<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Together We Stand: Siamese Networks for Similar Question Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpita</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Yenala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Chinnakotla</surname></persName>
							<email>manojc@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
							<email>m.shrivastava@iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
								<address>
									<region>Hyderabad</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Together We Stand: Siamese Networks for Similar Question Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="378" to="387"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
					<note>based models which use non-shared pa-rameters.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Community Question Answering (cQA) services like Yahoo! Answers 1 , Baidu Zhidao 2 , Quora 3 , StackOverflow 4 etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions. The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives. The main challenge in this task is the &quot;lexico-syntactic&quot; gap between the current and the previous questions. In this paper, we propose a novel approach called &quot;Siamese Convolutional Neural Network for cQA (SCQA)&quot; to find the semantic similarity between the current and the archived questions. SCQA consist of twin convolu-tional neural networks with shared parameters and a contrastive loss function joining them. SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA forum archives. The model projects semantically similar question pairs nearer to each other and dissimilar question pairs farther away from each other in the semantic space. Experiments on large scale real-life &quot;Yahoo! Answers&quot; dataset reveals that SCQA outperforms current state-of-the-art approaches based on translation models , topic models and deep neural network 1 https://answers.yahoo.com/ 2</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The cQA forums have emerged as popular and effective means of information exchange on the Web. Users post queries in these forums and re- ceive precise and compact answers in stead of a list of documents. Unlike in Web search, opinion based queries are also answered here by experts and users based on their personal experiences. The question and answers are also enhanced with rich metadata like categories, subcategories, user ex- pert level, user votes to answers etc.</p><p>One of the serious concerns in cQA is "question-starvation" <ref type="bibr" target="#b9">(Li and King, 2010</ref>) where a question does not get immediate answer from any user. When this happens, the question may take several hours and sometimes days to get sat- isfactory answers or may not get answered at all. This delay in response may be avoided by re- trieving semantically related questions from the cQA archives. If a similar question is found in the database of previous questions, then the corre- sponding best answer can be provided without any delay. However, the major challenge associated with retrieval of similar questions is the lexico- syntactic gap between them. Two questions may mean the same thing but they may differ lexically and syntactically. For example the queries "Why are yawns contagious?" and "Why do we yawn when we see somebody else yawning?" convey the same meaning but differ drastically from each other in terms of words and syntax.</p><p>Several techniques have been proposed in the literature for similar question retrieval and they could be broadly classified as follows: models like BM 25 <ref type="bibr" target="#b13">(Robertson et al., 1994)</ref> and Language modeling for Information Retrieval (LM IR) ( <ref type="bibr" target="#b15">Zhai and Lafferty, 2004</ref>) score the similarity based on the weights of the matching text terms between the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Translation Models:</head><p>Learning word or phrase level translation models from question-answer pairs in parallel corpora of same language <ref type="bibr" target="#b6">(Jeon et al., 2005;</ref><ref type="bibr" target="#b14">Xue et al., 2008;</ref>. The similarity function between questions is then defined as the probability of translating a given question into another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Topic Models: Learning topic models from</head><p>question-answer pairs ( <ref type="bibr" target="#b7">Ji et al., 2012;</ref><ref type="bibr" target="#b16">Zhang et al., 2014</ref>). Here, the similarity between questions, is defined in the latent topic space discovered by the topic model. Retrieving semantically similar questions can be thought of as a classification problem with large number of categories. Here, each category con- tains a set of related questions and the number of questions per category is small. It is possible that given a test question, we find that there are no questions semantically related to it in the archives, it will belong to a entirely new unseen category. Thus, only a subset of the categories is known dur- ing the time of training. The intuitive approach to solve this kind of problem would to learn a simi- larity metric between the question to be classified and the archive of previous questions. Siamese networks have shown promising results in such distance based learning methods ( <ref type="bibr" target="#b0">Bromley et al., 1993;</ref><ref type="bibr" target="#b2">Chopra et al., 2005</ref>). These networks pos- sess the capability of learning the similarity metric from the available data, without requiring specific information about the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Learning Based Approaches: Deep</head><p>In this paper, we propose a novel unified model called Siamese Convolutional Neural Network for cQA. SCQA architecture contain deep convolu- tional neural networks as twin networks with a contrastive energy function at the top. These twin networks share the weights with each other (pa- rameter sharing). The energy function used is suit- able for discriminative training for Energy-Based models ( <ref type="bibr" target="#b8">LeCun and Huang, 2005)</ref>. SCQA learns the shared model parameters and the similarity metric by minimizing the energy function connect- ing the twin networks. Parameter sharing guar- antees that question and its relevant answer are nearer to each other in the semantic space while the question and any answer irrelevant to it are far away from each other. For example, the rep- resentations of "President of USA" and "Barack Obama" should be nearer to each other than those of "President of USA" and "Tom Cruise lives in USA". The learnt similarity metric is used to retrieve semantically similar questions from the archives given a new posted question.</p><p>Similar question pairs are required to train SCQA which is usually hard to obtain in large numbers. Hence, SCQA overcomes this limita- tion by leveraging Question-Answer pairs (Q, A) from the cQA archives. This also has additional advantages such as:</p><p>• The knowledge and expertise of the answer- ers and askers usually differ in a cQA fo- rum. The askers, who are novices or non- experts, usually use less technical terminol- ogy whereas the answerers, who are typically experts, are more likely to use terms which are technically appropriate in the given realm of knowledge. Due to this, a model which learns from Question-Answer (Q, A) train- ing data has the advantage of learning map- pings from non-technical and simple terms to technical terms used by experts such as shortsight =&gt; myopia etc. This advan- tage will be lost if we learn from (Q, Q) pairs where both the questions are posed by non- experts only.</p><p>• Experts usually include additional topics that are correlated to the question topic which the original askers may not even be aware of. For example, for the question "how can I overcome short sight?", an expert may give an answer containing the concepts "laser surgery", "contact lens", "LASIK surgery" etc. Due to this, the concept short sight gets associated with these expanded concepts as well. Since, the askers are non-experts, such rich concept associations are hard to learn from (Q, Q) training archives even if they are available in large scale. Thus, leveraging (Q, A) training data leads to learning richer concept/term associations in SCQA.</p><p>In summary, the following are our main contri- butions in this paper:</p><p>• We propose a novel model SCQA based on Siamese Convolutional Neural Network which use shared parameters to learn the sim- ilarity metric between question-answer pairs in a cQA dataset.</p><p>• In SCQA, we overcome the non-availability of training data in the form of question- question pairs by leveraging existing question-answer pairs from the cQA archives which also helps in improving the effective- ness of the model.</p><p>• We reduce the computational complexity by directly using character-level representations of question-answer pairs in stead of us- ing sentence modeling based representations which also helps in handling spelling errors and out-of-vocabulary (OOV) words in docu- ments.</p><p>The rest of the paper is organized as follows. Sec- tion 2 presents the previous approaches to conquer the problem. Section 3 describes the architecture of SCQA. Sections 4 and 5 explain the training and testing phase of SCQA respectively. Section 6 introduces a variant of SCQA by adding tex- tual similarity to it. Section 7 describes the experi- mental set-up, details of the evaluation dataset and evaluation metrics. In Section 8, quantitative and qualitative results are presented. Finally, Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The classical retrieval models BM 25 ( <ref type="bibr" target="#b13">Robertson et al., 1994)</ref>, LM IR ( <ref type="bibr" target="#b15">Zhai and Lafferty, 2004</ref>) do not help much to capture semantic relatedness be- cause they mainly consider textual similarity be- tween queries. Researchers have used translation based models to solve the problem of question re- trieval. <ref type="bibr" target="#b6">Jeon et al. (2005)</ref> leveraged the similar- ity between the archived answers to estimate the translation probabilities. <ref type="bibr" target="#b14">Xue et al. (2008)</ref> en- hanced the performance of word based translation model by combining query likelihood language model to it.  used phrase based translation model where they considered question answer pairs as parallel corpus. However, <ref type="bibr" target="#b16">Zhang et al. (2014)</ref> stated that questions and answers cannot be considered parallel because they are het- erogeneous in lexical level and in terms of user be- haviors. To overcome these vulnerabilities topic modeling was introduced by <ref type="bibr" target="#b7">(Ji et al., 2012;</ref><ref type="bibr" target="#b16">Zhang et al., 2014</ref>). The approach assumes that questions and answers share some common latent topics. These techniques match questions not only on a term level but also on a topic level. <ref type="bibr" target="#b19">Zhou et al. (2015)</ref> used a fisher kernel to model the fixed size representation of the variable length questions. The model enhances the embedding of the questions with the metadata "category" in- volved with them. <ref type="bibr" target="#b17">Zhang et al. (2016)</ref> learnt representations of words and question categories simultaneously and incorporated the learnt repre- sentations into traditional language models.</p><p>Following the recent trends, deep learning is also employed to solve this problem. <ref type="bibr" target="#b12">Qiu et al. (2015)</ref> introduced convolutional neural tensor net- work (CNTN), which combines sentence model- ing and semantic matching. CNTN transforms the word tokens into vectors by a lookup layer, then encode the questions and answers to fixed-length vectors with convolutional and pooling layers, and finally model their interactions with a tensor layer. <ref type="bibr" target="#b3">Das et al. (2016)</ref> used deep structured topic mod- eling that combined topic model and paired con- volutional networks to retrieve related questions.  used a deep neural network (DNN) to map the question answer pairs to a com- mon semantic space and calculated the relevance of each answer given the query using cosine simi- larity between their vectors in that semantic space. Finally they fed the learnt semantic vectors into a learning to rank (LTR) framework to learn the rel- ative importance of each feature.</p><p>On a different line of research, several Textual-based Question Answering (QA) systems (Qanda 5 , QANUS 6 , QSQA 7 etc.) are developed that retrieve answers from the Web and other tex- tual sources. Similarly, structured QA systems</p><formula xml:id="formula_0">F(Q) Convolutional Neural Network F(A) Convolutional Neural Network W || F(Q) -F(A) || S Q A F(Q) F(A)</formula><p>Figure 1: Architecture of Siamese network.</p><p>(Aqualog 8 , NLBean 9 etc.) obtain answers from structured information sources with predefined on- tologies. QALL-ME Framework ( <ref type="bibr" target="#b5">Ferrandez et al., 2011</ref>) is a reusable multilingual QA architecture built using structured data modeled by an ontol- ogy. The reusable architecture of the system may be utilized later to incorporate multilingual ques- tion retrieval in SCQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Siamese Neural Network</head><p>Siamese Neural Networks (shown in <ref type="figure">Figure 1</ref>) were introduced by <ref type="bibr" target="#b0">Bromley et al. (1993)</ref> to solve the problem of signature verification. Later, <ref type="bibr" target="#b2">Chopra et al. (2005)</ref> used the architecture with discriminative loss function for face verification. Recently these networks are used extensively to enhance the quality of visual search ( <ref type="bibr" target="#b10">Liu et al., 2008;</ref><ref type="bibr" target="#b4">Ding et al., 2008)</ref>. Let, F (X) be the family of functions with set of parameters W . F (X) is assumed to be differen- tiable with respect to W . Siamese network seeks a value of the parameter W such that the symmet- ric similarity metric is small if X 1 and X 2 belong to the same category, and large if they belong to different categories. The scalar energy function S(Q, A) that measures the semantic relatedness between question answer pair (Q,A) can be de- fined as:</p><formula xml:id="formula_1">S(Q, A) = F (Q) − F (A)<label>(1)</label></formula><p>In SCQA question and relevant answer pairs are fed to train the network. The loss function is min- imized so that S(Q, A) is small if the answer A is relevant to the question Q and large otherwise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture of SCQA</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, SCQA consists of a pair of deep convolutional neural networks (CNN) with convolution, max pooling and rectified lin- ear (ReLU) layers and a fully connected layer at the top. CNN gives a non linear projection of the question and answer term vectors in the seman- tic space. The semantic vectors yielded are con- nected to a layer that measures distance or simi- larity between them. The contrastive loss function combines the distance measure and the label. The gradient of the loss function with respect to the weights and biases shared by the sub-networks, is computed using back-propagation. Stochastic Gradient Descent method is used to update the pa- rameters of the sub-networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inputs to SCQA</head><p>The size of training data used is in millions, thus representing every word with one hot vector would be practically infeasible. Word hashing introduced by <ref type="bibr" target="#b11">Mcnamee et al. (2004)</ref> involves letter n-gram to reduce the dimensionality of term vectors. For a word, say, "table" represented as (#table#) where # is used as delimiter, letter 3-grams would be #ta, tab, abl, ble, le#. Thus word hashing is charac- ter level representation of documents which takes care of OOV words and words with minor spelling errors. It represents a query using a lower di- mensional vector with dimension equal to num- ber of unique letter trigrams in the training dataset (48,536 in our case). The input to the twin networks of SCQA are word hashed term vectors of the question and answer pair and a label. The label indicates whether the sample should be placed nearer or far- ther in the semantic space. For positive samples (which are expected to be nearer in the semantic space), twin networks are fed with word hashed vectors of question and relevant answers which are marked as "best-answer" or "most voted an- swers" in the cQA dataset of</p><note type="other">Yahoo! Answers (question-relevant answer pair). For negative sam- ples (which are expected to be far away from each other in the semantic space), twin networks are fed with word hashed vectors of question and answer of any other random question from the dataset (question-irrelevant answer pair).</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolution</head><p>Each question-answer pair is word hashed into (q i - a i ) such that q i ∈ R nt and a i ∈ R nt where n t is the total number of unique letter trigrams in the train- ing data. Convolution layer is applied on the word hashed question answer vectors by convolving a filter with weights c ∈ R hxw where h is the filter height and w is the filter width. A filter consisting of a layer of weights is applied to a small patch of word hashed vector to get a single unit as output. The filter is slided across the length of vector such that the resulting connectivity looks like a series of overlapping receptive fields which output of width w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Max Pooling</head><p>Max pooling performs a kind of non-linear down- sampling. It splits the filter outputs into small non- overlapping grids (larger grids result to greater the signal reduction), and take the maximum value in each grid as the value in the output of reduced size. Max pooling layer is applied on top of the output given by convolutional network to extract the cru- cial local features to form a fixed-length feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ReLU</head><p>Non-linear function Rectified linear unit (ReLU) is applied element-wise to the output of max pool- ing layer. ReLU is defined as f (x) = max(0, x). ReLU is preferred because it simplifies backprop- agation, makes learning faster and also avoids sat- uration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fully Connected layer</head><p>The terminal layer of the convolutional neural sub- networks is a fully connected layer. It converts the output of the last ReLU layer into a fixed-length semantic vector s ∈ R ns of the input to the sub- network. We have empirically set the value of n s to 128 in SCQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>We train SCQA for a question while looking for semantic similarity with the answers relevant to it. SCQA is different from the other deep learning counterparts due to its property of parameter shar- ing. Training the network with a shared set of pa- rameters not only reduces number of parameters (thus, save lot of computations) but also ensures consistency of the representation of questions and answers in semantic space. The shared parameters of the network are learnt with the aim to minimize the semantic distance between the question and the relevant answers and maximize the semantic dis- tance between the question and the irrelevant an- swers.</p><p>Given an input {q i , a i } where q i and a i are the i th question answer pair, and a label y i with y i ∈ {1,-1}, the loss function is defined as:</p><formula xml:id="formula_2">loss(q i , a i ) = 1 − cos(q i , a i ), if y = 1; max(0, cos(q i , a i ) − m), if y = −1;</formula><p>where m is the margin which decides by how much distance dissimilar pairs should be moved away from each other. It generally varies be- tween 0 to 1. The loss function is minimized such that question answer pairs with label 1 (question- relevant answer pair) are projected nearer to each other and that with label -1 (question-irrelevant an- swer pair) are projected far away from each other in the semantic space. The model is trained by minimizing the overall loss function in a batch. The objective is to minimize :</p><formula xml:id="formula_3">L(Λ) = (q i ,a i )∈C∪C loss(q i , a i )<label>(2)</label></formula><p>where C contains batch of question-relevant answer pairs and C contain batch of question- irrelevant answer pairs. The parameters shared by the convolutional sub-networks are updated using Stochastic Gradient escent (SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Testing</head><p>While testing, we need to retrieve similar ques- tions given a query. During testing we make pairs of all the questions with the query and feed them to SCQA. The term vectors of the question pairs are word hashed and fed to the twin sub-networks. The trained shared weights of the SCQA projects the question vector in the semantic space. The similarity between the pairs is calculated using the similarity metric learnt during the training. Thus SCQA outputs a value of distance measure (score) for each pair of questions. The threshold is dynamically set to the average similarity score across questions and we output only those which have a similarity greater than the average similar- ity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Siamese Neural Network with Textual Similarity</head><p>SCQA is trained using question-relevant an- swer pairs as positive samples and question- irrelevant answer pairs as negative samples. It poorly models the basic text similarity because in the (Q, A) training pairs, the answerers of- ten do not repeat the question words while pro- viding the answer. For example: for the ques- tion "Who is the President of the US?", the answerer would just provide "Barrack Obama". Due to this, although the model learns that president the U S =&gt; Barrack Obama, the similarity for president =&gt; president wouldn't be much and hence needs to be augmented through BM 25 or some such similarity function.</p><p>Though SCQA can strongly model semantic relations between documents, it needs boosting in the area of textual similarity. The sense of word based similarity is infused to SCQA by using BM25 ranking algorithm. Lucene 10 is used to cal- culate the BM25 scores for question pairs. The score from similarity metric of SCQA is com- bined with the BM25 score. A new similarity score is calculated by the weighted combination of the SCQA and BM 25 score as:  similar. The value of α can be tuned according to the nature of dataset.</p><formula xml:id="formula_4">score = α * SCQA score + (1 − α) * BM 25 score (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We collected Yahoo! Answers dataset from Yahoo! Labs Webscope <ref type="bibr">11</ref> . Each question in the dataset contains title, description, best an- swer, most voted answers and meta-data like categories, sub categories etc. For training dataset, we randomly selected 2 million data and extracted question-relevant answer pairs and question-irrelevant answer pairs from them to train SCQA. Similarly, our validation dataset contains 400,000 question answer pairs. The hyperparam- eters of the network are tuned on the validation dataset. The values of the hyperparameters for which we obtained the best results is shown in Ta- ble 1.</p><p>We used the annotated survey dataset of 1018 questions released by <ref type="bibr" target="#b16">Zhang et al. (2014)</ref> as testset for all the models. On this gold data, we evaluated the performance of the models with three eval- uation criteria: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at K (P@K).</p><p>Each question and answer was pre-processed by lower-casing, stemming, stopword and special character removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Parameter Sharing</head><p>In order to find out whether parameter sharing helps in the present task we build two models named Deep Structured Neural Network for Com- munity Question Answering(DSQA) and Deep Structured Neural Network for Community Ques- tion Answering with Textual Similarity T-DSQA. DSQA and T-DSQA have the same architecture as SCQA and T-SCQA with the exception that in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>We did a comparative study of the results of the previous methods with respect to SCQA and T- SCQA. The baseline performance is shown by query likelihood language model (LM).  <ref type="table">Table 2</ref> that topic based approaches show slight improvement over translation based methods but they show sig- nificant improvement over baseline. The mod- <ref type="table">Table 2</ref>: Results on Yahoo! Answers dataset. The best re- sults are obtained by T-SCQA (bold faced). The difference between the results marked(*) and other methods are statisti- cally significant with p &lt;0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><note type="other">MAP MRR P@1 LMIR 0.762 0.844 0.717 translation(word) 0.786 0.870 0.807 translation+LM 0.787 0.869 0.804 translation(phrase) 0.789 0.875 0.817 Q-A topic model 0.787 0.879 0.810 Q-A topic model(s) 0.800 0.888 0.820 DSQA 0.755 0.921 0.751 T-DSQA 0.801 0.932 0.822 SCQA 0.811 0.895 0.830 T-SCQA 0.852 * 0.934 * 0.849 *</note><p>els DSQA and T-DSQA were built using convo- lutional neural sub-networks joined by a distance measure at the top. There is no sharing of parame- ters involved between the sub-networks of these models. It is clear from the comparison of re- sults between T-DSQA and T-SCQA that param- eter sharing definitely helps in the task of similar question retrieval in cQA forums. T-SCQA outper- forms all the previous approaches significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Quantitative Analysis</head><p>SCQA and T-SCQA learns the semantic relation- ship between the question and their best and most voted answers. It is observed that by varying the weights of SCQA and BM 25 scores, the value of MAP changes significantly ( <ref type="figure" target="#fig_6">Figure 5</ref>). The weight is tuned in the validation dataset. We trained our model for several epochs and observed how the results varied with the epochs. We found that the evaluation metrics changed with increasing the number of epochs but became saturated after epoch 60. The comparison of evaluation metrics with epochs can be visualised in <ref type="figure" target="#fig_3">Figure 3</ref>. The comparisons SCQA and T-SCQA with the previously proposed models is shown in <ref type="table">Table 2</ref>. For baseline we considered the traditional lan- guage model LM IR. The results in the table are consistent with the literature which says transla- tion based models outperform the baseline meth- ods and topic based approaches outperform the translational methods. Also, it is observed that deep learning based solution with parameter shar- ing is more helpful for this task than without pa- rameter sharing. Note, that the results of previous models stated in <ref type="table">Table 2</ref>   papers since we tried to re-implement those mod- els with our training data (to the best of our capa- bility). Though we use the test data released by <ref type="bibr" target="#b16">Zhang et al. (2014)</ref> we do not report their results in <ref type="table">Table 2</ref> due to the difference in training data used to train the models.</p><p>In the test dataset released by <ref type="bibr" target="#b16">Zhang et al. (2014)</ref>, there are fair amount of questions that pos- sess similarity in the word level hence T-SCQA performed better than SCQA for this dataset. T- SCQA gives the best performance in all evaluation measures. The results of T-SCQA in <ref type="table">Table 2</ref> uses the trained model at epoch 60 with the value of α as 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Qualitative Analysis</head><p>In <ref type="table">Table 3</ref> few examples are shown to depict how results of T-SCQA reflect strong semantic infor- mation when compared to other baseline methods. For Q1 we compare performance of LMIR and T- SCQA. LMIR outputs the question by consider- ing word based similarity. It focuses on match- ing the words "how", "become", "naturally" etc, hence it outputs "How can I be naturally funny?" which is irrelevant to the query. On the other hand, T −SCQA retrieves the questions that are seman- tically relevant to the query. For Q2 we compare the performance of T-SCQA with phrase based translation model ( ). The out- puts of translation(phrase) model shows that the translation of "nursery" and "pre-school" to "day- care", "going to university" to "qualifications" are highly probable. The questions retrieved are se- mantically related, however asking craft ideas for pre-school kids for the event of mother's day is ir- relevant in this context. The results of our model solely focuses on the qualifications, degrees and skills one needs to work in a nursery. For Q3 we compare the performance of T-SCQA with super- vised topic model ( <ref type="bibr" target="#b16">Zhang et al., 2014</ref>). The ques- tions retrieved by both the models revolve around the topic "effect of smoking on children". While the topic model retrieve questions which deal with smoking by mother and its effect on child, T- SCQA retrieve questions which deals not only with the affects of a mother smoking but also the effect of passive smoking on the child. For Q4 we com-Query Comment Q1: How can I become naturally happy? LM IR</p><p>1.How can I be naturally happy? LM IR performs 2.How can I become naturally funny? word based 1.Are some of us naturally born happy or do we learn how to matching using T-SCQA become happy? "how","become", 2.How can I become prettier and feel happier with myself?</p><p>"naturally" etc. Q2: Do you need to go to university to work in a nursery or pre-school?</p><p>For translation translation 1.What degree do you need to work in a nursery?</p><p>(phrase) (phrase) 2. I work at a daycare with pre-school kids <ref type="bibr">(3)</ref><ref type="bibr">(4)</ref><ref type="bibr">(5)</ref>. Any ideas on university-&gt;degree crafts for mother's day? nursery-&gt;daycare 1.Will my B.A hons in childhood studies put me in as an are highly probable unqualified nursery nurse?</p><p>translations but craft T-SCQA 2.What skills are needed to work in a nursery, or learned from ideas for daycare working in a nursery? is irrelevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q3: Does smoking affect an unborn child?</head><p>Both models Q-A topic 1.How do smoking cigarettes and drinking affect an unborn retrieve questions model(s) child? on topic "effect of 2.How badly will smoking affect an unborn child? smoking on children" 1.How does cigarette smoking and alcohol consumption by but T-SCQA could mothers affect unborn child?</p><p>retrieve based on T-SCQA 2.Does smoking by a father affect the unborn child? If there passive smoking is no passive smoking, then is it still harmful? through father.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q4: How do I put a video on YouTube?</head><p>T-DSQA could not 1.How can I download video from YouTube and put them decipher "put". T-DSQA on my Ipod? It relates "put" to 2.I really want to put videos from YouTube to my Ipod..how? download and 1.How do I post a video on YouTube?</p><p>transfer of videos T-SCQA 2.How can I make a channel on YouTube and upload videos while T-SCQA relates on it? plz help me... it to uploading videos. <ref type="table">Table 3</ref>: This table compares the qualitative performance of T-SCQA with LMIR, phrase based translation model transla- tion(phrase), supervised topic model Q-A topic model(s) and deep semantic model without parameter sharing T-DSQA. For queries Q1-4 T-SCQA show better performance than the previous models .</p><p>pare the performance of T-SCQA with T-DSQA. T- DSQA retrieves the questions that are related to downloading and transferring YouTube videos to other devices. Thus, T-DSQA cannot clearly clar- ify the meaning of "put" in Q4. However, the re- trieved questions of T-SCQA are more aligned to- wards the ways to record videos and upload them in YouTube. The questions retrieved by T-SCQA are semantically more relevant to the query Q4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>In this paper, we proposed SCQA for similar question retrieval which tries to bridge the lexico- syntactic gap between the question posed by the user and the archived questions. SCQA employs twin convolutional neural networks with shared parameters to learn the semantic similarity be- tween the question and answer pairs. Interpo- lating BM 25 scores into the model T-SCQA re- sults in improved matching performance for both textual and semantic matching. Experiments on large scale real-life "Yahoo! Answers" dataset re- vealed that T-SCQA outperforms current state-of- the-art approaches based on translation models, topic models and deep neural network based mod- els which use non-shared parameters.</p><p>As part of future work, we would like to en- hance SCQA with the meta-data information like categories, user votes, ratings, user reputation of the questions and answer pairs. Also, we would like to experiment with other deep neural archi- tectures such as Recurrent Neural Networks, Long Short Term Memory Networks, etc. to form the sub-networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Learning based models like (Zhou et al., 2016),(Qiu and Huang, 2015), (Das et al., 2016) use variants of neural network archi- tectures to model question-question pair sim- ilarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of SCQA. The network consists of repeating convolution, max pooling and ReLU layers and a fully connected layer. Also the weights W1 to W5 are shared between the sub-networks.</figDesc><graphic url="image-1.png" coords="4,323.81,62.81,181.42,159.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where α control the weights given to SCQA and BM 25 models. It range from 0 to 1. SCQA with this improved similarity metric is called Siamese Convolutional Neural Network for cQA with Textual Similartity (T-SCQA). Figure 4 de- picts the testing phase of T-SCQA. This model will give better performance in datasets with good mix of questions that are lexically and semantically 10 https://lucene.apache.org/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Variation of evaluation metrics with the epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For the translation based methods translation(word), translation + LM and translation(phrase) we implemented the papers by Jeon et al. (2005), Xue et al. (2008), Zhou et al. (2011) respec- tively. The first paper deals with word based trans- lation, the second enhanced the first by adding lan- guage model to it and the last paper implements phrase based translation method to bridge lexi- cal gap. As seen from Table 2, the translation based methods outperforms the baseline signifi- cantly. The models are trained using GIZA++ 12 tool with the question and best answer pair as the parallel corpus. For the topic based Q-A topic model and Q-A topic model (s), we implemented the models QATM -PR (Question-Answer Topic Model) Ji et al.(2012) and T BLM SQAT M −V (Su- pervised Question-Answer Topic Model with user votes as supervision) Zhang et al. (2014) respec- tively. Again it is visible from the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Testing phase of T-SCQA. Here the qi is the i th query and rij is the j th question retrieved by qi. The twin CNN networks share the parameters (W) with each other. The connecting distance metric layer outputs the SCQA score and the textual matching module outputs the BM 25 score. The weighted combination of these scores give the final score. rij is stated similar to the query qi if the final score of the pair exceeds an appropriate threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The variation of MAP with α.</figDesc><graphic url="image-4.png" coords="8,72.29,319.96,217.70,172.35" type="bitmap" /></figure>

			<note place="foot" n="1"> https://answers.yahoo.com/ 2 http://zhidao.baidu.com/ 3 http://www.quora.com/ 4 http://stackoverflow.com/ based models which use non-shared parameters.</note>

			<note place="foot" n="5"> http://www.openchannelfoundation.org/ projects/Qanda/ 6 http://www.qanus.com/ 7 http://www.dzonesoftware.com/ products/open-source-question-answer-software/</note>

			<note place="foot" n="8"> http://technologies.kmi.open.ac.uk/ aqualog/ 9 http://www.markwatson.com/opensource/</note>

			<note place="foot" n="11"> http://webscope.sandbox.yahoo.com/ catalog.php?datatype=l</note>

			<note place="foot" n="12"> http://www.statmt.org/moses/giza/ GIZA++.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yann LeCun</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Isabelle Guyon. IJPRAI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning the latent topics for question retrieval in community QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mirror on the wall: Finding similar questions with deep structured topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpita</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Chinnakotla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Using conditional random fields to extract contexts and answers of questions from online forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The qall-me framework: A specifiable-domain multilingual question answering architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Ferrandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Spurk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Ferrandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Orasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guenter</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Web semantics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Finding similar questions in large question and answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Jiwoon Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Ho</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Question-answer topic model for question retrieval in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Loss functions for discriminative training of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Routing questions to appropriate answerers in community question answering services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Understanding and summarizing answers in community-based question answering services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>ICCL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Character n-gram tokenization for european language text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Information retrieval</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for community-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Okapi at trec-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>NIST Special Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Retrieval models for question and answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>SIGIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Question retrieval with high quality answers in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning distributed representations of data in community question answering for question retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICWSDM</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Phrase-based translation model for question retrieval in community question answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACL:HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning continuous word embedding with metadata for question retrieval in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning semantic representation with neural networks for community question answering retrieval. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
