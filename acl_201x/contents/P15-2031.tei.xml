<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Learning Framework of Skip-Grams and Global Vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NTT Communication Science Laboratories</orgName>
								<orgName type="institution" key="instit2">NTT Corporation</orgName>
								<address>
									<addrLine>2-4 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<postCode>619-0237</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Learning Framework of Skip-Grams and Global Vectors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="186" to="191"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Log-bilinear language models such as SkipGram and GloVe have been proven to capture high quality syntactic and semantic relationships between words in a vector space. We revisit the relationship between SkipGram and GloVe models from a machine learning viewpoint, and show that these two methods are easily merged into a unified form. Then, by using the unified form, we extract the factors of the configurations that they use differently. We also empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural-network-inspired word embedding meth- ods such as Skip-Gram (SkipGram) have been proven to capture high quality syntactic and se- mantic relationships between words in a vector space <ref type="bibr" target="#b9">(Mikolov et al., 2013a)</ref>. A similar embed- ding method, called 'Global Vector (GloVe)', was recently proposed. It has demonstrated significant improvements over SkipGram on the widely used 'Word Analogy' and 'Word Similarity' benchmark datasets ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>). Unfortunately, a later deep re-evaluation has revealed that GloVe does not consistently outperform SkipGram ( <ref type="bibr" target="#b7">Levy et al., 2015)</ref>; both methods provided basically the same level of performance, and SkipGram even seems 'more robust (not yielding very poor re- sults)' than GloVe. Moreover, some other papers, i.e., <ref type="bibr" target="#b16">(Shi and Liu, 2014)</ref>, and some researchers in the community have discussed a relationship, and/or which is superior, SkipGram or GloVe.</p><p>From this background, we revisit the relation- ship between SkipGram and GloVe from a ma- chine learning viewpoint. We show that it is nat- V : set of vocabulary (set of words) |V| : vocabulary size, or number of words in V i : index of the input vector, where i ∈ {1, . . . , |V|} j : index of the output vector, where j ∈ {1, . . . , |V|} ei : input vector of the i-th word in V oj : output vector of the j-th word in V If i = j, then ei and oj are the input and output vec- tors of the same word in V, respectively. D : number of dimensions in input and output vectors mi,j : (i, j)-factor of matrix M si,j : dot product of input and output vectors,</p><formula xml:id="formula_0">si,j = ei · oj D : training data, D = {(in, jn)} N n=1 Ψ(·) : objective function σ(·) : sigmoid function, σ(x) = 1 1+exp(−x)</formula><p>ci,j : co-occurrence of the i-th and j-th words in D D : (virtual) negative sampling data c i,j : co-occurrence of the i-th and j-th words in D k : hyper-parameter of the negative sampling β(·) : 'weighting factor' of loss function Φ(·) : loss function <ref type="table">Table 1</ref>: List of notations used in this paper.</p><p>ural to think that these two methods are essen- tially identical, with the chief difference being their learning configurations.</p><p>The final goal of this paper is to provide a uni- fied learning framework that encompasses the con- figurations used in SkipGram and GloVe to gain a deeper understanding of the behavior of these em- bedding methods. We also empirically investigate which learning configuration most clearly eluci- dates the performance difference often observed in word similarity and analogy tasks. <ref type="table">Table 1</ref> shows the notations used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SkipGram and GloVe</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Matrix factorization view of SkipGram</head><p>SkipGram can be categorized as one of the simplest neural language models <ref type="bibr" target="#b12">(Mnih and Kavukcuoglu, 2013)</ref>. It generally assigns two dis- tinct D-dimensional vectors to each word in vo- cabulary V; one is 'input vector', and the other is 'output vector' 1 .</p><p>Roughly speaking, SkipGram models word-to- word co-occurrences, which are extracted within the predefined context window size, by the in- put and output vectors. Recently, SkipGram has been interpreted as implicitly factorizing the matrix, where the factors are calculated from co-occurrence information ( . Let m i,j be the (i, j)-factor of matrix M to be 'implicitly' factorized by SkipGram. Skip- Gram approximates each m i,j by the inner prod- uct of the corresponding input and output vectors, that is:</p><formula xml:id="formula_1">m i,j ≈ e i · o j ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">SkipGram with negative sampling</head><p>The primitive training sample for SkipGram is a pair of a target word and its corresponding con- text word. Thus, we can represent the training data of SkipGram as a list of input and output in- dex pairs, that is,</p><formula xml:id="formula_2">D = {(i n , j n )} N n=1</formula><p>. Thus the estimation problem of 'SkipGram with negative sampling (SGNS)' is defined as the minimization problem of objective function Ψ:</p><formula xml:id="formula_3">Ψ = − (in,jn)∈D log σ(e in · o jn ) − (in,jn)∈D log 1 − σ(e in · o jn ) ,<label>(2)</label></formula><p>where the optimization parameters are e i and o j for all i and j. Note that we explicitly represent the negative sampling data D ). Let us assume that, in a preliminary step, we count all co-occurrences in D. Then, the SGNS objective in Eq. 2 can be rewritten as follows by a simple reformulation:</p><formula xml:id="formula_4">Ψ = − i j c i,j log σ(e i · o j ) +c i,j log 1 − σ(e i · o j ) .<label>(3)</label></formula><p>Here, let us substitute e i · o j in Eq. 3 for s i,j , and then assume that all s i,j are free parameters. Namely, we can freely select the value of s i,j in- dependent from any other s i ,j , where i = i and j = j , respectively. The partial derivatives of Ψ with respect to s i,j take the following form:</p><formula xml:id="formula_5">∂ s i,j Ψ = − c i,j 1 − σ(s i,j ) − c i,j σ(s i,j ) .<label>(4)</label></formula><p>put' and 'output' to reduce the ambiguity since 'word' and 'context' are exchangeable by the definition of model (i.e., SkipGram or CBoW).</p><p>The minimizer can be obtained when ∂ s i,j Ψ = 0 for all s i,j . By using this relation, we can obtain the following closed form solution:</p><formula xml:id="formula_6">s i,j = log c i,j c i,j .<label>(5)</label></formula><p>Overall, SGNS approximates the log of the co- occurrence ratio between 'real' training data D and 'virtual' negative sampling data D by the in- ner product of the corresponding input and output vectors in terms of minimizing the SGNS objec- tive written in Eq. 2, and Eq. 3 as well. Therefore, we can obtain the following relation for SGNS:</p><formula xml:id="formula_7">m i,j = log c i,j c i,j ≈ e i · o j .<label>(6)</label></formula><p>Note that the expectation of c i,j is</p><formula xml:id="formula_8">kc i c j</formula><p>|D| if the negative sampling is assumed to follow unigram probability c j |D| , and the negative sampling data is k-times larger than the training data D, where c i = j c i,j and c j = i c i,j 2 . The above matches 'shifted PMI' as described in ( ) when we substitute c i,j for kc i c j |D| in Eq. 6, In addition, the word2vec implementation uses a smoothing factor α to reduce the selec- tion of high-occurrence-frequency words during the negative sampling. The expectation of c i,j can then be written as:</p><formula xml:id="formula_9">kc i (c j ) α j (c j ) α . We refer to log c i,j j (c j ) α kc i (c j ) α as 'α-parameterized shifted PMI (SPMI k,α )'.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Matrix factorization view of GloVe</head><p>The GloVe objective is defined in the following form ( <ref type="bibr" target="#b13">Pennington et al., 2014)</ref>:</p><formula xml:id="formula_10">Ψ = i j β(c i,j ) e i · o j − log(c i,j ) 2 , (7)</formula><p>where β(·) represent a 'weighting function'. In particular, β(·) satisfies the relations 0 ≤ β(x) &lt; ∞, and β(x) = 0 if x = 0. For example, the following weighting function has been introduced in ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>):</p><formula xml:id="formula_11">β(x) = min 1, x/x max γ .<label>(8)</label></formula><p>This is worth noting here that the original GloVe introduces two bias terms, b i and b j , and defines configuration SGNS GloVe training unit sample-wise co-occurrence loss function logistic (Eq. 11) squared (Eq. 12) neg. sampling explicit no sampling weight. func. β(·) fixed to 1 Eq. 8 fitting function SPMI k,α log(ci,j) bias none bi and bj </p><formula xml:id="formula_12">e i · o j + b i + b j instead of just e i · o j in Eq. 7.</formula><p>For simplicity and ease of discussion, we do not ex- plicitly introduce bias terms in this paper. This is because, without loss of generality, we can embed the effect of the bias terms in the input and output vectors by introducing two additional dimensions for all e i and o j , and fixing parameters e i,D+1 = 1 and o j,D+2 = 1. According to Eq. 7, GloVe can also be viewed as a matrix factorization method. Different from SGNS, GloVe approximates the log of co- occurrences:</p><formula xml:id="formula_13">m i,j = log(c i,j ) ≈ e i · o j ,<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unified Form of SkipGram and GloVe</head><p>An examination of the differences between Eqs. 6 and 9 finds that Eq. 6 matches Eq. 9 if c i,j = 1. Recall that c i,j is the number of co-occurrences of (i, j) in negative sampling data D . Therefore, what GloVe approximates is SGNS when the neg- ative sampling data D is constructed as 1 for all co-occurrences. From the viewpoint of matrix fac- torization, GloVe can be seen as a special case of SGNS, in that it utilizes a sort of uniform negative sampling method.</p><p>Our assessment of the original GloVe paper suggests that the name "Global Vector" mainly stands for the architecture of the two stage learn- ing framework. Namely, it first counts all the co-occurrences in D, and then, it leverages the gathered co-occurrence information for estimating (possibly better) parameters. In contrast, the name "SkipGram" stands mainly for the model type; how it counts the co-occurrences in D. The key points of these two methods seems different and do not conflict. Therefore, it is not surprising to treat these two similar methods as one method; for example, SkipGram model with two-stage global vector learning. The following objective function is a generalized form that subsumes Eqs. 3 and 7:</p><formula xml:id="formula_14">Ψ = i j β(c i,j )Φ(e i , o j , c i,j , c i,j ).<label>(10)</label></formula><p>hyper-parameter selected value word2vec glove context window (W ) 10 sub ( <ref type="bibr" target="#b7">Levy et al., 2015</ref>) dirty, t = 10 −5 - del ( <ref type="bibr" target="#b7">Levy et al., 2015</ref>) use 400,000 most frequent words cds ( <ref type="bibr" target="#b7">Levy et al., 2015)</ref> α = 3/4 - w+c ( <ref type="bibr" target="#b7">Levy et al., 2015)</ref> e <ref type="table">Table 3</ref>: Hyper-parameters in our experiments.</p><note type="other">+ o weight. func. (γ, xmax) - 3/4, 100 initial learning rate (η) 0.025 0.05 # of neg. sampling (k) 5 - # of iterations (T ) 5 20 # of threads 56 # of dimensions (D) 300</note><p>In particular, the original SGNS uses β(c i,j ) = 1 for all (i, j), and logistic loss function:</p><formula xml:id="formula_15">Φ(e i , o j , c i,j , c i,j ) = c i,j log σ(e i · o j ) +c i,j log 1 − σ(e i · o j ) .</formula><p>(11) In contrast, GloVe uses a least squared loss func- tion: <ref type="table" target="#tab_0">Table 2</ref> lists the factors of each configuration used differently in SGNS and GloVe.</p><formula xml:id="formula_16">Φ(e i , o j , c i,j , c i,j ) = e i · o j − log c i,j c i,j 2 .<label>(12)</label></formula><p>Note that this unified form also includes SkipGram with noise contrastive estimation (SGNCE) <ref type="bibr" target="#b12">(Mnih and Kavukcuoglu, 2013)</ref>, which approximates m i,j = log( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Following the series of neural word embedding pa- pers, our training data is taken from a Wikipedia dump (Aug. 2014). We tokenized and lowercased the data yielding about 1.8B tokens.</p><p>For the hyper-parameter selection, we mostly followed the suggestion made in ( <ref type="bibr" target="#b7">Levy et al., 2015)</ref>. <ref type="table">Table 3</ref> summarizes the default values of hyper-parameters used consistently in all our ex- periments unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark datasets for evaluation</head><p>We prepared eight word similarity benchmark datasets (WSimilarity), namely, R&amp;G <ref type="bibr" target="#b15">(Rubenstein and Goodenough, 1965</ref>  WSimilarity WAnalogy SGNS (original) 8856 65.4 (65.2, 65.7) 63.0 (62.2, 63.8) GloVe (original) 8243 57.6 (57.5, 57.9) 64.8 (64.6, 65.0) w/o bias terms 8027 57.6 (57.5, 57.7) 64.8 (64.5, 65.0) fitting=SPMI k,α 8332 57.5 (57.2, 57.8) 65.0 (64.8. 65.1) <ref type="table">Table 4</ref>: Results: the micro averages of Spear- man's rho (WSimilarity) and accuracy (WAnal- ogy) for all benchmark datasets.</p><p>et al., 2013). Moreover, we also prepared three analogy benchmark datasets (WAnalogy), that is, GSEM <ref type="bibr" target="#b9">(Mikolov et al., 2013a</ref>), <ref type="bibr">GSYN (Mikolov et al., 2013a)</ref>, and MSYN ( <ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>). <ref type="table">Table 4</ref> shows the training time and performance results gained from our benchmark data. The col- umn 'time' indicates average elapsed time (sec- ond) for model learning. All the results are the av- erage performance of ten runs. This is because the comparison methods have some randomized factors, such as initial value (since they are non- convex optimization problems) and (probabilistic) sampling method, which significantly impact the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SGNS and GloVe Results</head><p>At first, we compared the original SGNS as im- plemented in the word2vec package <ref type="bibr">3</ref> and the original GloVe as implemented in the glove package <ref type="bibr">4</ref> . These results are shown in the first and second rows in <ref type="table">Table 4</ref>. In our experiments, SGNS significantly outperformed GloVe in WSimilarity while GloVe significantly outperformed SGNS in WAnalogy. As we explained, these two methods can be easily merged into a unified form. Thus, there must be some differences in their configura- tions that yields such a large difference in the re- sults. Next, we tried to determine the clues as the differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of incorporating bias terms</head><p>The third row (w/o bias terms) in <ref type="table">Table 4</ref> shows the results of the configuration without using the bias terms in the glove package. A comparison with the results of the second row, finds no mean- ingful benefit to using the bias terms. In contrast, obviously, the elapsed time for model learning is consistently shorter since we can discard the bias term update.  <ref type="table" target="#tab_0">W =2  3  5  10  20  SGNS (original)</ref> 64.9 65.1 65.4 65.4 64.9 GloVe (original) 53.6 55.7 57.0 57.6 57.8 w/o harmonic func. 54.6 56.9 57.8 58.2 57.9</p><p>(b) WAnalogy method W <ref type="table" target="#tab_0">=2  3  5  10  20  SGNS (original)</ref> 62.8 63.5 63.9 63.0 61.3 GloVe (original) 51.7 58.4 62.3 64.8 66.1 w/o harmonic func. 52.6 58.0 60.5 61.6 60.7 <ref type="table">Table 5</ref>: Impact of the context window size, and harmonic function. <ref type="formula" target="#formula_1">(1)</ref> 38.5% 53.6% 64.5% 73.5% 78.4% <ref type="table">Table 6</ref>: The ratio of entries less than one in co- occurrence matrix.</p><formula xml:id="formula_17">W =2 3 5 10 20 (1) 0 &lt; ci,j &lt; 1 104M 213M 377M 649M 914M (2) 1 ≤ ci,j 167M 184M 207M 234M 251M non-zero ci,j 271M 398M 584M 883M 1165M ratio of</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of fitting function</head><p>The fourth row (fitting=SPMI k,α ) in <ref type="table">Table 4</ref> shows the performance when we substituted the fit- ting function of GloVe, namely, log(c i,j ), for SPMI k=5,α=3/4 used in SGNS. Clearly, the per- formance becomes nearly identical to the original GloVe. Accordingly, the selection of fitting func- tion has only a small impact. <ref type="table">Table 5</ref> shows the impact of context window size W . The results of SGNS seem more stable against W than those of GloVe. Additionally, we investigated the impact of the 'harmonic function' used in GloVe. The 'har- monic function' uses the inverse of context dis- tance, i.e., 1/a if the context word is a-word away from the target word, instead of just count 1 re- gardless of the distance when calculating the co- occurrences. Clearly, GloVe without using the harmonic function shown in the third row of Ta- ble 5 yielded significantly degraded performance on WAnalogy, and slight improvement on WSimi- larity. This fact may imply that the higher WAnal- ogy performance of GloVe was derived by the ef- fect of this configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of context window size and harmonic function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Link between harmonic function and negative sampling</head><p>This section further discusses a benefit of har- monic function.</p><p>Recall that GloVe does not explicitly consider 'negative samples'. It fixes c i,j = 1 for all (i, j) as shown in Eq. 7. However, the co-occurrence count given by using the harmonic function can take values less than 1, i.e., c i,j = 2/3, if the i- th word and the j-th word co-occurred twice with distance 3. As a result, the value of the fitting func- tion of GloVe becomes log(2/3). Interestingly, this is essentially equivalent to co-occur 3 times in the negative sampling data and 2 times in the real data since the fitting function of the unified form shown in Eq. 12 is log(c i,j /c i,j ) = log(2/3) when c i,j = 2 and c i,j = 3. It is not surprising that rare co-occurrence words that occur only in long range contexts may have almost no correlation between them. Thus treating them as negative samples will not create a problem in most cases. Therefore, the harmonic function seems to 'unexpectedly' mimic a kind of a negative sampling method; it is inter- preted as 'implicitly' generating negative data. <ref type="table">Table 6</ref> shows the ratio of the entries c i,j whose value is less than one in matrix M. Remember that vocabulary size was 400,000 in our experi- ments. Thus, we had a total of 400K×400K=160B elements in M, and most were 0. Here, we con- sider only non-zero entries. It is clear that longer context window sizes generated many more en- tries categorized in 0 &lt; c i,j &lt; 1 by the har- monic function. One important observation is that the ratio of 0 &lt; c i,j &lt; 1 is gradually increas- ing, which offers a similar effect to increasing the number of negative samples. This can be a rea- son why GloVe demonstrated consistent improve- ments in WAnalogy performance as context win- dow increased since larger negative sampling size often improves performance ( <ref type="bibr" target="#b7">Levy et al., 2015)</ref>. Note also that the number of 0 &lt; c i,j &lt; 1 always becomes 0 in the configuration without the har- monic function. This is equivalent to using uni- form negative sampling c i,j = 1 as described in Sec. 3. This fact also indicates the importance of the negative sampling method. <ref type="table">Table 7</ref> shows the impact of weighting function used in GloVe, namely, Eq 8. Note that 'β(·)=1' column shows the results when we fixed 1 for all non-zero entries <ref type="bibr">5</ref> . This is also clear that the weighting function Eq 8 with appropriate param- eters significantly improved the performance of both WSimilarity and WAnalogy tasks. How- ever unfortunately, the best parameter values for (a) WSimilarity hyper param.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impact of weighting function</head><p>β(·)=1 xmax = 1 10 100 10000 γ = 0.75 59.4 60.1 60.9 57.7 49.5 w/o harmonic func. 58. <ref type="bibr">2</ref> 58.0 60.7 58.2 56.0 γ = 1.0 (59.4) 60.1 59.4 55.9 36.1 w/o harmonic func. <ref type="figure">(58.2)</ref> 58.3 60.7 57.7 46.7 (b) WAnalogy hyper param.</p><p>β(·)=1 xmax = 1 10 100 10000 γ = 0.75 55.7 61.1 64.3 64.8 28.4 w/o harmonic func. 53.4</p><p>52.6 60.3 61.6 42.5 γ = 1.0 (55.7) 61.0 63.8 59.1 7.5 w/o harmonic func. <ref type="figure">(53.4)</ref> 54.1 60.8 60.1 20.3 <ref type="table">Table 7</ref>: Impact of the weighting function.</p><p>WSimilarity and WAnalogy tasks looks different. We emphasize that harmonic function discussed in the previous sub-section was still a necessary condition to obtain the best performance, and bet- ter performance in the case of 'β(·)=1' as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper reconsidered the relationship between SkipGram and GloVe models in machine learn- ing viewpoint. We showed that SGNS and GloVe can be easily merged into a unified form. We also extracted the factors of the configurations that are used differently. We empirically inves- tigated which learning configuration is responsi- ble for the performance difference often observed in widely examined word similarity and analogy tasks. Finally, we found that at least two config- urations, namely, the weighting function and har- monic function, had significant impacts on the per- formance. Additionally, we revealed a relation- ship between harmonic function and negative sam- pling. We hope that our theoretical and empirical analyses will offer a deeper understanding of these neural word embedding methods 6 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>)</head><label></label><figDesc>in matrix factoriza- tion view. This paper omits a detailed discussion of SGNCE for space restrictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of the different configura-
tions used in SGNS and GloVe. 

</table></figure>

			<note place="foot" n="1"> These two vectors are generally referred to as &apos;word (or target) vector&apos; and &apos;context vector&apos;. We use the terms &apos;in</note>

			<note place="foot" n="2"> Every input of the i-th word samples k words. Therefore, the negative sampling number is kci. Finally, the expectation can be obtained by multiplying count kci by probability c j |D| .</note>

			<note place="foot" n="3"> https://code.google.com/p/word2vec/ 4 http://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="5"> This is equivalent to set 0 to-x-max option in glove implementation.</note>

			<note place="foot" n="6"> The modified codes for our experiments will be available in author&apos;s homepage</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank three anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies</title>
		<meeting>Human Language Technologies</meeting>
		<imprint>
			<publisher>The</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Marius Pas¸caPas¸ca, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">word2vec Explained: Deriving Mikolov et al.&apos;s Negativesampling Word-embedding Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>abs/1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural Word Embedding as Implicit Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving Distributional Similarity with Lessons Learned from Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<title level="m">Contextual Correlates of Semantic Similarity. Language &amp; Cognitive Processes</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A word at a time: Computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on World Wide Web, WWW &apos;11</title>
		<meeting>the 20th International Conference on World Wide Web, WWW &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Linking GloVe with word2vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1411.5595</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
