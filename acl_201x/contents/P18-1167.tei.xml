<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
							<email>domhant@amazon.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1799" to="1808"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1799</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With recent advances in network ar-chitectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolu-tional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different ar-chitectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language, we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important for the encoder side than for the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the introduction of attention mecha- nisms ( <ref type="bibr" target="#b16">Luong et al., 2015</ref>) Neural Machine Translation (NMT) ) has shown some impressive results. Initially, approaches to NMT mainly relied on Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b16">Luong et al., 2015;</ref><ref type="bibr">Wu et al., 2016</ref>) such as Long Short-Term Memory (LSTM) networks (Hochre- iter and <ref type="bibr" target="#b9">Schmidhuber, 1997</ref>) or the Gated Recti- fied Unit (GRU) ( .</p><p>Recently, other approaches relying on con- volutional networks <ref type="bibr" target="#b11">(Kalchbrenner et al., 2016;</ref><ref type="bibr" target="#b7">Gehring et al., 2017)</ref> and self-attention ( <ref type="bibr">Vaswani et al., 2017</ref>) have been introduced. These ap- proaches remove the dependency between source language time steps, leading to considerable speed-ups in training time and improvements in quality. The Transformer, however, contains other differences besides self-attention, including layer normalization across the entire model, multiple source attention mechanisms, a multi-head dot at- tention mechanism, and the use of residual feed- forward layers. This raises the question of how much each of these components matters.</p><p>To answer this question we first introduce a flexible Architecture Definition Language (ADL) ( §2). In this language we standardize existing components in a consistent way making it eas- ier to compare structural differences of architec- tures. Additionally, it allows us to efficiently per- form a granular analysis of architectures, where we can evaluate the impact of individual compo- nents, rather than comparing entire architectures as a whole. This ability leads us to the following observations:</p><p>• Source attention on lower encoder layers brings no additional benefit ( §4.2).</p><p>• Multiple source attention layers and residual feed-forward layers are key ( §4.3).</p><p>• Self-attention is more important for the source than for the target side ( §4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Flexible Neural Machine Translation Architecture Combination</head><p>In order to experiment easily with different ar- chitecture variations we define a domain specific NMT Architecture Definition Language (ADL), consisting of combinable and nestable building blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>NMT is formulated as a sequence to sequence prediction task in which a source sentence X = x 1 , ..., x n is translated auto-regressively into a tar- get sentence Y = y 1 , ..., y m one token at a time as</p><formula xml:id="formula_0">p(y t |Y 1:t−1 , X; θ) = softmax(W o z L + b o ),<label>(1)</label></formula><p>where b o is a bias vector, W o projects a model de- pendent hidden vector z L of the Lth decoder layer to the dimension of the target vocabulary V trg and θ denotes the model parameters. Typically, during training Y 1:t−1 consists of the reference sequence tokens, rather then the predictions produced by the model, which is known as teacher-forcing. Train- ing is done by minimizing the cross-entropy loss between the predicted and the reference sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture Definition Language</head><p>In the following we specify the ADL which can be used to define any standard NMT architecture and combinations thereof.</p><p>Layers The basic building block of the ADL is a layer l. Layers can be nested, mean- ing that a layer can consist of several sub- layers. Layers optionally take set of named ar- guments l(k 1 =v 1 , k 2 =v 2 , ...) with names k 1 , k 2 , ... and values v 1 , v 2 , ... or positional arguments l(v 1 , v 2 , ...).</p><p>Layer definitions For each layer we have a cor- responding layer definition based on the hidden states of the previous layer and any additional ar- guments. Specifically, each layer takes T hid- den states h i 1 , ..., h i T , which in matrix form are H i ∈ R T ×d i , and produces a new set of hidden states h i+1 1 , ..., h i+1</p><p>T or H i+1 . While each layer can have a different number of hidden units d i , in the following we assume them to stay constant across layers and refer to the model dimensionality as d model . We distinguish the hidden states on the source side U 0 , ..., U Ls from the hidden states of the target side Z 0 , ..., Z L . These are produced by the source and target embeddings and L s source layers and L target layers.</p><p>Source attention layers play a special role in that their definition additionally makes use of any of the source hidden states U 0 , ..., U Ls .</p><p>Layer chaining Layers can be chained, feeding the output of one layer as the input to the next. We denote this as l 1 → l 2 ...l L . This is equivalent to writing l L (... l 2 (l 1 (H 0 ))) if none of the layers is a source attention layer.</p><p>In layer chains layers may also contain lay- ers that themselves take arguments. As an ex-</p><formula xml:id="formula_1">ample l 1 (k =v) → l 2 ... l L is equivalent to l L (... l 2 (l 1 (H 0 , k =v))).</formula><p>Note that unlike in the layer definition hidden states are not explic- itly stated in the layer chain, but rather implicitly defined through the preceding layers.</p><p>Encoder/Decoder structure A NMT model is fully defined through two layer chains, namely one describing the encoder and another describing the decoder. The first layer hidden states on the source U 0 are defined through the source embedding as</p><formula xml:id="formula_2">u 0 t = E src x t<label>(2)</label></formula><p>where x t ∈ {0, 1} |Vsrc| is the one-hot represen- tation of x t and E S x t ∈ R e×|Vsrc| an embedding matrix with embedding dimensionality e. Simi- larly, Z 0 is defined through the target embedding matrix E tgt . Given the final decoder hidden state Z L the next word predictions are done according to Equa- tion 1.</p><p>Layer repetition Networks often consist of sub- structures that are repeated several times. In order to support this we define a repetition layer as</p><formula xml:id="formula_3">repeat(n, l) = l 1 l 2 ...l n ,</formula><p>where l represents a layer chain and each one of l 1 , ..., l n an instantiation of that layer chain with a separate set of weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Layer Definitions</head><p>In this section we will introduce the concrete lay- ers and their definitions, which are available for composing NMT architectures. They are based on building blocks common to many current NMT models.</p><p>Dropout A dropout ( <ref type="bibr" target="#b20">Srivastava et al., 2014)</ref> layer, denoted as dropout(h t ), can be applied to hidden states as a form of regularization.</p><p>Fixed positional embeddings Fixed positional embeddings ( <ref type="bibr">Vaswani et al., 2017</ref>) add informa- tion about the position in the sequence to the hid- den states. With h t ∈ R d the positional embed- ding layer is defined as</p><formula xml:id="formula_4">pos(h t ) = dropout( √ d · h t + p t ) p t,j = sin(t/10000 2j/d ) p t,2j+1 = cos(t/10000 2j/d ).</formula><p>Linear We define a linear projection layer as</p><formula xml:id="formula_5">linear(h t , d o ) = Wh t + b,</formula><p>where W ∈ R do×d in .</p><p>Feed-forward Making use of the linear projec- tion layer a feed-forward layer with ReLU activa- tion and dropout is defined as</p><formula xml:id="formula_6">ff(h t , d o ) = dropout(max(0, linear(h t , d o )))</formula><p>and a version which temporarily upscales the num- ber of hidden units, as done by <ref type="bibr">Vaswani et al. (2017)</ref>, can be defined as</p><formula xml:id="formula_7">ffl(h t ) = ff(4d in )linear(d in )</formula><p>where</p><formula xml:id="formula_8">h t ∈ R d in .</formula><p>Convolution Convolutions run a small feed- forward network on a sliding window over the in- put. Formally, on the encoder side this is defined as</p><formula xml:id="formula_9">cnn(H, v, k) = v(W[h i−−k/2 ; ...; h i+k/2 ] + b)</formula><p>where k is the kernel size, and v is a non-linearity. The input is padded so that the number of hidden states does not change.</p><p>To preserve the auto-regressive property of the decoder we need to make sure to never take fu- ture decoder time steps into account, which can be achieved by adding k − 1 padding vectors h −k+1 = 0, . . . , h −1 = 0 such that the decoder convolution is given as</p><formula xml:id="formula_10">cnn(H, v, k) = v(W[h t−k+1 ; ...; h t ] + b).</formula><p>The non-linearity v can either be a ReLU or a Gated Linear Unit (GLU) ( <ref type="bibr" target="#b6">Dauphin et al., 2016)</ref>. With the GLU we set d i = 2d such that we can split h = [h A ; h B ] ∈ R 2d and compute the non- linearity as</p><formula xml:id="formula_11">glu([h A ; h B ]) = h A ⊗ σ(h B ).</formula><p>Identity We define an identity layer as id(h t ) = h t .</p><p>Concatenation To concatenate the output of p layer chains we define</p><formula xml:id="formula_12">concat(h t , l 1 , ..., l p ) = [l 1 (h t ); ...; l p (h t )].</formula><p>Recurrent Neural Network An RNN layer is defined as</p><formula xml:id="formula_13">rnn(h t ) = f rnn o (h t , s t−1 ) s t = f rnn h (h t , s t−1 )</formula><p>where f rnn o and f rnn h could be defined through either a GRU ( ) or a LSTM (Hochreiter and Schmidhuber, 1997) cell.</p><p>In addition, a bidirectional RNN layer birnn is available, which runs one rnn in forward and an- other in reverse direction and concatenates both re- sults.</p><p>Attention All attention mechanisms take a set of query vectors q 0 , ..., q M , key vectors k 0 , ..., k N and value vectors v 0 , ..., v N in order to produce one context vector per query, which is a linear combination of the value vectors. We define Q ∈ R M ×d , V ∈ R N ×d and K ∈ R N ×d as the con- catenation of these vectors. What is used as the query, key and value vectors depends on attention type and is defined below.</p><p>Dot product attention The scaled dot prod- uct attention ( <ref type="bibr">Vaswani et al., 2017</ref>) is defined as</p><formula xml:id="formula_14">dot att(Q, K, V, s) = softmax QK √ s V,</formula><p>where the scaling factor s is implicitly set to d un- less noted otherwise. Adding a projection to the queries, keys and values we get the projected dot attention as</p><formula xml:id="formula_15">proj dot att(Q, K, V, d p , s) = dot att(QW Q , KW K , VW V , s)</formula><p>where d p is dimensionality of the projected vec- <ref type="formula" target="#formula_0">(2017)</ref> further introduces a multi- head attention, which applies multiple attentions at a reduced dimensionality. With h heads multi- head attention is computed as</p><formula xml:id="formula_16">tors such that W Q ∈ R dq×dp , W K ∈ R d k ×dp and W V ∈ R dv×dp . Vaswani et al.</formula><formula xml:id="formula_17">mh dot att(Q, K, V, h, s) = [C 0 ; ...; C h ], C i = proj dot att(Q, K, V, d/h, s).</formula><p>Note that with h = 1 we recover the projected dot attention.</p><p>MLP attention The MLP attention ) computes the scores with a one- layer neural network as</p><formula xml:id="formula_18">mlp att(Q, K, V) = softmax (S) V, S ij = w T o tanh(W q q i + W k k j ).</formula><p>Source attention Using the source hidden vectors U, the source attentions are computed as</p><formula xml:id="formula_19">mh dot src att(H, U, h, s) = mh dot att(H, U, U, h, s), mlp src att(H, U) = mlp att(H, U, U), dot src att(H, U, s) = mh dot att(H, U, U, 1, s).</formula><p>Self-attention Self-attention ( <ref type="bibr">Vaswani et al., 2017</ref>) uses the hidden states as queries, keys and values such that</p><formula xml:id="formula_20">mh dot self att(H, s) = mh dot att(H, H, H, s).</formula><p>Please note that on the target side one needs to make sure to preserve the auto-regressive property by only attending to hidden states at the current or past steps h &lt; t, which is achieved by masking the attention mechanism.</p><p>Layer normalization Layer normalization ( <ref type="bibr">Ba et al., 2016</ref>) uses the mean and standard deviation for normalization. It is computed as</p><formula xml:id="formula_21">norm(h t ) = g σ t ⊗ (h t − µ t ) + b µ t = 1 d d i=1 h t,j σ t = 1 d d i=1 (h t,j − µ j ) 2</formula><p>where g and b are learned scale and shift parame- ters with the same dimensionality as h.</p><p>Residual layer A residual layer adds the output of an arbitrary layer chain l to the current hidden states. We define this as</p><formula xml:id="formula_22">res(h t , l) = h t + l(h t ).</formula><p>For convenience we also define res d(h t , l) = res(l(h t )dropout) and res nd(h t , l) = res(norml(h t )dropout).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Standard Architectures</head><p>Having defined the common building blocks we now show how standard NMT architectures can be constructed.</p><p>RNMT As RNNs have been around the longest in NMT, several smaller architecture variations exist. Similar to <ref type="bibr">Wu et al. (2016)</ref> in the following we use a bi-directional RNN followed by a stack of uni-directional RNNs with residual connections on the encoder side. Using the ADL an n layer en- coder can be expressed as</p><formula xml:id="formula_23">U Ls = dropoutbirnnrepeat(n − 1, res d(rnn)).</formula><p>For the decoder we use the architecture by <ref type="bibr" target="#b16">Luong et al. (2015)</ref>, which first runs a stacked RNN and then combines the context provided by a single at- tention mechanism with the hidden state provided by the RNN. This can be expressed by</p><formula xml:id="formula_24">Z L = dropoutrepeat(n, res d(rnn)) concat(id, mlp att)ff.</formula><p>If input feeding ( <ref type="bibr" target="#b16">Luong et al., 2015</ref>) is used the first layer hidden states are redefined as</p><formula xml:id="formula_25">z 0 t = [z L t−1 ; E tgt y t ].</formula><p>Note that this inhibits any parallelism across de- coder time steps. This is only an issue when using models other than RNNs, as RNNs already do not allow for parallelizing over decoder time steps.</p><p>ConvS2S Gehring et al. (2017) introduced a NMT model that fully relies on convolutions, both on the encoder and on the decoder side. The en- coder is defined as</p><formula xml:id="formula_26">U Ls = posrepeat(n, res(cnn(glu)dropout))</formula><p>and the decoder, which uses an unscaled single head dot attention is defined as</p><formula xml:id="formula_27">Z L = posres(dropoutcnn(glu)dropout</formula><p>res(dot src att(s=1))).</p><p>Note that unlike (Gehring et al., 2017) we do not project the query vectors before the attention and do not add the embeddings to the attention values.</p><p>Transformer The Transformer ( <ref type="bibr">Vaswani et al., 2017</ref>) makes use of self-attention, instead of RNNs or Convolutional Neural Networks (CNNs), as the basic computational block. Note that we use a slightly updated residual structure as im- plemented by tensor2tensor 1 than proposed origi- nally. Specifically, layer normalization is applied to the input of the residual block instead of ap- plying it between blocks. The Transformer uses a combination of self-attention and feed-forward layers on the encoder and additionally source at- tention layers on the decoder side. When defining the Transformer encoder block as t enc = res nd(mh dot self att)res nd(ffl), and the decoder block as</p><formula xml:id="formula_28">t dec = res nd(mh dot self att)</formula><p>res nd(mh dot src att)res nd(ffl).</p><p>the Transformer encoder is given as</p><formula xml:id="formula_29">U Ls = posrepeat(n, t enc )norm</formula><p>and the decoder as</p><formula xml:id="formula_30">Z L = posrepeat(n, t dec )norm.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The dot attention mechanism, now heavily used in the Transformer models, was introduced by <ref type="table">(Lu- ong</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>What follows is an extensive empirical analysis of current NMT architectures and how certain sub- layers as defined through our ADL affect perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>All experiments were run with an adapted ver- sion of SOCKEYE ( <ref type="bibr" target="#b8">Hieber et al., 2017)</ref>, which can parse arbitrary model definitions that are ex- pressed in the language described in Section 2.3. The code and configuration are available at https://github.com/awslabs/sockeye/tree/acl18 al- lowing researchers to easily replicate the experi- ments and to quickly try new NMT architectures by either making use of existing building blocks in novel ways or adding new ones.</p><p>In order to get data points on corpora of differ- ent sizes we ran experiments on both WMT and IWSLT data sets. For WMT we ran the majority of our experiments on the most recent WMT'17 data consisting of roughly 5.9 million training sentences for English-German (EN→DE) and 4.5 million sentences for Latvian-English (LV→EN). We used newstest2016 as validation data and re- port metrics calculated on newstest2017. For the smaller IWSLT'16 English-German corpus, which consists of roughly 200 thousand training sen- tences, we used TED.tst2013 as validation data and report numbers for TED.tst2014.</p><p>For both WMT'17 and IWSLT'16 we prepro- cessed all data using the Moses 2 tokenizer and apply Byte Pair Encoding (BPE) ( <ref type="bibr" target="#b19">Sennrich et al., 2015</ref>) with 32,000 merge operations. Unless noted otherwise we run each experiment three times with different random seeds and report the mean and standard deviation of the BLEU and ME- TEOR ( <ref type="bibr" target="#b14">Lavie and Denkowski, 2009</ref> In order to compare to previous work, we also ran an additional experiment on WMT'14 using the same data as <ref type="bibr">Vaswani et al. (2017)</ref> as provided in preprocessed form through ten- sor2tensor. <ref type="bibr">3</ref> This data set consists of WMT'16 training data, which has been tokenized and byte pair encoded with 32,000 merge operations. Eval- uation is done on tokenized and compound split newstest2014 data using multi-bleu.perl in order to get scores comparable to <ref type="bibr">Vaswani et al. (2017)</ref>. As seen in <ref type="table">Table 1</ref>, our Transformer implementa- tion achieves a score equivalent to the originally reported numbers.</p><p>On the smaller IWSLT data we use d model = 512 and on WMT d model = 256 for all mod- els. Models are trained with 6 encoder and 6 de- coder blocks, where in the Transformer model a layer refers to a full encoder or decoder block. All convolutional layers use a kernel of size 3 and a ReLU activation, unless noted otherwise. RNNs use LSTM cells. For training we use the Adam optimizer ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) with a learning rate of 0.0002. The learning rate is decayed by a factor of 0.7, whenever the validation perplexity does not improve for 8 consecutive checkpoints, where a checkpoint is created every 4,000 updates on WMT and 1,000 updates on IWSLT. All mod- els use label smoothing ( <ref type="bibr">Szegedy et al., 2016</ref>) with ls = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">What to attend to?</head><p>Source attention is typically based on the top en- coder block. With multiple source attention lay- ers one could hypothesize that it could be benefi- cial to allow attention encoder blocks other than the top encoder block. It might for example be beneficial for lower decoder blocks to use encoder blocks from the same level as they represent the same level of abstraction. Inversely, assuming that the translation is done in a coarse to fine manner it might help to first use the uppermost encoder block and use gradually lower level representa- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder block IWSLT WMT'17 upper</head><p>25.4 ± 0.2 27.6 ± 0.0 increasing 25.4 ± 0.1 27.3 ± 0.1 decreasing 25.3 ± 0.2 27.1 ± 0.1 The result of modifying the source attention mechanism to use different encoder blocks is shown in <ref type="table" target="#tab_1">Table 2</ref>. The variations include using the result of the encoder Transformer block at the same level as the decoder Transformer block (increasing) and using the upper encoder Trans- former block in the first decoder block and then gradually using the lower blocks (decreasing).</p><p>We can see that attention on the upper encoder block performs best and no gains can be observed by attention on different encoder layers in the source attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Structure</head><p>The Transformer sets itself apart from both standard RNN models and convolutional model by more than just the multi-head self-attention blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN to Transformer</head><p>The differences to the RNN include the multiple source attention layers, multi-head attention, layer normalization and the residual upscaling feed-forward layers. Addition- ally, RNN models typically use single head MLP attention instead of the dot attention. This raises the question of what aspect contributes most to the performance of the Transformer. <ref type="table" target="#tab_2">Table 3</ref> shows the result of taking an RNN and step by step changing the architecture to be simi- lar to the Transformer architecture. We start with a standard RNN architecture with MLP attention similar to <ref type="bibr" target="#b16">Luong et al. (2015)</ref> as described in Sec- tion 2.4 with and without input feeding denoted as RNMT.</p><p>Next, we take a model with a residual connec- tion around the encoder bi-RNN such that the en- coder is defined as</p><formula xml:id="formula_31">dropoutres d(birnn)repeat(5, res d(rnn)).</formula><p>The decoder uses a residual single head dot atten- tion and no input feeding and is defined as We denote this model as RNN in <ref type="table" target="#tab_2">Table 3</ref>. This model is then changed to use multi-head attention (mh), positional embeddings (pos), layer normal- ization on the inputs of the residual blocks (norm), an attention mechanism in a residual block after every RNN layer with multiple (multi-att) and a single head (multi-add-1h), and finally a residual <ref type="table">Model  BLEU  BLEU  METEOR  BLEU  METEOR  Transformer</ref> 25.4 ± 0.1 27.6 ± 0.0 47.2 ± 0.1 18.5 ± 0.0 51.3 ± 0.1 RNMT 23.2 ± 0.2 25.5 ± 0.2 45.1 ± 0.1 - - -input feeding 23.1 ± 0.2 24.6 ± 0.1 43.8 ± 0.2 - - RNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IWSLT EN→DE WMT'17 EN→DE WMT'17 LV→EN</head><p>22.8 ± 0.2 23.8 ± 0.1 43.3 ± 0.1 15.2 ± 0.1 45.9 ± 0.1 + mh 23.7 ± 0.4 24.4 ± 0.1 43.9 ± 0.1 16.0 ± 0.1 47.1 ± 0.1 + pos 23.9 ± 0.2 24.1 ± 0.1 43.5 ± 0.2 - - + norm 23.7 ± 0.1 24.0 ± 0.2 43.2 ± 0.1 15.2 ± 0.1 46.3 ± 0.2 + multi-att-1h</p><p>24.5 ± 0.0 25.2 ± 0.1 44.9 ± 0.1 16.6 ± 0.2 49.1 ± 0.2 / multi-att 24.4 ± 0.3 25.5 ± 0.0 45.3 ± 0.0 17.0 ± 0.2 49.4 ± 0.1 + ff 25.1 ± 0.1 26.7 ± 0.1 46.4 ± 0.2 17.8 ± 0.1 50.5 ± 0.1  Comparing this to the Transformer as defined in Section 2.4 we note that the model is identical to the Transformer, except that each self-attention has been replaced by an RNN or bi-RNN. <ref type="table" target="#tab_2">Table 3</ref> shows that not using input feeding has a negative effect on the result, which however can be compensated by the explored model variations. With just a single attention mechanism the model benefits from multiple attention heads. The gains are even larger when an attention mechanism is added to every layer. With multiple source atten- tion mechanisms the benefit of multiple heads de- creases. Layer normalization on the inputs of the residual blocks has a small negative effect in all settings and metrics. As RNNs can learn to encode positional information positional embeddings are not strictly necessary. Indeed, we can observe no gains but rather even a small drop in BLEU and METEOR for WMT'17 EN→DE when using them. Adding feed-forward layers leads to large and consistent performance boost. While the fi- nal model, which is a Transformer model where each self-attention has been replaced by an RNN, is able to make up for a large amount of the dif- ference between the baseline and the Transformer, it is still outperformed by the Transformer. The largest gains come from multiple attention mech- anisms and residual feed-forward layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN to Transformer</head><p>While the convolutional models have much more in common with the Transformer than the RNN based models, there are still some notable differences. Like the Trans- former, convolutional models have no dependency between decoder time steps during training, use multiple source attention mechanisms and use a slightly different residual structure, as seen in Sec- tion 2.4. The Transformer uses a multi-head scaled dot attention while the ConvS2S model uses an un- scaled single head dot attention. Other differences include the use of layer normalization as well as residual feed-forward blocks in the Transformer.</p><p>The result of making a CNN based architecture more and more similar to the Transformer can be seen in <ref type="table" target="#tab_3">Table 4</ref>. As a baseline we use a simple residual CNN structure with a residual single head dot attention. This is denoted as CNN in <ref type="table" target="#tab_3">Table 4</ref>. On the encoder side we have This is similar to, but slightly simpler than, the ConvS2S model described in Section 2.4. In the experiments we explore both the GLU and ReLU as non-linearities for the CNN.</p><p>Adding layer normalization (norm), multi-head attention (mh) and upsampling residual feed- forward layers (ff) we arrive at a model that is IWSLT <ref type="table">EN-DE  WMT'17 EN→DE  WMT'17 LV→EN  Model  BLEU  BLEU  METEOR  BLEU  METEOR  Transformer</ref> 25.4 ± 0.1 27.6 ± 0.0 47.2 ± 0.1 18.5 ± 0.0 51.3 ± 0.1 CNN GLU 24.3 ± 0.4 25.0 ± 0.3 44.4 ± 0.2 16.0 ± 0.5 47.4 ± 0.4</p><formula xml:id="formula_32">+ norm 24.1 ± 0.1 - - - - + mh</formula><p>24.2 ± 0.2 25.4 ± 0.1 44.8 ± 0.1 16.1 ± 0.1 47.6 ± 0.2 + ff 25.3 ± 0.1 26.8 ± 0.1 46.0 ± 0.1 16.4 ± 0.2 47.9 ± 0.2 CNN ReLU 23.6 ± 0.3 23.9 ± 0.1 43.4 ± 0.1 15.4 ± 0.1 46.4 ± 0.3 + norm 24.3 ± 0.1 24.3 ± 0.2 43.6 ± 0.1 16.0 ± 0.2 47.1 ± 0.5 + mh 24.2 ± 0.2 24.9 ± 0.1 44.4 ± 0.1 16.1 ± 0.1 47.5 ± 0.2 + ff 25.3 ± 0.3 26.9 ± 0.1 46.1 ± 0.0 16.4 ± 0.2 47.9 ± 0.1 identical to a Transformer where the self-attention layers have been replaced by CNNs. This means that we have the following architecture on the en- coder posrepeat(6, res nd(cnn)res nd(ffl))norm.</p><p>Whereas for the decoder we have posrepeat(6, res nd(cnn)</p><p>res nd(mh dot src att)res nd(ffl))norm.</p><p>While in the baseline the GLU activation works better than the ReLU activation, when layer normalization, multi-head attention attention and residual feed-forward layers are added, the perfor- mance is similar. Except for IWSLT multi-head attention gives consistent gains over single head attention. The largest gains can however be ob- served by the addition of residual feed-forward layers. The performance of the final model, which is very similar to a Transformer where each self- attention has been replaced by a CNN, matches the performance of the Transformer on IWSLT EN→DE but is still 0.7 BLEU points worse on WMT'17 EN→DE and two BLEU points on WMT'17 LV→EN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Self-attention variations</head><p>At the core of the Transformer are self-attentional layers, which take the role previously occupied by RNNs and CNNs. Self-attention has the advantage that any two positions are directly connected and that, similar to CNNs, there are no dependencies between consecutive time steps so that the com- putation can be fully parallelized across time. One disadvantage is that relative positional information is not directly represented and one needs to rely on the different heads to make up for this. In a CNN information is constrained to a local window which grows linearly with depth. Relative posi- tions are therefore taken into account. While an RNN keeps an internal state, which can be used in future time steps, it is unclear how well this works for very long range dependencies ( <ref type="bibr" target="#b13">Koehn and Knowles, 2017;</ref><ref type="bibr" target="#b2">Bentivogli et al., 2016)</ref>. Addi- tionally, having a dependency on the previous hid- den state inhibits any parallelization across time.</p><p>Given the different advantages and disadvan- tages we selectively replace self-attention on the encoder and decoder side in order to see where the model benefits most from self-attention.</p><p>We take the encoder and decoder block defined in Section 2.4 and try out different layers in place of the self-attention. Concretely, we have t enc = res nd(x enc )res nd(ffl), on the encoder side and t dec = res nd(x dec ) res nd(mh dot src att)res nd(ffl).</p><p>on the decoder side. <ref type="table">Table 5</ref> shows the result of replacing x enc and x dec with either self-attention, a CNN with ReLU activation or an RNN. Notice that with self-attention used in both x enc and x dec we recover the Transformer model. Additionally, we remove the residual block on the decoder side entirely (none). This results in a decoder block which only has information about the previous tar- get word y t through the word embedding that is fed as the input to the first layer. The decoder block is reduced to <ref type="table">Decoder  BLEU  BLEU  METEOR  BLEU  METEOR  self-att</ref> self-att 25.4 ± 0.2 27.6 ± 0.0 47.2 ± 0.1 18.3 ± 0.0 51.1 ± 0.1 self-att RNN 25.1 ± 0.1 27.4 ± 0.1 47.0 ± 0.1 18.4 ± 0.2 51.1 ± 0.1 self-att CNN 25.4 ± 0.4 27.6 ± 0.2 46.7 ± 0.1 18.0 ± 0.3 50.3 ± 0.3 RNN self-att 25.8 ± 0.1 27.2 ± 0.1 46.7 ± 0.1 17.8 ± 0.1 50.6 ± 0.1 CNN self-att 25.7 ± 0.1 26.6 ± 0.3 46.3 ± 0.1 16.8 ± 0.4 49.4 ± 0.4 RNN RNN 25.1 ± 0.1 26.7 ± 0.1 46.4 ± 0.2 17.8 ± 0.1 50.5 ± 0.1 CNN CNN 25.3 ± 0.3 26.9 ± 0.1 46.1 ± 0.0 16.4 ± 0.2 47.9 ± 0.2 self-att combined 25.1 ± 0.2 27.6 ± 0.2 47.2 ± 0.2 18.3 ± 0.1 51.1 ± 0.1 self-att none 23.7 ± 0.2 25.3 ± 0.2 43.1 ± 0.1 15.9 ± 0.1 45.1 ± 0.2 <ref type="table">Table 5</ref>: Different variations of the encoder and decoder self-attention layer.</p><note type="other">IWSLT EN→DE WMT'17 EN→DE WMT'17 LV→EN Encoder</note><p>In addition to that, we try a combination where the first and fourth block use self-attention, the second and fifth an RNN, the third and sixth a CNN (com- bined).</p><p>Replacing the self-attention on both the encoder and the decoder side with an RNN or CNN re- sults in a degradation of performance. In most settings, such as WMT'17 EN→DE for both vari- ations and WMT'17 LV→EN for the RNN, the performance is comparable when replacing the de- coder side self-attention. For the encoder how- ever, except for IWSLT, we see a drop in perfor- mance of up to 1.5 BLEU points when not using self-attention. Therefore, self-attention seems to be more important on the encoder side than on the decoder side. Despite the disadvantage of having a limited context window, the CNN performs as well as self-attention on the decoder side on IWLT and WMT'17 EN→DE in terms of BLEU and only slightly worse in terms of METEOR. The combi- nation of the three mechanisms (combined) on the decoder side performs almost identical to the full Transformer model, except for IWSLT where it is slightly worse.</p><p>It is surprising how well the model works with- out any self-attention as the decoder essentially looses any information about the history of gener- ated words. Translations are entirely based on the previous word, provided through the target side word embedding, and the current position, pro- vided through the positional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We described an ADL for specifying NMT archi- tectures based on composable building blocks. In- stead of committing to a single architecture, the language allows for combining architectures on a granular level. Using this language we ex- plored how specific aspects of the Transformer ar- chitecture can successfully be applied to RNNs and CNNs. We performed an extensive evalua- tion on IWSLT EN→DE, WMT'17 EN→DE and LV→EN, reporting both BLEU and METEOR over multiple runs in each setting.</p><p>We found that RNN based models benefit from multiple source attention mechanisms and resid- ual feed-forward blocks. CNN based models on the other hand can be improved through layer nor- malization and also feed-forward blocks. These variations bring the RNN and CNN based models close to the Transformer. Furthermore, we showed that one can successfully combine architectures. We found that self-attention is much more impor- tant on the encoder side than it is on the decoder side, where even a model without self-attention performed surprisingly well. For the data sets we evaluated on, models with self-attention on the en- coder side and either an RNN or CNN on the de- coder side performed competitively to the Trans- former model in most cases.</p><p>We make our implementation available so that it can be used for exploring novel architecture varia- tions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>et al., 2015) as part of an exploration of dif- ferent attention mechanisms for RNN based NMT models. Britz et al. (2017) performed an extensive ex- ploration of hyperparameters of RNN based NMT models. The variations explored include different attention mechanisms, RNN cells types and model depth. Similar to our work, Schrimpf et al. (2017) de- fine a language for exploring architectures. In this case the architectures are defined for RNN cells and not for the higher level model architec- ture. Using the language they perform an auto- matic search of RNN cell architectures. For the application of image classification there have been several recent successful efforts of automatically searching for successful architec- tures (Zoph and Le, 2016; Negrinho and Gordon, 2017; Liu et al., 2017). 1 https://github.com/tensorflow/tensor2tensor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>dropoutrepeat( 6 ,</head><label>6</label><figDesc>res d(rnn)) res d(dot src att)res d(ffl).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>upscaling feed-forward layer is added after each attention block (ff). The final architecture of the encoder after applying these variations is posres nd(birnn)res nd(ffl) repeat(5, res nd(rnn)res nd(ffl)norm and of the decoder posrepeat(6, res nd(rnn) res nd(mh dot src att)res nd(ffl))norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>posrepeat( 6 ,</head><label>6</label><figDesc>res d(cnn)) and for the decoder posrepeat(6, res d(cnn)res d(dot src att)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>) scores across runs. Evaluation scores are based on tokenized sequences and calculated with MultEval (Clark et al., 2011).</figDesc><table>Model 
WMT'14 
Vaswani et al. (2017) 
27.3 
Our Transformer base impl. 
27.5 

Table 1: BLEU scores on WMT'14 EN→DE. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores when varying the en-
coder block used in the source attention mecha-
nism of a Transformer on the EN→DE IWSLT and 
WMT'17 datasets. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Transforming an RNN into a Transformer style architecture. + shows the incrementally added 
variation. / denotes an alternative variation to which the subsequent + is relative to. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Transforming a CNN based model into a Transformer style architecture.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> https://github.com/moses-smt/mosesdecoder/</note>

			<note place="foot" n="3"> https://github.com/tensorflow/tensor2tensor/blob/ 765d33bb/tensor2tensor/data generators/translate ende.py</note>

			<note place="foot">t dec = res nd(mh dot src att)res nd(ffl).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural versus phrasebased machine translation quality: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04631</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08083</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sockeye: A Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05690</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03872</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The meteor metric for automatic evaluation of machine translation. Machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<title level="m">Deeparchitect: Automatically designing and training deep architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A flexible approach to automated rnn architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07316</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
