<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEXSEMTM: A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bennett</surname></persName>
							<email>awbennett0@gmail.com, tb@ldwin.net, jeyhan.lau@gmail.com, diana@dianamccarthy.co.uk, bond@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Dept of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Cambridge ♠ Linguistics and Multilingual Studies</orgName>
								<orgName type="institution" key="instit4">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Dept of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Cambridge ♠ Linguistics and Multilingual Studies</orgName>
								<orgName type="institution" key="instit4">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename><surname>Jey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Dept of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Cambridge ♠ Linguistics and Multilingual Studies</orgName>
								<orgName type="institution" key="instit4">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Dept of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Cambridge ♠ Linguistics and Multilingual Studies</orgName>
								<orgName type="institution" key="instit4">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♥</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Dept of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Cambridge ♠ Linguistics and Multilingual Studies</orgName>
								<orgName type="institution" key="instit4">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Dept of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Cambridge ♠ Linguistics and Multilingual Studies</orgName>
								<orgName type="institution" key="instit4">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bond</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Computing and Information Systems</orgName>
								<orgName type="department" key="dep2">Dept of Theoretical and Applied Linguistics</orgName>
								<orgName type="institution" key="instit1">The University of Melbourne</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Cambridge ♠ Linguistics and Multilingual Studies</orgName>
								<orgName type="institution" key="instit4">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEXSEMTM: A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1513" to="1524"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed. This paper analyses a state-of-the-art method for sense distribution learning, and op-timises it for application to the entire vocabulary of a given language. The optimised method is then used to produce LEXSEMTM: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polyse-mous, English simplex lemmas, which is released as a public resource to the community. Finally, the quality of this data is investigated, and the LEXSEMTM sense distributions are shown to be superior to those based on the WORDNET first sense for lemmas missing from SEMCOR, and at least on par with SEMCOR-based distributions otherwise.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP commu- nity for many years (for a detailed overview, see <ref type="bibr" target="#b0">Agirre and Edmonds (2007)</ref> and <ref type="bibr" target="#b35">Navigli (2009)</ref>). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for perform- ing WSD ( <ref type="bibr" target="#b39">Postma et al., 2015;</ref><ref type="bibr" target="#b7">Brody et al., 2006</ref>), as well as complementary problems dealing with word senses ( <ref type="bibr" target="#b22">Jin et al., 2009;</ref><ref type="bibr" target="#b28">Lau et al., 2014</ref>).</p><p>One such application has been the automatic learning of sense distributions <ref type="bibr" target="#b30">(McCarthy et al., 2004b;</ref><ref type="bibr" target="#b28">Lau et al., 2014)</ref>. A sense distribution is a probability distribution over the senses of a given lemma. For example, if the noun crane had two senses, bird and machine, then a hypo- thetical sense distribution could indicate that the noun is expected to take the machine meaning 60% of the time and the bird meaning 40% of the time in a representative corpus. Sense distribu- tions (or simple "first sense" information) are used widely in tasks including information extraction ( <ref type="bibr" target="#b41">Tandon et al., 2015)</ref>, novel word sense detection ( <ref type="bibr" target="#b27">Lau et al., 2012;</ref><ref type="bibr" target="#b28">Lau et al., 2014</ref>), semi-automatic dictionary construction ( <ref type="bibr" target="#b18">Cook et al., 2013)</ref>, lex- ical simplification <ref type="bibr" target="#b3">(Biran et al., 2011)</ref>, and tex- tual entailment <ref type="bibr" target="#b40">(Shnarch et al., 2011</ref>). Automat- ically acquired sense distributions themselves are also used to improve unsupervised WSD, for ex- ample by providing a most frequent sense heuris- tic ( <ref type="bibr" target="#b30">McCarthy et al., 2004b;</ref><ref type="bibr" target="#b22">Jin et al., 2009)</ref> or by improving unsupervised usage sampling strate- gies ( <ref type="bibr" target="#b1">Agirre and Martinez, 2004</ref>). Furthermore, the improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data ( <ref type="bibr" target="#b23">Koeling et al., 2005;</ref><ref type="bibr" target="#b13">Chan and Ng, 2006</ref>; <ref type="bibr" target="#b28">Lau et al., 2014</ref>).</p><p>In addition, there is great scope to use these techniques to improve existing sense frequency re- sources, which are currently limited by the bottle- neck of requiring manual sense annotation. The most prominent example of such a resource is WORDNET <ref type="bibr" target="#b20">(Fellbaum, 1998)</ref>, where the sense frequency data is based on SEMCOR <ref type="bibr" target="#b32">(Miller et al., 1993</ref>), a 220,000 word corpus that has been manually tagged with WORDNET senses. This data is full of glaring irregularities due to its age and the limited size of the corpus; for example, the word pipe has its most frequent sense listed as tobacco pipe, whereas one might expect this to be tube carrying water or gas in modern English ( <ref type="bibr" target="#b29">McCarthy et al., 2004a</ref>). This is likely due to the more common use of the tobacco pipe sense in mid-20th century liter- ature. The problem is particularly highlighted by the fact that out of the approximately 28,000 pol- ysemous simplex lemmas in WORDNET 3.0, ap- proximately 61% have no sense annotations at all, and less than half of the remaining lemmas have at least 5 sense annotations! Unfortunately, there has been a lack of work investigating how to apply sense learning tech- niques at the scale of a full lexical resource such as WORDNET. Updating language-wide sense frequency resources would require learning sense distributions over the entire vocabularies of lan- guages, which could be extremely computation- ally expensive. To make things worse, domain dif- ferences could require learning numerous distribu- tions per word. Despite this, though, we would not want to make these techniques scalable at the expense of sense distribution quality. Therefore, we would like to understand the tradeoff between the accuracy and computation time of these tech- niques, and optimise this tradeoff. This could be particularly critical in applying them in an indus- trial setting.</p><p>The current state-of-the-art technique for unsu- pervised sense distribution learning is HDP-WSI ( <ref type="bibr" target="#b28">Lau et al., 2014)</ref>. In order to address the above concerns, we provide a series of investigations ex- ploring how to best optimise HDP-WSI for large- scale application. We then use our optimised technique to produce LEXSEMTM, 1 a semantic and sense frequency dataset of unprecedented size, spanning the entire vocabulary of English. Finally, we use crowdsourced data to produce a new set of gold-standard sense distributions to accompany LEXSEMTM. We use these to investigate the qual- ity of the sense frequency data in LEXSEMTM with respect to SEMCOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Given the difficulty and expense of obtaining large-scale and robust annotated data, unsuper- vised approaches to problems involving word learning and recognising word senses have long been studied in NLP. Perhaps the most fa- mous such problem is word sense disambiguation (WSD), for which many unsupervised solutions have been proposed. Some methods are very com- plex, performing WSD separately for each word usage using information such as word embeddings of surrounding words ) or POS- tags ( <ref type="bibr" target="#b24">Lapata and Brew, 2004</ref>). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic <ref type="bibr" target="#b31">(McCarthy et al., 2007)</ref>, which assigns each usage of a given word-type to its most frequent sense.</p><p>Given the popularity of the MFS heuristic, much of the past work on unsupervised tech- niques has focused on identifying the most fre- quent sense. The original method of this kind was proposed by <ref type="bibr" target="#b30">McCarthy et al. (2004b)</ref>, which re- lied on finding distributionally similar words to the target word, and comparing these to the can- didate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its to- ken usages. Boyd-Graber and  for- malise the method of <ref type="bibr" target="#b30">McCarthy et al. (2004b)</ref> with a probabilistic model, while others take different approaches, such as adapting existing sense fre- quency data to specific domains <ref type="bibr" target="#b12">(Chan and Ng, 2005;</ref><ref type="bibr" target="#b13">Chan and Ng, 2006</ref>), using coarse grained thesaurus-like sense inventories <ref type="bibr" target="#b34">(Mohammad and Hirst, 2006</ref>), adapting information retrieval-based methods <ref type="bibr" target="#b25">(Lapata and Keller, 2007)</ref>, using ensem- ble learning ( <ref type="bibr" target="#b7">Brody et al., 2006</ref>), utilising the net- work structure of WORDNET (Boyd- , or making use of word embeddings <ref type="bibr" target="#b2">(Bhingardive et al., 2015</ref>). Alternatively, <ref type="bibr" target="#b22">Jin et al. (2009)</ref> focus on how best to use the MFS heuris- tic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of <ref type="bibr" target="#b28">Lau et al. (2014)</ref>, due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses.</p><p>The task we are interested in -namely, sense distribution learning -is in principle very simi- lar to identifying the MFS. Indeed, of these meth- ods for identifying the MFS, some of them are explicitly described in terms of sense distribution learning <ref type="bibr" target="#b12">(Chan and Ng, 2005;</ref><ref type="bibr" target="#b13">Chan and Ng, 2006;</ref><ref type="bibr" target="#b28">Lau et al., 2014</ref>), while the others implicitly learn sense distributions by calculating some kind of scores used to rank senses.</p><p>The state-of-the-art technique of <ref type="bibr" target="#b28">Lau et al. (2014)</ref> that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using non- parametric HDP ( <ref type="bibr" target="#b42">Teh et al., 2006</ref>) topic mod- els, as detailed in Section 3. The WSI compo- nent, HDP-WSI, is based on the work of <ref type="bibr" target="#b27">Lau et al. (2012)</ref>, which at the time was state-of-the-art. Since then, however, other competitive WSI ap- proaches have been developed, involving complex structures such as multi-layer topic models ( <ref type="bibr" target="#b14">Chang et al., 2014</ref>), or complex word embedding based approaches ( <ref type="bibr" target="#b36">Neelakantan et al., 2014</ref>). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future explo- ration. On the other hand, because HDP-WSI is implemented using topic models, it can be cus- tomised by replacing HDP with newer, more ef- ficient topic modelling algorithms. Recent work has produced more advanced topic modelling ap- proaches, some of which are extensions of existing approaches using more advanced learning algo- rithms or expanded models <ref type="bibr" target="#b8">(Buntine and Mishra, 2014)</ref>, while others are more novel, involving vari- ations such as neural networks ( <ref type="bibr" target="#b26">Larochelle and Murray, 2011;</ref><ref type="bibr" target="#b11">Cao et al., 2015)</ref>, or incorporat- ing distributional similarity of words ( <ref type="bibr" target="#b43">Xie et al., 2015)</ref>. Of these approaches, we chose to experi- ment with that of <ref type="bibr" target="#b8">Buntine and Mishra (2014)</ref> be- cause a working implementation was readily avail- able, it has previously shown very strong perfor- mance in terms of accuracy and speed, and it is similar to HDP and thus easy to incorporate into our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HDP-WSI Sense Learning</head><p>HDP-WSI ( <ref type="bibr" target="#b28">Lau et al., 2014</ref>) is a state-of-the- art unsupervised method for learning sense distri- butions, given a sense repository with per-sense glosses. It takes as input a collection of exam- ple usages of the target lemma 2 and the glosses for each target sense, and produces a probability distribution over the target senses.</p><p>At the heart of HDP-WSI is HDP ( <ref type="bibr" target="#b42">Teh et al., 2006</ref>), a nonparametric topic modelling technique. It is a generative probabilistic model and uses top- ics as a latent variable to allow statistical shar- ing between documents, providing a kind of soft- clustering mixture model. Each document is as- sumed to have a corresponding distribution over these topics, and each topic is assumed to have a corresponding distribution over words. Accord- ing to the model, each word for a given docu- ment is independently generated by first sampling a topic according to that document's distribution over topics, and then sampling a word according to the topic's distribution over words. Unlike older topic modelling methods such as LDA ( <ref type="bibr" target="#b4">Blei et al., 2003)</ref>, HDP is nonparametric, meaning the num- ber of topics used by the model is automatically learnt, and does not need to be set as a hyper- parameter. In other words, the model automati- cally learns the "right" number of topics for each lemma.</p><p>HDP-WSI follows a two-step process: word sense induction (WSI), followed by topic-sense alignment. WSI is performed using HDP based on the earlier work of <ref type="bibr" target="#b27">Lau et al. (2012)</ref>: each us- age of the target lemma is treated as a document, and HDP topic modelling is run on this document collection. This gives a variable number of learnt topics, which are the senses induced by WSI. A single topic is then assigned to each document, <ref type="bibr">3</ref> and a distribution over these topics is learnt using maximum likelihood estimation.</p><p>In the second step of HDP-WSI, we align the distribution over topics from WSI to the provided sense inventory. We first create a distribution over words for each sense, from the sense's gloss. <ref type="bibr">4</ref> Then a prevalence score is calculated for each sense by taking a weighted sum of the similarity of that sense with every topic, 5 weighting each similarity score by the topic's probability. These prevalence scores are finally normalised to give a distribution over senses.</p><p>Despite state-of-the-art results with HDP-WSI in past work ( <ref type="bibr" target="#b28">Lau et al., 2014</ref>), there are some con- done by <ref type="bibr" target="#b27">Lau et al. (2012)</ref>. <ref type="bibr">3</ref> The topic with the maximum probability is assigned. <ref type="bibr">4</ref> As with the lemma usages, the text is normalised via lem- matisation and stopword removal. Then a distribution is cre- ated using maximum likelihood estimation.</p><p>5 Defined in terms of Jensen Shannon divergence between the respective distributions over words. cerns in applying it to large-scale learning. Most importantly, in order to make HDP nonparamet- ric, it relies on relatively inefficient MCMC sam- pling techniques, typically based on a hierarchi- cal Chinese Restaurant Process ("CRP"). On the other hand, recent work has provided very ef- ficient topic modelling techniques given a fixed number of topics. While in previous work it was assumed that performance benefits of HDP over other techniques like LDA were based on it learning the "right" number of topics ( <ref type="bibr" target="#b27">Lau et al., 2012;</ref><ref type="bibr" target="#b28">Lau et al., 2014</ref>), more recent work chal- lenges this assumption. Rather, it is suggested that it is more important for topic modelling to use high-performance learning algorithms so that top- ics are learnt in correct proportions, in which case "junk" topics can easily be ignored <ref type="bibr" target="#b8">(Buntine and Mishra, 2014</ref>). In other words, it is likely that the previously-found performance advantage of HDP over LDA was actually due to properties of their respective Gibbs sampling algorithms.</p><p>Furthermore, in our experience using it for sense distribution learning, HDP seems to use a very consistent number of topics. In experiments we ran on the BNC 6 -the same dataset that <ref type="bibr" target="#b28">Lau et al. (2014)</ref> based their experiments on -the num- ber of topics was between 5 and 10 over 80% of the time, and over 99% of the time it was below 14. Because the number of topics is so consistent, it is likely we can safely use a fixed number with little risk that it will be too low.</p><p>In addition, there are some theoretical concerns with HDP. Firstly, it models topic and word al- locations using Dirichlet Processes ( <ref type="bibr" target="#b42">Teh et al., 2006</ref>). However, previous research has shown that phenomena such as word and sense frequen- cies follow power-law distributions according to Zipf's law <ref type="bibr" target="#b37">(Piantadosi, 2014)</ref>, and thus are better modelled using Pitman-Yor Processes <ref type="bibr" target="#b38">(Pitman and Yor, 1997</ref>). Another weakness is that HDP does not model burstiness. This is a phenomenon where words that occur at least once in a given discourse are disproportionately more likely to occur several times, even compared with other discourses about the same topic <ref type="bibr" target="#b17">(Church, 2000;</ref><ref type="bibr" target="#b19">Doyle and Elkan, 2009)</ref>.</p><p>We now present and evaluate HCA-WSI, which is an alternative to HDP-WSI that addresses the above concerns. It follows the same process as HDP-WSI, except that the HDP topic mod- elling is replaced with HCA 7 ( <ref type="bibr" target="#b8">Buntine and Mishra, 2014</ref>), a more advanced software suite for topic modelling. 8 HCA is based on a similar probabilis- tic model to HDP, except for a few differences: (1) it only has a fixed number of topics; (2) it mod- els word frequencies using a more general Pitman- Yor Process; and <ref type="formula">(3)</ref> it incorporates an extra com- ponent to the model to model burstiness (each doc- ument can individually have an elevated probabil- ity for some words, regardless of its distribution over topics). The second and third of these dif- ferences directly answer our theoretical concerns about using HDP.</p><p>The learning algorithm for HCA is called "table indicator sampling" <ref type="bibr" target="#b15">(Chen et al., 2011</ref>), which is a collapsed Gibbs sampling algorithm. The over- all probabilistic model is interpreted as a hierar- chical CRP, and some extra latent variables called table indicators are added to the model, which en- code the decisions made about creating new tables during the CRP. The use of these latent variables allows for a very efficient collapsed Gibbs sam- pling process, which is found to converge more quickly than competing Gibbs sampling and vari- ational Bayes techniques. The convergence is also shown to be more accurate, with topic models of lower perplexity being produced given the same underlying stochastic model. Compared to HDP, HCA has been shown to be orders of magnitude faster, with similar mem- ory overhead <ref type="bibr" target="#b8">(Buntine and Mishra, 2014)</ref>. There- fore, as long as the quality of the sense distribu- tions given by HCA-WSI are no worse than those from HDP-WSI, it should be worthwhile switch- ing in terms of scalability. This massive reduction in computation time would be of particular benefit to our intended large-scale application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>We evaluate HCA-WSI in comparison to HDP- WSI using one of the sense tagged datasets of <ref type="bibr">9</ref> which was also used by <ref type="bibr" target="#b28">Lau et al. (2014)</ref>. This dataset consists of 40 En- glish lemmas, and for each lemma it contains a set of usages of varying size from the BNC and a gold-standard sense distribution that was created by hand-annotating a subset of the usages with WORDNET 1.7 senses.</p><note type="other">0 20 40 10 2 10 3 10 4 Number of Lemma Usages (1000's) Topic Model Training Time (s) Training Time for HDP-WSI vs. HCA-WSI HDP-WSI HCA-WSI Figure 1: Comparison of the time taken to train the topic models of HDP-WSI and HCA-WSI for each lemma in the BNC dataset. For each method, one data point is plotted per lemma. Koeling et al. (2005),</note><p>Using this dataset, we can calculate the qual- ity of a candidate sense distribution by calculat- ing its Jensen Shannon divergence (JSD) with re- spect to the corresponding gold-standard distribu- tion. JSD is a measure of dissimilarity between two probability distributions, so a lower JSD score means the distribution is more similar to the gold- standard, and is therefore assumed to be of higher quality.</p><p>Given our finding on topic counts in Section 3, HCA was run using a fixed number of 10 topics. Other settings were configured as recommended in the HCA documentation, or according to the HDP settings used by <ref type="bibr" target="#b28">Lau et al. (2014)</ref>. <ref type="bibr">10</ref> This setup is also used in subsequent experiments, except where stated otherwise.</p><p>We proceeded by calculating the JSD scores of all lemmas in this dataset, using both methods. We performed a Wilcoxon signed-rank test on the two sequences of JSD scores, in order to test the hy- pothesis that switching to HCA-WSI has a system- atic impact on sense distribution quality. We found that the mean JSD score for HDP-WSI was 0.209 ± 0.116, slightly lower than the mean JSD score for HCA-WSI of 0.211 ± 0.117. However the two-sided p-value from the test was 0.221, which is insignificant at any reasonable decision thresh- old. In addition, we compared the time taken 11 to run topic modelling for every lemma using both methods, the results of which are displayed in <ref type="figure">Fig- ure 1</ref>. These results show that the computation time of HCA-WSI is consistently lower than that of HDP-WSI, by over an order of magnitude.</p><p>We conclude that HCA-WSI is far more compu- tationally efficient than HDP-WSI, and there is no significant evidence that it gives worse sense dis- tributions. Therefore, HCA-WSI is used instead of HDP-WSI for the remainder of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Large-Scale Learning with HCA-WSI</head><p>In order to apply HCA-WSI sense distribution learning on a language-wide scale, we need to understand how to optimise it to achieve a rea- sonable tradeoff between efficiency and sense dis- tribution quality. Most pertinently, we need to know how many lemma usages and iterations of Gibbs sampling are needed for high-quality re- sults, and whether this varies for different kinds of lemmas. To this end, we run experiments ex- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.04</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Usages (1000's) JSD-Mean Difference</head><p>JSD-Mean Convergence <ref type="figure">Figure 3</ref>: Convergence of mean JSD score for BNC dataset lemmas, using HCA-WSI. One line plotted per lemma, one data-point per bin. For each data- point, the difference between mean JSD within that bin and within the final bin of the lemma is plotted.</p><p>ploring how HCA-WSI converges over increasing numbers of lemma usages and topic model itera- tions. These experiments are all performed using the BNC dataset (see Section 4.1).</p><p>In order to explore the convergence of HCA- WSI over Gibbs sampling iterations, we trained HCA topic models for each lemma in the BNC dataset over a large number of iterations. The re- sults of this are displayed in <ref type="figure" target="#fig_0">Figure 2</ref>, which shows the convergence of log-perplexity for each lemma. We conclude that around 300 iterations of sam- pling appears to be sufficient for convergence in the vast majority of cases.</p><p>Next, we explored the convergence of HCA- WSI over lemma usages by subsampling from our training data. For each lemma in the BNC dataset, we created a large number of sense distributions using random subsets of the lemma's usages. <ref type="bibr">12</ref> Each distribution was generated by randomly se- lecting a number of usages between a minimum of 500 and the maximum available (uniformly), and randomly sampling that many usages without replacement. From these usages the sense distri- bution was created using HCA-WSI, and its JSD score relative to the gold-standard was calculated (as in Section 4.1). Finally, the results for each lemma were partitioned into 40 bins of approxi- mately equal size, according to the number of us- ages sampled.</p><p>The results of our subsampling experiment are plotted in <ref type="figure">Figure 3</ref>, which shows the convergence of mean JSD score for each lemma. We conclude from this that around 5,000-10,000 usages seem to be necessary for convergent results, and that this is fairly consistent across lemmas. <ref type="bibr">13</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LEXSEMTM Dataset</head><p>We now discuss the creation of the LEXSEMTM ("Lexical Semantic Topic Models") dataset, which contains trained topic models for the majority of simplex English lemmas. These can be aligned to any sense repository with glosses to produce sense distributions, or used directly in other ap- plications. In addition, the dataset contains distri- butions over WORDNET 3.0 senses.</p><p>In order to produce domain-neutral sense dis- tributions reflecting usage in modern English, we sampled all lemma usages from English Wikipedia. <ref type="bibr">14</ref> Our Wikipedia corpus was tokenised and POS-tagged using OpenNLP and lemmatised using Morpha ( <ref type="bibr" target="#b33">Minnen et al., 2001</ref>).</p><p>We trained topic models for every simplex lemma in WORDNET 3.0 with at least 20 us- ages in our processed Wikipedia corpus. This in- cluded lemmas for all POS (nouns, verbs, adjec- tives, and adverbs), and also nonpolysemous lem- mas. In Section 5, we concluded that approxi- mately 5,000-10,000 usages were needed for con- vergent results with the BNC dataset. On the other hand, given that we are working on a different cor- pus and with a wider range of lemmas there is uncertainty in this number, so we conservatively sampled up to 40,000 usages per lemma, if avail- able.</p><p>These usages were sampled from the corpus by locating all sentences where either the surface or lemmatised forms of the sentence contained the target lemma, along with a matching POS- tag. Processing of lemma usages was done almost identically to <ref type="bibr" target="#b28">Lau et al. (2014)</ref>. However, because we found the usages contained substantially fewer tokens on average compared to the BNC dataset, we included two sentences rather than one on ei- ther side of the target lemma location where pos- sible (giving 5 sentences in total), which gave a better match in usage size.</p><p>Topic models were trained using HCA, using al- most the same setup as described in Section 4.1. However, since some highly-polysemous lemmas may require a greater number of topics than the lemmas in the BNC dataset, we conservatively in- creased the number of topics used from 10 to 20. We similarly increased the number of Gibbs sam- pling iterations from 300 to 1,000. 15 Finally, for each polysemous lemma that we trained a topic model for, we also produced a sense distribu- tion over WORDNET 3.0 senses, using the default topic-sense alignment method discussed in Sec- tion 3.</p><p>In total, 62,721 lemmas were processed, and 8,801 of these had the desired number of at least 5,000 usages. Counting only polysemous lem- mas for which we also provide sense distribu- tions, 25,155 were processed in total, and 6,853 of these had at least 5,000 usages. This works out to approximately 88% coverage of polysemous WORDNET 3.0 lemmas in total, or 24% coverage with at least 5,000 usages (as compared to 39% coverage by lemmas in SEMCOR, or 17% with at least 5 sense-tagged occurrences in SEMCOR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation of LEXSEMTM against SEMCOR</head><p>Our final major contribution is an analysis of how our LEXSEMTM sense distributions compare with SEMCOR. We produce a new set of gold- standard sense distributions for a diverse set of simplex English lemmas tagged with WORDNET 3.0 senses, created using crowdsourced annota- tions of English Wikipedia usages. We use these gold-standard distributions to investigate when LEXSEMTM should be used in place of SEM- COR, and release them as a public resource, to facilitate the evaluation of future work involving LEXSEMTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Gold-Standard Distributions</head><p>One of our goals in creating this dataset was to determine whether there is a SEMCOR frequency cutoff, 16 below which our LEXSEMTM distribu- tions are clearly more accurate than SEMCOR. In order to have a diverse set of lemmas and be able to address this question, we partitioned the lem- mas in WORDNET 3.0 based on SEMCOR fre- quency.</p><p>In order to keep analysis simple and consistent with previous investigations, we first filtered out multiword lemmas, nonpolysemous lemmas, and non-nouns. <ref type="bibr">17</ref> Next, since in Section 5 we decided that at least around 5,000 usages were needed for stable and converged sense distributions, we fil- tered out all lemmas without at least 5,000 usages in our English Wikipedia corpus. The remaining lemmas were then split into 5 groups of approx- imately equal size based on SEMCOR frequency. The SEMCOR frequencies contained in each group are summarised in <ref type="table">Table 1</ref>.</p><p>From each of the SEMCOR frequency groups, we randomly sampled 10 lemmas, giving 50 lem- mas in total. Then for each lemma, we randomly sampled 100 usages to be annotated from English Wikipedia. This was done in the same way as the sampling of lemma usages for LEXSEMTM (see Section 6).</p><p>We obtained crowdsourced sense annotations for each lemma using Amazon Mechanical Turk (AMT: Callison-Burch and Dredze <ref type="formula">(2010)</ref>). The sentences for each lemma were split into 4 batches (25 sentences per batch). In addition, two con- trol sentences <ref type="bibr">18</ref> were created for each lemma, and added to each corresponding batch. Each batch of 27 items was annotated separately by 10 an- notators. For each item to be annotated, annota- tors were provided with the sentence containing the lemma, the gloss for each sense as listed in WORDNET 3.0 <ref type="bibr">19</ref> and a list of hypernyms and syn- onyms for each sense. Annotators were asked to assign each item to exactly one sense.</p><p>From these crowdsourced annotations, our gold-standard sense distributions were created us- ing MACE ( <ref type="bibr" target="#b21">Hovy et al., 2013)</ref>, which is a general- purpose tool for inferring item labels from multi- annotator, multi-item tasks. It provides a Bayesian framework for modelling item annotations, mod- elling the individual biases of each annotator, and supports semi-supervised training. MACE was run separately on the usage annotations of each lemma, with the control sentences included to guide training.</p><p>Gold-standard sense distributions were ob- tained from the output of MACE, which includes a list containing the mode label of each item. For each lemma, we removed the control sentence labels from this list, and constructed the gold- standard distribution from the remaining labels us- ing maximum likelihood estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluation of LEXSEMTM</head><p>We now use these gold-standard distributions to evaluate the sense distributions in LEXSEMTM relative to SEMCOR. For each of the 50 lemmas that we created gold-standard distributions for, we evaluate the corresponding LEXSEMTM distribu- tion against the gold-standard. In addition, we cre- ate benchmark sense distributions for each lemma from SEMCOR counts using maximum likelihood estimation, <ref type="bibr">20</ref> which we also evaluate against the gold-standards. Evaluation of sense distribution quality using gold-standard distributions is done by calculating JSD, as in Section 4.1.</p><p>First, we performed this comparison of LEXSEMTM to SEMCOR JSD scores for all 50 lemmas at once. As in Section 4.1, we calcu- lated the JSD scores for every lemma using each method individually, and compared the difference in values pairwise for statistical significance us- ing a Wilcoxon signed-rank test. The results of this comparison are detailed in <ref type="table">Table 1</ref> (final row: Group = All), which shows that JSD is clearly lower for LEXSEMTM distributions compared to SEMCOR, as would be hoped. This difference is statistically significant at p &lt; 0.05.</p><p>We then performed the same comparison sepa- rately within each SEMCOR frequency group (Ta- ble 1). First of all, we can see that LEXSEMTM sense distributions strongly outperform SEMCOR- based distributions in Group 1 (lemmas missing from SEMCOR). This is as would be expected, since the SEMCOR-based distributions for this group are based on which sense is listed first in WORDNET, which in the absence of SEMCOR counts is arbitrary. On the other hand, in all other groups (lemmas in SEMCOR) the difference be- tween LEXSEMTM and SEMCOR is not statisti-cally significant (p &gt; 0.1 in all cases). This still remains true when we pool together the results from these groups (second last row of <ref type="table">Table 1</ref>: Group = 2-5). While it appears that LEXSEMTM may still be outperforming SEMCOR on average over these groups (lower JSD on average), we do not have enough statistical power to be sure, given the high variance.</p><p>Returning to the initial question regarding a SEMCOR frequency cutoff, the only strong con- clusion we can make is that LEXSEMTM is clearly superior for lemmas missing from SEMCOR. Al- though it appears that LEXSEMTM may outper- form SEMCOR for lemmas with higher SEMCOR frequencies, the variance in our results is too high to be sure of this, let alone define a frequency cutoff. However, given that LEXSEMTM sense distributions never appear to be worse than SEM- COR-based distributions, regardless of SEMCOR frequency -and may actually be marginally su- perior -it seems reasonable to use our sense dis- tributions in general in place of SEMCOR.</p><p>We can contrast this result to the findings of <ref type="bibr" target="#b31">McCarthy et al. (2007)</ref>, who found that the au- tomatic first sense learning method of <ref type="bibr" target="#b30">McCarthy et al. (2004b)</ref> outperformed SEMCOR for words with SEMCOR frequency less than 5. However, their analysis was based on the accuracy of the first sense heuristic, rather than the entire sense distribution, and they used very different datasets to us. 21 Furthermore, their SEMCOR frequency cutoff result was only statistically significant for some variations of their method, and they evalu- ated over more lemmas <ref type="bibr">22</ref> meaning that statistical significance was easier to obtain. Given these rea- sons, their results likely do not contradict ours.</p><p>Given that LEXSEMTM contains sense fre- quencies for 88% of polysemous simplex lemmas in WORDNET, compared to only 39% for SEM- COR, the strong performance of our LEXSEMTM sense distributions for lemmas missing from SEMCOR is extremely significant. Technically these results are only relevant for lemmas where LEXSEMTM was trained on at least 5,000 us-Group Lemma Count SEMCOR Freqs. ages, which reduces the coverage of LEXSEMTM to 24%. However, even then this gives us sense frequencies for 1,602 polysemous lemmas miss- ing from SEMCOR, which accounts for over 5% of polysemous simplex lemmas in WORDNET. Furthermore, based on some additional ongoing analysis comparing LEXSEMTM distributions di- rectly to SEMCOR-based distributions across all of LEXSEMTM (not presented here), it appears the decrease in sense distribution quality for lemmas trained on fewer than 5,000 usages is on average fairly small. This is corroborated by our results in <ref type="figure">Figure 3</ref>: we can observe for the lemmas in the BNC dataset that when the number of usages was reduced to 500, the mean change in JSD for each lemma was almost always less than 0.02 and never greater than 0.04, which is small compared to the difference between LEXSEMTM and SEMCOR in each SEMCOR frequency group. This strongly suggests that our conclusions can be extended to lemmas with low LEXSEMTM frequency, though more work is needed to confirm this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean JSD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Future Work</head><p>The most immediate extension of our work would be to apply our sense learning method to a broader range of data. In particular, we intend to expand LEXSEMTM by applying HCA-WSI across the vocabularies of languages other than English, and also to multiword lemmas. Another obvious ex- tension would be to further explore the alignment component of HCA-WSI. We currently use a sim- ple approach, and we believe this process could be improved, e.g. by using word embeddings.</p><p>In addition, previous work by <ref type="bibr" target="#b27">Lau et al. (2012)</ref> and <ref type="bibr" target="#b28">Lau et al. (2014)</ref> also provided methods for detecting novel and unattested senses, us- ing the topic modelling output from the WSI step of HDP-WSI. These could be applied with LEXSEMTM-which contains this WSI output as well as sense frequencies -to search for novel and unattested senses throughout the entire vocab- ulary of English. This could be used to expand ex- isting sense inventories with new senses, for exam- ple using the methodology of <ref type="bibr" target="#b18">Cook et al. (2013)</ref>. Given that LEXSEMTM also contains WSI output for nonpolysemous WORDNET lemmas (37,566 in total), this could be lead to the discovery of many new polysemous lemmas.</p><p>In conclusion, we have created extensive re- sources for future work in NLP and related dis- ciplines. We have produced LEXSEMTM, which was trained on English Wikipedia and spans ap- proximately 88% of polysemous English lemmas. This dataset contains sense distributions for the majority of polysemous lemmas in WORDNET 3.0. It also contains lemma topic models, for both polysemous and nonpolysemous lemmas, which provide rich semantic information about lemma usage, and can be re-aligned to sense invento- ries to produce new sense distributions at triv- ial cost. In addition, we have produced gold- standard distributions for a subset of the lemmas in LEXSEMTM, which we have used to demonstrate that LEXSEMTM sense distributions are at least on-par with those based on SEMCOR for lemmas with a reasonable frequency in Wikipedia, and strongly superior for lemmas missing from SEM- COR. Finally, we demonstrated that HCA topic modelling is more efficient than HDP, providing guidance for others who wish to do large-scale un- supervised sense distribution learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convergence of log-perplexity of toic model for BNC dataset lemmas, using HCA-WSI. One line per lemma.</figDesc></figure>

			<note place="foot" n="1"> LEXSEMTM, as well as code for accessing LEXSEMTM and reproducing our experiments is available via: https://github.com/awbennett/LexSemTm</note>

			<note place="foot" n="2"> Except where stated otherwise, a lemma usage includes the sentence containing the lemma, and the two immediate neighbouring sentences (if available). It is assumed that each usage has been normalised via lemmatisation and stopword removal, and extra local-context tokens are added, as was</note>

			<note place="foot" n="6"> The British National Corpus (Burnard, 1995), which is a balanced corpus of English. 4 HCA-WSI Sense Learning</note>

			<note place="foot" n="7"> Version 0.61, obtained from: http://www.mloss.org/software/view/527 8 For simplicity we use HCA to refer to both the topic modelling algorithm implemented by Buntine and Mishra (2014) as well as the corresponding software suite, whereas elsewhere HCA often only refers to the software.</note>

			<note place="foot" n="9"> Koeling et al. (2005) also produced domain-specific datasets for the same lemmas, however in order to keep our analysis focussed we only use the domain-neutral BNC dataset. 10 Initial values for concentration and discount parameters for burstiness were set to 100 and 0.5 respectively, and the number of iterations was set to 300. Other hyperparameters were left with default values.</note>

			<note place="foot" n="11"> All benchmarking experiments were run using separate cores on Intel Xeon CPU E5-4650L processors, on a Dell R820 server with 503GiB of main memory.</note>

			<note place="foot" n="12"> Approximately 580 random sense distributions were created per lemma.</note>

			<note place="foot" n="13"> We also ran extensive experiments to test the impact of training single topic models over multiple lemmas, using a wide variety of sampling methods, but found the impact to be neutral at best in terms of both the quality of the learned sense distributions and the overall computational cost. 14 The English Wikipedia dump is dated 2009-11-28.</note>

			<note place="foot" n="15"> These changes had a very minor impact on the HCAWSI evaluation results obtained in Section 4.1, with an average increase in JSD of 0.001 ± 0.004. 16 The number of sense annotations in SEMCOR.</note>

			<note place="foot" n="17"> We chose to restrict our scope in this evaluation to nouns because much of the prior work has also focussed on nouns, and these are the words we would expect others to care the most about disambiguating, since they are more often context bearing. Also, introducing other POS would require a greater quantity of expensive annotated data. 18 These were created manually, to be as clear and unambiguous as possible. 19 Example sentences were removed only if they were for a different lemma within the corresponding synset.</note>

			<note place="foot" n="20"> For lemmas with no SEMCOR annotations, we assign one count to the first-listed sense in WORDNET 3.0.</note>

			<note place="foot" n="21"> Their evaluation on the all words task from SENSEVAL2, which will have more occurrences of the more frequent words, whereas ours is a lexical sample with 100 instances of each word. However, our experiment has a larger dataset (50 × 100 = 5000 instances, as opposed to 786 in total in the SENSEVAL-2 dataset) which makes it more reliable. 22 They evaluated over 63 lemmas with SEMCOR frequency between 1 and 5, whereas we only evaluated over 14 lemmas (Group 2, and part of Group 3).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by a Google Cloud Platform award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Word Sense Disambiguation: Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<pubPlace>Dordrecht, Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised WSD based on automatically retrieved examples: The importance of bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised most frequent sense detection using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Bhingardive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhirendra</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rudramurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanumant</forename><surname>Redkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1238" to="1243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Putting it simply: a context-aware approach to lexical simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="496" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PUTOP: Turning predominant senses into a topic model for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Graber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluation (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="277" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A topic model for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1024" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ensemble methods for unsupervised WSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Experiments with non-parametric topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2014)</title>
		<meeting>the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2014)<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">User reference guide British National Corpus version 1.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Burnard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Oxford University Computing Services</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Creating speech and language data with Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics Human Language Technologies<address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Workshop on Creating Speech and Text Language Data With Amazon&apos;s Mechanical Turk</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence<address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2210" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word sense disambiguation with distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 19th International Joint Conference on Artificial Intelligence<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1010" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating class priors in domain adaptation for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>Australia</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inducing word sense with automatically learned hidden concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaohong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sampling table configurations for the hierarchical Poisson-Dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6912</biblScope>
			<biblScope unit="page" from="296" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Empirical estimates of adaptation: The chance of two noriegas is closer to p/2 than p 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000)</title>
		<meeting>the 18th International Conference on Computational Linguistics (COLING 2000)<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="180" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A lexicographic appraisal of an automatic approach for detecting new word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of eLex 2013</title>
		<meeting>eLex 2013<address><addrLine>Tallinn, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accounting for burstiness in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning whom to trust with MACE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1120" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimating and exploiting the entropy of sense distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Peng Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Koeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT 2009): Short Papers</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT 2009): Short Papers<address><addrLine>Boulder, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain-specific sense distributions and predominant sense acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Koeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP 2005)</title>
		<meeting>the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP 2005)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Verb class disambiguation using informative priors. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="45" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An information retrieval approach to sense ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter</title>
		<meeting><address><addrLine>Rochester, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="348" to="355" />
		</imprint>
	</monogr>
	<note>Proceedings of the Main Conference</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Word sense induction for novel sense detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the EACL (EACL 2012)</title>
		<meeting>the 13th Conference of the EACL (EACL 2012)<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="591" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning word sense distributions, detecting unattested senses and identifying novel senses using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ranking WordNet senses automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Koeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<idno>569</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Department of Informatics, University of Sussex</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finding predominant word senses in untagged text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Koeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised acquisition of predominant word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Koeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="553" to="590" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>George A Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randee</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tengi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ARPA Workshop on Human Language Technology</title>
		<meeting>the ARPA Workshop on Human Language Technology<address><addrLine>Plainsboro, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Applied morphological processing of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Determining word sense dominance using a thesaurus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the EACL (EACL 2006)</title>
		<meeting>the 11th Conference of the EACL (EACL 2006)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Zipf&apos;s word frequency law in natural language: A critical review and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steven T Piantadosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1112" to="1130" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. The Annals of Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Yor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="855" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">VUA-background : When to use background information to perform word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval-2015)</title>
		<meeting>the 9th International Workshop on Semantic Evaluation (SemEval-2015)<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="345" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A probabilistic modeling framework for lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Shnarch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="558" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lights, camera, action: Knowledge extraction from movie scripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Gerard De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Incorporating word correlation knowledge into topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
