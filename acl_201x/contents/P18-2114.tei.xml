<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Split and Rephrase: Better Evaluation and a Stronger Baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Split and Rephrase: Better Evaluation and a Stronger Baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Linguistics (Short Papers), pages 719-724 Melbourne, Australia, July 15 -20, 2018. c 2018 Association for Computational Linguistics 719</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Processing long, complex sentences is challeng- ing. This is true either for humans in various circumstances <ref type="bibr" target="#b4">(Inui et al., 2003;</ref><ref type="bibr" target="#b14">Watanabe et al., 2009;</ref><ref type="bibr" target="#b2">De Belder and Moens, 2010)</ref> or in NLP tasks like parsing <ref type="bibr" target="#b13">(Tomita, 1986;</ref><ref type="bibr" target="#b8">McDonald and Nivre, 2011;</ref><ref type="bibr" target="#b5">Jelínek, 2014</ref>) and machine trans- lation ( <ref type="bibr" target="#b1">Chandrasekar et al., 1996;</ref><ref type="bibr" target="#b11">Pouget-Abadie et al., 2014;</ref><ref type="bibr" target="#b7">Koehn and Knowles, 2017</ref>). An auto- matic system capable of breaking a complex sen- tence into several simple sentences that convey the same meaning is very appealing.</p><p>A recent work by <ref type="bibr" target="#b10">Narayan et al. (2017)</ref> in- troduced a dataset, evaluation method and base- line systems for the task, naming it "Split-and- Rephrase". The dataset includes 1,066,115 in- stances mapping a single complex sentence to a sequence of sentences that express the same mean- ing, together with RDF triples that describe their semantics. They considered two system setups: a text-to-text setup that does not use the accompany- ing RDF information, and a semantics-augmented setup that does. They report a BLEU score of 48.9 for their best text-to-text system, and of 78.7 for the best RDF-aware one. We focus on the text-to- text setup, which we find to be more challenging and more natural.</p><p>We begin with vanilla SEQ2SEQ models with attention ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> and reach an ac- curacy of 77.5 BLEU, substantially outperforming the text-to-text baseline of <ref type="bibr" target="#b10">Narayan et al. (2017)</ref> and approaching their best RDF-aware method. However, manual inspection reveal many cases of unwanted behaviors in the resulting outputs: (1) many resulting sentences are unsupported by the input: they contain correct facts about rele- vant entities, but these facts were not mentioned in the input sentence; (2) some facts are re- peated-the same fact is mentioned in multiple output sentences; and (3) some facts are missing- mentioned in the input but omitted in the output.</p><p>The model learned to memorize entity-fact pairs instead of learning to split and rephrase. Indeed, feeding the model with examples containing enti- ties alone without any facts about them causes it to output perfectly phrased but unsupported facts <ref type="table" target="#tab_2">(Table 3)</ref>. Digging further, we find that 99% of the simple sentences (more than 89% of the unique ones) in the validation and test sets also appear in the training set, which-coupled with the good memorization capabilities of SEQ2SEQ models and the relatively small number of dis- tinct simple sentences-helps to explain the high BLEU score.</p><p>To aid further research on the task, we pro- pose a more challenging split of the data. We also establish a stronger baseline by extending the SEQ2SEQ approach with a copy mechanism, which was shown to be helpful in similar tasks ( <ref type="bibr" target="#b3">Gu et al., 2016;</ref><ref type="bibr" target="#b9">Merity et al., 2017;</ref><ref type="bibr" target="#b12">See et al., 2017)</ref>. On the original split, our models outperform the <ref type="table" target="#tab_1">count  unique  RDF entities  32,186  925  RDF relations  16,093  172  complex sentences  1,066,115 5,544  simple sentences  5,320,716 9,552  train complex sentences 886,857  4,438  train simple sentences  4,451,959 8,840  dev complex sentences  97,950  554  dev simple sentences  475,337  3,765  test complex sentences  81,308  554  test simple sentences</ref> 393,420 4,015 % dev simple in train 99.69% 90.9% % test simple in train 99.09% 89.8% % dev vocab in train 97.24% % test vocab in train 96.35% <ref type="table">Table 1</ref>: Statistics for the WEBSPLIT dataset.</p><p>best baseline of <ref type="bibr" target="#b10">Narayan et al. (2017)</ref> by up to 8.68 BLEU, without using the RDF triples. On the new split, the vanilla SEQ2SEQ models break com- pletely, while the copy-augmented models per- form better. In parallel to our work, an updated version of the dataset was released (v1.0), which is larger and features a train/test split protocol which is similar to our proposal. We report results on this dataset as well. The code and data to reproduce our results are available on Github. <ref type="bibr">1</ref> We encour- age future work on the split-and-rephrase task to use our new data split or the v1.0 split instead of the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary Experiments</head><p>Task Definition In the split-and-rephrase task we are given a complex sentence C, and need to produce a sequence of simple sentences T 1 , ..., T n , n ≥ 2, such that the output sentences convey all and only the information in C. As additional su- pervision, the split-and-rephrase dataset associates each sentence with a set of RDF triples that de- scribe the information in the sentence. Note that the number of simple sentences to generate is not given as part of the input.</p><p>Experimental Details We focus on the task of splitting a complex sentence into several simple ones without access to the corresponding RDF triples in either train or test time. For evaluation we follow <ref type="bibr" target="#b10">Narayan et al. (2017)</ref> and compute the averaged individual multi-reference BLEU score for each prediction. <ref type="bibr">2</ref> We split each prediction to 1 https://github.com/biu-nlp/ sprp-acl2018 2 Note that this differs from "normal" multi-reference BLEU (as implemented in multi-bleu.pl) since the number of references differs among the instances in the test-  Our models only differ in the LSTM cell size <ref type="bibr">(128, 256 and 512, respectively)</ref>. See the supplemen- tary material for training details and hyperparame- ters. We compare our models to the baselines pro- posed in <ref type="bibr" target="#b10">Narayan et al. (2017)</ref>. HYBRIDSIMPL and SEQ2SEQ are text-to-text models, while the other reported baselines additionally use the RDF information.</p><formula xml:id="formula_0">Model BLEU #S/C #T/S</formula><p>Results As shown in <ref type="table" target="#tab_1">Table 2</ref>, our 3 models ob- tain higher BLEU scores then the SEQ2SEQ base- line, with up to 28.35 BLEU improvement, de- spite being single-layer models vs. the 3-layer models used in <ref type="bibr" target="#b10">Narayan et al. (2017)</ref>. A possible explanation for this discrepancy is the SEQ2SEQ baseline using a dropout rate of 0.8, while we use 0.3 and only apply it on the LSTM out- puts. Our results are also better than the MUL- TISEQ2SEQ and SPLIT-MULTISEQ2SEQ mod- els, which use explicit RDF information. We also present the macro-average 5 number of sim- set. <ref type="bibr">3</ref> Using NLTK v3.2.5 https://www.nltk.org/ 4 https://github.com/OpenNMT/OpenNMT-py commit d4ab35a <ref type="bibr">5</ref> Since the number of references varies greatly from one complex sentence to another, (min: 1, max: 76,283, median: 16) we avoid bias towards the complex sentences with many references by performing macro average, i.e. we first average the number of simple sentences in each reference among the Input Prediction A Fortress of Grey Ice with ISBM 0-7653- 0633-6 has 672 pages .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.V. Jones authored A Fortress of Grey Ice . A Fortress of Grey Ice has 672 pages . The address , 11 Diagonal Street is located in South Africa where the leader is Cyril</head><p>Ramaphosa and some Asian South Africans live .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The address , 11 Diagonal Street is located in South Africa . The leader of South Africa is called Cyril Ramaphosa . The leader of South Africa is called Cyril Ramaphosa . The leader of South Africa is called Cyril Ramaphosa . Alan Shepard Alan Shepard Alan Shepard</head><p>Alan Shepard is dead . Alan Shepard was a test pilot . AFC Ajax AFC Ajax AFC Ajax AFC Ajax 's manager is Jong Ajax . AFC Ajax N.V. own Sportpark De Toekomst .  Analysis We begin analyzing the results by manually inspecting the model's predictions on the validation set. This reveals three common kinds of mistakes as demonstrated in <ref type="table" target="#tab_2">Table 3</ref>: un- supported facts, repetitions, and missing facts. All the unsupported facts seem to be related to enti- ties mentioned in the source sentence. Inspecting the attention weights <ref type="figure" target="#fig_0">(Figure 1</ref>) reveals a worry- ing trend: throughout the prediction, the model focuses heavily on the first word in of the first en- tity ("A wizard of Mars") while paying little atten- tion to other cues like "hardcover", "Diane" and references of a specific complex sentence, and then average these numbers.</p><p>"the ISBN number". This explains the abundance of "hallucinated" unsupported facts: rather than learning to split and rephrase, the model learned to identify entities, and spit out a list of facts it had memorized about them. To validate this assump- tion, we count the number of predicted sentences which appeared as-is in the training data. We find that 1645 out of the 1693 (97.16%) predicted sen- tences appear verbatim in the training set. <ref type="table">Table  1</ref> gives more detailed statistics on the WEBSPLIT dataset.</p><p>To further illustrate the model's recognize-and- spit strategy, we compose inputs containing an entity string which is duplicated three times, as shown in the bottom two rows of <ref type="table" target="#tab_2">Table 3</ref>. As expected, the model predicted perfectly phrased and correct facts about the given entities, although these facts are clearly not supported by the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">New Data-split</head><p>The original data-split is not suitable for measur- ing generalization, as it is susceptible to "cheat- ing" by fact memorization. We construct a new train-development-test split to better reflect our expected behavior from a split-and-rephrase model. We split the data into train, development and test sets by randomly dividing the 5,554 dis- tinct complex sentences across the sets, while us- ing the provided RDF information to ensure that:</p><p>1. Every possible RDF relation (e.g., BORNIN, LOCATEDIN) is represented in the training set (and may appear also in the other sets).</p><p>2. Every RDF triplet (a complete fact) is repre- sented only in one of the splits.</p><p>While the set of complex sentences is still di- vided roughly to 80%/10%/10% as in the original split, now there are nearly no simple sentences in <ref type="table" target="#tab_1">count  unique  train complex sentences 1,039,392 4,506  train simple sentences  5,239,279 7,865  dev complex sentences  13,294  535  dev simple sentences  39,703  812  test complex sentences  13,429  503  test simple sentences  41,734  879  # dev simple in train</ref> 35 (0.09%) # test simple in train 1 (0%) % dev vocab in train 62.99% % test vocab in train 61.67% dev entities in train 26/111 (23.42%) test entities in train 25/120 (20.83%) dev relations in train 34/34 (100%) test relations in train 37/37 (100%) <ref type="table">Table 4</ref>: Statistics for the RDF-based data split the development and test sets that appear verba- tim in the train-set. Yet, every relation appear- ing in the development and test sets is supported by examples in the train set. We believe this split strikes a good balance between challenge and fea- sibility: to succeed, a model needs to learn to iden- tify relations in the complex sentence, link them to their arguments, and produce a rephrasing of them. However, it is not required to generalize to unseen relations. <ref type="bibr">6</ref> The data split and scripts for creating it are available on Github. <ref type="bibr">7</ref> Statistics describing the data split are detailed in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Copy-augmented Model</head><p>To better suit the split-and-rephrase task, we aug- ment the SEQ2SEQ models with a copy mecha- nism. Such mechanisms have proven to be benefi- cial in similar tasks like abstractive summarization ( <ref type="bibr" target="#b3">Gu et al., 2016;</ref><ref type="bibr" target="#b12">See et al., 2017</ref>) and language modeling ( <ref type="bibr" target="#b9">Merity et al., 2017)</ref>. We hypothesize that biasing the model towards copying will im- prove performance, as many of the words in the simple sentences (mostly corresponding to enti- ties) appear in the complex sentence, as evident by the relatively high BLEU scores for the SOURCE baseline in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Copying is modeled using a "copy switch" probability p(z) computed by a sigmoid over a learned composition of the decoder state, the con- text vector and the last output embedding. It in- terpolates the p sof tmax distribution over the target vocabulary and a copy distribution p copy over the source sentence tokens. p copy is simply the com- puted attention weights. Once the above distribu- <ref type="bibr">6</ref> The updated dataset (v1.0, published by Narayan et al. after this work was accepted) follows (2) above, but not (1).</p><p>7 https://github.com/biu-nlp/ sprp-acl2018  <ref type="table">Table 5</ref>: Results over the test sets of the original, our proposed split and the v1.0 split tions are computed, the final probability for an out- put word w is:</p><formula xml:id="formula_1">BLEU #S/C #T/S original</formula><formula xml:id="formula_2">p(w) = p(z = 1)p copy (w) + p(z = 0)p sof tmax (w)</formula><p>In case w is not present in the output vocabulary, we set p sof tmax (w) = 0. We refer the reader to <ref type="bibr" target="#b12">See et al. (2017)</ref> for a detailed discussion regard- ing the copy mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>Models with larger capacities may have greater representation power, but also a stronger tendency to memorize the training data. We therefore per- form experiments with copy-enhanced models of varying LSTM widths (128, 256 and 512). We train the models using the negative log likelihood of p(w) as the objective. Other than the copy mechanism, we keep the settings identical to those in Section 2. We train models on the original split, our proposed data split and the v1.0 split.</p><p>Results <ref type="table">Table 5</ref> presents the results. On the orig- inal data-split, the COPY512 model outperforms all baselines, improving over the previous best by 8.68 BLEU points. On the new data-split, as ex- pected, the performance degrades for all models, as they are required to generalize to sentences not seen during training. The copy-augmented mod- els perform better than the baselines in this case as well, with a larger relative gap which can be ex- plained by the lower lexical overlap between the train and the test sets in the new split. On the v1.0 split the results are similar to those on our split, Input Prediction Madeleine L'Engle who is influenced by George Macdonald wrote " A Severed Wasp " .</p><p>A Severed Wasp was written by George Macdonald . A Severed Wasp is from the United States . The A.C. Lumezzane has 4150 members and play in the Lega Pro League .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.C. Lumezzane 's ground is in the Lega Pro League . A.C. Lumezzane has 4150 members . Irish English is the official language of Ireland , which is lead by Enda Kenny and home to Adare</head><p>Manor .</p><p>Adare Manor is located in Ireland . English is the language of Ireland . English is the language of Ireland . (repeated x3) <ref type="table">Table 6</ref>: Predictions from the COPY512 model, trained on the new data split.</p><p>in spite of it being larger <ref type="bibr">(1,331,515 vs. 886,857 examples)</ref>, indicating that merely adding data will not solve the task.</p><p>Analysis We inspect the models' predictions for the first 20 complex sentences of the original and new validation sets in <ref type="table" target="#tab_5">Table 7</ref>. We mark each sim- ple sentence as being "correct" if it contains all and only relevant information, "unsupported" if it contains facts not present in the source, and "re- peated" if it repeats information from a previous sentence. We also count missing facts. <ref type="figure" target="#fig_1">Figure  2</ref> shows the attention weights of the COPY512 model for the same sentence in <ref type="figure" target="#fig_0">Figure 1</ref>. Reassur- ingly, the attention is now distributed more evenly over the input symbols. On the new splits, all models perform catastrophically. <ref type="table">Table 6</ref> shows outputs from the COPY512 model when trained on the new split. On the original split, while SEQ2SEQ128 mainly suffers from missing infor- mation, perhaps due to insufficient memorization capacity, SEQ2SEQ512 generated the most unsup- ported sentences, due to overfitting or memoriza- tion. The overall number of issues is clearly re- duced in the copy-augmented models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We demonstrated that a SEQ2SEQ model can ob- tain high scores on the original split-and-rephrase task while not actually learning to split-and- rephrase. We propose a new and more challenging data-split to remedy this, and demonstrate that the cheating SEQ2SEQ models fail miserably on the new split. Augmenting the SEQ2SEQ models with a copy-mechanism improves performance on both data splits, establishing a new competitive base- line for the task. Yet, the split-and-rephrase task (on the new split) is still far from being solved. We strongly encourage future research to evaluate on our proposed split or on the recently released version 1.0 of the dataset, which is larger and also addresses the overlap issues mentioned here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SEQ2SEQ512's attention weights. Horizontal: input. Vertical: predictions.</figDesc><graphic url="image-1.png" coords="3,89.18,322.84,183.90,205.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention weights from the COPY512 model for the same input as in Figure 1.</figDesc><graphic url="image-2.png" coords="5,90.07,522.27,184.85,205.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores, simple sentences per 
complex sentence (#S/C) and tokens per simple 
sentence (#T/S), as computed over the test set. 
SOURCE are the complex sentences and REFER-
ENCE are the reference rephrasings from the test 
set. Models marked with * use the semantic RDF 
triples. 

sentences 3 and report the average number of sim-
ple sentences in each prediction, and the average 
number of tokens for each simple sentence. We 
train vanilla sequence-to-sequence models with at-
tention (Bahdanau et al., 2015) as implemented 
in the OPENNMT-PY toolkit (Klein et al., 2017). 4 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Predictions from a vanilla SEQ2SEQ model, illustrating unsupported facts, missing facts and 
repeated facts. The last two rows show inputs we composed to demonstrate that the models memorize 
entity-fact pairs. 

ple sentences per complex sentence in the ref-
erence rephrasings (REFERENCE) showing that 
the SPLIT-MULTISEQ2SEQ and SPLIT-SEQ2SEQ 
baselines may suffer from over-splitting since the 
reference splits include 2.52 simple sentences on 
average, while the mentioned models produced 
2.84 sentences. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Results of the manual analysis, showing 
the number of simple sentences with unsupported 
facts (unsup.), repeated facts, missing facts and 
correct facts, for 20 complex sentences from the 
original and new validation sets. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Shashi Narayan and Jan Botha for their useful comments. The work was supported by the Intel Collaborative Research Institute for Compu-tational Intelligence (ICRI-CI), the Israeli Science Foundation (grant number 1555/15), and the Ger-man Research Foundation via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Motivations and methods for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Text simplification for children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">De</forename><surname>Belder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR workshop on accessible search systems</title>
		<meeting>the SIGIR workshop on accessible search systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1154" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text simplification for reading assistance: a project note</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuro</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Iwakura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international workshop on Paraphrasing</title>
		<meeting>the second international workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improvements to dependency parsing using automatic simplification of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Jelínek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-4012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations. Association for Computational Linguistics</title>
		<meeting>ACL 2017, System Demonstrations. Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W17-3204" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics</title>
		<meeting>the First Workshop on Neural Machine Translation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyzing and integrating dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Split and rephrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D17-1064" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overcoming the curse of sentence length for neural machine translation using automatic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-4009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. Association for Computational Linguistics</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P17-1099" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient parsing for natural languagea fast algorithm for practical systems. int. series in engineering and computer science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Tomita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vinícius Rodriguez Uzêda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria Aluísio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willian</forename><surname>Massami Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaldo Candido</forename><surname>Junior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on Design of communication</title>
		<meeting>the 27th ACM international conference on Design of communication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Facilita: reading assistance for low-literacy readers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
