<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Do</roleName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Choe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Kertz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="512" to="516"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatically detecting verbal irony (roughly, sarcasm) is a challenging task because ironists say something other than-and often opposite to-what they actually mean. Discerning ironic intent exclusively from the words and syntax comprising texts (e.g., tweets, forum posts) is therefore not always possible: additional contextual information about the speaker and/or the topic at hand is often necessary. We introduce a new corpus that provides empirical evidence for this claim. We show that annota-tors frequently require context to make judgements concerning ironic intent, and that machine learning approaches tend to misclassify those same comments for which annotators required additional context.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction &amp; Motivation</head><p>This work concerns the task of detecting verbal irony online. Our principal argument is that sim- ple bag-of-words based text classification models -which, when coupled with sufficient data, have proven to be extremely successful for many natu- ral language processing tasks ( <ref type="bibr">Halevy et al., 2009)</ref> -are inadequate for irony detection. In this paper we provide empirical evidence that context is often necessary to recognize ironic intent. This is consistent with the large body of prag- matics/linguistics literature on irony and its us- age, which has emphasized the role that context plays in recognizing and decoding ironic utter- ances <ref type="bibr">(Grice, 1975;</ref><ref type="bibr">Clark and Gerrig, 1984;</ref><ref type="bibr">Sperber and Wilson, 1981)</ref>. But existing work on au- tomatic irony detection -reviewed in Section 2 -has not explicitly attempted to operationalize such theories, and has instead relied on features (mostly word counts) intrinsic to the texts that are to be classified as ironic. These approaches have achieved some success, but necessarily face an upper-bound: the exact same sentence can be both intended ironically and unironically, depending on the context (including the speaker and the topic at hand). Only obvious verbal ironies will be recog- nizable from intrinsic features alone.</p><p>Here we provide empirical evidence for the above claims. We also introduce a new annotated corpus that will allow researchers to build models that augment existing approaches to irony detec- tion with contextual information regarding the text (utterance) to be classified and its author. Briefly, our contributions are summarized as follows.</p><p>• We introduce the first version of the reddit irony corpus, composed of annotated com- ments from the social news website reddit. Each sentence in every comment in this cor- pus has been labeled by three independent an- notators as having been intended by the au- thor ironically or not. This dataset is publicly available. 1</p><p>• We provide empirical evidence that human annotators consistently rely on contextual in- formation to make ironic/unironic sentence judgements.</p><p>• We show that the standard 'bag-of-words' ap- proach to text classification fails to accurately judge ironic intent on those cases for which humans required additional context. This suggests that, as humans require context to make their judgements for this task, so too do computers.</p><p>Our hope is that these observations and this dataset will spur innovative new research on meth- ods for verbal irony detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>There has recently been a flurry of interesting work on automatic irony detection <ref type="bibr">(Tepperman et al., 2006;</ref><ref type="bibr">Davidov et al., 2010;</ref><ref type="bibr">Carvalho et al., 2009;</ref><ref type="bibr">Burfoot and Baldwin, 2009;</ref><ref type="bibr">Tsur et al., 2010;</ref><ref type="bibr">González-Ibáñez et al., 2011;</ref><ref type="bibr">Filatova, 2012;</ref><ref type="bibr">Reyes et al., 2012;</ref><ref type="bibr">Lukin and Walker, 2013;</ref><ref type="bibr">Riloff et al., 2013)</ref>. In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innova- tive approaches specific to detecting irony.</p><p>The most common data source used to experi- ment with irony detection systems has been Twit- ter ( <ref type="bibr">Reyes et al., 2012;</ref><ref type="bibr">González-Ibáñez et al., 2011;</ref><ref type="bibr">Davidov et al., 2010)</ref>, though Amazon prod- uct reviews have been used experimentally as well ( <ref type="bibr">Tsur et al., 2010;</ref><ref type="bibr">Davidov et al., 2010;</ref><ref type="bibr">Reyes et al., 2012;</ref><ref type="bibr">Filatova, 2012)</ref>. <ref type="bibr">Walker et al. (2012)</ref> also recently introduced the Internet Argument Corpus (IAC), which includes a sarcasm label (among others).</p><p>Some of the findings from these previous ef- forts have squared with intuition: e.g., overzealous punctuation (as in "great idea!!!!") is indicative of ironic intent <ref type="bibr">(Carvalho et al., 2009</ref>). Other works have proposed novel approaches specifically for irony detection: <ref type="bibr">Davidov et al. (2010)</ref>, for ex- ample, proposed a semi-supervised approach in which they look for sentence templates indicative of irony. Elsewhere, <ref type="bibr">Riloff et al. (2013)</ref> proposed a method that exploits contrasting sentiment in the same utterance to detect irony.</p><p>To our knowledge, however, no previous work on irony detection has attempted to leverage contextual information regarding the author or speaker (external to the utterance). But this is nec- essary in some cases, however. For example, in the case of Amazon product reviews, knowing the kinds of books that an individual typically likes might inform our judgement: someone who tends to read and review Dostoevsky is probably be- ing ironic if she writes a glowing review of Twi- light. Of course, many people genuinely do enjoy Twilight and so if the review is written subtly it will likely be difficult to discern the author's in- tent without this background. In the case of Twit- ter, it is likely to be difficult to classify utterances without considering the contextualizing exchange of tweets (i.e., the conversation) to which they be- long. Figure 1: The web-based tool used by our annotators to la- bel reddit comments. Enumerated interface elements are de- scribed as follows: 1 the text of the comment to be anno- tated -sentences marked as ironic are highlighted; 2 buttons to label sentences as ironic or unironic; 3 buttons to request additional context (the embedding discussion thread or asso- ciated webpage -see Section 3.2); 4 radio button to provide confidence in comment labels (low, medium or high).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Introducing the reddit Irony Dataset</head><p>Here we introduce the first version (β 1.0) of our irony corpus. Reddit (http://reddit. com) is a social-news website to which news stories (and other links) are posted, voted on and commented upon.</p><p>The forum compo- nent of reddit is extremely active: popular posts often have well into 1000's of user com- ments. Reddit comprises 'sub-reddits', which fo- cus on specific topics. For example, http:// reddit.com/r/politics features articles (and hence comments) centered around political news. The current version of the corpus is avail- able at: https://github.com/bwallace/ ACL-2014-irony. Data collection and annota- tion is ongoing, so we will continue to release new (larger) versions of the corpus in the future. The present version comprises 3,020 annotated com- ments scraped from the six subreddits enumerated in <ref type="table">Table 1</ref>. These comments in turn comprise a total of 10,401 labeled sentences. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation Process</head><p>Three university undergraduates independently annotated each sentence in the corpus. More specifically, annotators have provided binary 'la- bels' for each sentence indicating whether or not they (the annotator) believe it was intended by the author ironically (or not). This annotation was provided via a custom-built browser-based anno- tation tool, shown in <ref type="figure">Figure 1</ref>.</p><p>We intentionally did not provide much guid- ance to annotators regarding the criteria for what sub-reddit (URL) description number of labeled comments politics (r/politics) Political news and editorials; focus on the US. 873 conservative (r/conservative) A community for political conservatives. 573 progressive (r/progressive) A community for political progressives (liberals). 543 atheism (r/atheism) A community for non-believers. 442 Christianity (r/Christianity) News and viewpoints on the Christian faith. 312 technology (r/technology) Technology news and commentary. 277 <ref type="table">Table 1</ref>: The six sub-reddits that we have downloaded comments from and the corresponding number of comments for which we have acquired annotations in this β version of the corpus. Note that we acquired labels at the sentence level, whereas the counts above reflect comments, all of which contain at least one sentence.</p><p>constitutes an 'ironic' statement, for two reasons. First, verbal irony is a notoriously slippery concept ( <ref type="bibr">Gibbs and Colston, 2007)</ref> and coming up with an operational definition to be consistently applied is non-trivial. Second, we were interested in assess- ing the extent of natural agreement between an- notators for this task. The raw average agreement between all annotators on all sentences is 0.844. Average pairwise Cohen's Kappa <ref type="bibr">(Cohen, 1960</ref>) is 0.341, suggesting fair to moderate agreement ( <ref type="bibr">Viera and Garrett, 2005</ref>), as we might expect for a subjective task like this one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context</head><p>Reddit is a good corpus for the irony detection task in part because it provides a natural prac- tical realization of the otherwise ill-defined con- text for comments. In particular, each comment is associated with a specific user (the author), and we can view their previous comments. More- over, comments are embedded within discussion threads that pertain to the (usually external) con- tent linked to in the corresponding submission (see <ref type="figure" target="#fig_1">Figure 2</ref>). These pieces of information (previous comments by the same user, the external link of the embedding reddit thread, and the other com- ments in this thread) constitute our context. All of this is readily accessible. Labelers can opt to request these pieces of context via the annotation tool, and we record when they do so. Consider the following example comment taken from our dataset: "Great idea on the talkathon Cruz. Really made the republicans look like the sane ones." Did the author intend this statement ironically, or was this a subtle dig on Senator Ted Cruz? Without additional context it is diffi- cult to know. And indeed, all three annotators re- quested additional context for this comment. This context at first suggests that the comment may have been intended literally: it was posted in the r/conservative subreddit (Ted Cruz is a conserva- tive senator). But if we peruse the author's com- ment history, we see that he or she repeatedly de- rides Senator Cruz (e.g., writing "Ted Cruz is no Ronald Reagan. They aren't even close."). From this contextual information, then, we can reason- ably assume that the comment was intended iron- ically (and all three annotators did so after assess- ing the available contextual information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Humans Need Context to Infer Irony</head><p>We explore the extent to which human annotators rely on contextual information to decide whether or not sentences were intended ironically. Recall that our annotation tool allows labelers to request additional context if they cannot make a decision based on the comment text alone <ref type="figure">(Figure 1</ref>). On average, annotators requested additional context for 30% of comments (range across annotators of 12% to 56%). As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, annotators are consistently more confident once they have consulted this information.</p><p>We tested for a correlation between these re- quests for context and the final decisions regard- ing whether comments contain at least one ironic sentence. We denote the probability of at least one annotator requesting additional context for com- ment i by P (C i ). We then model the probability of this event as a linear function of whether or not 514 For all comments for which these annotators requested context, we show forced (before viewing the requested contextual content) and final (after) decisions regarding perceived ironic intent on behalf of the author. Each row shows one of four possible decision sequences (e.g., a judgement of ironic prior to seeing context and unironic after). Numbers correspond to counts of these sequences for each annotator (e.g., the first annotator changed their mind from ironic to unironic 86 times).</p><p>Cases that involve the annotator changing his or her mind are shown in red; those in which the annotator stuck with their initial judgement are shown in blue. Color intensity is proportional to the average confidence judgements the annotator provided: these are uniformly stronger after they have consulted contextualizing information. Note also that the context frequently results in annotators changing their judgement.</p><p>any annotator labeled any sentence in comment i as ironic. We code this via the indicator variable I i which is 1 when comment i has been deemed to contain an ironic sentence (by any of the three annotators) and 0 otherwise.</p><formula xml:id="formula_0">logit{P (C i )} = β 0 + β 1 I i (1)</formula><p>We used the regression model shown in Equa- tion 1, where β 0 is an intercept and β 1 captures the correlation between requests for context for a given comment and its ultimately being deemed to contain at least one ironic sentence. We fit this model to the annotated corpus, and found a signif- icant correlation: ˆ β 1 = 1.508 with a 95% confi- dence interval of (1.326, 1.690); p &lt; 0.001.</p><p>In other words, annotators request context sig- nificantly more frequently for those comments that (are ultimately deemed to) contain an ironic sentence. This would suggest that the words and punctuation comprising online comments alone are not sufficient to distinguish ironic from unironic comments. Despite this, most machine learning based approaches to irony detection have relied nearly exclusively on such intrinsic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Machines Probably do, too</head><p>We show that the misclassifications (with respect to whether comments contain irony or not) made by a standard text classification model signifi- cantly correlate with those comments for which human annotators requested additional context. This provides evidence that bag-of-words ap- proaches are insufficient for the general task of irony detection: more context is necessary.</p><p>We implemented a baseline classification ap- proach using vanilla token count features (binary bag-of-words). We removed stop-words and lim- ited the vocabulary to the 50,000 most frequently occurring unigrams and bigrams. We added ad- ditional binary features coding for the presence of punctuational features, such as exclamation points, emoticons (for example, ';)') and question marks: previous work ( <ref type="bibr">Davidov et al., 2010;</ref><ref type="bibr">Carvalho et al., 2009</ref>) has found that these are good indicators of ironic intent.</p><p>For our predictive model, we used a linear- kernel SVM (tuning the C parameter via grid- search over the training dataset to maximize F1 score). We performed five-fold cross-validation, recording the predictionsˆypredictionsˆ predictionsˆy i for each (held-out) comment i. Average F1 score over the five-folds was 0.383 with range (0.330, 0.412); mean recall was 0.496 (0.446, 0.548) and average precision was 0.315 (0.261, 0.380). The five most predictive tokens were: !, yeah, guys, oh and shocked. This represents reasonable performance (with intuitive predictive tokens); but obviously there is quite a bit of room for improvement. <ref type="bibr">3</ref> We now explore empirically whether these mis- classifications are made on the same comments for which annotators requested context. To this end, we introduce a variable M i for each comment i such that M i = 1 ifˆyifˆ ifˆy i = y i , i.e., M i is an in-dicator variable that encodes whether or not the classifier misclassified comment i. We then ran a second regression in which the output variable was the logit-transformed probability of the model misclassifying comment i, i.e., P (M i ). Here we are interested in the correlation of the event that one or more annotators requested additional con- text for comment i (denoted by C i ) and model mis- classifications (adjusting for the comment's true label). Formally:</p><formula xml:id="formula_1">logit{P (M i )} = θ 0 + θ 1 I i + θ 2 C i (2)</formula><p>Fitting this to the data, we estimatedˆθestimatedˆ estimatedˆθ 2 = 0.971 with a 95% CI of (0.810, 1.133); p &lt; 0.001. Put another way, the model makes mistakes on those comments for which annotators requested addi- tional context (even after accounting for the an- notator designation of comments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Directions</head><p>We have described a new (publicly available) cor- pus for the task of verbal irony detection. The data comprises comments scraped from the so- cial news website reddit. We recorded confidence judgements and requests for contextualizing infor- mation for each comment during annotation. We analyzed this corpus to provide empirical evidence that annotators quite often require context beyond the comment under consideration to discern irony; especially for those comments ultimately deemed as being intended ironically. We demonstrated that a standard token-based machine learning ap- proach misclassified many of the same comments for which annotators tend to request context. We have shown that annotators rely on contex- tual cues (in addition to word and grammatical fea- tures) to discern irony and argued that this implies computers should, too. The obvious next step is to develop new machine learning models that exploit the contextual information available in the corpus we have curated (e.g., previous comments by the same user, the thread topic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>This work was made possible by the Army Re- search Office (ARO), grant #64481-MA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustrative reddit comment (highlighted). The title ("Virginia Republican ...") links to an article, providing one example of contextualizing content. The conversational thread in which this comment is embedded provides additional context. The comment in question was presumably intended ironically, though without the aforementioned context this would be difficult to conclude with any certainty.</figDesc><graphic url="image-6.png" coords="3,310.66,196.51,209.92,103.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: This plot illustrates the effect of viewing contextual information for three annotators (one table for each annotator). For all comments for which these annotators requested context, we show forced (before viewing the requested contextual content) and final (after) decisions regarding perceived ironic intent on behalf of the author. Each row shows one of four possible decision sequences (e.g., a judgement of ironic prior to seeing context and unironic after). Numbers correspond to counts of these sequences for each annotator (e.g., the first annotator changed their mind from ironic to unironic 86 times). Cases that involve the annotator changing his or her mind are shown in red; those in which the annotator stuck with their initial judgement are shown in blue. Color intensity is proportional to the average confidence judgements the annotator provided: these are uniformly stronger after they have consulted contextualizing information. Note also that the context frequently results in annotators changing their judgement.</figDesc></figure>

			<note place="foot" n="1"> https://github.com/bwallace/ ACL-2014-irony</note>

			<note place="foot" n="2"> We performed na¨ıvena¨ıve &apos;segmentation&apos; of comments based on punctuation.</note>

			<note place="foot" n="3"> Some of the recently proposed strategies mentioned in Section 2 may improve performance here, but none of these address the fundamental issue of context.</note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
