<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linguistic Structured Sparsity in Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
							<email>dyogatama@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Linguistic Structured Sparsity in Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="786" to="796"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. These regularizers impose linguistic bias in feature weights, enabling us to incorporate prior knowledge into conventional bag-of-words models. We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis , and forecasting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>What is the best way to exploit linguistic infor- mation in statistical text processing models? For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open ques- tion, as cheap "bag-of-words" models often per- form well. Much recent work in NLP has fo- cused on linguistic feature engineering ( <ref type="bibr" target="#b12">Joshi et al., 2010</ref>) or representation learning <ref type="bibr" target="#b7">(Glorot et al., 2011;</ref><ref type="bibr" target="#b25">Socher et al., 2013)</ref>.</p><p>In this paper, we propose a radical alternative. We embrace the conventional bag-of-words repre- sentation of text, instead bringing linguistic bias to bear on regularization. Since the seminal work of <ref type="bibr" target="#b2">Chen and Rosenfeld (2000)</ref>, the importance of regularization in discriminative models of text- including language modeling, structured predic- tion, and classification-has been widely recog- nized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model).</p><p>Recently, structured (or composite) regulariza- tion has been introduced; simply put, it reasons about different weights jointly. The most widely explored variant, group lasso ( <ref type="bibr" target="#b34">Yuan and Lin, 2006</ref>) seeks to avoid large 2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology <ref type="bibr" target="#b13">(Kim and Xing, 2008)</ref>, signal processing ( <ref type="bibr" target="#b16">Lv et al., 2011</ref>), and NLP <ref type="bibr" target="#b4">(Eisenstein et al., 2011;</ref><ref type="bibr" target="#b17">Martins et al., 2011;</ref><ref type="bibr" target="#b18">Nelakanti et al., 2013</ref>). For text categorization problems, <ref type="bibr" target="#b32">Yogatama and Smith (2014)</ref> proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information.</p><p>In this paper, we show how linguistic informa- tion of various kinds-parse trees, thematic topics, and hierarchical word clusterings-can be used to construct group lasso variants that impose linguis- tic bias without introducing any new features. Our experiments demonstrate that structured regulariz- ers can squeeze higher performance out of conven- tional bag-of-words models on seven out of eight of text categorization tasks tested, in six cases with more compact models than the best-performing unstructured-regularized model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation</head><p>We represent each document as a feature vector x ∈ R V , where V is the vocabulary size. x v is the frequency of the vth word (i.e., this is a "bag of words" model).</p><p>Consider a linear model that predicts a binary response y ∈ {−1, +1} given x and weight vector w ∈ R V . We denote our training data of D doc- uments in the corpus by</p><formula xml:id="formula_0">{x d , y d } D d=1</formula><p>. The goal of the learning procedure is to estimate w by mini- mizing the regularized training data loss:</p><formula xml:id="formula_1">ˆ w = arg min w Ω(w) + D d=1 L(x d , w, y d ),</formula><p>where L(x, w, y) is the loss function for docu- ment d and Ω(w) is the regularizer.</p><p>In this work, we use the log loss:</p><formula xml:id="formula_2">L(x d , w, y d ) = − log(1 + exp(−y d w x d )),</formula><p>Other loss functions (e.g., hinge loss, squared loss) can also be used with any of the regularizers dis- cussed in this paper. Our focus is on the regularizer, Ω(w). For high dimensional data such as text, regularization is crucial to avoid overfitting. <ref type="bibr">1</ref> The usual starting points for regularization are the "lasso" <ref type="bibr" target="#b29">(Tibshirani, 1996)</ref> and the "ridge" <ref type="bibr" target="#b9">(Hoerl and Kennard, 1970)</ref>, based respectively on the 1 and squared 2 norms:</p><formula xml:id="formula_3">Ω las (w) = λ las w 1 = λ j |w j | Ω rid (w) = λ rid w 2 2 = λ j w 2 j</formula><p>Both methods disprefer weights of large magni- tude; smaller (relative) magnitude means a feature (here, a word) has a smaller effect on the predic- tion, and zero means a feature has no effect. <ref type="bibr">2</ref> The hyperparameter λ in each case is typically tuned on a development dataset. A linear combination of ridge and lasso is known as the elastic net ( <ref type="bibr" target="#b36">Zou and Hastie, 2005</ref>). The lasso, ridge, and elastic net are three strong baselines in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Group Lasso</head><p>Structured regularizers penalize estimates of w in which collections of weights are penalized jointly. For example, in the group lasso <ref type="bibr" target="#b34">(Yuan and Lin, 2006</ref>), predefined groups of weights (subvectors of w) are encouraged to either go to zero (as a group) or not (as a group)-this is known as "group sparsity." <ref type="bibr">3</ref> The variant of group lasso we explore here uses an 1,2 norm. Let g index the G predefined groups of weights and w g denote the subvector of w con- taining weights for group g:</p><formula xml:id="formula_4">Ω glas (w) =λ glas G g=1 λ g w g 2 , 1</formula><p>A Bayesian interpretation of regularization is as a prior on the weight vector w; in many cases Ω can be under- stood as a log-prior representing beliefs about the model held before exposure to data. For lasso regression, the prior is a zero-mean Laplace distribution, whereas for ridge regres- sion the prior is a zero-mean Gaussian distribution. For non- overlapping group lasso, the prior is a two-level hierarchical Bayes model <ref type="bibr" target="#b5">(Figueiredo, 2002</ref>). The Bayesian interpretation of overlapping group lasso is not yet well understood. <ref type="bibr">2</ref> The lasso leads to strongly sparse solutions, in which many elements of the estimated w are actually zero. This is an attractive property for efficiency and (perhaps) inter- pretability. The ridge encourages weights to go toward zero, but usually not all the way to zero; for this reason its solutions are known as "weakly" sparse. <ref type="bibr">3</ref> Other structured regularizers include the fused lasso ( <ref type="bibr" target="#b28">Tibshirani et al., 2005</ref>) and the elitist lasso ( <ref type="bibr" target="#b15">Kowalski and Torresani, 2009).</ref> where λ glas is a hyperparameter tuned on a devel- opment data, and λ g is a group specific weight. Typically the groups are non-overlapping, which offers computational advantages, but this need not be the case ( <ref type="bibr" target="#b11">Jenatton et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Structured Regularizers for Text</head><p>Past work applying the group lasso to NLP prob- lems has considered four ways of defining the groups. <ref type="bibr" target="#b4">Eisenstein et al. (2011)</ref> defined groups of coefficients corresponding to the same inde- pendent variable applied to different (continuous) output variables in multi-output regression. <ref type="bibr" target="#b17">Martins et al. (2011)</ref> defined groups based on fea- ture templates used in chunking and parsing tasks. <ref type="bibr" target="#b18">Nelakanti et al. (2013)</ref> defined groups based on n- gram histories for language modeling. In each of these cases, the groups were defined based on in- formation from feature types alone; given the fea- tures to be used, the groups were known.</p><p>Here we build on a fourth approach that exploits structure in the data. <ref type="bibr">4 Yogatama and Smith (2014)</ref> introduced the sentence regularizer, which uses patterns of word cooccurrence in the training data to define groups. We review this method, then ap- ply the idea to three more linguistically informed structure in text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentence Regularizer</head><p>The sentence regularizer exploits sentence bound- aries in each training document. The idea is to define a group g d,s for every sentence s in every training document d. The group contains coeffi- cients for words that occur in its sentence. This means that a word is a member of one group for every distinct (training) sentence it occurs in, and that the regularizer is based on word tokens, not types as in the approach of <ref type="bibr" target="#b17">Martins et al. (2011)</ref> and <ref type="bibr" target="#b18">Nelakanti et al. (2013)</ref>. The regularizer is:</p><formula xml:id="formula_5">Ω sen (w) = D d=1 S d s=1 λ d,s w d,s 2 ,</formula><p>where S d is the number of sentences in document d. This regularizer results in tens of thousands to millions of heavily overlapping groups, since a standard corpus typically contains thousands to millions of sentences and many words that appear in more than one sentence.  Figure 1: An example of a parse tree from the Stanford sen- timent treebank, which annotates sentiment at the level of every constituent (indicated here by + and ++; no mark- ing indicates neutral sentiment). The sentence is The ac- tors are fantastic. Our regularizer constructs nine groups for this sentence, corresponding to c0, c1, . . . , c8. gc 0 consists of 5 weights-w the , wactors , ware , w fantastic , w., exactly the same as the group in the sentence regularizer-gc 1 consists of 2 words, gc 4 of 3 words, etc. Notice that c2, c3, c6, c7, and c8 each consist of only 1 word. The Stanford sentiment treebank has an annotation of sentiments at the constituent level. As in this example, most constituents are annotated as neutral.</p><p>If the norm of w g d,s is driven to zero, then the learner has deemed the corresponding sentence ir- relevant to the prediction. It is important to point out that, while the regularizer prefers to zero out the weights for all words in irrelevant sentences, it also prefers not to zero out weights for words in relevant sentences. Since the groups overlap and may work against each other, the regularizer may not be able to drive many weights to zero on its own. <ref type="bibr" target="#b32">Yogatama and Smith (2014)</ref> used a linear combination of the sentence regularizer and the lasso (a kind of sparse group lasso; <ref type="bibr" target="#b6">Friedman et al., 2010</ref>) to also encourage weights of irrelevant word types to go to zero. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parse Tree Regularizer</head><p>Sentence boundaries are a rather superficial kind of linguistic structure; syntactic parse trees pro- vide more fine-grained information. We introduce a new regularizer, the parse tree regularizer, in which groups are defined for every constituent in every parse of a training data sentence. <ref type="figure">Figure 1</ref> illustrates the group structures derived from an example sentence from the Stanford sen- timent treebank <ref type="bibr" target="#b25">(Socher et al., 2013)</ref>. This regu- larizer captures the idea that phrases might be se- lected as relevant or (in most cases) irrelevant to a task, and is expected to be especially useful in sentence-level prediction tasks.</p><p>The parse-tree regularizer (omitting the group coefficients and λ) for one sentence with the parse tree shown in <ref type="figure">Figure 1</ref> is:</p><formula xml:id="formula_6">Ωtree (w) = p |w the | 2 + |wactors | 2 + |ware | 2 + |w fantastic | 2 + |w.| 2 + p |ware | 2 + |w fantastic | 2 + |w 2 . | + p |w the | 2 + |wactors | 2 + p |ware | 2 + |w fantastic | 2 + |w the | + |wactors | + |ware | + |w fantastic | + |w.|</formula><p>The groups have a tree structure, in that assign- ing zero values to the weights in a group corre- sponding to a higher-level constituent implies the same for those constituents that are dominated by it. This resembles the tree-guided group lasso in <ref type="bibr" target="#b13">Kim and Xing (2008)</ref>, although the leaf nodes in their tree represent tasks in multi-task regression.</p><p>Of course, in a corpus there are many parse trees (one per sentence, so the number of parse trees is the number of sentences). The parse-tree regular- izer is:</p><formula xml:id="formula_7">Ω tree (w) = D d=1 S d s=1 C d,s c=1 λ d,s,c w d,s,c 2 ,</formula><formula xml:id="formula_8">where λ d,s,c = λ glas × size(g d,s,c ), d</formula><p>ranges over (training) documents and c ranges over con- stituents in the parse of sentence s in docu- ment d. Similar to the sentence regularizer, the parse-tree regularizer operates on word to- kens. Note that, since each word token is it- self a constituent, the parse tree regularizer in- cludes terms just like the lasso naturally, penal- izing the absolute value of each word's weight in isolation. For the lasso-like penalty on each word, instead of defining the group weights to be 1 × the number of tokens for each word type, we tune one group weight for all word types on a de- velopment data. As a result, besides λ glas , we have an additional hyperparameter, denoted by λ las .</p><p>To gain an intuition for this regularizer, consider the case where we apply the penalty only for a sin- gle tree (sentence), which for ease of exposition is assumed not to use the same word more than once (i.e., x ∞ = 1). Because it instantiates the tree- structured group lasso, the regularizer will require bigger constituents to be "included" (i.e., their words given nonzero weight) before smaller con- stituents can be included. The result is that some words may not be included. Of course, in some sentences, some words will occur more than once, and the parse tree regularizer instantiates groups for constituents in every sentence in the training corpus, and these groups may work against each other. The parse tree regularizer should therefore be understood as encouraging group behavior of syntactically grouped words, or sharing of infor- mation by syntactic neighbors.</p><p>In sentence level prediction tasks, such as sentence-level sentiment analysis, it is known that most constituents (especially those that corre- spond to shorter phrases) in a parse tree are un- informative (neutral sentiment). This was verified by <ref type="bibr" target="#b25">Socher et al. (2013)</ref> when annotating phrases in a sentence for building the Stanford sentiment treebank. Our regularizer incorporates our prior expectation that most constituents should have no effect on prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LDA Regularizer</head><p>Another type of structure to consider is topics. For example, if we want to predict whether a pa- per will be cited or not <ref type="bibr" target="#b33">(Yogatama et al., 2011</ref>), the model can perform better if it knows before- hand the collections of words that represent certain themes (e.g., in ACL papers, these might include machine translation, parsing, etc.). As a result, the model can focus on which topics will increase the probability of getting citations, and penalize weights for words in the same topic together, in- stead of treating each word separately.</p><p>We do this by inferring topics in the training corpus by estimating the latent Dirichlet alloca- tion (LDA) model ( <ref type="bibr" target="#b0">Blei et al., 2003)</ref>). Note that LDA is an unsupervised method, so we can in- fer topical structures from any collection of docu- ments that are considered related to the target cor- pus (e.g., training documents, text from the web, etc.). This contrasts with typical semi-supervised learning methods for text categorization that com- bine unlabeled and labeled data within a genera- tive model, such as multinomial na¨ıvena¨ıve Bayes, via expectation-maximization ( <ref type="bibr" target="#b19">Nigam et al., 2000</ref>) or semi-supervised frequency estimation ( <ref type="bibr" target="#b26">Su et al., 2011)</ref>. Our method does not use unlabeled data to obtain more training documents or estimate the joint distributions of words better, but it allows the use of unlabeled data to induce topics. We leave comparison with other semi-supervised methods for future work.</p><p>There are many ways to associate inferred top- ics with group structure. In our experiments, we choose the R most probable words given a topic and create a group for them. <ref type="bibr">6</ref> The LDA regular- <ref type="bibr">6</ref> Another possibility is to group the smallest set of words whose total probability given a topic amounts to P (e.g., 0.99). mass of a topic. Preliminary experiments found this izer can be written as:</p><formula xml:id="formula_9">Ω lda (w) = K k=1 λ k w k 2 ,</formula><p>where k ranges over the K topics. Similar to our earlier notations, w k corresponds to the subvec- tor of w such that the corresponding features are present in topic k. Note that in this case we can also have overlapping groups, since words can ap- pear in the top R of many topics.  <ref type="table">Table 1</ref>: A toy example of K = 4 topics. The top R = 5 words in each topics are displayed. The LDA regularizer will construct four groups from these topics. The first group is wsoccer , w striker , w midfielder , w goal , w defender , the sec- ond group is winjury , w knee , w ligament , w shoulder , wcruciate , etc. In this example, there are no words occurring in the top R of more than one topic, but that need not be the case in general.</p><formula xml:id="formula_10">k = 1 k = 2 k = 3 k = 4</formula><p>To gain an intuition for this regularizer, consider the toy example in <ref type="table">Table 1</ref>. the case where we have K = 4 topics and we select R = 5 top words from each topic. Supposed that we want to clas- sify whether an article is a sports article or a sci- ence article. The regularizer might encourage the weights for the fourth topics' words toward zero, since they are less useful for the task. Addition- ally, the regularizer will penalize words in each of the other three groups collectively. Therefore, if (for example) ligament is deemed a useful feature for classifying an article to be about sports, then the other words in that topic will have a smaller ef- fective penalty for getting nonzero weights-even weights of the opposite sign as w ligament . It is im- portant to distinguish this from unstructured reg- ularizers such as the lasso, which penalize each word's weight on its own without regard for re- lated word types.</p><p>Unlike the parse tree regularizer, the LDA regu- larizer is not tree structured. Since the lasso-like penalty does not occur naturally in a non tree- structured regularizer, we add an additional lasso penalty for each word type (with hyperparameter λ las ) to also encourage weights of irrelevant words to go to zero. Our LDA regularizer is an instance of sparse group lasso ( <ref type="bibr" target="#b6">Friedman et al., 2010)</ref>. not to work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Brown Cluster Regularizer</head><p>Brown clustering is a commonly used unsuper- vised method for grouping words into a hierarchy of clusters ( <ref type="bibr" target="#b1">Brown et al., 1992)</ref>. Because it uses local information, it tends to discover words with similar syntactic behavior, though semantic group- ings are often evident, especially at the more fine- grained end of the hierarchy.</p><p>We incorporate Brown clusters into a regular- izer in a similar way to the topical word groups inferred using LDA in §4.3, but here we make use of the hierarchy. Specifically, we construct tree- structured groups, one per cluster (i.e., one per node in the hierarchy). The Brown cluster regu- larizer is:</p><formula xml:id="formula_11">Ω brown (w) = N v=1 λ v w v 2 ,</formula><p>where v ranges over the N nodes in the Brown cluster tree. As a tree structured regularizer, this regularizer enforces constraints that a node v's group is given nonzero weights only if those nodes that dominate v (i.e., are on a path from v to the root) have their groups selected. Consider a similar toy example to the LDA reg- ularizer (sports vs. science) and the hierarchical clustering of words in <ref type="figure" target="#fig_2">Figure 2</ref>. In this case, the Brown cluster regularizer will create 17 groups, one for every node in the clustering tree. The regu- larizer for this tree (omitting the group coefficients and λ) is:</p><formula xml:id="formula_12">Ω brown (w) = 7 i=0 w v i 2 + |w goal | + |w striker | + |w midfielder | + |w knee | + |w injury | + |w gravity | + |w moon | + |w sun |</formula><p>The regularizer penalizes words in a cluster to- gether, exploiting discovered syntactic related- ness. Additionally, the regularizer can zero out weights of words corresponding to any of the in- ternal nodes, such as v 7 if the words monday and sunday are deemed irrelevant to prediction.</p><p>Note that the regularizer already includes terms like the lasso naturally. Similar to the parse tree regularizer, for the lasso-like penalty on each word, we tune one group weight for all word types on a development data with a hyperparameter λ las .</p><p>A key difference between the Brown cluster regularizer and the parse tree regularizer is that there is only one tree for Brown cluster regularizer, whereas the parse tree regularizer can have mil- lions (one per sentence in the training data). The LDA and Brown cluster regularizers offer ways to incorporate unlabeled data, if we believe that the unlabeled data can help us infer better topics or clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predic- tive model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning</head><p>There are many optimization methods for learn- ing models with structured regularizers, particu- lary group lasso (Jacob et al., <ref type="bibr">2009</ref>  <ref type="bibr" target="#b8">Hestenes, 1969;</ref><ref type="bibr" target="#b22">Powell, 1969)</ref>. We review it here in brief, for com- pleteness, and show how it can be applied to tree- structured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically:</p><formula xml:id="formula_13">min w Ω(w) + D d=1 L(x d , w, y d ).</formula><p>Separating the lasso-like penalty for each word type from our group regularizers, we can rewrite this problem as: min</p><formula xml:id="formula_14">w,v Ω las (w) + Ω glas (v) + D d=1 L(x d , w, y d ) s.t. v = Mw</formula><p>where v consists of copies of the elements of w. Notice that we work directly on w instead of the copies for the lasso-like penalty, since it does not have overlaps and has its own hyper- parameters λ las . For the remaining groups with size greater than one, we create copies v of size L = G g=1 size(g). M ∈ {0, 1} L×V is a ma- trix whose 1s link elements of w to their copies. <ref type="bibr">7</ref> We now have a constrained optimization prob- lem, from which we can create an augmented La- grangian problem; let u be the Lagrange variables:</p><formula xml:id="formula_15">Ω las (w) + Ω glas (v) + L(w) + u (v − Mw) + ρ 2 v − Mw 2 2</formula><p>ADMM proceeds by iteratively updating each of w, v, and u, amounting to the following sub- problems:</p><formula xml:id="formula_16">min w Ω las (w) + L(w) − u Mw + ρ 2 v − Mw 2 2 (1) min v Ω glas (v) + u v + ρ 2 v − Mw 2 2 (2) u = u + ρ(v − Mw)<label>(3)</label></formula><p>Yogatama and Smith <ref type="formula">(2014)</ref> show that Eq. 1 can be rewritten in a form quite similar to 2 - regularized loss minimization. <ref type="bibr">8</ref> Eq. 2 is the proximal operator of 1 ρ Ω glas ap- plied to Mw − u ρ . As such, it depends on the form of M. Note that when applied to the col- lection of "copies" of the parameters, v, Ω glas no longer has overlapping groups. Defined M g as the rows of M corresponding to weight copies as- signed to group g. Let z g M g w − ug ρ . De- note λ g = λ glas size(g). The problem can be solved by applying the proximal operator used in non-overlapping group lasso to each subvector:</p><formula xml:id="formula_17">v g = prox Ω glas , λg ρ (z g ) =    0 if z g 2 ≤ λg ρ zg 2 − λg ρ zg 2 z g otherwise.</formula><p>For a tree structured regularizer, we can get speedups by working from the root node towards the leaf nodes when applying the proximal oper- ator in the second step. If g is a node in a tree which is driven to zero, all of its children h that has λ h ≤ λ g will also be driven to zero.</p><p>Eq. 3 is a simple update of the dual variable u. Algorithm 1 summarizes our learning procedure. 9 <ref type="bibr">7</ref> For the parse tree regularizer, L is the sum, over all training-data word tokens t, of the number of constituents t belongs to. For the LDA regularizer, L = R × K. For the Brown cluster regularizer, L = V − 1.</p><p>8 The difference lies in that the squared 2 norm in the penalty penalizes the difference between w and a vector that depends on the current values of u and v. This does not affect the algorithm or its convergence in any substantive way. <ref type="bibr">9</ref> We use relative changes in the 2 norm of the parameter vector w as our convergence criterion (threshold of 10 −3 ), and set the maximum number of iterations to 100. Other cri- teria can also be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ADMM for overlapping group lasso</head><p>Input: augmented Lagrangian variable ρ, regularization strengths λ glas and λ las while stopping criterion not met do w = arg min</p><formula xml:id="formula_18">w Ω las (w)+L(w)+ ρ 2 P V i=1 Ni(wi−µi) 2 for g = 1 to G do vg = prox Ω glas , λg ρ (zg)</formula><p>end for u = u + ρ(v − Mw) end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We use publicly available datasets to evaluate our model described in more detail below.</p><p>Topic classification. We consider four binary categorization tasks from the 20 Newsgroups dataset. <ref type="bibr">10</ref> Each task involves categorizing a document according to two related categories: comp.sys: ibm.pc.hardware vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space; and alt.atheism vs. soc.religion.christian. Sentiment analysis. One task in sentiment anal- ysis is predicting the polarity of a piece of text, i.e., whether the author is favorably inclined toward a (usually known) subject of discussion or proposi- tion ( <ref type="bibr" target="#b20">Pang and Lee, 2008)</ref>. Sentiment analysis, even at the coarse level of polarity we consider here, can be confused by negation, stylistic use of irony, and other linguistic phenomena. Our sen- timent analysis datasets consist of movie reviews from the Stanford sentiment treebank <ref type="bibr" target="#b25">(Socher et al., 2013)</ref>, <ref type="bibr">11</ref> and floor speeches by U.S. Congress- men alongside "yea"/"nay" votes on the bill under discussion ( <ref type="bibr" target="#b27">Thomas et al., 2006</ref>). <ref type="bibr">12</ref> For the Stan- ford sentiment treebank, we only predict binary classifications (positive or negative) and exclude neutral reviews.</p><p>Text-driven forecasting. Forecasting from text requires identifying textual correlates of a re- sponse variable revealed in the future, most of which will be weak and many of which will be spurious ( <ref type="bibr" target="#b14">Kogan et al., 2009</ref>). We consider two such problems. The first one is predicting whether a scientific paper will be cited or not within three years of its publication <ref type="bibr" target="#b33">(Yogatama et al., 2011</ref>  the dataset comes from the ACL Anthology and consists of research papers from the Association for Computational Linguistics and citation data ( <ref type="bibr" target="#b24">Radev et al., 2009)</ref>. The second task is predicting whether a legislative bill will be recommended by a Congressional committee ( <ref type="bibr" target="#b30">Yano et al., 2012</ref>). 13 <ref type="table" target="#tab_3">Table 2</ref> summarizes statistics about the datasets used in our experiments. In total, we evaluate our method on eight binary classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Setup</head><p>In all our experiments, we use unigram features plus an additional bias term which is not regu- larized. We compare our new regularizers with state-of-the-art methods for document classifica- tion: lasso, ridge, and elastic net regularization, as well as the sentence regularizer discussed in §4.1 <ref type="bibr" target="#b32">(Yogatama and Smith, 2014</ref>). <ref type="bibr">14</ref> We parsed all corpora using the Berkeley parser ( <ref type="bibr" target="#b21">Petrov and Klein, 2007)</ref>. <ref type="bibr">15</ref> For the LDA regular- izers, we ran LDA 16 on training documents with K = 1, 000 and R = 10. For the Brown cluster regularizers, we ran Brown clustering 17 on train- ing documents with 5, 000 clusters for the topic classification and sentiment analysis datasets, and 1, 000 for the larger text forecasting datasets (since they are bigger datasets that took more time).</p><p>13 http://www.ark.cs.cmu.edu/bills 14 Hyperparameters are tuned on a separate develop- ment dataset, using accuracy as the evaluation crite- rion. For lasso and ridge models, we choose λ from {10 −2 , 10 −1 , 1, 10, 10 2 , 10 3 }. For elastic net, we perform grid search on the same set of values as ridge and lasso experiments for λ rid and λ las . For the sentence, Brown cluster, and LDA regularizers, we perform grid search on the same set of values as ridge and lasso experiments for ρ, λ glas , λ las . For the parse tree regularizer, because there are many more groups than other regularizers, we choose λ glas from {10 −4 , 10 −3 , 10 −2 , 10 −1 , 10}, ρ and λ las from the same set of values as ridge and lasso experiments. If there is a tie on development data we choose the model with the smallest number of nonzero weights.  <ref type="table" target="#tab_5">Table 3</ref> shows the results of our experiments on the eight datasets. The results demonstrate the su- periority of structured regularizers. One of them achieved the best result on all but one dataset. <ref type="bibr">18</ref> It is also worth noting that in most cases all variants of the structured regularizers outperformed lasso, ridge, and elastic net. In four cases, the new regu- larizers in this paper outperform the sentence reg- ularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>We can see that the parse tree regularizer per- formed the best for the movie review dataset. The task is to predict sentence-level sentiment, so each training example is a sentence. Since constituent- level annotations are available for this dataset, we only constructed groups for neutral constituents (i.e., we drive neutral constituents to zero during training). It has been shown that syntactic in- formation is helpful for sentence-level predictions <ref type="bibr" target="#b25">(Socher et al., 2013)</ref>, so the parse tree regularizer is naturally suitable for this task.</p><p>The Brown cluster and LDA regularizers per- formed best for the forecasting scientific articles dataset. The task is to predict whether an article will be cited or not within three years after publi- cation. Regularizers that exploit the knowledge of semantic relations (e.g., topical categories), such as the Brown cluster and LDA regularizers, are therefore suitable for this type of prediction. <ref type="table" target="#tab_6">Table 4</ref> shows model sizes obtained by each of the regularizers for each dataset. While lasso prunes more aggressively, it almost always per- forms worse. Our structured regularizers were able to obtain a significantly smaller model (27%, 34%, 19% as large on average for parse tree, Brown, and LDA regularizers respectively) com- pared to the ridge model.</p><p>Topic and cluster features. Another way to in- corporate LDA topics and Brown clusters into a linear model is by adding them as additional fea- tures. For the 20N datasets, we also ran lasso, ridge, and elastic net with additional LDA topic and Brown cluster features. <ref type="bibr">19</ref> Note that these new baselines use more features than our model. We can also add these additional features to our model    and treat them as regular features (i.e., they do not belong to any groups and are regularized with standard regularizer such as the lasso penalty).</p><p>The results in <ref type="table" target="#tab_7">Table 5</ref> show that for these datasets, models that incorporate this information through structured regularizers outperformed models that encode this information as additional features in 4 out 4 of cases (LDA) and 2 out of 4 cases (Brown). Sparse models with Brown clusters ap- pear to overfit badly; recall that the clusters were learned on only the training data-clusters from a larger dataset would likely give stronger re- sults. Of course, better performance might also be achieved by incorporating new features as well as using structured regularizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Examples</head><p>To gain an insight into the models, we inspect group sparsity patterns in the learned models by looking at the parameter copies v. This lets us see which groups are considered important (i.e., "se- lected" vs. "removed"). For each of the proposed regularizers, we inspect the model a task in which it performed well.</p><p>For the parse tree regularizer, we inspect the model for the 20N:religion task. We observed that the model included most of the sentences (root node groups), but in some cases removed phrases from the parse trees, such as ozzy osbourne in the sentence ozzy osbourne , ex-singer and main char- acter of the black sabbath of good ole days past , is and always was a devout catholic .</p><p>For the LDA regularizer, we inspect zero and nonzero groups (topics) in the forecasting scien- tific articles task. In this task, we observed that 642 out of 1,000 topics are driven to zero by our model. <ref type="table">Table 6</ref> shows examples of zero and nonzero topics for the dev.-tuned hyperparameter values. We can see that in this particular case, the model kept meaningful topics such as parsing and speech processing, and discarded general topics that are not correlated with the content of the pa- pers (e.g., acknowledgment, document metadata, equation, etc.). Note that most weights for non- selected groups, even in w, are near zero.</p><p>For the Brown cluster regularizer, we inspect the model from the 20N:science task. 771 out of 5,775 groups were driven to zero for the best model tuned on the development set. Examples of zero and nonzero groups are shown in Ta- ble 7. Similar to the LDA example, the groups that were driven to zero tend to contain generic words that are not relevant to the predictions. We can also see the tree structure effect in the regu- larizer. The group {underwater, industrial} was = 0 "acknowledgment": workshop arpa program session darpa research papers spoken technology systems "document metadata": university references proceedings abstract work introduction new been research both "equation": pr w h probability wi gram context z probabilities complete "translation": translation target source german english length alignment hypothesis translations position = 0 "translation": korean translation english rules sentences parsing input evaluation machine verb "speech processing": speaker identification topic recognition recognizer models acoustic test vocabulary independent "parsing": parser parsing probabilistic prediction parse pearl edges chart phase theory "classification": documents learning accuracy bayes classification wt document naive method selection <ref type="table">Table 6</ref>: Examples of LDA regularizer-removed and -selected groups (in v) in the forecasting scientific articles dataset. Words with weights (in w) of magnitude greater than 10 −3 are highlighted in red (not cited) and blue (cited).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= 0</head><p>underwater industrial spotted hit reaped rejuvenated destroyed stretched undertake shake run seeing developing tingles diminishing launching finding investigating receiving maintaining adds engage explains builds = 0 failure reproductive ignition reproduction cyanamid planetary nikola fertility astronomical geophysical # lunar cometary supplying astronautical magnetic atmospheric std underwater hpr wordscan exclusively aneutronic industrial peoples obsessive congenital rare simple bowel hereditary breast <ref type="table">Table 7</ref>: Examples of Brown regularizer-removed and -selected groups (in v) in the 20N:science task. # denotes any numeral. Words with weights (in w) of magnitude greater than 10 −3 are highlighted in red (space) and blue (medical). driven to zero, but not once it combined with other words such as hpr, std, obsessive. Note that we ran Brown clustering only on the training docu- ments; running it on a larger collection of (unla- beled) documents relevant to the prediction task (i.e., semi-supervised learning) is worth exploring in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related and Future Work</head><p>Overall, our results demonstrate that linguistic structure in the data can be used to improve bag- of-words models, through structured regulariza- tion. State-of-the-art approaches to some of these problems have used additional features and repre- sentations ( <ref type="bibr" target="#b31">Yessenalina et al., 2010;</ref><ref type="bibr" target="#b25">Socher et al., 2013)</ref>. For example, for the vote sentiment analy- sis datasets, latent variable models of <ref type="bibr" target="#b31">Yessenalina et al. (2010)</ref> achieved a superior result of 77.67%. To do so, they sacrificed convexity and had to rely on side information for initialization. Our exper- imental focus has been on a controlled compari- son between regularizers for a fixed model family (the simplest available, linear with bag-of-words features). However, the improvements offered by our regularization methods can be applied in fu- ture work to other model families with more care- fully engineered features, metadata features (espe- cially important in forecasting), latent variables, etc. In particular, note that other kinds of weights (e.g., metadata) can be penalized conventionally, or incorporated into the structured regularization where it makes sense to do so (e.g., n-grams, as in <ref type="bibr" target="#b18">Nelakanti et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced three data-driven, linguistically informed structured regularizers based on parse trees, topics, and hierarchical word clusters. We empirically showed that models regularized us- ing our methods consistently outperformed stan- dard regularizers that penalize features in isolation such as lasso, ridge, and elastic net on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and fore- casting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustrative example of Brown clusters for N = 9. The Brown cluster regularizer constructs 17 groups, one per node in for this tree, v0, v1,. .. , v16. v0 contains 8 words, v1 contains 5, etc. Note that the leaves, v8, v9,. .. , v16, each contain one word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>15 https://code.google.com/p/berkeleyparser/ 16 http://www.cs.princeton.edu/ ˜ blei/lda-c/ 17 https://github.com/percyliang/brown-cluster</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) ;</head><label>;</label><figDesc></figDesc><table>Dataset 

D # Dev. # Test 
V 

20N 

science 
952 
235 
790 30,154 
sports 
958 
239 
796 20,832 
relig. 
870 
209 
717 24,528 
comp. 
929 
239 
777 20,868 

Sent. 
movie 
6,920 
872 
1,821 17,576 
vote 
1,175 
257 
860 24,508 

Fore. 
science 
3,207 
280 
539 42,702 
bill 37,850 
7,341 
6,571 10,001 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Descriptive statistics about the datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Classification 
accuracies on 
various datasets. 
"m.f.c." is the 
most frequent 
class baseline. 
Boldface shows 
best results. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model 
sizes (percentages 
of nonzero 
features in the 
resulting models) 
on various 
datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Classification accuracies on the 20N datasets for 
lasso, ridge, and elastic net models with additional LDA fea-
tures (top) and Brown cluster features (bottom). The last col-
umn shows structured regularized models from Table 3. 

</table></figure>

			<note place="foot" n="4"> This provides a compelling reason not to view such methods in a Bayesian framework: if the regularizer is informed by the data, then it does not truly correspond to a prior.</note>

			<note place="foot" n="5"> Formally, this is equivalent to including one additional group for each word type.</note>

			<note place="foot" n="10"> http://qwone.com/ ˜ jason/20Newsgroups 11 http://nlp.stanford.edu/sentiment/ 12 http://www.cs.cornell.edu/ ˜ ainur/data.html</note>

			<note place="foot" n="18"> This &quot;bill&quot; dataset, where they offered no improvement, is the largest by far (37,850 documents), and therefore the one where regularizers should matter the least. Note that the differences are small across regularizers for this dataset. 19 For LDA, we took the top 10 words in a topic as a feature. For Brown clusters, we add a cluster as an additional feature if its size is less than 50.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Brendan O'Connor for help with visualization and three anonymous review-ers for helpful feedback on an earlier draft of this paper. This research was supported in part by computing resources provided by a grant from the Pittsburgh Supercomputing Center, a Google re-search award, and the Intelligence Advanced Re-search Projects Activity via Department of In-terior National Business Center contract number D12PC00347. The U.S. Government is authorized to reproduce and distribute reprints for Govern-mental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily represent-ing the official policies or endorsements, either ex-pressed or implied, of IARPA, DoI/NBC, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of smoothing techniques for me models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smoothing proximal gradient method for general structured sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discovering sociolinguistic associations with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive sparseness using Jeffreys&apos; prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A note on the group lasso and a sparse group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshiran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiplier and gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Magnus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hestenes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="303" to="320" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Group lasso with overlap and graph lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanphilippe</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured variable selection with sparsity-inducing norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2777" to="2824" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Movie reviews and revenues: An experiment in text regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature selection via block-regularized regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting risk from financial reports with regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitry</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">R</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">S</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sparsity and persistence: mixed norms provide simple signal models with dependent coefficients. Signal, Image and Video Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="251" to="0264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The group lasso for stable recovery of block-sparse signal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoan</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunru</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1371" to="1382" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured sparsity in structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><forename type="middle">A T</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured penalties for log-linear language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Nelakanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A method for nonlinear constraints in minimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
		<editor>R. Fletcher</editor>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured sparsity via alternating direction methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Zhiwei (tony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1435" to="1468" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The ACL anthology network corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahed</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qazvinian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries</title>
		<meeting>of ACL Workshop on Natural Language essing and Information Retrieval for Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale text classication using semisupervised multinomial naive Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelber</forename><surname>Sayyad-Shirabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Get out the vote: Determining support or opposition from congressional floor-debate transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparsity and smoothness via the fused lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saharon</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Textual predictors of bill survival in congressional committees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-level structured models for document sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making the most of bag of words: Sentence regularization with alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predicting a scientific community&apos;s response to an article</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient methods for overlapping group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2104" to="2116" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
