<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Efficient Cross-lingual Model for Sentence Classification Using Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandi</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Efficient Cross-lingual Model for Sentence Classification Using Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="126" to="131"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a cross-lingual convolutional neural network (CNN) model that is based on word and phrase embeddings learned from unlabeled data in two languages and dependency grammar. Compared to traditional machine translation (MT) based methods for cross lingual sentence modeling, our model is much simpler and does not need parallel corpora or language specific features. We only use a bilingual dictionary and dependency parser. This makes our model particularly appealing for resource poor languages. We evaluate our model using English and Chinese data on several sentence classification tasks. We show that our model achieves a comparable and even better performance than the traditional MT-based method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid growth of global Internet, huge amounts of information are created in different lan- guages. It is important to develop cross-lingual NLP systems in order to leverage information from other languages, especially languages with rich annotations. Traditionally, cross-lingual systems rely highly on machine translation (MT) systems <ref type="bibr" target="#b15">Wan, 2011;</ref><ref type="bibr" target="#b13">Rigutini et al., 2005;</ref><ref type="bibr" target="#b7">Ling et al., 2008;</ref><ref type="bibr">Amini et al., 2009;</ref><ref type="bibr" target="#b3">Guo and Xiao, 2012;</ref><ref type="bibr" target="#b1">Chen and Ji, 2009;</ref><ref type="bibr" target="#b2">Duh et al., 2011</ref>). They translate data in one language into the other, and then apply monolingual models. One problem of such cross-lingual systems is that there is hardly any decent MT system for resource- poor languages. Another problem is the lack of high quality parallel corpora for resource-poor lan- guages, which is required by MT systems.</p><p>Other work tried to address these problems by developping language independent representation learning and structural correspondence learning (SCL) <ref type="bibr" target="#b12">(Prettenhofer and Stein, 2010;</ref><ref type="bibr" target="#b16">Xiao and Guo, 2013)</ref>. They showed some promising re- sults on document level classification tasks. How- ever, their methods require carefully designed lan- guage specific features and find the "pivot fea- tures" across languages, which can be very expen- sive and inefficient.</p><p>To solve these problems, we develop an effi- cient and feasible cross-lingual sentence model that is based on convolutional neural network (CNN). Sentence modeling using CNN has shown its great potential in recent years ( <ref type="bibr" target="#b5">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b6">Kim, 2014;</ref><ref type="bibr" target="#b8">Ma et al., 2015)</ref>. One of the advantages is that CNN requires much less exper- tise knowledge than traditional feature based mod- els. The only input of the model, word embed- dings, can be learned automatically from large un- labeled text data.</p><p>There are roughly two main differences between different languages, lexicon and grammar. Lex- icon can be seen as a set of symbols with each symbol representing certain meanings. A bilin- gual dictionary easily enables us to map from one symbol set to another. As for grammar, it decides the organization of lexical symbols, i.e., word or- der. Different languages organize their words in different manners (see <ref type="figure" target="#fig_0">Figure 1a</ref> for an example). To reduce grammar difference, we propose to use dependency grammar as an intermediate grammar. As shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, dependency grammar can yield a similar dependency tree between two sen- tences in different languages.</p><p>To bridge two different languages from aspects of both lexicon and grammar, our CNN-based cross-lingual model consists of two components, bilingual word embedding learning and CNN in- corporating dependency information. We propose a method to learn bilingual word embeddings as the input of CNN, using only a bilingual dic- tionary and unlabeled corpus. We then adopt a dependency-based CNN (DCNN) ( <ref type="bibr" target="#b8">Ma et al., 2015)</ref> to incorporate dependency tree information. We also design lexical features and phrase-based bilin- gual embeddings to improve our cross-lingual sen- tence model. We evaluate our model on English and Chinese data. We train a cross-lingual model on English data and then test it on Chinese data. Our experi- ments show that compared to the MT based cross- lingual model, our model achieves a comparable and even better performance on several sentence classification tasks including question classifica- tion, opinion analysis and sentence level event de- tection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Our method is based on the CNN sentence clas- sification model. It consists of two key compo- nents. First, we propose a method to learn bilin- gual word embeddings with only a bilingual dic- tionary and unlabeled corpus. This includes both word and phrase based embeddings. Second, for the CNN model, we use dependency grammar as the intermediate grammar, i.e., dependency-based CNN (DCNN) ( <ref type="bibr" target="#b8">Ma et al., 2015</ref>) where we also pro- pose some useful modifications to make the model more suitable for the cross-lingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilingual word and phrase embeddings</head><p>In order to train the bilingual word embeddings, we first construct an artificial bilingual corpus con- taining mix-language texts. We assume that the embeddings for a word and its translation in an- other language should be similar. We thus aim to create a synthetic similar context for a bilingual word pair. For example, assume we have an En- glish unlabeled corpus and we want to learn word embeddings of Chinese word "夏 威 夷" and its English counter-part "Hawaii", we can substitute half of "Hawaii" in the English corpus into "夏威 夷". Based on the modified corpus, we can ob- tain similar embeddings for the bilingual word pair "Hawaii" and "夏威夷". Similarly, we can also substitute Chinese words in the Chinese unlabeled data with their English counter-parts.</p><p>We use a bilingual dictionary to find bilingual word pairs. Each word w in the corpus has 1/2 chance to be replaced by its counter-part word in the other language. If there are multiple trans- lations for w in the bilingual dictionary, we ran- domly choose its replacement with probability 1/k, where k is the number of translations for w in the bilingual dictionary.</p><p>In the bilingual dictionary, many translations are phrase based, for example, "how many" and "多少". Intuitively phrases should be treated as a whole and translated to words or phrases in the other languages. Otherwise, "how many" will be translated word by word as "如何 很 多", which makes no sense in Chinese. Therefore, we pro- pose a simple method to learn phrase based bilin- gual word embeddings. When creating the artifi- cial mixed language corpus, if we need to substi- tute a word with its translated phrase, we connect all the words in the phrase with underscores so that they can be treated as one unit during word em- bedding learning. We also preprocess the data by identifying all the phrases and concatenating all the words in the phrases that appear in the bilingual dictionary. We thus can learn phrase based bilin- gual embeddings.</p><p>The original English and Chinese corpora are still useful for encoding pure monolingual infor- mation. Therefore, we mix them together with the artificial mixed language corpus to form the final corpus for word embedding learning. In the data, phrases are also identified and connected using the same strategy. We use the CBOW model ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>) for the bilingual word embedding learning. CBOW follows the assumption that sim- ilar words are more likely to appear in similar context. It casts word embedding learning into a word prediction problem given the context of the word. Because the CBOW model ignores word order within the window of contextual words, it may fail to capture the grammar or word order difference between two languages. We set a rel- atively larger CBOW window size <ref type="bibr">(20)</ref> so that the window can cover an average sentence length. This is expected to ignore the grammar difference within a sentence and allow the CBOW model to learn bilingual word embeddings based on sen- tence level word co-occurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dependency grammar based CNN</head><p>Using the learned bilingual word embeddings as input, we adopt CNN for sentence modeling. When doing convolution and max pooling, each window is treated as a unit, therefore, only local words' relations are captured. Due to different grammars, local words' relation may vary across different languages. For example in <ref type="figure" target="#fig_0">Figure 1a</ref>, the basic CNN model will create six windows with size of 3 for each sentence: {Padding, Padding, What}, {Padding, What, is}, {What, is, Hawaii}, {is, Hawaii, 's}, {Hawaii, 's, state}, {'s, state, flower} for English, and {Padding, Padding, 夏威 夷}, {Padding, 夏威夷, 的}, {夏威夷, 的, 州}, {的, 州, 花}, {州, 花, 是}, {花, 是, 什么} for Chi- nese. We can see that four windows in each sen- tence (out of six windows in that sentence) have different word ordering from the corresponding window in the other language.</p><p>To make relations captured in a window more meaningful for CNN, we adopt dependency based grammar as an intermediate grammar. As shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, a dependency based CNN cre- ates windows {What, ROOT, ROOT}, {is, What, ROOT}, {Hawaii, flower, What}, {'s, Hawaii, flower}, {state, flower, What}, {flower, What, ROOT} for English, and {夏威夷, 花, 什么}, {的, 夏威夷, 花}, {州, 花, 什么}, {花, 什么, ROOT}, {是, 什么, ROOT}, {什么, ROOT, ROOT} for Chinese. These dependency based windows cap- ture similar word order and co-occurrence across languages. The order of the windows is not im- portant as the max pooling layer ignores the global window order.</p><p>We therefore propose to incorporate depen- dency information into CNN. We evaluate the fol- lowing three different setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Dependency based CNN (DCNN):</head><p>We adopt the dependency based CNN proposed by <ref type="bibr" target="#b8">Ma et al. (2015)</ref>, where instead of the natural word or- ders within a window, dependency based orders are used. For example, let x be the word em- bedding of current word w, then a dependency based window with size of 3 is x ⊕ P arent 1 (x) ⊕ P arent 2 (x), where P arent 1 (x) and P arent 2 (x) are the embeddings of the parent and the grandpar- ent of x respectively; ⊕ is concatenation operation. The dependency based windows will be passed through the convolution layer and max pooling layer and finally a softmax layer for classification. We use a window size of 3 (a short dependency path) here in order to make the model more robust across different languages.</p><p>(b) DCNN incorporating lexical features: Al- though dependency grammar is a good intermedi- ate grammar, dependencies across languages are still not exactly the same. Second, dependency parsing is not perfect, especially for resource-poor languages. Therefore, it is possible that some word co-occurrence patterns cannot be captured. We thus add lexical features by adding an additional channel with window size equal to one, that is, each window has only one word. This lexicon in- put (a single word embedding) also passes through an independent convolution and pooling layer and the resulting feature is concatenated with the other abstract features.</p><p>(c) DCNN with phrase based grammar: In or- der to utilize phrase based bilingual embeddings, we make a modification in the dependency based CNN. If the input sentence contains a phrase in the bilingual dictionary, we combine the word nodes from the same phrase into a phrase node in the de- pendency tree. The combined phrase node will in- herit all the parents and children from its contained word nodes. Then the phrase node will be treated as a single unit in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks and datasets</head><p>To evaluate our model, we select four sentence classification tasks including question classifica- tion, sentiment classification on movie review, sentiment classification on product review and sentence level event detection. For each task, we either use existing data or collect our own. It is dif- ficult to find cross-lingual data with identical an- notation schema for all the tasks. We thus collect English and Chinese corpora from tasks with sim- ilar annotation schema and take the overlapping part. For all the tasks, we train our model on En- glish data, and test on Chinese data. To tune our model, we split Chinese dataset into validation and test sets.</p><p>Question classification (QC) aims to determine the category of a given question sentence. For En- glish, we use the TREC 1 dataset. For Chinese, we use a QA corpus from HIT-IRLab 2 . We kept the six overlapped question types for both English and Chinese corpora. The final corpus includes 4,313 English questions and 4,031 Chinese ques- tions (859 for testing, 859 for validation and 2,313 for training 3 ).</p><p>Sentiment classification on movie review (SC-M) aims to classify a piece of given movie re- view into positive or negative. For English, we use IMDB polarity movie reviews from ( <ref type="bibr" target="#b11">Pang and Lee, 2004</ref>) (5,331 positive and 5,331 negative). For Chinese, we use the short Chinese movie reviews from Douban 4 . Like IMDB, users from Douban leave their comments along with a score for the movie. We collected 250 one star reviews (lowest score), and 250 five star reviews (highest score). We randomly split the 500 reviews into 200 for val- idation and 300 for testing.</p><p>Sentiment classification on product review (SC-P) aims to classify a piece of given prod- uct review into positive or negative. We use cor- pora from <ref type="bibr" target="#b15">(Wan, 2011)</ref>. Their Chinese dataset contains mostly short reviews. However, their English Amazon product reviews are generally longer, containing several sentences. Although our model is designed to take a single sentence as input, CNN can actually handle any input length. We remove reviews that are longer than 100 words and treat the remaining review as a single sentence. For dependency parsing, we combine the root of each sentence and make it a global dependency tree. In the end, we got 3,134 English product re- views (1,707 positive, 1,427 negative); 1000 (549 positive, 451 negative) and 314 (163 positive, 151 negative) Chinese ones for validation and testing respectively.</p><p>Sentence level event detection (ED) aims to determine if a sentence contains an event. ACE 2005 corpus 5 is ideal for cross-lingual tasks, be-cause it contains annotated data for different lan- guages with the same definition of events. Sen- tence is the smallest unit that contains a set of com- plete event information, i.e., triggers and corre- sponding arguments. To build the sentence level corpus, we first split document into sentences. For each sentence, if an event occurs (event triggers and arguments exist), we label the sentence as pos- itive. Otherwise, we label it as negative. In the end we have 11,090 English sentences (3,688 positive, 7,402 negative). From the Chinese data we ran- domly selected 500 Chinese sentences (157 pos- itive, 343 negative) for test, and 500 (138 posi- tive, 362 negative) for validation. The remaining 5,039 ones (1767 positive, 3772 negative) are kept as training set. Because this is a detection task, we report F-score for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We compare our bilingual word embedding based strategy to MT-based approach on the above four cross-lingual sentence classification tasks. Be- sides, we also evaluate the effectiveness of in- corporating dependency information into CNN for sentence modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setup</head><p>For the traditional MT-based cross-lingual method, we use the state-of-the-art statistical MT system Moses 6 . Language model is trained on Chinese gi- gawords corpus <ref type="bibr">7</ref> with SRILM 8 . The parallel cor- pora used are from LDC 9 . We first translate En- glish data into Chinese, and then apply the model trained on the translated dataset to the Chinese test data. For sentence classification, we use both basic <ref type="bibr">CNN and DCNN (Ma et al., 2015</ref> gradient descent (SGD) learning method. We ap- ply random dropout ( <ref type="bibr" target="#b4">Hinton et al., 2012</ref>) on the last fully connected layer for regularization. We use ADADELTA <ref type="bibr" target="#b17">(Zeiler, 2012)</ref> algorithm to au- tomatically control the learning rate and progress. The batch size for SGD and feature maps are tuned on the validation set for each task and fixed across different configurations. We preprocess all our corpora with Stanford CoreNLP ( <ref type="bibr" target="#b9">Manning et al., 2014</ref>), including word segmentation, sentence seg- mentation and dependency parsing. <ref type="table">Table 1</ref> shows the results of different systems. When using the MT based methods, the basic CNN achieves better results than DCNN. One possible reason is that the translation system produces er- rors, which may affect the performance of depen- dency parsing. For our method using bilingual word embeddings, basic CNN encodes only lex- icon mapping information, and is not good at cap- turing grammar patterns. Therefore, it is natu- ral this system has the lowest result. DCNN per- forms better than CNN, because it is able to cap- ture additional grammar patterns across two lan- guages by incorporating dependency information. Adding lexical features (DCNN+Lex) further im- proves performance. Given the fact that depen- dency parser is not perfect and dependency gram- mar between languages is not exactly the same, the grammar patterns that DCNN learned are not always reliable. The lexical feature here acts as an additional evidence to make the model more robust. DCNN+Lex+Phrase yields the best per- formance. The bilingual lexicon dictionary we use contains 54,168 Chinese words, and <ref type="bibr">29,</ref><ref type="bibr">355</ref> of them have phrase-based translations <ref type="bibr">(54.19%)</ref>. Therefore, phrase-based bilingual word embed- dings can represent sentences more accurately, and thus yield better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Compared to the MT-based approach, our cross- lingual model achieves comparable and even better performance. The advantage of our method is that we only use s dependency parser and bilingual dic- tionary, instead of a much more complicated ma- chine translation system, which requires expertise knowledge about different languages, human de- signed features and expensive parallel corpus. Our method can be easily applied to any language pairs whose dependency parsers exist.</p><p>We further compare our cross-lingual model with a monolingual model for question classifica- tion and event detection. We have labeled Chinese training data for both tasks. We train a DCNN model on Chinese training data and then test on Chinese test set. For question classification, the monolingual model has an accuracy of 93.02%, and for event detection, its F-score is 87.28%. The event detection corpus has a consistent defi- nition across two languages. Therefore, our cross- lingual system achieves close performance as the monolingual one. However, for question classifi- cation, the English and Chinese labeled data are constructed by two different teams and their an- notation schemes are not identical. Therefore, the monolingual model performs much better than our cross-lingual model. Domain adaptation between two data sets may improve the performance for the bilingual model, but it is not the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an efficient way to model cross-lingual sentences with only a bilingual dic- tionary and dependency parser. We evaluated our method on Chinese and English data and showed comparable and even better results than the tradi- tional MT-based method on several sentence clas- sification tasks. In addition, our method does not rely on expertise knowledge, human designed fea- tures and annotated resources. Therefore, it is easy to apply it to any language pair as long as there ex- ist dependency parsers and a bilingual dictionary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example to show dependency grammar can yield unified grammar between languages (Chinese and English).</figDesc><graphic url="image-2.png" coords="3,80.76,138.89,198.41,94.84" type="bitmap" /></figure>

			<note place="foot" n="1"> http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2 http://ir.hit.edu.cn 3 For QC and ED, we kept some samples as training set for an in-domain supervised model (refer to Section 4.2). 4 http://www.douban.com 5 http://projects.ldc.upenn.edu/ace/</note>

			<note place="foot" n="6"> http://www.statmt.org/moses/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is partially supported by DARPA Con-tract No. FA8750-13-2-0041 and AFOSR award No. FA9550-15-1-0346.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from multiple partially observed views-an application to multilingual text categorization</title>
		<ptr target="http://www.statmt.org/lm-benchmark11http://www.mandarintools.com/cedict.html" />
	</analytic>
	<monogr>
		<title level="m">References Massih Amini, Nicolas Usunier, and Cyril Goutte</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can one language bootstrap the other: a case study on event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing</title>
		<meeting>the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is machine translation ripe for cross-lingual sentiment classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="429" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cross language text classification via subspace co-regularized multiview learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6481</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can chinese web pages be classified with english data source?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="969" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency-based convolutional neural networks for sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="174" to="179" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crosslanguage text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An EM based training algorithm for crosslanguage text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Rigutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Maggini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Intelligence, 2005. Proceedings. The 2005 IEEE/WIC/ACM International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Biweighting domain adaptation for cross-language text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1535" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilingual co-training for sentiment classification of chinese product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="587" to="616" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised representation learning for cross-lingual text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1465" to="1475" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
