<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Frame Identification with Distributed Word Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<postCode>OX1 3QD</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>76 9th Avenue</addrLine>
									<postCode>10011</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<email>jaseweston@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>76 9th Avenue</addrLine>
									<postCode>10011</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>76 9th Avenue</addrLine>
									<postCode>10011</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Frame Identification with Distributed Word Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel technique for semantic frame identification using distributed representations of predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-of-the-art results on FrameNet-style frame-semantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be en- compassed in a discrete representation, such ap- proaches have successfully been applied to tasks such as sentiment analysis <ref type="bibr" target="#b23">(Socher et al., 2011</ref>), topic classification ( <ref type="bibr" target="#b15">Klementiev et al., 2012</ref>) or word-word similarity <ref type="bibr" target="#b20">(Mitchell and Lapata, 2008)</ref>.</p><p>We present a new technique for semantic frame identification that leverages distributed word rep- resentations. According to the theory of frame se- mantics <ref type="bibr" target="#b11">(Fillmore, 1982)</ref>, a semantic frame rep- resents an event or scenario, and possesses frame elements (or semantic roles) that participate in the * The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument iden- tification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame's semantic roles ( <ref type="bibr" target="#b6">Das et al., 2010;</ref><ref type="bibr" target="#b7">Das et al., 2014)</ref>. <ref type="bibr">1</ref> Here, we focus on the first sub- task of frame identification for given predicates; we use our novel method ( §3) in conjunction with a standard argument identification model ( §4) to perform full frame-semantic parsing.</p><p>We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus ( <ref type="bibr" target="#b0">Baker et al., 1998;</ref><ref type="bibr" target="#b10">Fillmore et al., 2003)</ref>, we outperform the prior state of the art ( <ref type="bibr" target="#b7">Das et al., 2014</ref>). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Sec- ond, we present results on PropBank-style seman- tic role labeling <ref type="bibr" target="#b21">(Palmer et al., 2005;</ref><ref type="bibr" target="#b19">Meyers et al., 2004;</ref><ref type="bibr" target="#b18">M` arquez et al., 2008)</ref>, that approach strong baselines, and are on par with prior state of the art <ref type="bibr" target="#b22">(Punyakanok et al., 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Early work in frame-semantic analysis was pio- neered by <ref type="bibr" target="#b12">Gildea and Jurafsky (2002)</ref>. Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the lat- ter has been more popular. Since the <ref type="bibr">CoNLL 2004</ref><ref type="bibr">CoNLL -2005</ref> shared tasks (Carreras and M` arquez, John bought a car .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMMERCE_BUY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>buy.V</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Buyer Goods</head><p>John bought a car .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>buy.01</head><p>buy.V</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A0 A1</head><p>Mary sold a car .</p><formula xml:id="formula_0">COMMERCE_BUY sell.V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seller Goods</head><p>Mary sold a car .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sell.01</head><p>sell.V</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A0 A1</head><p>(a) (b) 2004; <ref type="bibr" target="#b4">Carreras and M` arquez, 2005</ref>) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its interaction with argument identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Frame-Semantic Parsing</head><p>Closely related to SRL, frame-semantic parsing consists of the resolution of predicate sense into a frame, and the analysis of the frame's argu- ments. Work in this area exclusively uses the FrameNet full text annotations. <ref type="bibr" target="#b14">Johansson and Nugues (2007)</ref> presented the best performing sys- tem at <ref type="bibr">SemEval 2007</ref><ref type="bibr" target="#b1">(Baker et al., 2007</ref>, and <ref type="bibr" target="#b6">Das et al. (2010)</ref> improved performance, and later set the current state of the art on this task ( <ref type="bibr" target="#b7">Das et al., 2014</ref>). We briefly discuss FrameNet, and subse- quently PropBank annotation conventions here.</p><p>FrameNet The FrameNet project ( <ref type="bibr" target="#b0">Baker et al., 1998</ref>) is a lexical database that contains informa- tion about words and phrases (represented as lem- mas conjoined with a coarse part-of-speech tag) termed as lexical units, with a set of semantic frames that they could evoke. For each frame, there is a list of associated frame elements (or roles, henceforth), that are also distinguished as core or non-core. <ref type="bibr">2</ref> Sentences are annotated us- ing this universal frame inventory. For exam- ple, consider the pair of sentences in <ref type="figure" target="#fig_0">Figure 1</ref>(a). COMMERCE BUY is a frame that can be evoked by morphological variants of the two example lexical units buy.V and sell.V. Buyer, Seller and Goods are some example roles for this frame.</p><p>PropBank The PropBank project <ref type="bibr" target="#b21">(Palmer et al., 2005</ref>) is another popular resource related to se- mantic role labeling. The PropBank corpus has verbs annotated with sense frames and their ar- guments. Like FrameNet, it also has a lexi- cal database that stores type information about verbs, in the form of sense frames and the possi- ble semantic roles each frame could take. There are modifier roles that are shared across verb frames, somewhat similar to the non-core roles in FrameNet. <ref type="figure" target="#fig_0">Figure 1</ref>(b) shows annotations for two verbs "bought" and "sold", with their lemmas (akin to the lexical units in FrameNet) and their verb frames buy.01 and sell.01. Generic core role labels (of which there are seven, namely A0-A5 and AA) for the verb frames are marked in the figure. <ref type="bibr">3</ref> A key difference between the two annotation sys- tems is that PropBank uses a local frame inven- tory, where frames are predicate-specific. More- over, role labels, although few in number, take spe- cific meaning for each verb frame. <ref type="figure" target="#fig_0">Figure 1</ref> high- lights this difference: while both sell.V and buy.V are members of the same frame in FrameNet, they evoke different frames in PropBank. In spite of this difference, nearly identical statistical models could be employed for both frameworks.</p><p>Modeling In this paper, we model the frame- semantic parsing problem in two stages: frame identification and argument identification. As mentioned in §1, these correspond to a frame dis- ambiguation stage, 4 and a stage that finds the var- ious arguments that fulfill the frame's semantic roles within the sentence, respectively. This re- sembles the framework of <ref type="bibr" target="#b7">Das et al. (2014)</ref>, who solely focus on FrameNet corpora, unlike this pa- per. The novelty of this paper lies in the frame identification stage ( §3). Note that this two-stage approach is unusual for the PropBank corpora when compared to prior work, where the vast ma- jority of published papers have not focused on the verb frame disambiguation problem at all, only fo- cusing on the role labeling stage (see the overview paper of M` arquez et al. (2008) for example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributed Frame Identification</head><p>We present a model that takes word embeddings as input and learns to identify semantic frames. A word embedding is a distributed representa- tion of meaning where each word is represented as a vector in R n . Such representations allow a model to share meaning between similar words, and have been used to capture semantic, syntac- tic and morphological content <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr">Turian et al., 2010, inter alia)</ref>. We use word embeddings to represent the syntactic con- text of a particular predicate instance as a vector. For example, consider the sentence "He runs the company." The predicate runs has two syntac- tic dependents -a subject and direct object (but no prepositional phrases or clausal complements). We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 . . . n in the vector correspond to the subject dependent, n+1 . . . 2n correspond to the clausal complement dependent, and so forth. Thus, the context is a vector in R nk with the embedding of He at the subject position, the embedding of company in direct object posi- tion and zeros everywhere else. Given input vec- tors of this form for our training data, we learn a matrix that maps this high dimensional and sparse representation into a lower dimensional space. Si- multaneously, the model learns an embedding for all the possible labels (i.e. the frames in a given lexicon). At inference time, the predicate-context is mapped to the low dimensional space, and we choose the nearest frame label as our classifica- tion. We next describe this model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Frame Identification with Embeddings</head><p>We continue using the example sentence from §2.2: "He runs the company." where we want to disambiguate the frame of runs in context. First, we extract the words in the syntactic context of runs; next, we concatenate their word embeddings as described in §2.2 to create an initial vector space representation. Subsequently, we learn a map- ping from this initial representation into a low- dimensional space; we also learn an embedding for each possible frame label in the same low- dimensional space. The goal of learning is to make sure that the correct frame label is as close as possible to the mapped context, while competing frame labels are farther away.</p><p>Formally, let x represent the actual sentence with a marked predicate, along with the associated syntactic parse tree; let our initial representation of the predicate context be g <ref type="bibr">(x)</ref>. Suppose that the word embeddings we start with are of dimension n. Then g is a function from a parsed sentence x to R nk , where k is the number of possible syn- tactic context types. For example g selects some important positions relative to the predicate, and reserves a block in its output space for the embed- ding of words found at that position. Suppose g considers clausal complements and direct objects. Then g : X → R 2n and for the example sentence it has zeros in positions 0 . . . n and the embedding of the word company in positions n+1 . . . 2n.</p><formula xml:id="formula_1">g(x) = [0, . . . , 0, embedding of company].</formula><p>Section 3.1 describes the context positions we use in our experiments. Let the low dimensional space we map to be R m and the learned mapping be M : R nk → R m . The mapping M is a linear trans- formation, and we learn it using the WSABIE algo- rithm <ref type="bibr" target="#b28">(Weston et al., 2011</ref>). WSABIE also learns an embedding for each frame label (y, henceforth). In our setting, this means that each frame corre- sponds to a point in R m . If we have F possi- ble frames we can store those parameters in an F × m matrix, one m-dimensional point for each frame, which we will refer to as the linear map- ping Y . Let the lexical unit (the lemma conjoined with a coarse POS tag) for the marked predicate be . We denote the frames that associate with in the frame lexicon 5 and our training corpus as F . WSABIE performs gradient-based updates on an objective that tries to minimize the distance between M (g(x)) and the embedding of the cor- rect label Y (y), while maintaining a large distance between M (g(x)) and the other possible labels Y (¯ y) in the confusion set F . At disambiguation time, we use a simple dot product similarity as our distance metric, meaning that the model chooses a label by computing the argmax y s(x, y) where</p><formula xml:id="formula_2">s(x, y) = M (g(x)) · Y (y)</formula><p>, where the argmax iter- ates over the possible frames y ∈ F if was seen in the lexicon or the training data, or y ∈ F , if it was unseen. <ref type="bibr">6</ref> Model learning is performed using the margin ranking loss function as described in <ref type="figure">Figure 2</ref>: Context representation extraction for the embedding model. Given a dependency parse (1) the model extracts all words matching a set of paths from the frame evoking predicate and its direct de- pendents (2). The model computes a composed rep- resentation of the predicate instance by using dis- tributed vector representations for words (3) -the (red) vertical embedding vectors for each word are concatenated into a long vector. Finally, we learn a linear transformation function parametrized by the context blocks (4). <ref type="bibr" target="#b28">Weston et al. (2011)</ref>, and in more detail in section 3.2.</p><p>Since WSABIE learns a single mapping from g(x) to R m , parameters are shared between different words and different frames. So for example "He runs the company" could help the model disam- biguate "He owns the company." Moreover, since g(x) relies on word embeddings rather than word identities, information is shared between words. For example "He runs the company" could help us to learn about "She runs a corporation".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Representation Extraction</head><p>In principle g(x) could be any feature function, but we performed an initial investigation of two partic- ular variants. In both variants, our representation is a block vector where each block corresponds to a syntactic position relative to the predicate, and each block's values correspond to the embedding of the word at that position. Direct Dependents The first context function we considered corresponds to the examples in §3. To elaborate, the positions of interest are the labels of the direct dependents of the predicate, so k is the number of labels that the dependency parser can produce. For example, if the label on the edge be- tween runs and He is nsubj, we would put the em- bedding of He in the block corresponding to nsubj. If a label occurs multiple times, then the embed- dings of the words below this label are averaged.</p><p>Unfortunately, using only the direct dependents can miss a lot of useful information. For exam- ple, topicalization can place discriminating infor- mation farther from the predicate. Consider "He runs the company." vs. "It was the company that he runs." In the second sentence, the discrim- inating word, company dominates the predicate runs. Similarly, predicates in embedded clauses may have a distant agent which cannot be captured using direct dependents. Consider "The athlete ran the marathon." vs. "The athlete prepared him- self for three months to run the marathon." In the second example, for the predicate run, the agent The athlete is not a direct dependent, but is con- nected via a longer dependency path. Dependency Paths To capture more relevant context, we developed a second context function as follows. We scanned the training data for a given task (either the PropBank or the FrameNet domains) for the dependency paths that connected the gold predicates to the gold semantic argu- ments. This set of dependency paths were deemed as possible positions in the initial vector space rep- resentation. In addition, akin to the first context function, we also added all dependency labels to the context set. Thus for this context function, the block cardinality k was the sum of the number of scanned gold dependency path types and the num- ber of dependency labels. Given a predicate in its sentential context, we therefore extract only those context words that appear in positions warranted by the above set. See <ref type="figure">Figure 2</ref> for an illustration of this process.</p><p>We performed initial experiments using con- text extracted from 1) direct dependents, 2) de- pendency paths, and 3) both. For all our experi- ments, setting 3) which concatenates the direct de- pendents and dependency path always dominated the other two, so we only report results for this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning</head><p>We model our objective function following <ref type="bibr" target="#b28">Weston et al. (2011)</ref>, using a weighted approximate- rank pairwise loss, learned with stochastic gradi- ent descent. The mapping from g(x) to the low dimensional space R m is a linear transformation, so the model parameters to be learnt are the matrix M ∈ R nk×m as well as the embedding of each possible frame label, represented as another ma- trix Y ∈ R F ×m where there are F frames in total. The training objective function minimizes:</p><formula xml:id="formula_3">x ¯ y L rank y (x) max(0, γ+s(x, y)−s(x, ¯ y)).</formula><p>where x, y are the training inputs and their cor- responding correct frames, and ¯ y are negative frames, γ is the margin. Here, rank y (x) is the rank of the positive frame y relative to all the neg- ative frames:</p><formula xml:id="formula_4">rank y (x) = ¯ y I(s(x, y) ≤ γ + s(x, ¯ y)),</formula><p>and L(η) converts the rank to a weight. Choos- ing L(η) = Cη for any positive constant C opti- mizes the mean rank, whereas a weighting such as L(η) = η i=1 1/i (adopted here) optimizes the top of the ranked list, as described in <ref type="bibr" target="#b25">(Usunier et al., 2009</ref>). To train with such an objective, stochastic gradient is employed. For speed the computation of rank y (x) is then replaced with a sampled approximation: sample N items ¯ y until a violation is found, i.e. max(0, γ + s(x, ¯ y) − s(x, y))) &gt; 0 and then approximate the rank with (F − 1)/N , see <ref type="bibr" target="#b28">Weston et al. (2011)</ref> for more details on this procedure. For the choices of the stochastic gradient learning rate, margin (γ) and dimensionality (m), please refer to §5.4- §5.5.</p><p>Note that an alternative approach could learn only the matrix M , and then use a k-nearest neigh- bor classifier in R m , as in <ref type="bibr" target="#b27">Weinberger and Saul (2009)</ref>. The advantage of learning an embedding for the frame labels is that at inference time we need to consider only the set of labels for classi- fication rather than all training examples. Addi- tionally, since we use a frame lexicon that gives us the possible frames for a given predicate, we usually only consider a handful of candidate la- bels. If we used all training examples for a given predicate for finding a nearest-neighbor match at inference time, we would have to consider many more candidates, making the process very slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Argument Identification</head><p>Here, we briefly describe the argument identifi- cation model used in our frame-semantic parsing experiments, post frame identification. Given x, the sentence with a marked predicate, the argu- ment identification model assumes that the pred- icate frame y has been disambiguated. From a frame lexicon, we look up the set of semantic roles R y that associate with y. This set also contains the null role r ∅ . From x, a rule-based candidate argu- ment extraction algorithm extracts a set of spans A that could potentially serve as the overt 7 argu- 7 By overtness, we mean the non-null instantiation of a semantic role in a frame-semantic parse.</p><p>• starting word of a • POS of the starting word of a • ending word of a</p><p>• POS of the ending word of a • head word of a</p><p>• POS of the head word of a • bag of words in a</p><p>• bag of POS tags in a • a bias feature</p><p>• voice of the predicate use • word cluster of a's head • word cluster of a's head conjoined with word cluster of the predicate *</p><note type="other">• dependency path between a's head and the predicate • the set of dependency labels of the predicate's children • dependency path conjoined with the POS tag of a's head • dependency path conjoined with the word cluster of a's head • position of a with respect to the predicate (before, after, overlap or identical)</note><p>• whether the subject of the predicate is missing (miss- ingsubj)</p><p>• missingsubj, conjoined with the dependency path • missingsubj, conjoined with the dependency path from the verb dominating the predicate to a's head <ref type="table">Table 1</ref>: Argument identification features. The span in con- sideration is termed a. Every feature in this list has two ver- sions, one conjoined with the given role r and the other con- joined with both r and the frame y. The feature with a * su- perscript is only conjoined with the role to reduce its sparsity.</p><p>ments A y for y (see §5.4- §5.5 for the details of the candidate argument extraction algorithms). Learning Given training data of the form</p><formula xml:id="formula_5">x (i) , y (i) , M (i) N i=1</formula><p>, where,</p><formula xml:id="formula_6">M = {(r, a} : r ∈ R y , a ∈ A ∪ A y },<label>(1)</label></formula><p>a set of tuples that associates each role r in R y with a span a according to the gold data. Note that this mapping associates spans with the null role r ∅ as well. We optimize the following log-likelihood to train our model:</p><formula xml:id="formula_7">max θ N i=1 |M (i) | j=1 log p θ (r, a) j |x, y, R y − Cθ 2 2</formula><p>where p θ is a log-linear model normalized over the set R y , with features described in <ref type="table">Table 1</ref>. We set C = 1.0 and use L-BFGS ( <ref type="bibr" target="#b17">Liu and Nocedal, 1989</ref>) for training. Inference Although our learning mechanism uses a local log-linear model, we perform infer- ence globally on a per-frame basis by applying hard structural constraints. Following <ref type="bibr" target="#b7">Das et al. (2014)</ref> and <ref type="bibr" target="#b22">Punyakanok et al. (2008)</ref> we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles sub- ject to hard constraints described in §5.4 and §5.5.</p><p>We use an off-the-shelf ILP solver for inference.</p><p>In this section, we present our experiments and the results achieved. We evaluate our novel frame identification approach in isolation and also con- joined with argument identification resulting in full frame-semantic structures; before presenting our model's performance we first focus on the datasets, baselines and the experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We evaluate our models on both FrameNet-and PropBank-style structures. For FrameNet, we use the full-text annotations in the FrameNet 1.5 re- lease <ref type="bibr">8</ref> which was used by <ref type="bibr">Das et al. (2014, §3.2)</ref>. We used the same test set as Das et al. contain- ing 23 documents with 4,458 predicates. Of the remaining 55 documents, 16 documents were ran- domly chosen for development. <ref type="bibr">9</ref> For experiments with PropBank, we used the Ontonotes corpus ( <ref type="bibr" target="#b13">Hovy et al., 2006</ref>), version 4.0, and only made use of the Wall Street Journal doc- uments; we used sections 2-21 for training, sec- tion 24 for development and section 23 for testing. This resembles the setup used by <ref type="bibr" target="#b22">Punyakanok et al. (2008)</ref>. All the verb frame files in Ontonotes were used for creating our frame lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Frame Identification Baselines</head><p>For comparison, we implemented a set of baseline models, with varying feature configurations. The baselines use a log-linear model that models the following probability at training time:</p><formula xml:id="formula_8">p(y|x, ) = e ψ·f (y,x,,) ¯ y∈F e ψ·f (¯ y,x,,)<label>(2)</label></formula><p>At test time, this model chooses the best frame as argmax y ψ · f (y, x, ) where argmax iterates over the possible frames y ∈ F if was seen in the lexicon or the training data, or y ∈ F , if it was un- seen, like the disambiguation scheme of §3. We train this model by maximizing L 2 regularized log-likelihood, using L-BFGS; the regularization constant was set to 0.1 in all experiments.</p><p>For comparison with our model from §3, which we call WSABIE EMBEDDING, we implemented two baselines with the log-linear model. Both the baselines use features very similar to the input rep- resentations described in §3.1. The first one com- putes the direct dependents and dependency paths as described in §3.1 but conjoins them with the word identity rather than a word embedding. Ad- ditionally, this model uses the un-conjoined words as backoff features. This would be a standard NLP approach for the frame identification problem, but is surprisingly competitive with the state of the art. We call this baseline LOG-LINEAR WORDS. The sec- ond baseline, tries to decouple the WSABIE training from the embedding input, and trains a log linear model using the embeddings. So the second base- line has the same input representation as WSABIE EMBEDDING but uses a log-linear model instead of WSABIE. We call this model LOG-LINEAR EMBED- DING.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Common Experimental Setup</head><p>We process our PropBank and FrameNet training, development and test corpora with a shift-reduce dependency parser that uses the Stanford conven- tions ( <ref type="bibr" target="#b8">de Marneffe and Manning, 2013)</ref> and uses an arc-eager transition system with beam size of 8; the parser and its features are described by <ref type="bibr" target="#b30">Zhang and Nivre (2011)</ref>. Before parsing the data, it is tagged with a POS tagger trained with a condi- tional random field ( <ref type="bibr" target="#b16">Lafferty et al., 2001</ref>) with the following emission features: word, the word clus- ter, word suffixes of length 1, 2 and 3, capitaliza- tion, whether it has a hyphen, digit and punctua- tion. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned us- ing the algorithm of <ref type="bibr" target="#b26">Uszkoreit and Brants (2008)</ref> on a large English newswire corpus for cluster fea- tures. We use the same word clusters for the argu- ment identification features in <ref type="table">Table 1</ref>.</p><p>We learn the initial embedding representations for our frame identification model ( §3) using a deep neural language model similar to the one pro- posed by <ref type="bibr" target="#b2">Bengio et al. (2003)</ref>. We use 3 hidden layers each with 1024 neurons and learn a 128- dimensional embedding from a large corpus con- taining over 100 billion tokens. In order to speed up learning, we use an unnormalized output layer and a hinge-loss objective. The objective tries to ensure that the correct word scores higher than a random incorrect word, and we train with mini- batch stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Setup for FrameNet</head><p>Hyperparameters For our frame identification model with embeddings, we search for the WSA- BIE hyperparameters using the development data.   We search for the stochastic gradient learning rate in {0.0001, 0.001, 0.01}, the margin γ ∈ {0.001, 0.01, 0.1, 1} and the dimensionality of the final vector space m ∈ {256, 512}, to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame. The underlined values are the chosen hyperparameters used to an- alyze the test data. Argument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in §4) was adapted from the algorithm of <ref type="bibr" target="#b29">Xue and Palmer (2004)</ref> applied to dependency trees. Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates: we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the pred- icate to the rightmost index of the subtree headed by the predicate's head; this helped capture cases like "a few months" (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate's head to the position immediately before the predicate, for cases like "your gift to Goodwill" (where to is the predicate and your gift is the argument). 10 10 Note that <ref type="bibr" target="#b7">Das et al. (2014)</ref> describe the state of the art in FrameNet-based analysis, but their argument identifica- tion strategy considered all possible dependency subtrees in Frame Lexicon In our experimental setup, we scanned the XML files in the "frames" directory of the FrameNet 1.5 release, which lists all the frames, the corresponding roles and the associ- ated lexical units, and created a frame lexicon to be used in our frame and argument identification models. We noted that this renders every lexical unit as seen; in other words, at frame disambigua- tion time on our test set, for all instances, we only had to score the frames in F for a predicate with lexical unit (see §3 and §5.2). We call this setup FULL LEXICON. While comparing with prior state of the art on the same corpus, we noted that <ref type="bibr" target="#b7">Das et al. (2014)</ref> found several unseen predicates at test time. <ref type="bibr">11</ref> For fair comparison, we took the lexical units for the predicates that Das et al. considered as seen, and constructed a lexicon with only those; training instances, if any, for the unseen predicates under Das et al.'s setup were thrown out as well. We call this setup SEMAFOR LEXICON. <ref type="bibr">12</ref> We also experimented on the set of unseen instances used by Das et al. ILP constraints For FrameNet, we used three ILP constraints during argument identification ( §4). 1) each span could have only one role, 2) each core role could be present only once, and 3) all overt arguments had to be non-overlapping. a parse, resulting in a much larger search space.   <ref type="table">Table 5</ref>: Full frame-structure prediction results for Propbank. This is a metric that takes into account frames and arguments together. See §5.7 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experimental Setup for PropBank</head><p>Hyperparameters As in §5.4, we made a hyper- parameter sweep in the same space. The chosen learning rate was 0.01, while the other values were γ = 0.01 and m = 512. Ambiguous lexical units were used for this selection process. Argument Candidates For PropBank we use the algorithm of <ref type="bibr" target="#b29">Xue and Palmer (2004)</ref> applied to dependency trees. Frame Lexicon For the PropBank experiments we scanned the frame files for propositions in Ontonotes 4.0, and stored possible core roles for each verb frame. The lexical units were simply the verb associating with the verb frames. There were no unseen verbs at test time. ILP constraints We used the constraints of <ref type="bibr" target="#b22">Punyakanok et al. (2008)</ref>. <ref type="table" target="#tab_1">Table 2</ref> presents accuracy results on frame iden- tification. <ref type="bibr">13</ref> We present results on all predicates, ambiguous predicates seen in the lexicon or the training data, and rare ambiguous predicates that appear ≤ 11 times in the training data. The WS- ABIE EMBEDDING model from §3 performs signif- icantly better than the LOG-LINEAR WORDS base- line, while LOG-LINEAR EMBEDDING underperforms in every metric. For the SEMAFOR LEXICON setup, we also compare with the state of the art from Das <ref type="bibr">13</ref> We do not report partial frame accuracy that has been reported by prior work.  et al. <ref type="formula" target="#formula_6">(2014)</ref>, who used a semi-supervised learn- ing method to improve upon a supervised latent- variable log-linear model. For unseen predicates from the Das et al. system, we perform better as well. Finally, for the FULL LEXICON setting, the ab- solute accuracy numbers are even better for our best model. <ref type="table" target="#tab_2">Table 3</ref> presents results on the full frame-semantic parsing task (measured by a reim- plementation of the SemEval 2007 shared task evaluation script) when our argument identifica- tion model ( §4) is used after frame identification. We notice similar trends as in <ref type="table" target="#tab_1">Table 2</ref>, and our re- sults outperform the previously published best re- sults, setting a new state of the art. <ref type="table" target="#tab_4">Table 4</ref> shows frame identification results on the PropBank data. On the development set, our best model performs with the highest accuracy on all and ambiguous predicates, but performs worse on rare ambiguous predicates. On the test set, the LOG-LINEAR WORDS baseline performs best by a very narrow margin. See §6 for a discussion. <ref type="table">Table 5</ref> presents results where we measure pre- cision, recall and F 1 for frames and arguments to- gether; this strict metric penalizes arguments for mismatched frames, like in <ref type="table" target="#tab_2">Table 3</ref>. We see the same trend as in <ref type="table" target="#tab_4">Table 4</ref>. Finally, <ref type="table" target="#tab_6">Table 6</ref> presents SRL results that measures argument performance only, irrespective of the frame; we use the eval- uation script from <ref type="bibr">CoNLL 2005 (Carreras and</ref><ref type="bibr" target="#b4">M` arquez, 2005</ref>). We note that with a better frame identification model, our performance on SRL im- proves in general. Here, too, the embedding model barely misses the performance of the best baseline, but we are at par and sometimes better than the sin- gle parser setting of a state-of-the-art SRL system <ref type="bibr" target="#b22">(Punyakanok et al., 2008)</ref>. <ref type="bibr">14</ref> For FrameNet, the WSABIE EMBEDDING model we propose strongly outperforms the baselines on all metrics, and sets a new state of the art. We be- lieve that the WSABIE EMBEDDING model performs better than the LOG-LINEAR EMBEDDING baseline (that uses the same input representation) because the former setting allows examples with differ- ent labels and confusion sets to share informa- tion; this is due to the fact that all labels live in the same label space, and a single projection ma- trix is shared across the examples to map the input features to this space. Consequently, the WSABIE EMBEDDING model can share more information be- tween different examples in the training data than the LOG-LINEAR EMBEDDING model. Since the LOG- LINEAR WORDS model always performs better than the LOG-LINEAR EMBEDDING model, we conclude that the primary benefit does not come from the input embedding representation. <ref type="bibr">15</ref> On the PropBank data, we see that the LOG- LINEAR WORDS baseline has roughly the same per- formance as our model on most metrics: slightly better on the test data and slightly worse on the development data. This can be partially explained with the significantly larger training set size for PropBank, making features based on words more useful. Another important distinction between PropBank and FrameNet is that the latter shares frames between multiple lexical units. The ef- fect of this is clearly observable from the "Rare" column in <ref type="table" target="#tab_4">Table 4</ref>. WSABIE EMBEDDING performs poorly in this setting while LOG-LINEAR EMBEDDING performs well. Part of the explanation has to do with the specifics of WSABIE training. Recall that the WSABIE EMBEDDING model needs to estimate the label location in R m for each frame. In other words, it must estimate 512 parameters based on at most 10 training examples. However, since the input representation is shared across all frames, every other training example from all the lexical units affects the optimal estimate, since they all modify the joint parameter matrix M . By contrast, in the log-linear models each label has its own set of parameters, and they interact only via the normalization constant. The LOG-LINEAR WORDS model does not have this entanglement, but cannot share information between words. For PropBank, combination of two syntactic parsers as input. <ref type="bibr">15</ref> One could imagine training a WSABIE model with word features, but we did not perform this experiment. these drawbacks and benefits balance out and we see similar performance for LOG-LINEAR WORDS and LOG-LINEAR EMBEDDING. For FrameNet, esti- mating the label embedding is not as much of a problem because even if a lexical unit is rare, the potential frames can be frequent. For example, we might have seen the SENDING frame many times, even though telex.V is a rare lexical unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">FrameNet Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">PropBank Results</head><p>In comparison to prior work on FrameNet, even our baseline models outperform the previous state of the art. A particularly interesting comparison is between our LOG-LINEAR WORDS baseline and the supervised model of <ref type="bibr" target="#b7">Das et al. (2014)</ref>. They also use a log-linear model, but they incorporate a la- tent variable that uses WordNet <ref type="bibr">(Fellbaum, 1998)</ref> to get lexical-semantic relationships and smooths over frames for ambiguous lexical units. It is possible that this reduces the model's power and causes it to over-generalize. Another difference is that when training the log-linear model, they nor- malize over all frames, while we normalize over the allowed frames for the current lexical unit. This would tend to encourage their model to ex- pend more of its modeling power to rule out pos- sibilities that will be pruned out at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a simple model that outper- forms the prior state of the art on FrameNet- style frame-semantic parsing, and performs at par with one of the previous-best single-parser sys- tems on PropBank SRL. Unlike <ref type="bibr" target="#b7">Das et al. (2014)</ref>, our model does not rely on heuristics to con- struct a similarity graph and leverage WordNet; hence, in principle it is generalizable to varying domains, and to other languages. Finally, we pre- sented results on PropBank-style semantic role la- beling with a system that included the task of au- tomatic verb frame identification, in tune with the FrameNet literature; we believe that such a sys- tem produces more interpretable output, both from the perspective of human understanding as well as downstream applications, than pipelines that are oblivious to the verb frame, only focusing on ar- gument analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number Filename dev-1</head><p>LUCorpus-v0.3 20000420 xin eng-NEW.xml dev-2 NTI SouthAfrica Introduction.xml dev-3</p><p>LUCorpus-v0.3 CNN AARONBROWN ENG 20051101 215800.partial-NEW.xml dev-4</p><p>LUCorpus-v0.3 AFGP-2002-600045-Trans.xml dev-5</p><p>PropBank TicketSplitting.xml dev-6</p><p>Miscellaneous Hijack.xml dev-7</p><p>LUCorpus-v0.3 artb 004 A1 E1 NEW.xml dev-8 NTI WMDNews 042106.xml dev-9 C-4 C-4Text.xml dev-10 ANC EntrepreneurAsMadonna.xml dev-11 NTI LibyaCountry1.xml dev-12 NTI NorthKorea NuclearOverview.xml dev-13</p><p>LUCorpus-v0.3 20000424 nyt-NEW.xml dev-14 NTI WMDNews 062606.xml dev-15 ANC 110CYL070.xml dev-16</p><p>LUCorpus-v0.3 CNN ENG 20030614 173123.4-NEW-1.xml <ref type="table">Table 7</ref>: List of files used as development set for the FrameNet 1.5 corpus.</p><p>A Development Data <ref type="table">Table 7</ref> features a list of the 16 randomly selected documents from the FrameNet 1.5 corpus, which we used for development. The resultant develop- ment set consists of roughly 4,500 predicates. We use the same test set as in <ref type="bibr" target="#b7">Das et al. (2014)</ref>, con- taining 23 documents and 4,458 predicates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example sentences with frame-semantic analyses. FrameNet annotation conventions are used in (a) while (b) denotes PropBank conventions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Frame identification results for FrameNet. See  §5.6. 

SEMAFOR LEXICON 
FULL LEXICON 
Model 
Precision Recall 
F1 
Precision Recall 
F1 

Development Data 

LOG-LINEAR WORDS 
76.97 
63.37 69.51 
77.02 
63.55 69.64 
WSABIE EMBEDDING ( §3) 
78.33 
64.51 70.75 
78.33 
64.53 70.76 

Test Data 

Das et al. supervised 
67.81 
60.68 64.05 
Das et al. best 
68.33 
61.14 64.54 
LOG-LINEAR WORDS 
71.21 
63.37 67.06 
73.31 
65.20 69.01 
WSABIE EMBEDDING ( §3) 
73.00 
64.87 68.69 
74.29 
66.02 69.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Frame identification accuracy results for PropBank. 
The model and the column names have the same semantics 
as Table 2. 

Model 
P 
R 
F1 
LOG-LINEAR WORDS 
80.02 75.58 77.74 
WSABIE EMBEDDING ( §3) 80.06 75.74 77.84 
Dev data ↑ ↓ Test data 
Model 
P 
R 
F1 
LOG-LINEAR WORDS 
81.55 77.83 79.65 
WSABIE EMBEDDING ( §3) 81.32 77.97 79.61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Argument only evaluation (semantic role labeling 
metrics) using the CoNLL 2005 shared task evaluation script 
(Carreras and M` arquez, 2005). Results from Punyakanok et 
al. (2008) are taken from Table 11 of that paper. 

</table></figure>

			<note place="foot" n="1"> There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007).</note>

			<note place="foot" n="2"> Additional information such as finer distinction of the coreness properties of roles, the relationship between frames, and that of roles are also present, but we do not leverage that information in this work.</note>

			<note place="foot" n="3"> NomBank (Meyers et al., 2004) is a similar resource for nominal predicates, but we do not consider it in our experiments. 4 For example in PropBank, the lexical unit buy.V has three verb frames and in sentential context, we want to disambiguate its frame. (Although PropBank never formally uses the term lexical unit, we adopt its usage from the frame semantics literature.)</note>

			<note place="foot" n="5"> The frame lexicon stores the frames, corresponding semantic roles and the lexical units associated with the frame. 6 This disambiguation scheme is similar to the one adopted by Das et al. (2014), but they use unlemmatized words to define their confusion set.</note>

			<note place="foot" n="8"> See https://framenet.icsi.berkeley.edu. 9 These documents are listed in appendix A.</note>

			<note place="foot" n="14"> The last row of Table 6 refers to a system which used the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Emily Pitler for comments on an early draft, and the anonymous reviewers for their valu-able feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval2007 Task 19: Frame semantic structure extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2004 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 shared task: semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic frame-semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frame-semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="56" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Stanford typed dependencies manual</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">WordNet: an electronic lexical database</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Background to FrameNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="250" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frame Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistics in the Morning Calm</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Hanshin Publishing Co</publisher>
			<date type="published" when="1982" />
			<biblScope unit="page" from="111" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<title level="m">Ontonotes: The 90 In Proceedings of NAACL-HLT</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LTH: semantic structure extraction using nonprojective dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic role labeling: an introduction to the special issue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The NomBank project: An interim report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL/HLT Workshop on Frontiers in Corpus Annotation</title>
		<meeting>NAACL/HLT Workshop on Frontiers in Corpus Annotation</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLHLT</title>
		<meeting>ACLHLT</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="287" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ranking with ordered weighted pairwise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed word clustering for large scale class-based language modeling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Calibrating features for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
