<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approximation Strategies for Multi-Structure Sentence Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
							<email>kapil@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University New York</orgName>
								<address>
									<postCode>10025</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Approximation Strategies for Multi-Structure Sentence Compression</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1241" to="1251"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming. We explore instead the use of Lagrangian relaxation to decou-ple the two subproblems and solve them separately. While dynamic programming is viable for bigram-based sentence compression , finding optimal compressed trees within graphs is NP-hard. We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms, yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers. Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence compression is a text-to-text genera- tion task in which an input sentence must be transformed into a shorter output sentence which accurately reflects the meaning in the input and also remains grammatically well-formed. The compression task has received increasing attention in recent years, in part due to the availability of datasets such as the Ziff-Davis cor- pus <ref type="bibr" target="#b23">(Knight and Marcu, 2000</ref>) and the Edinburgh compression corpora <ref type="bibr" target="#b4">(Clarke and Lapata, 2006</ref>), from which the following example is drawn. <ref type="bibr">Original: In 1967</ref> Chapman, who had cultivated a con- ventional image with his ubiquitous tweed jacket and pipe, by his own later admission stunned a party attended by his friends and future Python colleagues by coming out as a homosexual.</p><p>Compressed: In 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual.</p><p>Following an assumption often used in compres- sion systems, the compressed output in this corpus is constructed by dropping tokens from the input sentence without any paraphrasing or reordering. <ref type="bibr">1</ref> A number of diverse approaches have been proposed for deletion-based sentence compres- sion, including techniques that assemble the out- put text under an n-gram factorization over the input text <ref type="bibr" target="#b35">(McDonald, 2006;</ref><ref type="bibr" target="#b6">Clarke and Lapata, 2008)</ref> or an arc factorization over input depen- dency parses <ref type="bibr" target="#b15">(Filippova and Strube, 2008;</ref><ref type="bibr" target="#b16">Galanis and Androutsopoulos, 2010;</ref><ref type="bibr" target="#b14">Filippova and Altun, 2013)</ref>. Joint methods have also been proposed that invoke integer linear programming (ILP) formu- lations to simultaneously consider multiple struc- tural inference problems-both over n-grams and input dependencies  or n-grams and all possible dependencies <ref type="bibr" target="#b44">(Thadani and McKeown, 2013)</ref>. However, it is well- established that the utility of ILP for optimal infer- ence in structured problems is often outweighed by the worst-case performance of ILP solvers on large problems without unique integral solu- tions. Furthermore, approximate solutions can often be adequate for real-world generation sys- tems, particularly in the presence of linguistically- motivated constraints such as those described by <ref type="bibr" target="#b6">Clarke and Lapata (2008)</ref>, or domain-specific pruning strategies such as the use of sentence tem- plates to constrain the output.</p><p>In this work, we develop approximate inference strategies to the joint approach of <ref type="bibr" target="#b44">Thadani and McKeown (2013)</ref> which trade the optimality guar- antees of exact ILP for faster inference by sep- arately solving the n-gram and dependency sub- problems and using Lagrange multipliers to en- force consistency between their solutions. How- ever, while the former problem can be solved efficiently using the dynamic programming ap- proach of <ref type="bibr" target="#b35">McDonald (2006)</ref>, there are no efficient algorithms to recover maximum weighted non- projective subtrees in a general directed graph. Maximum spanning tree algorithms, commonly used in non-projective dependency parsing <ref type="bibr" target="#b34">(McDonald et al., 2005</ref>), are not easily adaptable to this task since the maximum-weight subtree is not necessarily a part of the maximum spanning tree.</p><p>We therefore consider methods to recover ap- proximate solutions for the subproblem of finding the maximum weighted subtree in a graph, com- mon among which is the use of a linear program- ming relaxation. This linear program (LP) ap- pears empirically tight for compression problems and our experiments indicate that simply using the non-integral solutions of this LP in Lagrangian re- laxation can empirically lead to reasonable com- pressions. In addition, we can recover approxi- mate solutions to this problem by using the Chu- Liu Edmonds algorithm for recovering maximum spanning trees ( <ref type="bibr" target="#b3">Chu and Liu, 1965;</ref><ref type="bibr" target="#b13">Edmonds, 1967)</ref> over the relatively sparse subgraph defined by a solution to the relaxed LP. Our proposed ap- proximation strategies are evaluated using auto- mated metrics in order to address the question: un- der what conditions should a real-world sentence compression system implementation consider ex- act inference with an ILP or approximate infer- ence? The contributions of this work include:</p><p>• An empirically-useful technique for approx- imating the maximum-weight subtree in a weighted graph using LP-relaxed inference.</p><p>• Multiple approaches to generate good ap- proximate solutions for joint multi-structure compression, based on Lagrangian relaxation to enforce equality between the sequential and syntactic inference subproblems.</p><p>• An analysis of the tradeoffs incurred by joint approaches with regard to runtime as well as performance under automated measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-Structure Sentence Compression</head><p>Even though compression is typically formulated as a token deletion task, it is evident that drop- ping tokens independently from an input sentence will likely not result in fluent and meaningful com- pressive text. Tokens in well-formed sentences participate in a number of syntactic and seman- tic relationships with other tokens, so one might expect that accounting for heterogenous structural relationships between tokens will improve the co- herence of the output sentence. Furthermore, much recent work has focused on the challenge of joint sentence extraction and compression, also known as compressive summarization <ref type="bibr" target="#b1">Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013;</ref><ref type="bibr" target="#b29">Li et al., 2013;</ref><ref type="bibr" target="#b40">Qian and Liu, 2013)</ref>, in which questions of efficiency are paramount due to the larger problems in- volved; however, these approaches largely restrict compression to pruning parse trees, thereby im- posing a dependency on parser performance. We focus in this work on a sentence-level compression system to approximate the ILP-based inference of <ref type="bibr" target="#b44">Thadani and McKeown (2013)</ref> which does not re- strict compressions to follow input parses but per- mits the generation of novel dependency relations in output compressions. The rest of this section is organized as fol- lows: §2.1 provies an overview of the joint se- quential and syntactic objective for compression from <ref type="bibr" target="#b44">Thadani and McKeown (2013)</ref> while §2.2 discusses the use of Lagrange multipliers to en- force consistency between the different structures considered. Following this, §2.3 discusses a dy- namic program to find maximum weight bigram subsequences from the input sentence, while §2.4 covers LP relaxation-based approaches for ap- proximating solutions to the problem of finding a maximum-weight subtree in a graph of potential output dependencies. Finally, §2.5 discusses the features and model training approach used in our experimental results which are presented in §3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Joint objective</head><p>We begin with some notation. For an input sen- tence S comprised of n tokens including dupli- cates, we denote the set of tokens in S by T {t i : 1 ≤ i ≤ n}. Let C represent a compres- sion of S and let x i ∈ {0, 1} denote an indicator variable whose value corresponds to whether to- ken t i ∈ T is present in the compressed sentence C. In addition, we define bigram indicator vari- ables y ij ∈ {0, 1} to represent whether a particular order-preserving bigram 2 t i , t j from S is present as a contiguous bigram in C as well as dependency indicator variables z ij ∈ {0, 1} corresponding to whether the dependency arc t i → t j is present in the dependency parse of C. The score for a given compression C can now be defined to factor over its tokens, n-grams and dependencies as follows.</p><formula xml:id="formula_0">score(C) = t i ∈T x i · θ tok (t i ) + t i ∈T ∪{START}, t j ∈T ∪{END} y ij · θ bgr (t i , t j ) + t i ∈T ∪{ROOT}, t j ∈T z ij · θ dep (t i → t j ) (1)</formula><p>where θ tok , θ bgr and θ dep are feature-based scoring functions for tokens, bigrams and dependencies respectively. Specifically, each θ v (·) ≡ w v φ v (·) where φ v (·) is a feature map for a given vari- able type v ∈ {tok, bgr, dep} and w v is the cor- responding vector of learned parameters.</p><p>The inference task involves recovering the high- est scoring compression C * under a particular set of model parameters w.</p><formula xml:id="formula_1">C * = arg max C score(C) = arg max x,y,z x θ tok + y θ bgr + z θ dep (2)</formula><p>where the incidence vector x x i t i ∈T repre- sents an entire token configuration over T , with y and z defined analogously to represent configura- tions of bigrams and dependencies. θ v θ v (·) denotes a corresponding vector of scores for each variable type v under the current model parame- ters. In order to recover meaningful compressions by optimizing (2), the inference step must ensure:</p><p>1. The configurations x, y and z are consistent with each other, i.e., all configurations cover the same tokens. 2. The structural configurations y and z are non-degenerate, i.e, the bigram configuration y represents an acyclic path while the depen- dency configuration z forms a tree.</p><p>These requirements naturally rule out simple ap- proximate inference formulations such as search- based approaches for the joint objective. <ref type="bibr">3</ref> An ILP-based inference solution is demonstrated in <ref type="bibr" target="#b44">Thadani and McKeown (2013)</ref> that makes use of linear constraints over the boolean variables x i , y ij and z ij to guarantee consistency, as well as aux- iliary real-valued variables and constraints repre- senting the flow of commodities <ref type="bibr" target="#b30">(Magnanti and Wolsey, 1994</ref>) in order to establish structure in y and z. In the following section, we propose an al- ternative formulation that exploits the modularity of this joint objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Lagrangian relaxation</head><p>Dual decomposition ( <ref type="bibr" target="#b25">Komodakis et al., 2007</ref>) and Lagrangian relaxation in general are often used for solving joint inference problems which are decomposable into individual subproblems linked by equality constraints ( <ref type="bibr" target="#b26">Koo et al., 2010;</ref><ref type="bibr" target="#b42">Rush and Collins, 2011;</ref><ref type="bibr" target="#b12">DeNero and Macherey, 2011;</ref><ref type="bibr" target="#b33">Martins et al., 2011;</ref><ref type="bibr" target="#b10">Das et al., 2012;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013)</ref>. This approach permits sub-problems to be solved sepa- rately using problem-specific efficient algorithms, while consistency over the structures produced is enforced through Lagrange multipliers via itera- tive optimization. Exact solutions are guaranteed when the algorithm converges on a consistent pri- mal solution, although this convergence itself is not guaranteed and depends on the tightness of the underlying LP relaxation. The primary advan- tage of this technique is the ability to leverage the underlying structure of the problems in inference rather than relying on a generic ILP formulation while still often producing exact solutions. The multi-structure inference problem de- scribed in the previous section seems in many ways to be a natural fit to such an approach since output scores factor over different types of struc- ture that comprise the output compression. Even if ILP-based approaches perform reasonably at the scale of single-sentence compression problems, the exponential worst-case complexity of general- purpose ILPs will inevitably pose challenges when scaling up to (a) handle larger inputs, (b) use higher-order structural fragments, or (c) incorpo- rate additional models.</p><p>Consider once more the optimization problem characterized by <ref type="formula">(2)</ref> The two structural problems that need to be solved in this formulation are the extraction of a maximum-weight acyclic sub- sequence of bigrams y from the lattice of all order-preserving bigrams from S and the recov- ery of a maximum-weight directed subtree z. Let α(y) ∈ {0, 1} n denote the incidence vector of tokens contained in the n-gram sequence y and β(z) ∈ {0, 1} n denote the incidence vector of words contained in the dependency tree z. We can now rewrite the objective in (2) while enforcing the constraint that the words contained in the se- quence y are the same as the words contained in the tree z, i.e., α(y) = β(z), by introducing a vector of Lagrange multipliers λ ∈ R n . In addi- tion, the token configuration x can be rewritten in the form of a weighted combination of α(y) and β(z) to ensure its consistency with y and z. This results in the following Lagrangian:</p><formula xml:id="formula_2">L(λ, y, z) = y θ bgr + z θ dep + θ tok (ψ · α(y) + (1 − ψ) · β(z)) + λ (α(y) − β(z))<label>(3)</label></formula><p>Finding the y and z that maximize this Lagrangian above yields a dual objective, and the dual prob- lem corresponding to the primal objective speci- fied in (2) is therefore the minimization of this ob- jective over the Lagrange multipliers λ.</p><formula xml:id="formula_3">min λ max y,z L(λ, y, z) = min λ max y y θ bgr + (λ + ψ · θ tok ) α(y) + max z z θ dep − (λ + (ψ − 1) · θ tok ) β(z) = min λ max y f (y, λ, ψ, θ) + max z g(z, λ, ψ, θ)<label>(4)</label></formula><p>This can now be solved with the iterative subgra- dient algorithm illustrated in Algorithm 1. In each iteration i, the algorithm solves for y (i) and z (i) under λ (i) , then generates λ (i+1) to penalize in- consistencies between α(y (i) ) and β(z (i) ). When α(y (i) ) = β(z (i) ), the resulting primal solution is exact, i.e., y (i) and z (i) represent the optimal struc- tures under (2). Otherwise, if the algorithm starts oscillating between a few primal solutions, the un- derlying LP must have a non-integral solution in which case approximation heuristics can be em-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Subgradient-based joint inference</head><p>Input: scores θ, ratio ψ, repetition limit l max , iteration limit i max , learning rate schedule η Output: token configuration x</p><formula xml:id="formula_4">1: λ (0) ← 0 n 2: M ← ∅, M repeats ← ∅ 3: for iteration i &lt; i max do 4: ˆ y ← arg max y f (y, λ, ψ, θ) 5: ˆ z ← arg max z g(z, λ, ψ, θ) 6:</formula><p>if α(ˆ y) = β(ˆ z) then return α(ˆ y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>if α(ˆ y) ∈ M then 8:</p><formula xml:id="formula_5">M repeats ← M repeats ∪ {α(ˆ y)} 9:</formula><p>if β(ˆ z) ∈ M then 10:</p><formula xml:id="formula_6">M repeats ← M repeats ∪ {β(ˆ z)} 11:</formula><p>if |M repeats | ≥ l max then break 12:</p><formula xml:id="formula_7">M ← M ∪ {α(ˆ y), β(ˆ z)} 13: λ (i+1) ← λ (i) − η i (α(ˆ y) − β(ˆ z)) return arg max x∈Mrepeats score(x)</formula><p>ployed. <ref type="bibr">4</ref> The application of this Lagrangian relax- ation strategy is contingent upon the existence of algorithms to solve the maximization subproblems for f (y, λ, ψ, θ) and g(z, λ, ψ, θ). The following sections discuss our approach to these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bigram subsequences</head><p>McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest- scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. The latter requires a dynamic programming table Q <ref type="bibr">[i]</ref>[r] which repre- sents the best score for a compression of length r ending at token i. The table can be populated us- ing the following recurrence:</p><formula xml:id="formula_8">Q[i][1] = score(S, START, i) Q[i][r] = max j&lt;i Q[j][r − 1] + score(S, i, j) Q[i][R + 1] = Q[i][R] + score(S, i, END)</formula><p>where R is the required number of output tokens and the scoring function is defined as score(S, i, j) θ bgr (t i , t j ) + λ j + ψ · θ tok (t j ) so as to solve f (y, λ, ψ, θ) from (4). This ap- proach requires O(n 2 R) time in order to identify  the highest scoring sequence y and corresponding token configuration α(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dependency subtrees</head><p>The maximum-weight non-projective subtree problem over general graphs is not as easily solved. Although the maximum spanning tree for a given token configuration can be recovered ef- ficiently, <ref type="figure" target="#fig_1">Figure 1</ref> illustrates that the maximum- scoring subtree is not necessarily found within it. The problem of recovering a maximum-weight subtree in a graph has been shown to be NP-hard even with uniform edge weights ( <ref type="bibr" target="#b28">Lau et al., 2006</ref>).</p><p>In order to produce a solution to this subprob- lem, we use an LP relaxation of the relevant portion of the ILP from <ref type="bibr" target="#b44">Thadani and McKeown (2013)</ref> by omitting integer constraints over the to- ken and dependency variables in x and z respec- tively. For simplicity, however, we describe the ILP version rather than the relaxed LP in order to motivate the constraints with their intended pur- pose rather than their effect in the relaxed prob- lem. The objective for this LP is given by</p><formula xml:id="formula_9">max x,z x θ tok + z θ dep (5)</formula><p>where the vector of token scores is redefined as</p><formula xml:id="formula_10">θ tok (1 − ψ) · θ tok − λ<label>(6)</label></formula><p>in order to solve g(z, λ, ψ, θ) from (4). Linear constraints are introduced to produce de- pendency structures that are close to the optimal dependency trees. First, tokens in the solution must only be active if they have a single active in- coming dependency edge. In addition, to avoid producing multiple disconnected subtrees, only one dependency is permitted to attach to the ROOT pseudo-token.</p><formula xml:id="formula_11">x j − i z ij = 0, ∀t j ∈ T<label>(7)</label></formula><formula xml:id="formula_12">j z ij = 1, if t i = ROOT<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROOT</head><p>Production was closed down at Ford last night . In order to avoid cycles in the dependency tree, we include additional variables to establish single- commodity flow <ref type="bibr" target="#b30">(Magnanti and Wolsey, 1994)</ref> be- tween all pairs of tokens. These γ ij variables carry non-negative real values which must be consumed by active tokens that they are incident to.</p><formula xml:id="formula_13">γ ij ≥ 0, ∀t i , t j ∈ T<label>(9)</label></formula><formula xml:id="formula_14">i γ ij − k γ jk = x j , ∀t j ∈ T<label>(10)</label></formula><p>These constraints ensure that cyclic structures are not possible in the non-relaxed ILP. In addition, they serve to establish connectivity for the de- pendency structure z since commodity can only originate in one location-at the pseudo-token ROOT which has no incoming commodity vari- ables. However, in order to enforce these prop- erties on the output dependency structure, this acyclic, connected commodity structure must con- strain the activation of the z variables.</p><formula xml:id="formula_15">γ ij − C max z ij ≤ 0, ∀t i , t j ∈ T<label>(11)</label></formula><p>where C max is an arbitrary upper bound on the value of γ ij variables. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates how these commodity flow variables constrain the out- put of the ILP to be a tree. However, the effect of these constraints is diminished when solving an LP relaxation of the above problem.</p><p>In the LP relaxation, x i and z ij are redefined as real-valued variables in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, potentially resulting in fractional values for dependency and token indi- cators. As a result, the commodity flow network is able to establish connectivity but cannot enforce a tree structure, for instance, directed acyclic struc- tures are possible and token indicators x i may be partially be assigned to the solution structure. This poses a challenge in implementing β(z) which is needed to recover a token configuration from the solution of this subproblem.</p><p>We propose two alternative solutions to address this issue in the context of the joint inference strat- egy. The first is to simply use the relaxed token configuration identified by the LP in Algorithm 1, i.e., to set β(˜ z) = ˜ x where˜xwhere˜ where˜x and˜zand˜and˜z represent the real-valued counterparts of the incidence vectors x and z. The viability of this approximation strategy is due to the following:</p><p>• The relaxed LP is empirically fairly tight, yielding integral solutions 89% of the time on the compression datasets described in §3.</p><p>• The bigram subproblem is guaranteed to re- turn a well-formed integral solution which obeys the imposed compression rate, so we are assured of a source of valid-if non- optimal-solutions in line 13 of Algorithm 1.</p><p>We also consider another strategy that attempts to approximate a valid integral solution to the depen- dency subproblem. In order to do this, we first include an additional constraint in the relaxed LP which restrict the number of tokens in the output to a specific number of tokens R that is given by an input compression rate.</p><formula xml:id="formula_16">i x i = R<label>(12)</label></formula><p>The addition of this constraint to the relaxed LP reduces the rate of integral solutions drastically- from 89% to approximately 33%-but it serves to ensure that the resulting token configuratioñ x has at least as many non-zero elements as R, i.e., there are at least as many tokens activated in the LP so- lution as are required in a valid solution.</p><p>We then construct a subgraph G(˜ z) consisting of all dependency edges that were assigned non- zero values in the solution, assigning to each edge a score equal to the score of that edge in the LP as well as the score of its dependent word, i.e., each z ij in G(˜ z) is assigned a score of θ dep (t i , t j ) − λ j + (1 − ψ) · θ tok (t j ). Since the commodity flow constraints in (9)-(11) ensure a connected˜zconnected˜connected˜z, it is therefore possible to recover a maximum-weight spanning tree from G(˜ z) using the Chu-Liu Ed- monds algorithm ( <ref type="bibr" target="#b3">Chu and Liu, 1965;</ref><ref type="bibr" target="#b13">Edmonds, 1967)</ref>. <ref type="bibr">5</ref> Although the runtime of this algorithm is cubic in the size of the input graph, it is fairly speedy when applied on relatively sparse graphs such as the solutions to the LP described above. The resulting spanning tree is a useful integral approximation of˜zof˜of˜z but, as mentioned previously, may contain more nodes than R due to fractional values iñ x; we therefore repeatedly prune leaves with the lowest incoming edge weight in the cur- rent tree until exactly R nodes remain. The result- ing tree is assumed to be a reasonable approxima- tion of the optimal integral solution to this LP. The Chu-Liu Edmonds algorithm is also em- ployed for another purpose: when the underly- ing LP for the joint inference problem is not tight-a frequent occurrence in our compression experiments-Algorithm 1 will not converge on a single primal solution and will instead oscillate between solutions that are close to the dual opti- mum. We identify this phenomenon by counting repeated solutions and, if they exceed some thresh- old l max with at least one repeated solution from either subproblem, we terminate the update proce- dure for Lagrange multipliers and instead attempt to identify a good solution from the repeating ones by scoring them under (2). It is straightforward to recover and score a bigram configuration y from a token configuration β(z). However, scoring so- lutions produced by the dynamic program from §2.3 also requires the score over a corresponding parse tree; this can be recovered by constructing a dependency subgraph containing across only the tokens that are active in α(y) and retrieving the maximum spanning tree for that subgraph using the Chu-Liu Edmonds algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Learning and Features</head><p>The features used in this work are largely based on the features from <ref type="bibr" target="#b44">Thadani and McKeown (2013)</ref>.</p><p>• φ tok contains features for part-of-speech (POS) tag sequences of length up to 3 around the token, features for the dependency label of the token conjoined with its POS, lexical features for verb stems and non-word sym- bols and morphological features that identify capitalized sequences, negations and words in parentheses.</p><p>• φ bgr contains features for POS patterns in a bigram, the labels of dependency edges in- cident to it, its likelihood under a Gigaword language model (LM) and an indicator for whether it is present in the input sentence.</p><p>• φ dep contains features for the probability of a dependency edge under a smoothed depen- dency grammar constructed from the Penn Treebank and various conjunctions of the fol- lowing features: (a) whether the edge appears as a dependency or ancestral relation in the input parse (b) the directionality of the depen-dency (c) the label of the edge (d) the POS tags of the tokens incident to the edge and (e) the labels of their surrounding chunks and whether the edge remains within the chunk.</p><p>For the experiments in the following section, we trained models using a variant of the structured perceptron <ref type="bibr" target="#b9">(Collins, 2002</ref>) which incorporates minibatches (Zhao and Huang, 2013) for easy par- allelization and faster convergence. <ref type="bibr">6</ref> Overfitting was avoided by averaging parameters and mon- itoring performance against a held-out develop- ment set during training. All models were trained using variants of the ILP-based inference approach of Thadani and McKeown (2013). We followed  in using LP-relaxed inference during learning, assuming algorithmic separabil- ity ( <ref type="bibr" target="#b27">Kulesza and Pereira, 2007</ref>) for these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We ran compression experiments over the newswire (NW) and broadcast news transcription (BN) corpora compiled by <ref type="bibr" target="#b6">Clarke and Lapata (2008)</ref> which contain gold compressions pro- duced by human annotators using only word deletion. The datasets were filtered to eliminate instances with less than 2 and more than 110 tokens for parser compatibility and divided into training/development/test sections following the splits from <ref type="bibr" target="#b6">Clarke and Lapata (2008)</ref>, yielding 953/63/603 instances for the NW corpus and 880/78/404 for the BN corpus. Gold dependency parses were approximated by running the Stanford dependency parser 7 over reference compressions. Following evaluations in machine translation as well as previous work in sentence compres- sion ( <ref type="bibr" target="#b46">Unno et al., 2006;</ref><ref type="bibr" target="#b6">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b38">Napoles et al., 2011b;</ref><ref type="bibr" target="#b44">Thadani and McKeown, 2013)</ref>, we evaluate sys- tem performance using F 1 metrics over n-grams and dependency edges produced by parsing sys- tem output with RASP ( <ref type="bibr" target="#b2">Briscoe et al., 2006</ref>) and the Stanford parser. All ILPs and LPs were solved using Gurobi, 8 a high-performance commercial- grade solver. Following a recent analysis of com- pression evaluations ( <ref type="bibr" target="#b38">Napoles et al., 2011b</ref>) which revealed a strong correlation between system com- pression rate and human judgments of compres- sion quality, we constrained all systems to produce compressed output at a specific rate-determined by the the gold compressions available for each instance-to ensure that the reported differences between the systems under study are meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Systems</head><p>We report results over the following systems grouped into three categories of models: tokens + n-grams, tokens + dependencies, and joint models.</p><p>• 3-LM: A reimplementation of the unsuper- vised ILP of Clarke and Lapata <ref type="formula" target="#formula_12">(2008)</ref> which infers order-preserving trigram variables pa- rameterized with log-likelihood under an LM and a significance score for token variables inspired by <ref type="bibr" target="#b19">Hori and Furui (2004)</ref>, as well as various linguistically-motivated constraints to encourage fluency in output compressions.</p><p>• DP: The bigram-based dynamic program of</p><p>McDonald <ref type="formula" target="#formula_10">(2006)</ref>  • DP+LP→MST: An approximate joint infer- ence approach based on Lagrangian relax- ation that uses DP for the maximum weight subsequence problem and LP→MST for the maximum weight subtree problem.</p><p>• DP+LP: Another Lagrangian relaxation ap- proach that pairs DP with the non-integral solutions from an LP relaxation of the maxi- mum weight subtree problem (cf. §2.4).</p><p>• ILP-Joint: The full ILP from <ref type="bibr" target="#b44">Thadani and McKeown (2013)</ref>, which provides an upper bound on the performance of the proposed approximation strategies.</p><p>The learning rate schedule for the Lagrangian re- laxation approaches was set as η i τ /(τ + i), 10 while the hyperparameter ψ was tuned using the    <ref type="table" target="#tab_1">Tables 1 and 2</ref> summarize the results from our compression experiments on the BN and NW cor- pora respectively. Starting with the n-gram ap- proaches, the performance of 3-LM leads us to observe that the gains of supervised learning far outweigh the utility of higher-order n-gram factor- ization, which is also responsible for a significant increase in wall-clock time. In contrast, DP is an order of magnitude faster than all other approaches studied here although it is not competitive under parse-based measures such as RASP F 1 % which is known to correlate with human judgments of grammaticality ( <ref type="bibr" target="#b4">Clarke and Lapata, 2006</ref>). We were surprised by the strong performance of the dependency-based inference techniques, which yielded results that approached the joint model in both n-gram and parse-based measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The exact ILP-Dep approach halves the run- time of ILP-Joint to produce compressions that have similar (although statistically distinguish- able) scores. Approximating dependency-based inference with LP→MST yields similar perfor- mance for a further halving of runtime; however, the performance of this approach is notably worse.</p><p>Turning to the joint approaches, the strong performance of ILP-Joint is expected; less so is the relatively high but yet practically reason- able runtime that it requires. We note, how- ever, that these ILPs are solved using a highly- optimized commercial-grade solver that can uti- lize all CPU cores 12 while our approximation approaches are implemented as single-processed Python code without significant effort toward op- timization. Comparing the two approximation strategies shows a clear performance advantage for DP+LP over DP+LP→MST: the latter ap- proach entails slower inference due to the over- head of running the Chu-Liu Edmonds algorithm at every dual update, and furthermore, the error in- troduced by approximating an integral solution re-sults in a significant decrease in dependency recall. In contrast, DP+LP directly optimizes the dual problem by using the relaxed dependency solution to update Lagrange multipliers and achieves the best performance on parse-based F 1 outside of the slower ILP approaches. Convergence rates also vary for these two techniques: DP+LP has a lower rate of empirical convergence (15% on BN and 4% on NW) when compared to DP+LP→MST (19% on BN and 6% on NW). <ref type="figure" target="#fig_5">Figure 3</ref> shows the effect of input sentence length on inference time and performance for ILP- Joint and DP+LP over the NW test corpus. <ref type="bibr">13</ref> The timing results reveal that the approximation strat- egy is consistently faster than the ILP solver. The variation in RASP F 1 % with input size indicates the viability of a hybrid approach which could bal- ance accuracy and speed by using ILP-Joint for smaller problems and DP+LP for larger ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Sentence compression is one of the better-studied text-to-text generation problems and has been ob- served to play a significant role in human summa- rization <ref type="bibr" target="#b22">(Jing, 2000;</ref><ref type="bibr" target="#b21">Jing and McKeown, 2000</ref>). Most approaches to sentence compression are su- pervised ( <ref type="bibr" target="#b24">Knight and Marcu, 2002;</ref><ref type="bibr" target="#b41">Riezler et al., 2003;</ref><ref type="bibr" target="#b45">Turner and Charniak, 2005;</ref><ref type="bibr" target="#b35">McDonald, 2006;</ref><ref type="bibr" target="#b46">Unno et al., 2006;</ref><ref type="bibr" target="#b17">Galley and McKeown, 2007;</ref><ref type="bibr" target="#b39">Nomoto, 2007;</ref><ref type="bibr" target="#b8">Cohn and Lapata, 2009;</ref><ref type="bibr" target="#b16">Galanis and Androutsopoulos, 2010;</ref><ref type="bibr" target="#b18">Ganitkevitch et al., 2011;</ref><ref type="bibr" target="#b37">Napoles et al., 2011a;</ref><ref type="bibr" target="#b14">Filippova and Altun, 2013</ref>) following the release of datasets such as the Ziff-Davis corpus <ref type="bibr" target="#b23">(Knight and Marcu, 2000</ref>) and the Edinburgh compression cor- pora ( <ref type="bibr" target="#b4">Clarke and Lapata, 2006;</ref><ref type="bibr" target="#b6">Clarke and Lapata, 2008</ref>), although unsupervised approaches- largely based on ILPs-have also received con- sideration <ref type="bibr" target="#b5">(Clarke and Lapata, 2007;</ref><ref type="bibr" target="#b6">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b15">Filippova and Strube, 2008)</ref>. Com- pression has also been used as a tool for document summarization <ref type="bibr" target="#b11">(Daumé and Marcu, 2002;</ref><ref type="bibr" target="#b48">Zajic et al., 2007;</ref><ref type="bibr" target="#b5">Clarke and Lapata, 2007;</ref><ref type="bibr" target="#b1">Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b47">Woodsend and Lapata, 2012;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013;</ref><ref type="bibr" target="#b36">Molina et al., 2013;</ref><ref type="bibr" target="#b29">Li et al., 2013;</ref><ref type="bibr" target="#b40">Qian and Liu, 2013)</ref>, with recent work formulating the summarization task as joint sentence extrac- tion and compression and often employing ILP or Lagrangian relaxation. Monolingual compression <ref type="bibr">13</ref> Similar results were observed for the BN test corpus. also faces many obstacles common to decoding in machine translation, and a number of approaches which have been proposed to combine phrasal and syntactic models <ref type="bibr" target="#b20">(Huang and Chiang, 2007;</ref><ref type="bibr" target="#b42">Rush and Collins, 2011</ref>) inter alia offer directions for future research into compression problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented approximate inference strate- gies to jointly compress sentences under bigram and dependency-factored objectives by exploiting the modularity of the task and considering the two subproblems in isolation. Experiments show that one of these approximation strategies produces re- sults comparable to a state-of-the-art integer linear program for the same joint inference task with a 60% reduction in average inference time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of the difficulty of recovering the maximum-weight subtree (B→C, B→D) from the maximum spanning tree (A→C, C→B, B→D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of commodity values for a valid solution of the non-relaxed ILP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>described in §2.3. 9 • LP→MST: An approximate inference ap- proach based on an LP relaxation of ILP- Dep. As discussed in §2.4, a maximum span- ning tree is recovered from the output of the LP and greedily pruned in order to generate a valid integral solution while observing the imposed compression rate. • ILP-Dep: A version of the joint ILP of Thadani and McKeown (2013) without n- gram variables and corresponding features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Inference</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of input size on (a) inference time, and (b) the corresponding difference in RASP F 1 % (ILP-Joint-DP+LP) on the NW corpus.</figDesc><graphic url="image-1.png" coords="9,304.02,62.80,217.70,202.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results for the NW corpus with all systems compressing to the size of the gold 
compression, yielding an average compression rate of 70.24%. In both tables, bold entries show signifi-
cant gains within a column under the paired t-test (p &lt; 0.05) and Wilcoxon's signed rank test (p &lt; 0.01). 

development split of each corpus. 11 

</table></figure>

			<note place="foot" n="1"> This is referred to as extractive compression by Cohn and Lapata (2008) &amp; Galanis and Androutsopoulos (2010) following the terminology used in document summarization.</note>

			<note place="foot" n="2"> Although Thadani and McKeown (2013) is not restricted to bigrams or order-preserving n-grams, we limit our discussion to this scenario as it also fits the assumptions of McDonald (2006) and the datasets of Clarke and Lapata (2006).</note>

			<note place="foot" n="3"> This work follows Thadani and McKeown (2013) in recovering non-projective trees for inference. However, recovering projective trees is tractable when a total ordering of output tokens is assumed. This will be addressed in future work.</note>

			<note place="foot" n="4"> Heuristic approaches (Komodakis et al., 2007; Rush et al., 2010), tightening (Rush and Collins, 2011) or branch and bound (Das et al., 2012) can still be used to retrieve optimal solutions, but we did not explore these strategies here.</note>

			<note place="foot" n="5"> A detailed description of the Chu-Liu Edmonds algorithm for MSTs is available in McDonald et al. (2005).</note>

			<note place="foot" n="6"> We used a minibatch size of 4 in all experiments. 7 http://nlp.stanford.edu/software/ 8 http://www.gurobi.com</note>

			<note place="foot" n="9"> For consistent comparisons with the other systems, our reimplementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. 10 τ was set to 100 for aggressive subgradient updates.</note>

			<note place="foot" n="11"> We were surprised to observe that performance improved significantly when ψ was set closer to 1, thereby emphasizing token features in the dependency subproblem. The final values chosen were ψBN = 0.9 and ψNW = 0.8.</note>

			<note place="foot" n="12"> 16 cores in our experimental environment.</note>

			<note place="foot" n="14"> The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author is grateful to Alexander Rush for help-ful discussions and to the anonymous reviewers for their comments. This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior Na-tional Business Center (DoI/NBC) contract num-ber D11PC20153. The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernmental purposes notwithstanding any copy-right annotation thereon. <ref type="bibr">14</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and robust compressive summarization with dual decomposition and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The second release of the RASP system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-COLING Interactive Presentation Sessions</title>
		<meeting>the ACL-COLING Interactive Presentation Sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Models for sentence compression: a comparison across domains, training requirements and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-COLING</title>
		<meeting>ACL-COLING</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelling compression with discourse constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: an integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal for Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentence compression beyond word deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sentence compression as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="637" to="674" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An exact dual decomposition algorithm for shallow semantic parsing with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), SemEval &apos;12</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics (*SEM), SemEval &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A noisychannel model for document compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modelbased aligner combination using dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edmonds</surname></persName>
		</author>
		<title level="m">Optimum branchings. Journal of Research of the National Bureau of Standards</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dependency tree based sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG</title>
		<meeting>INLG</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An extractive supervised two-stage method for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="885" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lexicalized Markov grammars for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="180" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1168" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech summarization: an approach through word extraction and a method for evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadaoki</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cut and paste based text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentence reduction for automatic text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Applied Natural Language Processing</title>
		<meeting>the Conference on Applied Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="310" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: a probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MRF optimization via dual decomposition: Message-passing revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual decomposition for parsing with non-projective head automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Structured learning with approximate inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<editor>John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis</editor>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding a length-constrained maximum-sum or maximum-density subtree and its application to logistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><forename type="middle">Hieu</forename><surname>Hoong Chuin Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><forename type="middle">Nguyen</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Optimization</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="391" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="490" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optimal trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Magnanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolsey</surname></persName>
		</author>
		<idno>290-94</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Summarization with a joint model for sentence extraction and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing</title>
		<meeting>the Workshop on Integer Linear Programming for Natural Langauge Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Concise integer linear programming formulations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual decomposition with many overlapping components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="238" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-HLT</title>
		<meeting>EMNLP-HLT</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discursive sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><forename type="middle">Eugenio</forename><surname>Iria Da Cunha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sierra Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">7817</biblScope>
			<biblScope unit="page" from="394" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Paraphrastic sentence compression with a character-based metric: tightening without deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Monolingual Text-To-Text Generation</title>
		<meeting>the Workshop on Monolingual Text-To-Text Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating sentence compression: pitfalls and suggested remedies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Monolingual Text-ToText Generation</title>
		<meeting>the Workshop on Monolingual Text-ToText Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Discriminative sentence compression with conditional random fields. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-11" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1571" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast joint compression and summarization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1492" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><forename type="middle">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exact decoding of syntactic translation models through Lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="72" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On dual decomposition and linear programming relaxations for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sentence compression with joint structural inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised learning for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenine</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Trimming CFG parse trees for sentence compression using machine learning approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-COLING</title>
		<meeting>ACL-COLING</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="850" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple aspect summarization using integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-11" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1549" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Minibatch and parallelization for online large margin structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
