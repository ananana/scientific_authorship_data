<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POS induction with distributional and morphological information using a distance-dependent Chinese restaurant process</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Sirts</surname></persName>
							<email>sirts@ioc.ee</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Cybernetics at</orgName>
								<orgName type="department" key="dep2">School of Interactive Computing Georgia Institute of Technology</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">School of Informatics</orgName>
								<orgName type="institution" key="instit1">Tallinn University of Technology</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
							<email>jacobe@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Cybernetics at</orgName>
								<orgName type="department" key="dep2">School of Interactive Computing Georgia Institute of Technology</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">School of Informatics</orgName>
								<orgName type="institution" key="instit1">Tallinn University of Technology</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
							<email>melsner0@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Cybernetics at</orgName>
								<orgName type="department" key="dep2">School of Interactive Computing Georgia Institute of Technology</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">School of Informatics</orgName>
								<orgName type="institution" key="instit1">Tallinn University of Technology</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Cybernetics at</orgName>
								<orgName type="department" key="dep2">School of Interactive Computing Georgia Institute of Technology</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="department" key="dep4">School of Informatics</orgName>
								<orgName type="institution" key="instit1">Tallinn University of Technology</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">POS induction with distributional and morphological information using a distance-dependent Chinese restaurant process</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="265" to="271"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new approach to inducing the syntactic categories of words, combining their distributional and morphological properties in a joint nonparametric Bayesian model based on the distance-dependent Chinese Restaurant Process. The prior distribution over word clusterings uses a log-linear model of morphological similarity ; the likelihood function is the probability of generating vector word embeddings. The weights of the morphology model are learned jointly while inducing part-of-speech clusters, encouraging them to co-here with the distributional features. The resulting algorithm outperforms competitive alternatives on English POS induction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The morphosyntactic function of words is reflected in two ways: their distributional properties, and their morphological structure. Each information source has its own advantages and disadvantages. Distributional similarity varies smoothly with syn- tactic function, so that words with similar syntactic functions should have similar distributional proper- ties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears.</p><p>These observations suggest that a general ap- proach to the induction of syntactic categories should leverage both distributional and morpho- logical features <ref type="bibr" target="#b8">(Clark, 2003;</ref><ref type="bibr" target="#b6">Christodoulopoulos et al., 2010)</ref>. But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demon- strated the utility of continuous vector represen- tations, or "embeddings" ( <ref type="bibr" target="#b15">Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Luong et al., 2013;</ref><ref type="bibr" target="#b12">Kim and de Marneffe, 2013;</ref><ref type="bibr" target="#b23">Turian et al., 2010)</ref>. In contrast, morphology is often represented in terms of sparse, discrete fea- tures (such as morphemes), or via pairwise mea- sures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity.</p><p>In this paper we present a new approach for in- ducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant pro- cess (ddCRP; <ref type="bibr" target="#b4">Blei and Frazier, 2011</ref>). In the dd- CRP, each data point (word type) selects another point to "follow"; this chain of following links corresponds to a partition of the data points into clusters. The probability of word w 1 following w 2 depends on two factors: 1) the distributional simi- larity between all words in the proposed partition containing w 1 and w 2 , which is encoded using a Gaussian likelihood function over the word embed- dings; and 2) the morphological similarity between w 1 and w 2 , which acts as a prior distribution on the induced clustering. We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning.</p><p>We apply our model to the English section of the the Multext-East corpus <ref type="bibr" target="#b10">(Erjavec, 2004</ref>) in or- der to evaluate both against the coarse-grained and fine-grained tags, where the fine-grained tags en- code detailed morphological classes. We find that our model effectively combines morphological fea- tures with distributional similarity, outperforming comparable alternative approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Unsupervised POS tagging has a long history in NLP. This paper focuses on the POS induction problem (i.e., no tag dictionary is available), and here we limit our discussion to very recent sys- tems. A review and comparison of older systems is provided by <ref type="bibr" target="#b6">Christodoulopoulos et al. (2010)</ref>, who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morpho- logical and distributional information yields the best results, especially cross-linguistically <ref type="bibr" target="#b8">(Clark, 2003;</ref><ref type="bibr" target="#b2">Berg-Kirkpatrick et al., 2010)</ref>. Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain pro- totypes for clusters ( <ref type="bibr" target="#b0">Abend et al., 2010)</ref>, or as features in a generative model ( <ref type="bibr" target="#b13">Lee et al., 2010;</ref><ref type="bibr" target="#b7">Christodoulopoulos et al., 2011;</ref><ref type="bibr" target="#b20">Sirts and Alumäe, 2012)</ref>, or a representation-learning algorithm <ref type="bibr" target="#b26">(Yatbaz et al., 2012)</ref>. Several of these systems use a small fixed set of orthographic and/or suffix fea- tures, sometimes obtained from an unsupervised morphological segmentation system ( <ref type="bibr" target="#b0">Abend et al., 2010;</ref><ref type="bibr" target="#b13">Lee et al., 2010;</ref><ref type="bibr" target="#b7">Christodoulopoulos et al., 2011;</ref><ref type="bibr" target="#b26">Yatbaz et al., 2012)</ref>. <ref type="bibr" target="#b5">Blunsom and Cohn's (2011)</ref> model learns an n-gram character model over the words in each cluster; we learn a log- linear model, which can incorporate arbitrary fea- tures. <ref type="bibr" target="#b2">Berg-Kirkpatrick et al. (2010)</ref> also include a log-linear model of morphology in POS induc- tion, but they use morphology in the likelihood term of a parametric sequence model, thereby en- couraging all elements that share a tag to have the same morphological features. In contrast, we use pairwise morphological similarity as a prior in a non-parametric clustering model. This means that the membership of a word in a cluster requires only morphological similarity to some other element in the cluster, not to the cluster centroid; which may be more appropriate for languages with multiple morphological paradigms. Another difference is that our non-parametric formulation makes it un- necessary to know the number of tags in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distance-dependent CRP</head><p>The ddCRP ( <ref type="bibr" target="#b4">Blei and Frazier, 2011</ref>) is an extension of the CRP; like the CRP, it defines a distribution over partitions ("table assignments") of data points ("customers"). Whereas in the regular CRP each customer chooses a table with probability propor- tional to the number of customers already sitting there, in the ddCRP each customer chooses another customer to follow, and sits at the same table with that customer. By identifying the connected compo- nents in this graph, the ddCRP equivalently defines a prior over clusterings.</p><p>If c i is the index of the customer followed by customer i, then the ddCRP prior can be written</p><formula xml:id="formula_0">P (c i = j) ∝ f (d ij ) if i = j α if i = j,<label>(1)</label></formula><p>where d ij is the distance between customers i and j and f is a decay function. A ddCRP is sequential if customers can only follow previous customers, i.e., d ij = ∞ when i &gt; j and f (∞) = 0. In this case, if d ij = 1 for all i &lt; j then the ddCRP reduces to the CRP. Separating the distance and decay function makes sense for "natural" distances (e.g., the num- ber of words between word i and j in a document, or the time between two events), but they can also be collapsed into a single similarity function. We wish to assign higher similarities to pairs of words that share meaningful suffixes. Because we do not know which suffixes are meaningful a priori, we use a maximum entropy model whose features in- clude all suffixes up to length three that are shared by at least one pair of words. Our prior is then:</p><formula xml:id="formula_1">P (c i = j|w, α) ∝ e w T g(i,j) if i = j α if i = j,<label>(2)</label></formula><p>where g s (i, j) is 1 if suffix s is shared by ith and jth words, and 0 otherwise. We can create an infinite mixture model by com- bining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments. Since we are using continuous-valued vectors (word embeddings) to represent the distri- butional characteristics of words, we use a multi- variate Gaussian likelihood. We will marginalize over the mean µ and covariance Σ of each clus- ter, which in turn are drawn from Gaussian and inverse-Wishart (IW) priors respectively:</p><formula xml:id="formula_2">Σ ∼ IW (ν 0 , Λ 0 ) µ ∼ N (µ 0 , Σ / κ 0 ) (3)</formula><p>The full model is then:</p><formula xml:id="formula_3">P (X,c, µ, Σ|Θ, w, α) (4) = K k=1 P (Σ k |Θ)p(µ k |Σ k , Θ) × n i=1 (P (c i |w, α)P (x i |µ z i , Σ z i )),</formula><p>where Θ are the hyperparameters for (µ, Σ) and z i is the (implicit) cluster assignment of the ith word x i . With a CRP prior, this model would be an infi- nite Gaussian mixture model <ref type="bibr">(IGMM;</ref><ref type="bibr" target="#b17">Rasmussen, 2000</ref>), and we will use the IGMM as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head><p>The Gibbs sampler for the ddCRP integrates over the Gaussian parameters, sampling only follower variables. At each step, the follower link c i for a single customer i is sampled, which can implicitly shift the entire block of n customers fol(i) who fol- low i into a new cluster. Since we marginalize over the cluster parameters, computing P (c i = j) re- quires computing the likelihood P (fol(i), X j |Θ), where X j are the k customers already clustered with j. However, if we do not merge fol(i) with X j , then we have P (X j |Θ) in the overall joint probability. Therefore, we can decompose P (fol(i), X j |Θ) = P (fol(i)|X j , Θ)P (X j |Θ) and need only compute the change in likelihood due to merging in fol(i): 1 :</p><formula xml:id="formula_4">P (fol(i)|X j , Θ) = π −nd/2 κ d/2 k |Λ k | ν k /2 κ d/2 n+k |Λ n+k | ν n+k /2 × d i=1 Γ ν n+k +1−i 2 Γ ν k +1−i 2 ,<label>(5)</label></formula><p>where the hyperparameters are updated as κ n = κ 0 + n, ν n = ν 0 + n, and</p><formula xml:id="formula_5">µ n = κ 0 µ 0 + ¯ x κ 0 + n<label>(6)</label></formula><formula xml:id="formula_6">Λ n = Λ 0 + Q + κ 0 µ 0 µ 0 T − κ n µ n µ T n ,<label>(7)</label></formula><p>where Q = n i=1 x i x T i . Combining this likelihood term with the prior, the probability of customer i following j is Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by <ref type="bibr" target="#b21">Socher et al. (2011)</ref>. Also, the block of customers being moved around can po- tentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results- although we report results without this exponent as well. This technique was also used by <ref type="bibr" target="#b22">Titov and Klementiev (2012)</ref> and <ref type="bibr" target="#b9">Elsner et al. (2012)</ref>.</p><formula xml:id="formula_7">P (c i = j|X , Θ, w, α) ∝ P (fol(i)|X j , Θ)P (c i = j|w, α).<label>(</label></formula><p>Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior <ref type="bibr" target="#b22">(Titov and Klementiev, 2012)</ref>. We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization ( <ref type="bibr" target="#b25">Wei and Tanner, 1990</ref>). We do not apply the exponentiation parame- ter a when training the weights because this proce- dure affects the follower structure only, and we do not have to worry about the magnitude of the like- lihood. Before the first iteration we initialize the follower structure: for each word, we choose ran- domly a word to follow from amongst those with the longest shared suffix of up to 3 characters. The number of clusters starts around 750, but decreases substantially after the first sampling iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Data For our experiments we used the English word embeddings from the Polyglot project (Al- Rfou' et al., 2013) 2 , which provides embeddings trained on Wikipedia texts for 100,000 of the most frequent words in many languages.</p><p>We evaluate on the English part of the Multext- East (MTE) corpus <ref type="bibr" target="#b10">(Erjavec, 2004)</ref>, which provides both coarse-grained and fine-grained POS labels for the text of Orwell's "1984". Coarse labels con- sist of 11 main word classes, while the fine-grained tags (104 for English) are sequences of detailed morphological attributes. Some of these attributes are not well-attested in English (e.g. gender) and some are mostly distinguishable via semantic anal- ysis (e.g. 1st and 2nd person verbs). Many tags are assigned only to one or a few words. Scores for the fine-grained tags will be lower for these reasons, but we argue below that they are still informative.</p><p>Since take the intersection of these two sets for training and evaluation. <ref type="table">Table 1</ref> shows corpus statistics.</p><p>Evaluation With a few exceptions <ref type="bibr" target="#b3">(Biemann, 2006</ref>; Van Gael et al., 2009), POS induction sys- tems normally require the user to specify the num- ber of desired clusters, and the systems are evalu- ated with that number set to the number of tags in the gold standard. For corpora such as MTE with both fine-grained and coarse-grained tages, pre- vious evaluations have scored against the coarse- grained tags. Though coarse-grained tags have their place <ref type="bibr" target="#b16">(Petrov et al., 2012)</ref>, in many cases the distributional and morphological distinctions between words are more closely aligned with the fine-grained tagsets, which typically distinguish between verb tenses, noun number and gender, and adjectival scale (comparative, superlative, etc.), so we feel that the evaluation against fine-grained tagset is more relevant here. For better comparison with previous work, we also evaluate against the coarse-grained tags; however, these numbers are not strictly comparable to other scores reported on MTE because we are only able to train and evalu- ate on the subset of words that also have Polyglot embeddings. To provide some measure of the dif- ficulty of the task, we report baseline scores using K-means clustering, which is relatively strong base- line in this task <ref type="bibr" target="#b7">(Christodoulopoulos et al., 2011</ref>).</p><p>There are several measures commonly used for unsupervised POS induction. We report greedy one-to-one mapping accuracy (1-1) <ref type="bibr" target="#b11">(Haghighi and Klein, 2006</ref>) and the information-theoretic score V- measure (V-m), which also varies from 0 to 100% <ref type="bibr" target="#b19">(Rosenberg and Hirschberg, 2007)</ref>. In previous work it has been common to also report many-to- one (m-1) mapping but this measure is particularly sensitive to the number of induced clusters (more clusters yield higher scores), which is variable for our models. V-m can be somewhat sensitive to the number of clusters <ref type="bibr" target="#b18">(Reichart and Rappoport, 2009</ref>) but much less so than m-1 <ref type="bibr" target="#b6">(Christodoulopoulos et al., 2010)</ref>. With different number of induced and gold standard clusters the 1-1 measure suffers because some induced clusters cannot be mapped to gold clusters or vice versa. However, almost half the gold standard clusters in MTE contain just a few words and we do not expect our model to be able to learn them anyway, so the 1-1 measure is still useful for telling us how well the model learns the bigger and more distinguishable classes.</p><p>In unsupervised POS induction it is standard to report accuracy on tokens even when the model it- self works on types. Here we report also type-based measures because these can reveal differences in model behavior even when token-based measures are similar.</p><p>Experimental setup For baselines we use K- means and the IGMM, which both only learn from the word embeddings. The CRP prior in the IGMM has one hyperparameter (the concentration param- eter α); we report results for α = 5 and 20. Both the IGMM and ddCRP have four hyperparameters controlling the prior over the Gaussian cluster pa- rameters: Λ 0 , µ 0 , ν 0 and κ 0 . We set the prior scale matrix Λ 0 by using the average covariance from a K-means run with K = 200. When setting the average covariance as the expected value of the IW distribution the suitable scale matrix can be com- puted as</p><formula xml:id="formula_8">Λ 0 = E [X] (ν 0 − d − 1)</formula><p>, where ν 0 is the prior degrees of freedom (which we set to d + 10) and d is the data dimensionality (64 for the Poly- glot embeddings). We set the prior mean µ 0 equal to the sample mean of the data and κ 0 to 0.01.</p><p>We experiment with three different priors for the ddCRP model. All our ddCRP models are non- sequential <ref type="bibr" target="#b21">(Socher et al., 2011)</ref>, allowing cycles to be formed. The simplest model, ddCRP uni- form, uses a uniform prior that sets the distance between any two words equal to one. <ref type="bibr">3</ref> The second model, ddCRP learned, uses the log-linear prior with weights learned between each two Gibbs iter- ations as explained in section 4. The final model, ddCRP exp, adds the prior exponentiation. The α parameter for the ddCRP is set to 1 in all experi- ments. For ddCRP exp, we report results with the exponent a set to 5.  <ref type="table" target="#tab_2">Table 2</ref>: Results of baseline and ddCRP models evaluated on word types and tokens using fine-grained tags, and on tokens using coarse-grained tags. For each model we present the number of induced clusters K (or fixed K for K-means) and 1-1 / V-m scores. The second column under each evaluation setting gives the scores for K-means with K equal to the number of clusters induced by the model in that row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head><p>with different random initializations. For each eval- uation setting we provide two sets of scores-first are the 1-1 and V-m scores for the given model, second are the comparable scores for K-means run with the same number of clusters as induced by the non-parametric model. These results show that all non-parametric mod- els perform better than K-means, which is a strong baseline in this task <ref type="bibr" target="#b7">(Christodoulopoulos et al., 2011</ref>). The poor performace of K-means can be explained by the fact that it tends to find clusters of relatively equal size, although the POS clus- ters are rarely of similar size. The common noun singular class is by far the largest in English, con- taining roughly a quarter of the word types. Non- parametric models are able to produce cluster of different sizes when the evidence indicates so, and this is clearly the case here.</p><p>From the token-based evaluation it is hard to say which IGMM hyperparameter value is better even though the number of clusters induced differs by a factor of 2. The type-base evaluation, how- ever, clearly prefers the smaller value with fewer clusters. Similar effects can be seen when com- paring IGMM and ddCRP uniform. We expected these two models perform on the same level, and their token-based scores are similar, but on the type- based evaluation the ddCRP is clearly superior. The difference could be due to the non-sequentiality, or becuase the samplers are different-IGMM en- abling resampling only one item at a time, ddCRP performing blocked sampling.</p><p>Further we can see that the ddCRP uniform and learned perform roughly the same. Although the prior in those models is different they work mainly using the the likelihood. The ddCRP with learned prior does produce nice follower structures within each cluster but the prior is in general too weak compared to the likelihood to influence the cluster- ing decisions. Exponentiating the prior reduces the number of induced clusters and improves results, as it can change the cluster assignment for some words where the likelihood strongly prefers one cluster but the prior clearly indicates another.</p><p>The last column shows the token-based evalua- tion against the coarse-grained tagset. This is the most common evaluation framework used previ- ously in the literature. Although our scores are not directly comparable with the previous results, our V-m scores are similar to the best published 60.5 ( <ref type="bibr" target="#b6">Christodoulopoulos et al., 2010</ref>) and <ref type="bibr">66.7 (Sirts and Alumäe, 2012)</ref>.</p><p>In preliminary experiments, we found that di- rectly applying the best-performing English model to other languages is not effective. Different lan- guages may require different parametrizations of the model. Further study is also needed to verify that word embeddings effectively capture syntax across languages, and to determine the amount of unlabeled text necessary to learn good embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper demonstrates that morphology and dis- tributional features can be combined in a flexi- ble, joint probabilistic model, using the distance- dependent Chinese Restaurant Process. A key ad- vantage of this framework is the ability to include arbitrary features in the prior distribution. Future work may exploit this advantage more thoroughly: for example, by using features that incorporate prior knowledge of the language's morphological structure. Another important goal is the evaluation of this method on languages beyond English.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Wikipedia and MTE are from different domains their lexicons do not fully overlap; we Wikipedia tokens 1843M Multext-East tokens 118K Multext-East types 9193 Multext-East &amp; Wiki types 7540 Table 1: Statistics for the English Polyglot word embeddings and English part of MTE: number of Wikipedia tokens used to train the embeddings, number of tokens/types in MTE, and number of types shared by both datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 presents all re- sults. Each number is an average of 5 experiments</head><label>2</label><figDesc></figDesc><table>Fine types 

Fine tokens 
Coarse tokens 

Model 
K 
Model 
K-means 
Model 
K-means 
Model 
K-means 

K-means 
104 or 11 16.1 / 47.3 -
39.2 / 62.0 -
44.4 / 45.5 -
IGMM, α = 5 
55.6 
41.0 / 45.9 23.1 / 49.5 48.0 / 64.8 37.2 / 61.0 48.3 / 58.3 40.8 / 55.0 
IGMM, α = 20 
121.2 
35.0 / 47.1 14.7 / 46.9 50.6 / 67.8 44.7 / 65.5 48.7 / 60.0 48.3 / 57.9 
ddCRP uniform 
80.4 
50.5 / 52.9 18.6 / 48.2 52.4 / 68.7 35.1 / 60.3 52.1 / 62.2 40.3 / 54.2 
ddCRP learned 
89.6 
50.1 / 55.1 17.6 / 48.0 51.1 / 69.7 39.0 / 63.2 48.9 / 62.0 41.1 / 55.1 
ddCRP exp, a = 5 47.2 
64.0 / 60.3 25.0 / 50.3 55.1 / 66.4 33.0 / 59.1 47.8 / 55.1 36.9 / 53.1 

</table></figure>

			<note place="foot" n="2"> https://sites.google.com/site/rmyeid/ projects/polyglot</note>

			<note place="foot" n="3"> In the sequential case this model would be equivalent to the IGMM (Blei and Frazier, 2011). Due to the nonsequentiality this equivalence does not hold, but we do expect to see similar results to the IGMM.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved unsupervised pos induction through prototype discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Annual Conference on Natural Language Learning</title>
		<meeting>the Thirteenth Annual Conference on Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">B</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging employing efficient graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distance dependent chinese restaurant processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2488" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hierarchical pitman-yor process hmm for unsupervised part of speech induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="865" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two decades of unsupervised POS induction: How far have we come?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Bayesian mixture model for part-of-speech induction using multiple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining distributional and morphological information for part of speech induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European chapter of the ACL</title>
		<meeting>the European chapter of the ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrapping a unified model of lexical and phonetic acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MULTEXT-East version 3: Multilingual morphosyntactic specifications, lexicons and corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaž</forename><surname>Erjavec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prototype-driven learning for sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deriving adjectival scales from continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple type-level unsupervised pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Yoong Keok Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="853" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Annual Conference on Natural Language Learning</title>
		<meeting>the Thirteenth Annual Conference on Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The infinite Gaussian mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The nvi clustering evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Annual Conference on Natural Language Learning</title>
		<meeting>the Ninth Annual Conference on Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">V-measure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="410" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hierarchical Dirichlet process model for joint part-of-speech and morphology induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Alumäe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="407" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectral chinese restaurant processes: Nonparametric clustering based on similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fifteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="698" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised semantic role induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The infinite HMM for unsupervised PoS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Van Gael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A monte carlo implementation of the em algorithm and the poor man&apos;s data augmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">411</biblScope>
			<biblScope unit="page" from="699" to="704" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning syntactic categories using paradigmatic representations of word context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Mehmet Ali Yatbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="940" to="951" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
