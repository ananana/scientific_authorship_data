<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Neural Networks for Syntactic Parsing of Morphologically Rich Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Legrand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ecole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Neural Networks for Syntactic Parsing of Morphologically Rich Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="573" to="578"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Morphologically rich languages (MRL) are languages in which much of the structural information is contained at the word-level, leading to high level word-form variation. Historically, syntactic parsing has been mainly tackled using genera-tive models. These models assume input features to be conditionally independent, making difficult to incorporate arbitrary features. In this paper, we investigate the greedy discriminative parser described in (Legrand and Collobert, 2015), which relies on word embeddings, in the context of MRL. We propose to learn morphological embeddings and propagate morphological information through the tree using a recur-sive composition procedure. Experiments show that such embeddings can dramatically improve the average performance on different languages. Moreover, it yields state-of-the art performance for a majority of languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphologically rich languages (MRL) are lan- guages for which important information concern- ing the syntactic structure is expressed through word formation, rather than constituent-order pat- terns. Unlike English, they can have complex word structure as well as flexible word order. A common practice when dealing with such lan- guages is to incorporate morphological informa- tion explicitly ( <ref type="bibr">Tsarfaty et al., 2013)</ref>. However this poses two problems to the classical generative models: they assume input features to be condi- tionally independent which makes the incorpora- * All research was conducted at the Idiap Research Insti- tute, before Ronan Collobert joined Facebook AI Research tion of arbitrary features difficult. Moreover, re- fining input features leads to a data sparsity issue.</p><p>In the other hand, neural network-based mod- els using continuous word representations as input have been able to overcome the data sparsity prob- lem inherent in NLP ( <ref type="bibr" target="#b6">Huang and Yates, 2009)</ref>. Furthermore, neural networks allow to incorporate arbitrary features and learn complex non-linear relations between them. <ref type="bibr">Legrand and Collobert (2015)</ref> introduced a greedy syntactic parser, based on neural networks which relies on word embed- dings. This model maintains a history of the previ- ous node predictions, in the form of vector repre- sentations, by leveraging a recursive composition procedure.</p><p>In this paper, we propose to enhance this model for syntactic parsing of MRL, by learning morpho- logical embeddings. We take advantage of a re- cursive composition procedure similar to the one used in ( <ref type="bibr">Legrand and Collobert, 2015)</ref> to propa- gate morphological information during the pars- ing process. We evaluate our approach on the SPMRL (Syntactic Parsing of MRL) Shared Task 2014 ( <ref type="bibr">Seddah et al., 2013</ref>) on nine different lan- guages. Each of them comes with a set of morpho- logical features allowing to augment words with information such as their grammatical functions, relation with other words in the sentence, prefixes, affixes and lemmas. We show that integrating mor- phological features allows to increase dramatically the average performance and yields state-of-the- art performance for a majority of languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Both the baseline (Berkeley parser) and the current state-of-the-art model on the SPMRL Shared Task <ref type="bibr" target="#b0">(Björkelund et al., 2014</ref>) rely on probabilistic context free grammar (PCFG)-based features. The latter uses a product of PCFG with latent annota- tion based models <ref type="bibr">(Petrov, 2010)</ref>, with a coarse-to-   fine decoding strategy. The output is then discrim- inatively reranked <ref type="bibr" target="#b1">(Charniak and Johnson, 2005)</ref> to select the best analysis. In contrast, the parser used in this paper constructs the parse tree in a greedy manner and relies only on word, POS tags and morphological embeddings. Several other papers have reported results for the SPMRL <ref type="bibr">Shared Task 2014</ref><ref type="bibr" target="#b5">. (Hall et al., 2014</ref>) introduced an approach where, instead of propa- gating contextual information from the leaves of the tree to internal nodes in order to refine the grammar, the structural complexity of the gram- mar is minimized. This is done by moving as much context as possible onto local surface fea- tures. This work was refined in <ref type="bibr" target="#b3">(Durrett and Klein, 2015)</ref>, taking advantage of continuous word rep- resentations. The system used in this paper also leverages words embeddings but has two major differences. First, it proceeds step-by-step in a greedy manner <ref type="bibr" target="#b3">(Durrett and Klein, 2015</ref>) by using structured inference (CKY). Second, it leverages a compositional node feature which propagates in- formation from the leaves to internal nodes, which is exactly what is claimed not to be done.</p><p>( <ref type="bibr" target="#b4">Fernández-González and Martins, 2015</ref>) pro- posed a procedure to turn a dependency tree into a constituency tree. They showed that encoding order information in the dependency tree make it isomorphic to the constituent tree, allowing any dependency parser to produce constituents. Like the parser we used, their parser do not need to binarize the treebank as most of the others con- stituency parsers. Unlike this system, we do not use the dependency structure as an intermediate representation and directly perform constituency parsing over raw words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent greedy parsing</head><p>In this paper, we used the model presented in ( <ref type="bibr">Legrand and Collobert, 2015)</ref>. It is a NN-based model which performs parsing in a greedy recur- rent way. It follows a bottom-up iterative pro- cedure: the tree is built starting from the termi- nal nodes (sentence words), as shown in <ref type="figure">Figure 1</ref>. Each step can be seen as a sequence tagging task. A BIOES 1 prefixing scheme is used to rewrite this chunk (here node) prediction problem into a word tagging problem. Each iteration of the procedure merges input constituents into new nodes by ap- plying the following steps:</p><p>• Node tagger: a neural network sliding win- dow is applied over the input sequence of constituents (leaves or heads of trees pre- dicted so far). This procedure (see <ref type="figure" target="#fig_3">Figure  2</ref>) outputs for each constituent a score s i for each BIOES-prefixed parsing tag t ∈ T (T being the parsing tags ensemble).</p><p>• Dynamic programming: a coherent path of BIOES tags is retrieved by decoding over a constrained graph. This insures (for instance) that a B-A can be followed only by a I-A or a E-A (for all parsing tag A).</p><p>• Compositional procedure: new nodes are created, merging input constituents, accord- ing to the dynamic programming predictions. A neural network composition module is then used to compute vector representations for the new nodes, according to the representa- tions of the merged constituents, as well as their corresponding tags (POS or parsing).</p><p>The procedure ends when the top node is pro- duced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parsing Morphologically Rich Languages</head><formula xml:id="formula_0">X i−2 X i−1 X i X i+1 X i+2 . . . . . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional features</head><p>Word embeddings  A standard two-layers neural network outputs a score s i for each BIOES-prefixed parsing tag. Ad- ditional features can be easily fed to the network. Each category is assigned a new lookup table con- taining a vector of feature for every possible tag.</p><formula xml:id="formula_1">. . POS tags Concat h(M 1 × .) M 2 × .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Morphological features</head><p>Morphological features enable the augmentation of input tokens with information expressed at a word level, such as grammatical function or rela- tion to other words. For parsing MRL, they have proven to be very helpful <ref type="bibr" target="#b2">(Cowan and Collins, 2005</ref>). The SMPRL corpus provides a different set of morphological features associated to the Figure 3: Recursive composition of the morpho- logical feature gender (male (m) / female (f) / not applicable (n/a)). C gen i are the correspond- ing composition modules. The representation g 2 is first computed using the 3-inputs module C gen 3 . g 4 is obtained by using the 2-inputs module C gen 2 .</p><p>tree terminals (tokens) for every language. These features include morphosyntactic features such as case, number, gender, person and type, as well as specific morphological information such as verbal mood, proper/common noun distinction, lemma, grammatical function. They also include many language-specific features. For more details about the morphological features available, the reader can refer to (Seddah et al., 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Morphological Embeddings</head><p>The parser from ( <ref type="bibr">Legrand and Collobert, 2015)</ref> re- lies only on word and tag embeddings. Besides these features, our model takes advantage of ad- ditional morphological features. As illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>, each additional feature m is assigned a different lookup table containing morphological feature vectors of size d m . The output vectors of the different morphological lookup-tables are sim- ply concatenated to form the input of the next neu- ral network layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Morphological composition</head><p>Morphological features are available only for leaves. To propagate morphological information to the nodes, we take advantage of a composi- tion procedure similar to the one used in ( <ref type="bibr">Legrand and Collobert, 2015</ref>) for words and POS. As il- lustrated in <ref type="figure">Figure 3</ref>, every morphological feature m is assigned a set on composition modules C m i which take as input i morphological embeddings  <ref type="table">Table 1</ref>: Results for all languages in terms of F1-score, using gold POS and morphological tags. Berke- ley+POS and Berkeley RAW are the two baseline system results provided by the organizers of the shared task. Our experiments used an ensemble of 5 models, trained starting from different random initializa- tions.</p><p>of dimension d m . Each composition module per- form a matrix-vector operation followed by a non- linearity</p><formula xml:id="formula_2">C m i (x) = h(M i m .x)</formula><p>where M i m ∈ R dm×idm is a matrix of parame- ters to be trained and h a pointwise non-linearity function. x = [x 1 ...x i ] is the concatenation of the corresponding input morphological embeddings. Note that given a morphological feature we have a different matrix of weight for every possible size i. In practice most tree nodes do not merge more than a few constituents and we only consider com- position sizes &lt; 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus</head><p>Experiments were conducted on the SPMRL cor- pus provided for the Shared Task 2014 <ref type="bibr">(Seddah et al., 2013)</ref>. It provides sentences and tree anno- tations for 9 different languages (Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish) coming from various sources. For each language, gold part-of-speech and mor- phological tags are provided. Results for two base- line baseline system are provided in order to eval- uate our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>The model was trained using a stochastic gradient descent over the available training data. Hyper- parameters were tuned on the provided validation sets. The word embedding size and POS/parsing tag size were set to D W = 100 and D T = 30, re- spectively. The morphological tag embedding size was set to 10. The window size of the tagger was set to K = 7 and its number of hidden units to 300. All parameters were initialized randomly (in- cluding the words embeddings). As suggested in <ref type="bibr">(Plaut and Hinton, 1987)</ref>, the learning rate was di- vided by the size of the input vector of each layer. We applied the same dropout regularization as in ( <ref type="bibr">Legrand and Collobert, 2015</ref>). <ref type="table">Table 2</ref> presents the influence of adding morpho- logical features to the model. We observe signif- icant improvement for every languages except for Hebrew. On average, morphological features al- lowed to overcome the original model by 2 F1- score.  <ref type="table">Table 2</ref>: Influence of the additional morphological embeddings in terms of F1-score <ref type="table">Table 1</ref> compares the performance in F1-score (obtained with the provided EVALB SPMRL tool) of different systems, using the provided gold POS and morphological features. We compare our re- sults with the two baselines provided with the task: (1) Berkeley parser with provided POS Tags (Berkeley+POS). (2) Berkeley Parser in raw mode where the parser do its own POS tagging (Berke- ley RAW). We also report the results of the current state-of-the art model for this task <ref type="bibr" target="#b0">(Björkelund et al., 2014</ref>  <ref type="bibr" target="#b4">Fernández and Martins, 2015)</ref> n/a 85.9 78.7 78.7 89.0 88.2 79.3 91.2 82.8 84. <ref type="bibr">2 (Björkelund et al., 2014)</ref> 81.3 87.9 81.8 81.3 89.5 91.8 84.3 87.5 84.0 85.5 Proposed approach 80.4 87.5 80.8 82.0 91.6 90.0 84.8 93.0 80.5 85.6 <ref type="table">Table 3</ref>: Results for all languages in terms of F1-score using predicted POS and morphological tags. Berkeley+POS and Berkeley RAW are the two baseline system results provided by the organizers of the shared task. Our experiments used an ensemble of 5 models, trained starting from different random initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>each iteration of the greedy parsing procedure, the BIOES-tag scores are averaged and the new node representations (words+POS and morpho- logical composition) are computed for each model by composing the sub-tree representations corre- sponding to the given model, using its own com- positional network. One can observe that the pro- posed model outperforms the best model by 1.1 F1-score on average. Moreover, it yields state-of- the art performance for 6 among the 9 available languages. Finally, <ref type="table">Table 3</ref> compares the performance of different systems for a more realistic parsing sce- nario where the gold POS and morphological tags are unknown. For these experiments, we use the same tags as in <ref type="bibr" target="#b0">(Björkelund et al., 2014</ref>) 2 obtained using the freely available tool <ref type="bibr">MarMoT (Mueller et al., 2013)</ref>. We compare our results with the same model as for the the gold tags experiences. Additionnaly, we compare our results with two recent models reporting results for the SPMRL Shared Task 2014. We see that the proposed model yields state-of-the art performance for 4 out of 9 available languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed to extend the parser introduced in ( <ref type="bibr">Legrand and Collobert, 2015)</ref> by learning morphological embeddings. We take ad- vantage of a recursive procedure to propagate mor- phological information through the tree during the parsing process. We showed that using the mor- phological embeddings boosts the F1-score and allows to outperform the current state-of-the-art model on the SPMRL Shared Task 2014 corpus. Moreover, our approach yields state-of-the art per- formance for a majority of languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Did</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>IW</head><label></label><figDesc>Figure 1: Greedy parsing algorithm (3 iterations), on the sentence "Did you hear the falling bombs ?". I W , I T and O stand for input words (or composed word representations R i ), input syntactic tags (parsing or part-of-speech) and output tags (parsing), respectively. The tree produced after 3 greedy iterations can be reconstructed as the following: (SQ (VBD Did) (NP (PRP you)) (VP (VB hear) (NP (DT the) (VBG falling) (NNS bombs))) (. ?)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A constituent X i (word or node previously predicted) is tagged by considering a fixed size context window of size K (here K = 5). The concatenated output of the compositional history and constituent tags is fed as input to the tagger. A standard two-layers neural network outputs a score s i for each BIOES-prefixed parsing tag. Additional features can be easily fed to the network. Each category is assigned a new lookup table containing a vector of feature for every possible tag.</figDesc></figure>

			<note place="foot" n="1"> (Begin, Intermediate, Other, End, Single)</note>

			<note place="foot" n="2"> The tags used are available here: http://cistern. cis.lmu.de/marmot/models/CURRENT/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknoledgments</head><p>This work was supported by NEC Laboratories America. We would like to thank Dimitri Palaz for our fruitful discussions and Marc Ferras for proof-reading this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introducing the ims-wrocław-szeged-cis entry at the SPMRL 2014 shared task: Reranking and morpho-syntax meet unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Cetinoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Falenska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of NonCanonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of NonCanonical Languages</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Wolfgang Seeker, and Zsolt Szántó</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coarseto-fine N-best parsing and MaxEnt discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Morphology and reranking for the statistical parsing of spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural CRF parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parsing as reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-González</forename><surname>Andr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Less grammar, more features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional representations for handling sparsity in supervised sequence-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
