<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Structured Natural Language Representations for Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Saraswat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
						</author>
						<title level="a" type="main">Learning Structured Natural Language Representations for Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="44" to="55"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1005</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their de-notations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEO-QUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing is the task of mapping natu- ral language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes se- mantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar <ref type="bibr" target="#b32">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b34">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b26">Wong and Mooney, 2006;</ref><ref type="bibr" target="#b8">Kwiatkowksi et al., 2010;</ref><ref type="bibr" target="#b14">Liang et al., 2011;</ref><ref type="bibr">Berant et al., 2013;</ref><ref type="bibr">Flanigan et al., 2014;</ref><ref type="bibr" target="#b17">Pasupat and Liang, 2015;</ref><ref type="bibr">Groschwitz et al., 2015)</ref>. Under the second ap- proach, the utterance is first parsed to an inter- mediate task-independent representation tied to a syntactic parser and then mapped to a grounded representation ( <ref type="bibr" target="#b9">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b19">Reddy et al., , 2014</ref><ref type="bibr" target="#b7">Krishnamurthy and Mitchell, 2015;</ref><ref type="bibr">Gardner and Krishnamurthy, 2017)</ref>. A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially en- ables the handling of unseen words and knowledge transfer across domains <ref type="bibr">(Bender et al., 2015)</ref>.</p><p>The successful application of encoder-decoder models ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014</ref>) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a se- quence transduction problem where an utterance is mapped to a target meaning representation in string format <ref type="bibr">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b1">Jia and Liang, 2016;</ref><ref type="bibr" target="#b5">Kočisk´Kočisk´y et al., 2016)</ref>. Such models still fall under the first approach, however, in con- trast to previous work ( <ref type="bibr" target="#b32">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b34">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b14">Liang et al., 2011</ref>) they reduce the need for domain-specific assump- tions, grammar learning, and more generally ex- tensive feature engineering. But this modeling flexibility comes at a cost since it is no longer pos- sible to interpret how meaning composition is per- formed. Such knowledge plays a critical role in understand modeling limitations so as to build bet- ter semantic parsers. Moreover, without any task- specific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets).</p><p>In this work, we propose a neural semantic parser that alleviates the aforementioned prob- lems. Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates. However, rather than using an external parser ( <ref type="bibr" target="#b19">Reddy et al., 2014</ref> or manually specified CCG grammars ( <ref type="bibr" target="#b9">Kwiatkowski et al., 2013)</ref>, we induce intermediate representa- tions in the form of predicate-argument structures from data. This is achieved with a transition-based approach which by design yields recursive seman- tic structures, avoiding the problem of generating ill-formed meaning representations. Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy <ref type="bibr" target="#b6">(Krishnamurthy and Mitchell, 2012;</ref><ref type="bibr">Cai and Yates, 2013;</ref><ref type="bibr">Berant et al., 2013;</ref><ref type="bibr">Berant and Liang, 2014</ref>), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomor- phic. <ref type="bibr">2</ref> As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding.</p><p>The whole network is trained end-to-end on natural language utterances paired with anno- tated logical forms or their denotations. We conduct experiments on four datasets, including GEOQUERY (which has logical forms; Zelle and Mooney 1996), SPADES <ref type="bibr">(Bisk et al., 2016)</ref>, WEB- QUESTIONS ( <ref type="bibr">Berant et al., 2013)</ref>, and GRAPH- QUESTIONS ( <ref type="bibr" target="#b22">Su et al., 2016</ref>) (which have deno- tations). Our semantic parser achieves the state of the art on SPADES and GRAPHQUESTIONS, while obtaining competitive results on GEOQUERY and WEBQUESTIONS. A side-product of our mod- eling framework is that the induced intermedi- ate representations can contribute to rationalizing neural predictions ( <ref type="bibr" target="#b11">Lei et al., 2016)</ref>. Specifically, they can shed light on the kinds of representations (especially predicates) useful for semantic pars- ing. Evaluation of the induced predicate-argument relations against syntax-based ones reveals that they are interpretable and meaningful compared to heuristic baselines, but they sometimes deviate from linguistic conventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Problem Formulation Let K denote a knowl- edge base or more generally a reasoning system, and x an utterance paired with a grounded mean- ing representation G or its denotation y. Our prob- lem is to learn a semantic parser that maps x to G via an intermediate ungrounded representation U . When G is executed against K, it outputs denota-  tion y.</p><p>Grounded Meaning Representation We repre- sent grounded meaning representations in FunQL ( <ref type="bibr" target="#b3">Kate et al., 2005</ref>) amongst many other alterna- tives such as lambda calculus <ref type="bibr" target="#b34">(Zettlemoyer and Collins, 2005</ref>), λ-DCS (Liang, 2013) or graph queries <ref type="bibr">(Holzschuher and Peinl, 2013;</ref><ref type="bibr">Harris et al., 2013)</ref>. FunQL is a variable-free query lan- guage, where each predicate is treated as a func- tion symbol that modifies an argument list. For example, the FunQL representation for the utter- ance which states do not border texas is:</p><formula xml:id="formula_0">answer(exclude(state(all), next to(texas)))</formula><p>where next to is a domain-specific binary predi- cate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states border- ing Texas) as its denotation. all is a special predi- cate that returns a collection of entities. exclude is a predicate that returns the difference between two input sets. An advantage of FunQL is that the resulting s-expression encodes semantic compositionality and derivation of the logical forms. This prop- erty makes FunQL logical forms convenient to be predicted with recurrent neural networks ( <ref type="bibr" target="#b25">Vinyals et al., 2015;</ref><ref type="bibr">Choe and Charniak, 2016;</ref>. However, FunQL is less expressive than lambda calculus, partially due to the elimination of variables. A more compact logical formulation which our method also applies to is λ-DCS <ref type="bibr" target="#b13">(Liang, 2013)</ref>. In the absence of anaphora and composite binary predicates, conversion algorithms exist be- tween FunQL and λ-DCS. However, we leave this to future work.</p><p>Ungrounded Meaning Representation We also use FunQL to express ungrounded meaning representations. The latter consist primarily of natural language predicates and domain-general predicates. Assuming for simplicity that domain- general predicates share the same vocabulary in ungrounded and grounded representations, the ungrounded representation for the example utterance is:</p><p>answer(exclude(states(all), border(texas))) where states and border are natural language pred- icates. In this work we consider five types of domain-general predicates illustrated in <ref type="table" target="#tab_1">Table 1</ref>. Notice that domain-general predicates are often implicit, or represent extra-sentential knowledge. For example, the predicate all in the above utter- ance represents all states in the domain which are not mentioned in the utterance but are critical for working out the utterance denotation. Finally, note that for certain domain-general predicates, it also makes sense to extract natural language rationales (e.g., not is indicative for exclude). But we do not find this helpful in experiments.</p><p>In this work we constrain ungrounded represen- tations to be structurally isomorphic to grounded ones. In order to derive the target logical forms, all we have to do is replacing predicates in the ungrounded representations with symbols in the knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modeling</head><p>In this section, we discuss our neural model which maps utterances to target logical forms. The se- mantic parsing task is decomposed in two stages: we first explain how an utterance is converted to an intermediate representation (Section 3.1), and then describe how it is grounded to a knowledge base (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating Ungrounded Representations</head><p>At this stage, utterances are mapped to interme- diate representations with a transition-based algo- rithm. In general, the transition system generates the representation by following a derivation tree (which contains a set of applied rules) and some canonical generation order (e.g., depth-first). For FunQL, a simple solution exists since the repre- sentation itself encodes the derivation. Consider again answer(exclude(states(all), border(texas))) which is tree structured. Each predicate (e.g., bor- der) can be visualized as a non-terminal node of the tree and each entity (e.g., texas) as a terminal. The predicate all is a special case which acts as a terminal directly. We can generate the tree with a top-down, depth first transition system reminis- cent of recurrent neural network grammars (RN- NGs; ). Similar to RNNG, our algorithm uses a buffer to store input tokens in the utterance and a stack to store partially com- pleted trees. A major difference in our semantic parsing scenario is that tokens in the buffer are not fetched in a sequential order or removed from the buffer. This is because the lexical alignment be- tween an utterance and its semantic representation is hidden. Moreover, some predicates cannot be clearly anchored to a token span. Therefore, we allow the generation algorithm to pick tokens and combine logical forms in arbitrary orders, condi- tioning on the entire set of sentential features. Al- ternative solutions in the traditional semantic pars- ing literature include a floating chart parser <ref type="bibr" target="#b17">(Pasupat and Liang, 2015</ref>) which allows to construct logical predicates out of thin air.</p><p>Our transition system defines three actions, namely NT, TER, and RED, explained below.</p><p>NT(X) generates a Non-Terminal predicate. This predicate is either a natural language expression such as border, or one of the domain-general predicates exemplified in <ref type="table" target="#tab_1">Table 1</ref> (e.g., exclude). The type of predicate is determined by the place- holder X and once generated, it is pushed onto the stack and represented as a non-terminal followed by an open bracket (e.g., 'border('). The open bracket will be closed by a reduce operation. TER(X) generates a TERminal entity or the spe- cial predicate all. Note that the terminal choice does not include variable (e.g., $0, $1), since FunQL is a variable-free language which suffi- ciently captures the semantics of the datasets we work with. The framework could be extended to generate directly acyclic graphs by incorporat- ing variables with additional transition actions for handling variable mentions and co-reference.</p><p>RED stands for REDuce and is used for subtree completion. It recursively pops elements from the stack until an open non-terminal node is encoun- tered. The non-terminal is popped as well, af- ter which a composite term representing the entire subtree, e.g., border(texas), is pushed back to the stack. If a RED action results in having no more open non-terminals left on the stack, the transition system terminates. <ref type="table">Table 2</ref> shows the transition actions used to generate our running example.</p><p>The model generates the ungrounded represen- tation U conditioned on utterance x by recursively calling one of the above three actions. Note that U is defined by a sequence of actions (denoted Sentence: which states do not border texas Non-terminal symbols in buffer: which, states, do, not, border Terminal symbols in buffer: texas <ref type="table">Table 2</ref>: Actions taken by the transition system for generating the ungrounded meaning representation of the example utterance. Symbols in red indicate domain-general predicates.</p><formula xml:id="formula_1">Stack Action NT choice TER choice NT answer answer ( NT exclude answer ( exclude ( NT states answer ( exclude ( states ( TER all answer ( exclude ( states ( all RED answer ( exclude ( states ( all ) NT border answer ( exclude ( states ( all ) , border ( TER texas answer ( exclude ( states ( all ) , border ( texas RED answer ( exclude ( states ( all ) , border ( texas ) RED answer ( exclude ( states ( all ) , border ( texas ) ) RED answer ( exclude ( states ( all ) , border ( texas ) ) )</formula><p>by a) and a sequence of term choices (denoted by u) as shown in <ref type="table">Table 2</ref>. The conditional proba- bility p(U |x) is factorized over time steps as:</p><formula xml:id="formula_2">p(U |x) = p(a, u|x) = T t=1 p(a t |a &lt;t , x)p(u t |a &lt;t , x) I(at =RED) (1)</formula><p>where I is an indicator function.</p><p>To predict the actions of the transition system, we encode the input buffer with a bidirectional LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) and the output stack with a stack-LSTM ( <ref type="bibr">Dyer et al., 2015)</ref>. At each time step, the model uses the rep- resentation of the transition system e t to predict an action:</p><formula xml:id="formula_3">p(a t |a &lt;t , x) ∝ exp(W a · e t ) (2)</formula><p>where e t is the concatenation of the buffer repre- sentation b t and the stack representation s t . While the stack representation s t is easy to retrieve as the top state of the stack-LSTM, obtaining the buffer representation b t is more involved. This is because we do not have an explicit buffer repre- sentation due to the non-projectivity of semantic parsing. We therefore compute at each time step an adaptively weighted representation of b t (Bah- danau et al., 2015) conditioned on the stack rep- resentation s t . This buffer representation is then concatenated with the stack representation to form the system representation e t . When the predicted action is either NT or TER, an ungrounded term u t (either a predicate or an entity) needs to be chosen from the candidate list depending on the specific placeholder X. To se- lect a domain-general term, we use the same rep- resentation of the transition system e t to compute a probability distribution over candidate terms:</p><formula xml:id="formula_4">p(u GENERAL t |a &lt;t , x) ∝ exp(W p · e t ) (3)</formula><p>To choose a natural language term, we directly compute a probability distribution of all natural language terms (in the buffer) conditioned on the stack representation s t and select the most relevant term (Jia and Liang, 2016):</p><formula xml:id="formula_5">p(u NL t |a &lt;t , x) ∝ exp(s t )<label>(4)</label></formula><p>When the predicted action is RED, the com- pleted subtree is composed into a single represen- tation on the stack. For the choice of composition function, we use a single-layer neural network as in <ref type="bibr">Dyer et al. (2015)</ref>, which takes as input the con- catenated representation of the predicate and argu- ment of the subtree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generating Grounded Representations</head><p>Since we constrain the network to learn un- grounded structures that are isomorphic to the target meaning representation, converting un- grounded representations to grounded ones be- comes a simple lexical mapping problem. For sim- plicity, hereafter we do not differentiate natural language and domain-general predicates.</p><p>To map an ungrounded term u t to a grounded term g t , we compute the conditional probability of g t given u t with a bi-linear neural network:</p><formula xml:id="formula_6">p(g t |u t ) ∝ exp u t · W ug · g t<label>(5)</label></formula><p>where u t is the contextual representation of the un- grounded term given by the bidirectional LSTM, g t is the grounded term embedding, and W ug is the weight matrix.</p><p>The above grounding step can be interpreted as learning a lexicon: the model exclusively re- lies on the intermediate representation U to pre- dict the target meaning representation G without taking into account any additional features based on the utterance. In practice, U may provide suf- ficient contextual background for closed domain semantic parsing where an ungrounded predicate often maps to a single grounded predicate, but is a relatively impoverished representation for pars- ing large open-domain knowledge bases like Free- base. In this case, we additionally rely on a dis- criminative reranker which ranks the grounded representations derived from ungrounded repre- sentations (see Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objective</head><p>When the target meaning representation is avail- able, we directly compare it against our predic- tions and back-propagate. When only denotations are available, we compare surrogate meaning rep- resentations against our predictions ( <ref type="bibr" target="#b19">Reddy et al., 2014</ref>). Surrogate representations are those with the correct denotations. When there exist multi- ple surrogate representations, <ref type="bibr">3</ref> we select one ran- domly and back-propagate. The global effect of the above update rule is close to maximizing the marginal likelihood of denotations, which differs from recent work on weakly-supervised seman- tic parsing based on reinforcement learning <ref type="bibr" target="#b16">(Neelakantan et al., 2017)</ref>.</p><p>Consider utterance x with ungrounded mean- ing representation U , and grounded meaning rep- resentation G. Both U and G are defined with a sequence of transition actions (same for U and G) and a sequence of terms (different for U and G). Recall that a = [a 1 , · · · , a n ] denotes the transition action sequence defining U and G; let u = [u 1 , · · · , u k ] denote the ungrounded terms (e.g., predicates), and g = [g 1 , · · · , g k ] the grounded terms. We aim to maximize the likelihood of the grounded meaning representa- tion p(G|x) over all training examples. This likelihood can be decomposed into the likelihood of the grounded action sequence p(a|x) and the grounded term sequence p(g|x), which we opti- mize separately.</p><p>For the grounded action sequence (which by design is the same as the ungrounded action se- quence and therefore the output of the transition system), we can directly maximize the log likeli- hood log p(a|x) for all examples:</p><formula xml:id="formula_7">L a = x∈T log p(a|x) = x∈T n t=1 log p(a t |x) (6)</formula><p>where T denotes examples in the training data.</p><p>For the grounded term sequence g, since the intermediate ungrounded terms are latent, we maximize the expected log likelihood of the grounded terms u [p(u|x) log p(g|u, x)] for all examples, which is a lower bound of the log like- lihood log p(g|x):</p><formula xml:id="formula_8">L g = x∈T u [p(u|x) log p(g|u, x)] = x∈T u p(u|x) k t=1 log p(g t |u t )<label>(7)</label></formula><p>The final objective is the combination of L a and L g , denoted as L G = L a + L g . We opti- mize this objective with the method described in <ref type="bibr" target="#b11">Lei et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reranker</head><p>As discussed above, for open domain semantic parsing, solely relying on the ungrounded repre- sentation would result in an impoverished model lacking sentential context useful for disambigua- tion decisions. For all Freebase experiments, we followed previous work <ref type="bibr">(Berant et al., 2013;</ref><ref type="bibr">Berant and Liang, 2014;</ref><ref type="bibr" target="#b19">Reddy et al., 2014</ref>) in addi- tionally training a discriminative ranker to re-rank grounded representations globally.</p><p>The discriminative ranker is a maximum- entropy model <ref type="bibr">(Berant et al., 2013</ref>). The objective is to maximize the log likelihood of the correct an- swer y given x by summing over all grounded can- didates G with denotation y (i.e., <ref type="bibr">[[G]</ref>] K = y):</p><formula xml:id="formula_9">L y = (x,y)∈T log [[G]] K =y p(G|x)<label>(8)</label></formula><formula xml:id="formula_10">p(G|x) ∝ exp{f (G, x)}<label>(9)</label></formula><p>where f (G, x) is a feature function that maps pair (G, x) into a feature vector. We give details on the features we used in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we verify empirically that our se- mantic parser derives useful meaning representa- tions. We give details on the evaluation datasets and baselines used for comparison. We also describe implementation details and the features used in the discriminative ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated our model on the following datasets which cover different domains, and use differ- ent types of training data, i.e., pairs of natural language utterances and grounded meanings or question-answer pairs. GEOQUERY (Zelle and Mooney, 1996) con- tains 880 questions and database queries about US geography. The utterances are compositional, but the language is simple and vocabulary size small. The majority of questions include at most one en- tity. SPADES ( <ref type="bibr">Bisk et al., 2016</ref>) contains 93,319 questions derived from CLUEWEB09 ( <ref type="bibr">Gabrilovich et al., 2013</ref>) sentences. Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs ( <ref type="bibr" target="#b19">Reddy et al., 2014</ref>). The sentences include two or more entities and although they are not very compositional, they constitute a large-scale dataset for neural network training. WEBQUESTIONS ( <ref type="bibr">Berant et al., 2013)</ref> contains 5,810 question-answer pairs. Similar to SPADES, it is based on Freebase and the questions are not very compositional. However, they are real questions asked by people on the Web. Fi- nally, GRAPHQUESTIONS (Su et al., 2016) con- tains 5,166 question-answer pairs which were cre- ated by showing 500 Freebase graph queries to Amazon Mechanical Turk workers and asking them to paraphrase them into natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Amongst the four datasets described above, GEO- QUERY has annotated logical forms which we di- rectly use for training. For the other three datasets, we treat surrogate meaning representations which lead to the correct answer as gold standard. The surrogates were selected from a subset of candi- date Freebase graphs, which were obtained by en- tity linking. Entity mentions in SPADES have been automatically annotated with Freebase entities ( <ref type="bibr">Gabrilovich et al., 2013</ref>). For WEBQUESTIONS and GRAPHQUESTIONS, we follow the procedure described in . We identify po- tential entity spans using seven handcrafted part- of-speech patterns and associate them with Free- base entities obtained from the Freebase/KG API. <ref type="bibr">4</ref> We use a structured perceptron trained on the enti- ties found in WEBQUESTIONS and GRAPHQUES- TIONS to select the top 10 non-overlapping entity disambiguation possibilities. We treat each possi- bility as a candidate input utterance, and use the perceptron score as a feature in the discriminative reranker, thus leaving the final disambiguation to the semantic parser.</p><p>Apart from the entity score, the discriminative ranker uses the following basic features. The first feature is the likelihood score of a grounded rep- resentation aggregating all intermediate represen- tations. The second set of features include the em- bedding similarity between the relation and the ut- terance, as well as the similarity between the rela- tion and the question words. The last set of fea- tures includes the answer type as indicated by the last word in the Freebase relation ( <ref type="bibr" target="#b27">Xu et al., 2016)</ref>.</p><p>We used the Adam optimizer for training with an initial learning rate of 0.001, two momentum parameters [0.99, 0.999], and batch size 1. The di- mensions of the word embeddings, LSTM states, entity embeddings and relation embeddings are <ref type="bibr">[50,</ref><ref type="bibr">100,</ref><ref type="bibr">100,</ref><ref type="bibr">100]</ref>. The word embeddings were initialized with Glove embeddings ( <ref type="bibr" target="#b18">Pennington et al., 2014</ref>). All other embeddings were randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Experimental results on the four datasets are sum- marized in <ref type="table" target="#tab_2">Tables 3-6</ref>. We present comparisons of our system which we call SCANNER (as a short- hand for SymboliC meANiNg rEpResentation) against a variety of models previously described in the literature.</p><p>GEOQUERY results are shown in <ref type="table" target="#tab_4">Table 5</ref>. The first block contains symbolic systems, whereas neural models are presented in the second block. We report accuracy which is defined as the pro- portion of the utterance that are correctly parsed to their gold standard logical forms. All previ- ous neural systems (Dong and Lapata, 2016; Jia and Liang, 2016) treat semantic parsing as a se- quence transduction problem and use LSTMs to directly map utterances to logical forms. SCAN- NER yields performance improvements over these Models F1 <ref type="bibr">Berant et al. (2013)</ref> 35.7 Yao and Van Durme <ref type="formula" target="#formula_5">(2014)</ref> 33.0 Berant and Liang <ref type="bibr">(2014)</ref> 39.9 <ref type="bibr">Bast and Haussmann (2015)</ref>  <ref type="bibr">49.4 Berant and Liang (2015)</ref> 49.7  50.3 <ref type="bibr">Bordes et al. (2014)</ref> 39.2 <ref type="bibr">Dong et al. (2015)</ref> 40.8 <ref type="bibr" target="#b30">Yih et al. (2015)</ref> 52.5 <ref type="bibr" target="#b27">Xu et al. (2016)</ref> 53.3 Neural Baseline 48.3 SCANNER 49.4  systems when using comparable data sources for training. Jia and Liang (2016) achieve better results with synthetic data that expands GEO- QUERY; we could adopt their approach to improve model performance, however, we leave this to fu- ture work. <ref type="table" target="#tab_5">Table 6</ref> reports SCANNER's performance on SPADES. For all Freebase related datasets we use average F1 <ref type="bibr">(Berant et al., 2013)</ref> as our evalua- tion metric. Previous work on this dataset has used a semantic parsing framework similar to ours where natural language is converted to an interme- diate syntactic representation and then grounded to Freebase. Specifically, <ref type="bibr">Bisk et al. (2016)</ref> evalu- ate the effectiveness of four different CCG parsers on the semantic parsing task when varying the amount of supervision required. As can be seen, SCANNER outperforms all CCG variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons. For fair comparison, we also built a neu- ral baseline that encodes an utterance with a recur- rent neural network and then predicts a grounded meaning representation directly <ref type="bibr" target="#b24">(Ture and Jojic, 2016;</ref><ref type="bibr" target="#b31">Yih et al., 2016)</ref>. Again, we observe that SCANNER outperforms this baseline.</p><p>Results on WEBQUESTIONS are summarized in <ref type="table" target="#tab_2">Table 3</ref>. SCANNER obtains performance on par with the best symbolic systems (see the first block in the table). It is important to note that <ref type="bibr">Bast and Haussmann (2015)</ref> develop a question answering system, which contrary to ours can- Models Accuracy Zettlemoyer and <ref type="bibr" target="#b34">Collins (2005)</ref> 79.3 <ref type="bibr" target="#b33">Zettlemoyer and Collins (2007)</ref> 86.1 <ref type="bibr" target="#b8">Kwiatkowksi et al. (2010)</ref> 87.9 <ref type="bibr" target="#b10">Kwiatkowski et al. (2011)</ref> 88.6 <ref type="bibr" target="#b9">Kwiatkowski et al. (2013)</ref> 88.0 <ref type="bibr" target="#b35">Zhao and Huang (2015)</ref> 88.9 <ref type="bibr" target="#b14">Liang et al. (2011)</ref> 91.1 <ref type="bibr">Dong and Lapata (2016)</ref> 84.6 Jia and Liang <ref type="formula">(2016)</ref> 85.0 Jia and <ref type="bibr" target="#b1">Liang (2016)</ref> with extra data 89.1 SCANNER 86.7  not produce meaning representations whereas Be- rant and Liang <ref type="formula" target="#formula_6">(2015)</ref> propose a sophisticated agenda-based parser which is trained borrowing ideas from imitation learning. SCANNER is con- ceptually similar to  who also learn a semantic parser via intermediate repre- sentations which they generate based on the out- put of a dependency parser. SCANNER performs competitively despite not having access to any linguistically-informed syntactic structures. The second block in <ref type="table" target="#tab_2">Table 3</ref> reports the results of sev- eral neural systems. <ref type="bibr" target="#b27">Xu et al. (2016)</ref> represent the state of the art on WEBQUESTIONS. Their sys- tem uses Wikipedia to prune out erroneous candi- date answers extracted from Freebase. Our model would also benefit from a similar post-processing step. As in previous experiments, SCANNER out- performs the neural baseline, too. Finally, <ref type="table" target="#tab_3">Table 4</ref> presents our results on GRAPHQUESTIONS. We report F1 for SCANNER, the neural baseline model, and three symbolic sys- tems presented in <ref type="bibr" target="#b22">Su et al. (2016)</ref>. SCANNER achieves a new state of the art on this dataset with a gain of 4.23 F1 points over the best previously reported model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Intermediate Representations</head><p>Since a central feature of our parser is that it learns intermediate representations with natural language predicates, we conducted additional experiments in order to inspect their quality. For GEOQUERY Metrics Accuracy Exact match 79.3 Structure match 89.6 Token match 96.5 <ref type="table">Table 7</ref>: GEOQUERY evaluation of ungrounded meaning representations. We report accuracy against a manually created gold standard.</p><p>which contains only 280 test examples, we manu- ally annotated intermediate representations for the test instances and evaluated the learned represen- tations against them. The experimental setup aims to shows how humans can participate in improving the semantic parser with feedback at the interme- diate stage. In terms of evaluation, we use three metrics shown in <ref type="table">Table 7</ref>. The first row shows the percentage of exact matches between the predicted representations and the human annotations. The second row refers to the percentage of structure matches, where the predicted representations have the same structure as the human annotations, but may not use the same lexical terms. Among struc- turally correct predictions, we additionally com- pute how many tokens are correct, as shown in the third row. As can be seen, the induced meaning representations overlap to a large extent with the human gold standard. We also evaluated the intermediate represen- tations created by SCANNER on the other three (Freebase) datasets.</p><p>Since creating a man- ual gold standard for these large datasets is time-consuming, we compared the induced rep- resentations against the output of a syntactic parser. Specifically, we converted the ques- tions to event-argument structures with EASY- CCG ( <ref type="bibr" target="#b12">Lewis and Steedman, 2014</ref>), a high cover- age and high accuracy CCG parser. EASYCCG extracts predicate-argument structures with a la- beled F-score of 83.37%. For further comparison, we built a simple baseline which identifies pred- icates based on the output of the Stanford POS- tagger ( ) following the order- ing VBD VBN VB VBP VBZ MD.</p><p>As shown in <ref type="table">Table 8</ref>, on SPADES and WE- BQUESTIONS, the predicates learned by our model match the output of EASYCCG more closely than the heuristic baseline. But for GRAPHQUESTIONS which contains more compo- sitional questions, the mismatch is higher. How- ever, since the key idea of our model is to cap- ture salient meaning for the task at hand rather than strictly obey syntax, we would not expect the Dataset SCANNER Baseline SPADES 51.2 45.5 -conj <ref type="formula" target="#formula_5">(1422)</ref> 56.1 66.4 -control <ref type="formula">(132)</ref> 28.3 40.5 -pp <ref type="formula" target="#formula_5">(3489)</ref> 46.2 23.1 -subord <ref type="formula" target="#formula_8">(76)</ref> 37.9 52.9 WEBQUESTIONS 42.1 25.5 GRAPHQUESTIONS 11.9 15.3 <ref type="table">Table 8</ref>: Evaluation of predicates induced by SCANNER against EASYCCG. We report F1(%) across datasets. For SPADES, we also provide a breakdown for various utterance types.</p><p>predicates induced by our system to entirely agree with those produced by the syntactic parser. To further analyze how the learned predicates differ from syntax-based ones, we grouped utterances in SPADES into four types of linguistic constructions: coordination (conj), control and raising (control), prepositional phrase attachment (pp), and subor- dinate clauses (subord). <ref type="table">Table 8</ref> also shows the breakdown of matching scores per linguistic con- struction, with the number of utterances in each type. In <ref type="table" target="#tab_6">Table 9</ref>, we provide examples of predi- cates identified by SCANNER, indicating whether they agree or not with the output of EASYCCG. As a reminder, the task in SPADES is to predict the entity masked by a blank symbol ( ).</p><p>As can be seen in <ref type="table">Table 8</ref>, the match- ing score is relatively high for utterances in- volving coordination and prepositional phrase attachments.</p><p>The model will often identify informative predicates (e.g., nouns) which do not necessarily agree with linguistic intuition. For example, in the utterance wilhelm maybach and his son started maybach in 1909 (see <ref type="table" target="#tab_6">Table 9</ref>), SCANNER identifies the predicate- argument structure son(wilhelm maybach) rather than started(wilhelm maybach). We also observed that the model struggles with control and subor- dinate constructions. It has difficulty distinguish- ing control from raising predicates as exemplified in the utterance ceo john thain agreed to leave from <ref type="table" target="#tab_6">Table 9</ref>, where it identifies the raising predi- cate agreed. For subordinate clauses, SCANNER tends to take shortcuts identifying as predicates words closest to the blank symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We presented a neural semantic parser which converts natural language utterances to grounded meaning representations via intermediate predicate-argument structures.</p><p>Our model conj the boeing company was founded in 1916 and is headquartered in , illinois . nstar was founded in 1886 and is based in boston , . the is owned and operated by zuffa , llc , headquarted in las vegas , nevada . hugh attended and then shifted to uppingham school in england .</p><p>was incorporated in 1947 and is based in new york city . the ifbb was formed in 1946 by president ben weider and his brother . wilhelm maybach and his son started maybach in 1909 .</p><p>was founded in 1996 and is headquartered in chicago .</p><p>control threatened to kidnap russ . has also been confirmed to play captain haddock . hoffenberg decided to leave .</p><p>is reportedly trying to get impregnated by djimon now . for right now , are inclined to trust obama to do just that .</p><p>agreed to purchase wachovia corp . ceo john thain agreed to leave . so nick decided to create . salva later went on to make the non clown-based horror . eddie dumped debbie to marry when carrie was 2 .</p><p>pp is the home of the university of tennessee . chu is currently a physics professor at . youtube is based in , near san francisco , california . mathematica is a product of . jobs will retire from . the nab is a strong advocacy group in . this one starred robert reed , known mostly as .</p><p>is positively frightening as detective bud white .</p><p>subord the is a national testing board that is based in toronto . is a corporation that is wholly owned by the city of edmonton . unborn is a scary movie that stars .</p><p>'s third wife was actress melina mercouri , who died in 1994 . sure , there were who liked the shah .</p><p>founded the , which is now also a designated terrorist group .</p><p>is an online bank that ebay owns . zoya akhtar is a director , who has directed the upcoming movie . imelda staunton , who plays , is genius .</p><p>is the important president that american ever had . plus mitt romney is the worst governor that has had . An assumption our model imposes is that un- grounded and grounded representations are struc- turally isomorphic. An advantage of this assump- tion is that tokens in the ungrounded and grounded representations are strictly aligned. This allows the neural network to focus on parsing and lexi- cal mapping, sidestepping the challenging struc- ture mapping problem which would result in a larger search space and higher variance. On the negative side, the structural isomorphism assump- tion restricts the expressiveness of the model, es- pecially since one of the main benefits of adopt- ing a two-stage parser is the potential of captur- ing domain-independent semantic information via the intermediate representation. While it would be challenging to handle drastically non-isomorphic structures in the current model, it is possible to perform local structure matching, i.e., when the mapping between natural language and domain- specific predicates is many-to-one or one-to-many.</p><p>For instance, Freebase does not contain a rela- tion representing daughter, using instead two rela- tions representing female and child. Previous work ( <ref type="bibr" target="#b9">Kwiatkowski et al., 2013</ref>) models such cases by introducing collapsing (for many-to-one map- ping) and expansion (for one-to-many mapping) operators. Within our current framework, these two types of structural mismatches can be han- dled with semi-Markov assumptions <ref type="bibr" target="#b21">(Sarawagi and Cohen, 2005;</ref><ref type="bibr" target="#b4">Kong et al., 2016</ref>) in the pars- ing (i.e., predicate selection) and the grounding steps, respectively. Aside from relaxing strict iso- morphism, we would also like to perform cross- domain semantic parsing where the first stage of the semantic parser is shared across domains. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>List of domain-general predicates. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : WEBQUESTIONS results.</head><label>3</label><figDesc></figDesc><table>Models 
F1 

SEMPRE (Berant et al., 2013) 
10.80 
PARASEMPRE (Berant and Liang, 2014) 12.79 
JACANA (Yao and Van Durme, 2014) 
5.08 
Neural Baseline 
16.24 
SCANNER 
17.02 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>GRAPHQUESTIONS results. Numbers for comparison systems are from Su et al. (2016).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : GEOQUERY results.</head><label>5</label><figDesc></figDesc><table>Models 
F1 
Unsupervised CCG (Bisk et al., 2016) 
24.8 
Semi-supervised CCG (Bisk et al., 2016) 28.4 
Neural baseline 
28.6 
Supervised CCG (Bisk et al., 2016) 
30.9 
Rule-based system (Bisk et al., 2016) 
31.4 
SCANNER 
31.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 : SPADES results.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 9 :</head><label>9</label><figDesc>Informative predicates identified by SCANNER in various types of utterances. Yellow predi- cates were identified by both SCANNER and EASYCCG, red predicates by SCANNER alone, and green predicates by EASYCCG alone. essentially jointly learns how to parse natural language semantics and the lexicons that help grounding. Compared to previous neural semantic parsers, our model is more interpretable as the intermediate structures are useful for inspecting what the model has learned and whether it matches linguistic intuition.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>learning to align and translate. In Proceedings of ICLR 2015. San Diego, California.</head><label>learning</label><figDesc></figDesc><table>Hannah Bast and Elmar Haussmann. 2015. More ac-
curate question answering on Freebase. In Proceed-
ings of the 24th ACM International on Conference 
on Information and Knowledge Management. ACM, 
pages 1431-1440. 

Emily M Bender, Dan Flickinger, Stephan Oepen, 
Woodley Packard, and Ann Copestake. 2015. Lay-
ers of interpretation: On grammar and composition-
ality. In Proceedings of the 11th International Con-
ference on Computational Semantics. London, UK, 
pages 239-249. 

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy 
Liang. 2013. Semantic parsing on Freebase from 
question-answer pairs. In Proceedings of the 2013 
Conference on Empirical Methods in Natural Lan-
guage Processing. Seattle, Washington, pages 1533-
1544. 

Jonathan Berant and Percy Liang. 2014. Semantic 
parsing via paraphrasing. In Proceedings of the 
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). Balti-
more, Maryland, pages 1415-1425. 

Jonathan Berant and Percy Liang. 2015. Imitation 
learning of agenda-based semantic parsers. Trans-
actions of the Association for Computational Lin-
guistics 3:545-558. 

Yonatan Bisk, Siva Reddy, John Blitzer, Julia Hock-
enmaier, and Mark Steedman. 2016. Evaluating in-
duced CCG parsers on grounded semantic parsing. 
In Proceedings of the 2016 Conference on Empirical 
Methods in Natural Language Processing. Austin, 
Texas, pages 2022-2027. 

Antoine Bordes, Sumit Chopra, and Jason Weston. 
2014. Question answering with subgraph embed-
dings. In Proceedings of the 2014 Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP). Doha, Qatar, pages 615-620. 

Qingqing Cai and Alexander Yates. 2013. Large-scale 
semantic parsing via schema matching and lexicon 
extension. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Sofia, Bulgaria, pages 
423-433. 

Do Kook Choe and Eugene Charniak. 2016. Parsing 
as language modeling. In Proceedings of the 2016 
Conference on Empirical Methods in Natural Lan-
guage Processing. Austin, Texas, pages 2331-2336. 

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the 
54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers). 
Berlin, Germany, pages 33-43. 

Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2015. 
Question answering over Freebase with multi-
column convolutional neural networks. In Proceed-
ings of the 53rd Annual Meeting of the Association 
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers). Beijing, China, 
pages 260-269. 

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin 
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual 
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference 
on Natural Language Processing (Volume 1: Long 
Papers). Beijing, China, pages 334-343. 

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, 
and Noah A. Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. San Diego, California, pages 
199-209. 

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, 
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning 
representation. In Proceedings of the 52nd Annual 
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Baltimore, Mary-
land, pages 1426-1436. 

Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. FACC1: Freebase anno-
tation of ClueWeb corpora, version 1 (release date 
2013-06-26, format version 1, correction level 0) . 

Matt Gardner and Jayant Krishnamurthy. 2017. Open-
Vocabulary Semantic Parsing with both Distribu-
tional Statistics and Formal Knowledge. In Pro-
ceedings of the 31st AAAI Conference on Artificial 
Intelligence. San Francisco, California, pages 3195-
3201. 

Jonas Groschwitz, Alexander Koller, and Christoph Te-
ichmann. 2015. Graph parsing with s-graph gram-
mars. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics 
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers). 
Beijing, China, pages 1481-1490. 

Steve Harris, 
Andy Seaborne, 
and Eric 
Prud'hommeaux. 2013. 
SPARQL 1.1 query 
language. W3C recommendation 21(10). 

Sepp Hochreiter and Jürgen Schmidhuber. 1997. 
Long short-term memory. Neural computation 
9(8):1735-1780. 

Florian Holzschuher and René Peinl. 2013. Perfor-
mance of graph query languages: comparison of </table></figure>

			<note place="foot" n="1"> Our code is available at https://github.com/ cheng6076/scanner.</note>

			<note place="foot" n="3"> The average Freebase surrogate representations obtained with highest denotation match (F1) is 1.4.</note>

			<note place="foot" n="4"> http://developers.google.com/ freebase/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly cypher, gremlin and native access in Neo4j</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint EDBT/ICDT 2013 Workshops</title>
		<meeting>the Joint EDBT/ICDT 2013 Workshops</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germany</forename><surname>Berlin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to Transform Natural to Formal Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings for the 20th National Conference on Artificial Intelligence. Pittsburgh</title>
		<meeting>for the 20th National Conference on Artificial Intelligence. Pittsburgh<address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmental recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2016</title>
		<meeting>ICLR 2016<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a Compositional Semantics for Freebase with an Open Predicate Vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="257" to="270" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowksi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling Semantic Parsers with On-the-Fly Ontology Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods on Natural Language Processing</title>
		<meeting>Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A* CCG parsing with a supertag-factored model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="990" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4408</idno>
		<title level="m">Lambda dependency-based compositional semantics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017</title>
		<meeting>ICLR 2017<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without questionanswer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transforming dependency structures to logical forms for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On generating characteristic-rich question sets for qa evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Simple and effective question answering with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Jojic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05029</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL</title>
		<meeting>the Human Language Technology Conference of the NAACL<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Main Conference</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Question answering on Freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2326" to="2336" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with Freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to Parse Database Queries Using Inductive Logic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th National Conference on Artificial Intelligence. Portland, Oregon</title>
		<meeting>the 13th National Conference on Artificial Intelligence. Portland, Oregon</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Prague, Czech Republic</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Prague, Czech Republic</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 21st Conference in Uncertainilty in Artificial Intelligence</title>
		<meeting>21st Conference in Uncertainilty in Artificial Intelligence<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1416" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
