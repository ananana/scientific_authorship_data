<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Parser with a Discriminative Recognition Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
						</author>
						<title level="a" type="main">A Generative Parser with a Discriminative Recognition Algorithm</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="118" to="124"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2019</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by dis-criminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling <ref type="bibr">(Collins, 1999;</ref><ref type="bibr">Henderson, 2003;</ref><ref type="bibr">Titov and Henderson, 2007;</ref><ref type="bibr">Petrov and Klein, 2007;</ref><ref type="bibr">Dyer et al., 2016)</ref>. How- ever, they often impose strong independence as- sumptions which restrict the use of arbitrary fea- tures for effective disambiguation. Moreover, gen- erative parsers are typically trained by maximiz- ing the joint probability of the parse tree and the sentence-an objective that only indirectly relates to the goal of parsing. At test time, these mod- els require a relatively expensive recognition algo-rithm <ref type="bibr">(Collins, 1999;</ref><ref type="bibr">Titov and Henderson, 2007)</ref> to recover the parse tree, but the parsing per- formance consistently lags behind their discrim- inative competitors <ref type="bibr">(Nivre et al., 2007;</ref><ref type="bibr">Huang, 2008;</ref><ref type="bibr">Goldberg and Elhadad, 2010)</ref>, which are di- rectly trained to maximize the conditional proba- bility of the parse tree given the sentence, where linear-time decoding algorithms exist (e.g., for transition-based parsers).</p><p>In this work, we propose a parsing and lan- guage modeling framework that marries a gener- ative model with a discriminative recognition al- gorithm in order to have the best of both worlds. The idea of combining these two types of mod- els is not new. For example, <ref type="bibr">Collins and Koo (2005)</ref> propose to use a generative model to gen- erate candidate constituency trees and a discrimi- native model to rank them. <ref type="bibr">Sangati et al. (2009)</ref> follow the opposite direction and employ a gener- ative model to re-rank the dependency trees pro- duced by a discriminative parser. However, pre- vious work combines the two types of models in a goal-oriented, pipeline fashion, which lacks model interpretations and focuses solely on parsing.</p><p>In comparison, our framework unifies genera- tive and discriminative parsers with a single objec- tive, which connects to expectation maximization and variational inference in grammar induction settings. In a nutshell, we treat parse trees as latent factors generating natural language sentences and parsing as a posterior inference task. We showcase the framework using Recurrent Neural Network Grammars (RNNGs; <ref type="bibr">Dyer et al. 2016</ref>), a recently proposed probabilistic model of phrase-structure trees based on neural transition systems. Differ- ent from this work which introduces separately trained discriminative and generative models, we integrate the two in an auto-encoder which fits our training objective. We show how the framework enables grammar induction, parsing and language modeling within a single implementation. On the English Penn Treebank, we achieve competitive performance on constituency parsing and state-of- the-art single-model language modeling score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section we briefly describe Recurrent Neu- ral Network Grammars (RNNGs; <ref type="bibr">Dyer et al. 2016</ref>), a top-down transition-based algorithm for parsing and generation. There are two versions of RNNG, one discriminative, the other generative. We follow the original paper in presenting the dis- criminative variant first.</p><p>The discriminative RNNG follows a shift- reduce parser that converts a sequence of words into a parse tree. As in standard shift-reduce parsers, the RNNG uses a buffer to store unpro- cessed terminal symbols and a stack to store par- tially completed syntactic constituents. At each timestep, one of the following three operations 2 is performed:</p><p>• NT(X) introduces an open non-terminal X onto the top of the stack, represented as an open parenthesis followed by X, e.g., (NP.</p><p>• SHIFT fetches the terminal in the front of the buffer and pushes it onto the top of the stack.</p><p>• REDUCE completes a subtree by repeatedly popping the stack until an open non-terminal is encountered. The non-terminal is popped as well, after which a composite term repre- senting the entire subtree is pushed back onto the top of the stack, e.g., (NP the cat).</p><p>The above transition system can be adapted with minor modifications to an algorithm that gen- erates trees and sentences. In generator tran- sitions, there is no input buffer of unprocessed words but there is an output buffer for storing words that have been generated. To reflect the change, the previous SHIFT operation is modified into a GEN operation defined as follows:</p><p>• GEN generates a terminal symbol and add it to the stack and the output buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our framework unifies generative and discrimi- native parsers within a single training objective. For illustration, we adopt the two RNNG variants introduced above with our customized features. Our starting point is the generative model ( § 3.1), which allows us to make explicit claims about the generative process of natural language sentences. Since this model alone lacks a bottom-up recog- nition mechanism, we introduce a discriminative recognition model ( § 3.2) and connect it with the generative model in an encoder-decoder setting.</p><p>To offer a clear interpretation of the training objec- tive ( § 3.3), we first consider the parse tree as la- tent and the sentence as observed. We then discuss extensions that account for labeled parse trees. Fi- nally, we present various inference techniques for parsing and language modeling within the frame- work ( § 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decoder (Generative Model)</head><p>The decoder is a generative RNNG that models the joint probability p(x, y) of a latent parse tree y and an observed sentence x. Since the parse tree is defined by a sequence of transition actions a, we write p(x, y) as p(x, a). <ref type="bibr">3</ref> The joint distribu- tion p(x, a) is factorized into a sequence of transi- tion probabilities and terminal probabilities (when actions are GEN), which are parametrized by a transitional state embedding u:</p><formula xml:id="formula_0">p(x, a) = p(a)p(x|a) = |a| t=1 p(a t |u t )p(x t |u t ) I(at=GEN) (1)</formula><p>where I is an indicator function and u t represents the state embedding at time step t. Specifically, the conditional probability of the next action is:</p><formula xml:id="formula_1">p(a t |u t ) = |a| t=1 exp(a t u T t + b a ) a ∈A exp(a u T t + b a )<label>(2)</label></formula><p>where a t represents the action embedding at time step t, A the action space and b a the bias. Simi- larly, the next word probability (when GEN is in- voked) is computed as:</p><formula xml:id="formula_2">p(w t |u t ) = |a| t=1 exp(w t u T t + b w ) w ∈W exp(w u T t + b w )<label>(3)</label></formula><p>where W denotes all words in the vocabulary.</p><p>To satisfy the independence assumptions im- posed by the generative model, u t uses only a restricted set of features defined over the output buffer and the stack -we consider p(a) as a con- text insensitive prior distribution. Specifically, we use the following features: 1) the stack embed- ding d t which encodes the stack of the decoder and is obtained with a stack-LSTM ( <ref type="bibr">Dyer et al., 2015</ref><ref type="bibr">Dyer et al., , 2016</ref>); 2) the output buffer embedding o t ; we use a standard LSTM to compose the output buffer and o t is represented as the most recent state of the LSTM; and 3) the parent non-terminal em- bedding n t which is accessible in the generative model because the RNNG employs a depth-first generation order. Finally, u t is computed as:</p><formula xml:id="formula_3">u t = W 2 tanh(W 1 [d t , o t , n t ] + b d ) (4)</formula><p>where Ws are weight parameters and b d the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder (Recognition Model)</head><p>The encoder is a discriminative RNNG that com- putes the conditional probability q(a|x) of the transition action sequence a given an observed sentence x. This conditional probability is factor- ized over time steps as:</p><formula xml:id="formula_4">q(a|x) = |a| t=1 q(a t |v t ) (5)</formula><p>where v t is the transitional state embedding of the encoder at time step t. The next action is predicted similarly to Equa- tion (2), but conditioned on v t . Thanks to the dis- criminative property, v t has access to any contex- tual features defined over the entire sentence and the stack -q(a|x) acts as a context sensitive pos- terior approximation. Our features 4 are: 1) the stack embedding e t obtained with a stack-LSTM that encodes the stack of the encoder; 2) the in- put buffer embedding i t ; we use a bidirectional LSTM to compose the input buffer and repre- sent each word as a concatenation of forward and backward LSTM states; i t is the representation of the word on top of the buffer; 3) to incorpo- rate more global features and a more sophisticated look-ahead mechanism for the buffer, we also use an adaptive buffer embedding ¯ i t ; the latter is com- puted by having the stack embedding e t attend to all remaining embeddings on the buffer with the attention function in <ref type="bibr">Vinyals et al. (2015)</ref>; and 4) the parent non-terminal embedding n t . Finally, v t is computed as follows:</p><formula xml:id="formula_5">v t = W 4 tanh(W 3 [e t , i t , ¯ i t , n t ] + b e )<label>(6)</label></formula><p>where Ws are weight parameters and b e the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Consider an auto-encoder whose encoder infers the latent parse tree and the decoder generates the observed sentence from the parse tree. <ref type="bibr">5</ref> The max- imum likelihood estimate of the decoder parame- ters is determined by the log marginal likelihood of the sentence:</p><formula xml:id="formula_6">log p(x) = log a p(x, a)<label>(7)</label></formula><p>We follow expectation-maximization and varia- tional inference techniques to construct an ev- idence lower bound of the above quantity (by Jensen's Inequality), denoted as follows:</p><formula xml:id="formula_7">log p(x) ≥ E q(a|x) log p(x, a) q(a|x) = L x<label>(8)</label></formula><p>where p(x, a) = p(x|a)p(a) comes from the de- coder or the generative model, and q(a|x) comes from the encoder or the recognition model. The objective function 6 in Equation <ref type="formula" target="#formula_7">(8)</ref>, denoted by L x , is unsupervised and suited to a grammar in- duction task. This objective can be optimized with the methods shown in <ref type="bibr">Miao and Blunsom (2016)</ref>. Next, consider the case when the parse tree is observed. We can directly maximize the log likelihood of the parse tree for the encoder out- put log q(a|x) and the decoder output log p(a):</p><formula xml:id="formula_8">L a = log q(a|x) + log p(a)<label>(9)</label></formula><p>This supervised objective leverages extra informa- tion of labeled parse trees to regularize the distri- bution q(a|x) and p(a), and the final objective is:</p><formula xml:id="formula_9">L = L x + L a<label>(10)</label></formula><p>where L x and L a can be balanced with the task focus (e.g, language modeling or parsing). <ref type="table" target="#tab_1">Pretrained word embedding dimensions  50  POS tag embedding dimensions  20  Encoder LSTM dimensions  128  Decoder LSTM dimensions  256  LSTM layer  2  Encoder dropout</ref> 0.2 Decoder dropout 0.3 <ref type="table">Table 1</ref>: Hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>We consider two inference tasks, namely parsing and language modeling.</p><p>Parsing In parsing, we are interested in the parse tree that maximizes the posterior p(a|x) (or the joint p(a, x)). However, the decoder alone does not have a bottom-up recognition mecha- nism for computing the posterior. Thanks to the encoder, we can compute an approximated pos- terior q(a|x) in linear time and select the parse tree that maximizes this approximation. An al- ternative is to generate candidate trees by sam- pling from q(a|x), re-rank them with respect to the joint p(x, a) (which is proportional to the true posterior), and select the sample that maximizes the true posterior.</p><p>Language Modeling In language modeling, our goal is to compute the marginal probability p(x) = a p(x, a), which is typically intractable. To approximate this quantity, we can use Equa- tion (8) to compute a lower bound of the log like- lihood log p(x) and then exponentiate it to get a pessimistic approximation of p(x). <ref type="bibr">7</ref> Another way of computing p(x) (without lower bounding) would be to use the variational approxi- mation q(a|x) as the proposal distribution as in the importance sampler of <ref type="bibr">Dyer et al. (2016)</ref>. How- ever, this is beyond the scope of this work and we leave detailed discussions to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our framework is related to a class of variational autoencoders <ref type="bibr">(Kingma and Welling, 2014</ref>), which use neural networks for posterior approximation in variational inference. This technique has been previously used for topic modeling (Miao et al., <ref type="formula" target="#formula_1">(2013)</ref> 90.4 <ref type="bibr">Dyer et al. (2016)</ref> 91.7 Cross and Huang (2016) 89.9 <ref type="bibr">Vinyals et al. (2015)</ref> 92.8</p><note type="other">Discrimina- tive parsers Socher et al. (2013) 90.4 Zhu et al.</note><p>Generative parsers</p><p>Petrov and Klein <ref type="formula" target="#formula_1">(2007)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We performed experiments on the English Penn Treebank dataset; we used sections 2-21 for train- ing, 24 for validation, and 23 for testing. Follow- ing <ref type="bibr">Dyer et al. (2015)</ref>, we represent each word in three ways: as a learned vector, a pretrained vector, and a POS tag vector. The encoder word embedding is the concatenation of all three vec- tors while the decoder uses only the first two since we do not consider POS tags in generation. Ta- ble 1 presents details on the hyper-parameters we used. To find the MAP parse tree argmax a p(a, x) (where p(a, x) is used rank the output of q(a|x)) and to compute the language modeling perplex- ity with the evidence lower bound (where a ∼ q(a|x)), we collect 100 samples from q(a|x), same as <ref type="bibr">Dyer et al. (2016)</ref>.</p><p>Experimental results for constituency parsing and language modeling are shown in <ref type="table" target="#tab_1">Tables 2  and 3</ref>, respectively. As can be seen, the single framework we propose obtains competitive pars-KN-5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="255.2">LSTM 113.4 Dyer et al. (2016) 102.4 This work: a ∼ q(a|x)</head><p>99.8 On language modeling, our framework achieves lower perplexity compared to <ref type="bibr">Dyer et al. (2016)</ref> and baseline models. This gain possibly comes from the joint optimization of both the genera- tive and discriminative components towards a lan- guage modeling objective. However, we acknowl- edge a subtle difference between <ref type="bibr">Dyer et al. (2016)</ref> and our approach compared to baseline language models: while the latter incrementally estimate the next word probability, our approach <ref type="bibr">(and Dyer et al. 2016</ref>) directly assigns probability to the en- tire sentence. Overall, the advantage of our frame- work compared to <ref type="bibr">Dyer et al. (2016)</ref> is that it opens an avenue to unsupervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a framework that integrates a gen- erative parser with a discriminative recognition model and showed how it can be instantiated with RNNGs. We demonstrated that a unified frame- work, which relates to expectation maximization and variational inference, enables effective pars- ing and language modeling algorithms. Evalua- tion on the English Penn Treebank, revealed that our framework obtains competitive performance on constituency parsing and state-of-the-art results on single-model language modeling. In the fu- ture, we would like to perform grammar induc- tion based on Equation <ref type="formula" target="#formula_7">(8)</ref>, with gradient descent and posterior regularization techniques ( <ref type="bibr">Ganchev et al., 2010</ref>  <ref type="bibr">Federico Sangati, Willem Zuidema, and Rens Bod. 2009</ref>. A generative re-ranking model for depen- dency parsing. <ref type="table">In Proceedings of the 11th In- ternational Conference on Parsing Technologies  (IWPT'09</ref> A Comparison to Importance Sampling ( <ref type="bibr">Dyer et al., 2016)</ref> In this appendix we highlight the connections be- tween importance sampling and variational infer- ence, thereby comparing our method with <ref type="bibr">Dyer et al. (2016)</ref>. Consider a simple directed graphical model with discrete latent variables a (e.g., a is the tran- sition action sequence) and observed variables x (e.g., x is the sentence). The model evidence, or the marginal likelihood p(x) = a p(x, a) is of- ten intractable to compute. Importance sampling transforms the above quantity into an expectation over a distribution q(a), which is known and easy to sample from:</p><formula xml:id="formula_10">p(x) = a p(x, a) q(a) q(a) = E q(a) w(x, a) (11)</formula><p>where q(a) is the proposal distribution and w(x, a) = p(x,a) q(a) the importance weight. 123</p><p>The proposal distribution can potentially depend on the observations x, i.e., q(a) q(a|x).</p><p>A challenge with importance sampling lies in choosing a proposal distribution which leads to low variance. As shown in <ref type="bibr">Rubinstein and Kroese (2008)</ref>, the optimal choice of the proposal distri- bution is in fact the true posterior p(a|x), in which case the importance weight p(a,x) p(a|x) = p(x) is con- stant with respect to a. In <ref type="bibr">Dyer et al. (2016)</ref>, the proposal distribution depends on x, i.e., q(a) q(a|x), and is computed with a separately-trained, discriminative model. This proposal choice is close to optimal, since in a fully supervised setting a is also observed and the discriminative model can be trained to approximate the true posterior well. We hypothesize that the performance of their importance sampler is dependent on this specific proposal distribution. Besides, their training strat- egy does not generalize to an unsupervised setting.</p><p>In comparison, variational inference approach approximates the log marginal likelihood log p(x) with the evidence lower bound. It is a natural choice when one aims to optimize Equation (11) directly: log p(x) = log a p(x, a) q(a) q(a)</p><p>≥ E q(a) log p(x, a) q(a)</p><p>where q(a) is the variational approximation of the true posterior. Again, the variational approx- imation can potentially depend on the observa- tion x (i.e., q(a) q(a|x)) and can be com- puted with a discriminative model. Equation <ref type="formula" target="#formula_1">(12)</ref> is a well-defined, unsupervised training objec- tive which allows us to jointly optimize genera- tive (i.e., p(x, a)) and discriminative (i.e., q(a|x)) models. To further support the observed vari- able a, we augment this objective with supervised terms shown in Equation (10), following <ref type="bibr">Kingma et al. (2014)</ref> and <ref type="bibr">Miao and Blunsom (2016)</ref>. Equation (12) can be also used to approximate the marginal likelihood p(x) (e.g., in language modeling) with its lower bound. An alternative choice without lower bounding is to use the vari- ational approximation q(a|x) as the proposal dis- tribution in importance sampling (Equation <ref type="formula">(11)</ref>). <ref type="bibr">Ghahramani and Beal (2000)</ref> show that this pro- posal distribution leads to improved results of im- portance samplers. However, a potential drawback of importance sampling-based approach is that it is prone to numerical underflow. In practice, we observed similar language modeling performance for both methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parsing results (F1) on the PTB test set. 

2016) and sentence compression (Miao and Blun-
som, 2016). Another interpretation of the pro-
posed framework is from the perspective of guided 
policy search in reinforcement learning (Bachman 
and Precup, 2015), where a generative parser is 
trained to imitate the trace of a discriminative 
parser. Further connections can be drawn with 
the importance-sampling based inference of Dyer 
et al. (2016). There, a generative RNNG and a 
discriminative RNNG are trained separately; dur-
ing language modeling, the output of the discrim-
inative model serves as the proposal distribution 
of an importance sampler p(x) = E q(a|x) 

p(x,a) 

q(a|x) . 
Compared to their work, we unify the generative 
and discriminative RNNGs in a single framework, 
and adopt a joint training objective. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Language modeling results (perplexity).</head><label>3</label><figDesc></figDesc><table>ing performance. Comparing the two inference 
methods for parsing, ranking approximated MAP 
trees from q(a|x) with respect to p(a, x) yields a 
small improvement, as in Dyer et al. (2016). It is 
worth noting that our parsing performance lags be-
hind Dyer et al. (2016). We believe this is due to 
implementation disparities, such as the modeling 
of the reduce operation. While Dyer et al. (2016) 
use an LSTM as the syntactic composition func-
tion of each subtree, we adopt a rather simple com-
position function based on embedding averaging 
for memory concern. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>References 

Philip Bachman and Doina Precup. 2015. Data gener-
ation as sequential decision making. In Advances in 
Neural Information Processing Systems, MIT Press, 
pages 3249-3257. 

Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania, Pennsylvania, Philadel-
phia. 

Michael Collins and Terry Koo. 2005. Discriminative 
reranking for natural language parsing. Computa-
tional Linguistics 31(1):25-70. 

James Cross and Liang Huang. 2016. Incremental 
parsing with minimal features using bi-directional 
lstm. In Proceedings of the 54th Annual Meeting of 
the Association for Computational Linguistics (Vol-
ume 2: Short Papers). Berlin, Germany, pages 32-
37. 

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin 
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual 
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference 
on Natural Language Processing (Volume 1: Long 
Papers). Beijing, China, pages 334-343. 

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, 
and Noah A. Smith. 2016. Recurrent neural net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. San Diego, California, pages 
199-209. 

Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, 
et al. 2010. Posterior regularization for structured 
latent variable models. Journal of Machine Learn-
ing Research 11(Jul):2001-2049. 

Zoubin Ghahramani and Matthew J. Beal. 2000. Vari-
ational inference for Bayesian mixtures of factor 
analysers. In Advances in Neural Information Pro-
cessing Systems, MIT Press, pages 449-455. 

Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies: 
The 2010 Annual Conference of the North American 
Chapter of the Association for Computational Lin-
guistics. Los Angeles, California, pages 742-750. James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of the 2003 Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics. Edmon-
ton, Canada, pages 24-31. 

Liang Huang. 2008. Forest reranking: Discriminative 
parsing with non-local features. In Proceedings of 
ACL-08: HLT. Columbus, Ohio, pages 586-594. 

Diederik P Kingma, Shakir Mohamed, Danilo Jimenez 
Rezende, and Max Welling. 2014. Semi-supervised 
learning with deep generative models. In Advances 
in Neural Information Processing Systems, MIT 
Press, pages 3581-3589. 

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational Bayes. In Proceedings of the 
International Conference on Learning Representa-
tions. Banff, Canada. 

Yishu Miao and Phil Blunsom. 2016. Language as a 
latent variable: Discrete generative models for sen-
tence compression. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language 
Processing. Austin, Texas, pages 319-328. 

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In Pro-
ceedings of The 33rd International Conference on 
Machine Learning. New York, New York, USA, 
pages 1727-1736. 

Andriy Mnih and Karol Gregor. 2014. Neural vari-
ational inference and learning in belief networks. 
In Proceedings of the 31st International Conference 
on Machine Learning. Beijing, China, pages 1791-
1799. 

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas 
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav 
Marinov, and Erwin Marsi. 2007. Maltparser: A 
language-independent system for data-driven de-
pendency parsing. Natural Language Engineering 
13(2):95-135. 

Jeffrey Pennington, Richard Socher, and Christopher 
Manning. 2014. Glove: Global vectors for word 
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP). Doha, Qatar, pages 1532-
1543. 

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language 
Technologies 2007: The Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence. Rochester, New York, pages 404-411. 

Reuven Y Rubinstein and Dirk P Kroese. 2008. Simu-
lation and the Monte Carlo method. John Wiley &amp; 
Sons. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>). Paris, France, pages 238-241.</head><label></label><figDesc></figDesc><table>Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and 
Masaaki Nagata. 2012. Bayesian symbol-refined 
tree substitution grammars for syntactic parsing. 
In Proceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics: Long 
Papers-Volume 1. Jeju Island, Korea, pages 440-
448. 

Richard Socher, John Bauer, Christopher D. Manning, 
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st 
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Sofia, 
Bulgaria, pages 455-465. 

Ivan Titov and James Henderson. 2007. Constituent 
parsing with incremental sigmoid belief networks. 
In Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics. Prague, 
Czech Republic, pages 632-639. 

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, 
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural 
Information Processing Systems, MIT Press, pages 
2773-2781. 

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, 
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the 
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). Sofia, 
Bulgaria, pages 434-443. 

</table></figure>

			<note place="foot" n="1"> Our code is available at https://github.com/ cheng6076/virnng.git.</note>

			<note place="foot" n="2"> To be precise, the total number of operations under our description is |X|+2 since the NT operation varies with the non-terminal choice X.</note>

			<note place="foot" n="3"> We assume that the action probability does not take the actual terminal choice into account.</note>

			<note place="foot" n="4"> Compared to Dyer et al. (2016), the new features we introduce are 3) and 4), which we found empirically useful.</note>

			<note place="foot" n="5"> Here, GEN and SHIFT refer to the same action with different definitions for encoding and decoding. 6 See § 4 and Appendix A for comparison between this objective and the importance sampler of Dyer et al. (2016).</note>

			<note place="foot" n="7"> As a reminder, the language modeling objective is exp(N LL/T ), where N LL denotes the total negative log likelihood of the test data and T the token counts.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
