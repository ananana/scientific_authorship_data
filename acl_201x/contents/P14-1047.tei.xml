<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California Institute for Creative Technologies</orgName>
								<address>
									<addrLine>12015 Waterfront Drive</addrLine>
									<postCode>90094</postCode>
									<settlement>Playa Vista</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Nelson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California Institute for Creative Technologies</orgName>
								<address>
									<addrLine>12015 Waterfront Drive</addrLine>
									<postCode>90094</postCode>
									<settlement>Playa Vista</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California Institute for Creative Technologies</orgName>
								<address>
									<addrLine>12015 Waterfront Drive</addrLine>
									<postCode>90094</postCode>
									<settlement>Playa Vista</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="500" to="510"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario. Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Q-learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The dialogue policy of a dialogue system decides on which actions the system should perform given a particular dialogue state (i.e., dialogue context). Building a dialogue policy can be a challenging task especially for complex applications. For this reason, recently much attention has been drawn to machine learning approaches to dialogue man- agement and in particular Reinforcement Learning (RL) of dialogue policies <ref type="bibr" target="#b39">(Williams and Young, 2007;</ref><ref type="bibr" target="#b33">Rieser et al., 2011;</ref><ref type="bibr" target="#b21">Jurčíček et al., 2012</ref>).</p><p>Typically there are three main approaches to the problem of learning dialogue policies using RL: (1) learn against a simulated user (SU), i.e., a model that simulates the behavior of a real user ; <ref type="bibr" target="#b34">Schatzmann et al., 2006</ref>); (2) learn directly from a corpus ( <ref type="bibr" target="#b19">Henderson et al., 2008;</ref><ref type="bibr" target="#b24">Li et al., 2009)</ref>; or (3) learn via live interac- tion with human users ( <ref type="bibr" target="#b35">Singh et al., 2002;</ref><ref type="bibr" target="#b11">Gaši´Gaši´c et al., 2011;</ref><ref type="bibr" target="#b13">Gaši´Gaši´c et al., 2013)</ref>.</p><p>We propose a fourth approach: concurrent learning of the system policy and the SU policy using multi-agent RL techniques. Both agents are trained simultaneously and there is no need for building a SU separately or having access to a cor- pus. <ref type="bibr">1</ref> As we discuss below, concurrent learning could potentially be used for learning via live in- teraction with human users. Moreover, for negoti- ation in particular there is one more reason in fa- vor of concurrent learning as opposed to learning against a SU. Unlike slot-filling domains, in nego- tiation the behaviors of the system and the user are symmetric. They are both negotiators, thus build- ing a good SU is as difficult as building a good system policy.</p><p>So far research on using RL for dialogue pol- icy learning has focused on single-agent RL tech- niques. Single-agent RL methods make the as- sumption that the system learns by interacting with a stationary environment, i.e., an environment that does not change over time. Here the environ- ment is the user. Generally the assumption that users do not significantly change their behavior over time holds for simple information providing tasks (e.g., reserving a flight). But this is not nec- essarily the case for other genres of dialogue, in- cluding negotiation. Imagine a situation where a negotiator is so uncooperative and arrogant that the other negotiators decide to completely change their negotiation strategy in order to punish her. Therefore it is important to investigate RL ap- proaches that do not make such assumptions about the user/environment. Multi-agent RL is designed to work for non- stationary environments. In this case the envi- ronment of a learning agent is one or more other agents that can also be learning at the same time. Therefore, unlike single-agent RL, multi-agent RL can handle changes in user behavior or in the be- havior of other agents participating in the inter- action, and thus potentially lead to more realis- tic dialogue policies in complex dialogue scenar- ios. This ability of multi-agent RL can also have important implications for learning via live inter- action with human users. Imagine a system that learns to change its strategy as it realizes that a particular user is no longer a novice user, or that a user no longer cares about five star restaurants.</p><p>We apply multi-agent RL to a resource alloca- tion negotiation scenario. Two agents with dif- ferent preferences negotiate about how to share resources. We compare Q-learning (a single- agent RL algorithm) with two multi-agent RL al- gorithms: Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) <ref type="bibr" target="#b2">(Bowling and Veloso, 2002</ref>). We vary the scenario complexity (i.e., the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate.</p><p>Our research contributions are as follows: (1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of cur- rent approaches to dialogue policy learning (i.e., the need for SUs and corpora), which may also potentially prove useful for learning via live inter- action with human users; (2) we show that concur- rent learning can address changes in user behav- ior over time, and requires multi-agent RL tech- niques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC- WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC- WoLF in such a variety of situations (varying a large number of parameters).</p><p>The paper is structured as follows. Section 2 presents related work. Section 3 provides a brief introduction to single-agent RL and multi-agent RL. Section 4 describes our negotiation domain and experimental setup. In section 5 we present our results. Finally, section 6 concludes and pro- vides some ideas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most research in RL for dialogue management has been done in the framework of slot-filling applica- tions such as restaurant recommendations ( <ref type="bibr" target="#b38">Thomson and Young, 2010;</ref><ref type="bibr" target="#b12">Gaši´Gaši´c et al., 2012;</ref><ref type="bibr" target="#b8">Daubigney et al., 2012)</ref>, flight reser- vations ( <ref type="bibr" target="#b19">Henderson et al., 2008)</ref>, sightseeing rec- ommendations ( <ref type="bibr" target="#b27">Misu et al., 2010)</ref>, appointment scheduling ( <ref type="bibr" target="#b16">Georgila et al., 2010)</ref>, etc. RL has also been applied to question-answering ( <ref type="bibr" target="#b28">Misu et al., 2012)</ref>, tutoring domains <ref type="bibr" target="#b37">(Tetreault and Litman, 2008;</ref><ref type="bibr" target="#b5">Chi et al., 2011)</ref>, and learning negotiation dialogue policies <ref type="bibr" target="#b18">(Heeman, 2009;</ref><ref type="bibr" target="#b14">Georgila and Traum, 2011;</ref><ref type="bibr" target="#b17">Georgila, 2013)</ref>.</p><p>As mentioned in section 1, there are three main approaches to the problem of learning dialogue policies using RL.</p><p>In the first approach, a SU is hand-crafted or learned from a small corpus of human-human or human-machine dialogues. Then the dialogue pol- icy can be learned by having the system interact with the SU for a large number of dialogues (usu- ally thousands of dialogues). Depending on the application, building a realistic SU can be just as difficult as building a good dialogue policy. Fur- thermore, it is not clear what constitutes a good SU for dialogue policy learning. Should the SU resemble real user behavior as closely as possi- ble, or should it exhibit some degree of random- ness to explore a variety of interaction patterns? Despite much research on the issue, these are still open questions ( <ref type="bibr" target="#b34">Schatzmann et al., 2006;</ref><ref type="bibr" target="#b1">Ai and Litman, 2008;</ref><ref type="bibr" target="#b31">Pietquin and Hastie, 2013)</ref>.</p><p>In the second approach, no SUs are required. Instead the dialogue policy is learned directly from a corpus of human-human or human-machine dia- logues. For example, <ref type="bibr" target="#b19">Henderson et al. (2008)</ref> used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation do- main, whereas <ref type="bibr" target="#b24">Li et al. (2009)</ref> used Least-Squares Policy Iteration ( <ref type="bibr" target="#b22">Lagoudakis and Parr, 2003)</ref>, an RL-based technique that can learn directly from corpora, in a voice dialer application. However, collecting such corpora is not trivial, especially in new domains. Typically, data are collected in a Wizard-of-Oz setup where human users think that they interact with a system while in fact they inter- act with a human pretending to be the system, or by having human users interact with a preliminary version of the dialogue system. In both cases the resulting interactions are expected to be quite dif-ferent from the interactions of human users with the final system. In practice this means that dia- logue policies learned from such data could be far from optimal.</p><p>The first experiment on learning via live inter- action with human users (third approach) was re- ported by <ref type="bibr" target="#b35">Singh et al. (2002)</ref>. They used RL to help the system with two choices: how much ini- tiative it should allow the user, and whether or not to confirm information provided by the user. Re- cently, learning of "full" dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possi- ble with the use of Gaussian processes <ref type="bibr" target="#b9">(Engel et al., 2005;</ref><ref type="bibr" target="#b32">Rasmussen and Williams, 2006</ref>). Typi- cally learning a dialogue policy is a slow process requiring thousands of dialogues, hence the need for SUs. Gaussian processes have been shown to speed up learning. This fact together with easy access to a large number of human users through crowd-sourcing has allowed dialogue policy learn- ing via live interaction with human users <ref type="bibr" target="#b11">(Gaši´Gaši´c et al., 2011;</ref><ref type="bibr" target="#b13">Gaši´Gaši´c et al., 2013</ref>).</p><p>Space constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management. Thus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains.</p><p>So far research on RL in the dialogue commu- nity has focused on using single-agent RL tech- niques where the stationary environment is the user. Most approaches assume that the user goal is fixed and that the behavior of the user is ratio- nal. Other approaches account for changes in user goals <ref type="bibr" target="#b26">(Ma, 2013)</ref>. In either case, one can build a user simulation model that is the average of dif- ferent user behaviors or learn a policy from a cor- pus that contains a variety of interaction patterns, and thus safely assume that single-agent RL tech- niques will work. However, in the latter case if the behavior of the user changes significantly over time then the assumption that the environment is stationary will no longer hold.</p><p>There has been a lot of research on multi-agent RL in the optimal control and robotics communi- ties <ref type="bibr" target="#b25">(Littman, 1994;</ref><ref type="bibr" target="#b20">Hu and Wellman, 1998;</ref><ref type="bibr" target="#b3">Busoniu et al., 2008)</ref>. Here two or more agents learn si- multaneously. Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time. Therefore the environment is no longer stationary and single-agent RL techniques do not work well or do not work at all. We are particu- larly interested in the work of <ref type="bibr" target="#b2">Bowling and Veloso (2002)</ref> who proposed the PHC and PHC-WoLF al- gorithms that we use in this paper. We chose these two algorithms because, unlike other multi-agent RL methods <ref type="bibr" target="#b25">(Littman, 1994;</ref><ref type="bibr" target="#b20">Hu and Wellman, 1998</ref>), they do not make assumptions that do not always hold and do not require quadratic or linear programming that does not always scale. <ref type="bibr" target="#b10">English and Heeman (2005)</ref> were the first in the dialogue community to explore the idea of con- current learning of dialogue policies. However, English and Heeman (2005) did not use multi- agent RL but only standard single-agent RL, in particular an on-policy Monte Carlo method <ref type="bibr" target="#b36">(Sutton and Barto, 1998)</ref>. But single-agent RL tech- niques are not well suited for concurrent learning where each agent is trained against a continuously changing environment. Indeed, English and Hee- man (2005) reported problems with convergence.  proposed a frame- work for co-adaptation of the dialogue policy and the SU using single-agent RL. They applied In- verse Reinforcement Learning (IRL) ( <ref type="bibr" target="#b0">Abbeel and Ng, 2004</ref>) to a corpus in order to learn the reward functions of both the system and the SU. Further- more, <ref type="bibr" target="#b7">Cuayáhuitl and Dethlefs (2012)</ref> used hier- archical multi-agent RL for co-ordinating the ver- bal and non-verbal actions of a robot. Cuayáhuitl and Dethlefs (2012) did not use PHC or PHC- WoLF and did not compare against single-agent RL methods.</p><p>With regard to using RL for learning negotia- tion policies, the amount of research that has been performed is very limited compared to slot-filling. <ref type="bibr" target="#b10">English and Heeman (2005)</ref> learned negotiation policies for a furniture layout task. Then Hee- man (2009) extended this work by experiment- ing with different representations of the RL state in the same domain (this time learning against a hand-crafted SU). In both cases, to reduce the search space, the RL state included only infor- mation about e.g., whether there was a pending proposal rather than the actual value of this pro- posal. <ref type="bibr" target="#b30">Paruchuri et al. (2009)</ref> performed a theo- retical study on how Partially Observable Markov Decision Processes (POMDPs) can be applied to negotiation domains. <ref type="bibr" target="#b14">Georgila and Traum (2011)</ref> built argumentation dialogue policies for negotiation against users of different cultural norms in a one-issue negotiation scenario. To learn these policies they trained SUs on a spoken dialogue corpus in a florist-grocer negotiation domain, and then tweaked these SUs towards a particular cultural norm using hand- crafted rules. Georgila (2013) learned argumen- tation dialogue policies from a simulated corpus in a two-issue negotiation scenario (organizing a party). Finally, <ref type="bibr" target="#b29">Nouri et al. (2012)</ref> used IRL to learn a model for cultural decision-making in a simple negotiation game (the Ultimatum Game).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Single-Agent vs. Multi-Agent Reinforcement Learning</head><p>Reinforcement Learning (RL) is a machine learn- ing technique used to learn the policy of an agent, i.e., which action the agent should perform given its current state <ref type="bibr" target="#b36">(Sutton and Barto, 1998</ref> An MDP is defined as a tuple (S, A, T , R, γ) where S is the set of states (representing different contexts) which the agent may be in, A is the set of actions of the agent, T is the transition func- tion S × A × S → [0, 1] which defines a set of transition probabilities between states after taking an action, R is the reward function S × A → which defines the reward received when taking an action from the given state, and γ is a factor that discounts future rewards. Solving the MDP means finding a policy π : S → A. The quality of the policy π is measured by the expected discounted (with discount factor γ) future reward also called Q-value, Q π : S × A → .</p><p>A stochastic game is defined as a tuple (n, S, A 1...n , T , R 1...n , γ) where n is the number of agents, S is the set of states, A i is the set of ac- tions available for agent i (and A is the joint ac- tion space A 1 × A 2 × ... × A n ), T is the transi- tion function S × A × S → [0, 1] which defines a set of transition probabilities between states af- ter taking a joint action, R i is the reward function for the ith agent S × A → , and γ is a factor that discounts future rewards. The goal is for each agent i to learn a mixed policy π i : S × A i → <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> </p><note type="other">that maps states to mixed strategies, which are probability distributions over the agent's actions, so that the agent's expected discounted (with dis- count factor γ) future reward is maximized. Stochastic games are a generalization of MDPs for multi-agent RL. In stochastic games there are many agents that select actions and the next state and rewards depend on the joint action of all these agents. The agents can have different reward functions. Partially Observable Stochastic Games (POSGs) are the equivalent of POMDPs for multi- agent RL. In POSGs, the agents have different ob- servations, and uncertainty about the state they are in and the beliefs of their interlocutors. POSGs are very hard to solve but new algorithms continu- ously emerge in the literature.</note><p>In this paper we use three algorithms: Q- learning, Policy Hill-Climbing (PHC), and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF). PHC is an extension of Q-learning. For all three algorithms, Q-values are updated as follows:</p><formula xml:id="formula_0">Q(s, a) ← (1−α)Q(s, a)+α r + γmax a Q(s , a )</formula><p>(1) In Q-learning, for a given state s, the agent performs the action with the highest Q-value for that state. In addition to Q-values, PHC and PHC-WoLF also maintain the current mixed pol- icy π(s, a). In each step the mixed policy is up- dated by increasing the probability of selecting the highest valued action according to a learning rate δ (see equations (2), (3), and (4) below).</p><formula xml:id="formula_1">π(s, a) ← π(s, a) + ∆ sa<label>(2)</label></formula><formula xml:id="formula_2">∆ sa = −δ sa if a = argmax a Q(s, a ) Σ a =a δ sa otherwise<label>(3)</label></formula><formula xml:id="formula_3">δ sa = min π(s, a), δ |A i | − 1<label>(4)</label></formula><p>The difference between PHC and PHC-WoLF is that PHC uses a constant learning rate δ whereas PHC-WoLF uses a variable learning rate (see equation <ref type="formula" target="#formula_4">(5)</ref> below). The main idea is that when the agent is "winning" the learning rate δ W should be low so that the opponents have more time to adapt to the agent's policy, which helps with con- vergence. On the other hand when the agent is "losing" the learning rate δ LF should be high so that the agent has more time to adapt to the other agents' policies, which also facilitates conver- gence. Thus PHC-WoLF uses two learning rates δ W and δ LF . PHC-WoLF determines whether the agent is "winning" or "losing" by comparing the current policy's π(s, a) expected payoff with that of the average policy˜πpolicy˜ policy˜π(s, a) over time. If the cur- rent policy's expected payoff is greater then the agent is "winning", otherwise it is "losing".</p><formula xml:id="formula_4">δ =        δ W if Σ α π(s, α )Q(s, α ) &gt; Σ α ˜ π(s, α )Q(s, α ) δ LF otherwise<label>(5)</label></formula><p>More details about Q-learning, PHC, and PHC- WoLF can be found in ( <ref type="bibr" target="#b36">Sutton and Barto, 1998;</ref><ref type="bibr" target="#b2">Bowling and Veloso, 2002</ref>).</p><p>As discussed in sections 1 and 2, single-agent RL techniques, such as Q-learning, are not suit- able for multi-agent RL. Nevertheless, despite its shortcomings Q-learning has been used success- fully for multi-agent RL <ref type="bibr" target="#b6">(Claus and Boutilier, 1998)</ref>. Indeed, as we see in section 5, Q-learning can converge to the optimal policy for small state spaces. However, as the state space size increases the performance of Q-learning drops (compared to PHC and PHC-WoLF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Domain and Experimental Setup</head><p>Our domain is a resource allocation negotiation scenario. Two agents negotiate about how to share resources. For the sake of readability from now on we will refer to apples and oranges.</p><p>The two agents have different goals. Also, they have human-like constraints of imperfect in- formation about each other; they do not know each other's reward function or degree of rational- ity (during learning our agents can be irrational). Thus a Nash equilibrium (if there exists one) can- not be computed in advance. Agent 1 cares more about apples and Agent 2 cares more about or- anges. <ref type="table">Table 1</ref> shows the points that Agents 1 and 2 earn for each apple and each orange that they have at the end of the negotiation.  <ref type="table" target="#tab_1">2  apple  300  200  orange  200  300   Table 1</ref>: Points earned by Agents 1 and 2 for each apple and each orange that they have at the end of the negotiation. We use a simplified dialogue model with two types of speech acts: offers and acceptances. The dialogue proceeds as follows: one agent makes an offer, e.g., "I give you 3 apples and 1 orange", and the other agent may choose to accept it or make a new offer. The negotiation finishes when one of the agents accepts the other agent's offer or time runs out.</p><p>We compare Q-learning with PHC and PHC- WoLF. For all algorithms and experiments each agent is rewarded only at the end of the dialogue based on the negotiation outcome (see <ref type="table">Table 1</ref>). Thus the two agents have different reward func- tions. There is also a penalty of -10 for each agent action to ensure that dialogues are not too long. Also, to avoid long dialogues, if none of the agents accepts the other agent's offers, the negotiation finishes after 20 pairs of exchanges between the two agents (20 offers from Agent 1 and 20 offers from Agent 2).</p><p>An example interaction between the two agents is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. As we can see, each agent can offer any combination of apples and oranges. So if we have X apples and Y oranges for sharing, there can be (X + 1) × (Y + 1) possible offers. For example if we have 2 apples and 2 oranges for sharing, there can be 9 possible offers: "offer- 0-0", "offer-0-1", ..., "offer-2-2". For our exper- iments we vary the number of fruits to be shared and choose to keep X equal to Y . <ref type="table" target="#tab_1">Table 2</ref> shows our state representation, i.e., the state variables that we keep track of with all the possible values they can take, where X is the num-Current offer: (X + 1) × (Y + 1) possible values How many times the current offer has already been rejected: (0, 1, 2, 3, or 4) Is the current offer accepted: yes, no ber of apples and Y is the number of oranges to be shared. The third variable is always set to "no" un- til one of the agents accepts the other agent's offer. <ref type="table">Table 3</ref> shows the state and action space sizes for different numbers of apples and oranges to be shared used in our experiments below. The num- ber of actions includes the acceptance of an of- fer. <ref type="table">Table 3</ref> also shows the number of state-action pairs (Q-values). As we will see in section 5, even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the as- sumption of interacting with a stationary environ- ment no longer holds. <ref type="bibr">For comparison, in (English and Heeman, 2005</ref>) the state specification for each agent included 5 binary variables resulting in 32 possible states. <ref type="bibr" target="#b10">English and Heeman (2005)</ref> kept track of whether there was an offer on the table but not of the actual value of the offer. For our task it is essential to keep track of the offer values, which of course results in much larger state spaces. Also, in <ref type="bibr" target="#b10">(English and Heeman, 2005</ref>) there were 5 possi- ble actions resulting in 160 state-action pairs. Our state and action spaces are much larger and fur- thermore we explore the effect of different state and action space sizes on convergence.</p><p>During learning the two agents interact for 5 epochs. Each epoch contains N number of episodes. We vary N from 25,000 up to 400,000 with a step of 25,000 episodes. English and Hee- man (2005) trained their agents for 200 epochs, where each epoch contained 200 episodes.</p><p>We also vary the exploration rate per epoch. In particular, in the experiments reported in sec- tion 5.1 the exploration rate is set as follows: 0.95 for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3 for epoch 4, and 0.1 for epoch 5. Section 5.2 re- ports results again with 5 epochs of training but a constant exploration rate per epoch set to 0.3. An exploration rate of 0.3 means that 30% of the time the agent will select an action randomly.</p><p>Finally, we vary the learning rate. For PHC- #States #Actions #State-Action <ref type="table" target="#tab_1">Pairs  1 A &amp; O  40  5  200  2 A &amp; O  90  10  900  3 A &amp; O  160  17  2720  4 A &amp; O  250  26  6500  5 A &amp; O  360  37  13320  6 A &amp; O  490  50  24500  7 A &amp; O  640  65  41600   Table 3</ref>: State space, action space, and state-action space sizes for different numbers of apples and or- anges to be shared (A: apples, O: oranges).</p><p>WoLF we set δ W = 0.05 and δ LF = 0.2 (see sec- tion 3). These values were chosen with exper- imentation and the basic idea is that the agent should learn faster when "losing" and slower when "winning". For PHC we explore two cases. In the first case which from now on will be referred to as PHC-W, we set δ to be equal to δ W (also used for PHC-WoLF). In the second case which from now on will be referred to as PHC-LF, we set δ to be equal to δ LF (also used for PHC-WoLF). So unlike PHC-WoLF, PHC-W and PHC-LF do not use a variable learning rate. PHC-W always learns slowly and PHC-LF always learns fast. In all the above cases, training stops after 5 epochs. Then we test the learned policies against each other for one more epoch the size of which is the same as the size of the epochs used for train- ing. For example, if the policies were learned for 5 epochs with each epoch containing 25,000 episodes, then for testing the two policies will in- teract for another 25,000 episodes. For compari- son, English and Heeman (2005) had their agents interact for 5,000 dialogues during testing. To en- sure that the policies do not converge by chance, we run the training and test sessions 20 times each and we report averages. Thus all results presented in section 5 are averages of 20 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Given that Agent 1 is more interested in apples and Agent 2 cares more about oranges, the maxi- mum total utility solution would be the case where each agent offers to get all the fruits it cares about and to give its interlocutor all the fruits it does not care about, and the other agent accepts this of- fer. Thus, when converging to the maximum to- tal utility solution, in the case of 4 fruits (4 ap-ples and 4 oranges), the average reward of the two agents should be 1200 minus 10 for making or accepting an offer. For 5 fruits the average re- ward should be 1500 minus 10, and so forth. We call 1200 (or 1500) the convergence reward, i.e., the reward after converging to the maximum to- tal utility solution if we do not take into account the action penalty. For example, in the case of 4 fruits, if Agent 1 starts the negotiation, after con- verging to the maximum total utility solution the optimal interaction should be: Agent 1 makes an offer to Agent 2, namely 0 apples and 4 oranges, and Agent 2 accepts. Thus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the av- erage reward of the two agents is also 1190. Also, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200.</p><p>Below, in all the graphs that we provide, we show the average distance from the convergence reward. This is to make all graphs comparable because in all cases the optimal average distance from the convergence reward of the two agents should be equal to 10 (make the optimal offer or accept the optimal offer that the other agent makes). The formulas for calculating the average distance from the convergence reward are:</p><formula xml:id="formula_5">AD 1 = nr j=1 |CR 1 − R 1j | n r (6) AD 2 = nr j=1 |CR 2 − R 2j | n r (7) AD = AD 1 + AD 2 2 (8)</formula><p>where CR 1 is the convergence reward for Agent 1, R 1j is the reward of Agent 1 for run j, CR 2 is the convergence reward for Agent 2, and R 2j is the reward of Agent 2 for run j. Moreover, AD 1 is the average distance from the convergence reward for Agent 1, AD 2 is the average distance from the convergence reward for Agent 2, and AD is the average of AD 1 and AD 2 . All graphs of section 5</p><p>show AD values. Also, n r is the number of runs (in our case always equal to 20). Thus in the case of 4 fruits, we will have CR 1 =CR 2 =1200, and if for all runs R 1j =R 2j =1190, then AD=10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Variable Exploration Rate</head><p>In this section we report results with different ex- ploration rates per training epoch (see section 4).  <ref type="table">Table 4</ref>: Average distance from convergence re- ward over 20 runs for 100,000 episodes per epoch and for different numbers of fruits to be shared (A: apples, O: oranges). The best possible value is 10. <ref type="table">Table 4</ref> shows the average distance from the con- vergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC- W, and PHC-WoLF). It is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence. Also, for 1, 2, and 3 fruits all algorithms converge and perform comparably. As the number of fruits in- creases, Q-learning starts performing worse than the multi-agent RL algorithms. For 7 fruits PHC- W appears to perform worse than Q-learning but this is because, as we can see in <ref type="figure" target="#fig_5">Figure 5</ref>, in this case more than 400,000 episodes per epoch are re- quired for convergence. Thus after only 100,000 episodes per epoch all policies still behave some- what randomly.</p><p>Figures 2, 3, 4, and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively. For 4 fruits it takes about 125,000 episodes per epoch and for 5 fruits it takes about 225,000 episodes per epoch for the policies to converge. This number rises to ap- proximately 350,000 for 6 fruits and becomes even higher for 7 fruits. Q-learning consistently per- forms worse than the rest of the algorithms. The differences between PHC-LF, PHC-W, and PHC- WoLF are insignificant, which is a bit surprising given that <ref type="bibr" target="#b2">Bowling and Veloso (2002)</ref> showed that PHC-WoLF performed better than PHC in a series of benchmark tasks. In <ref type="figure" target="#fig_2">Figures 2 and 3</ref>, PHC-LF appears to be reaching convergence slightly faster than PHC-W and PHC-WoLF but this is not statis- tically significant.  Average distance from convergence reward during testing (20 runs). The best possible value is 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Constant Exploration Rate</head><p>In this section we report results with a constant exploration rate for all training epochs (see sec- tion 4). <ref type="figure" target="#fig_6">Figures 6 and 7</ref> show the average dis- tance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively. Clearly having a constant exploration rate in all epochs is problem- atic. For 4 fruits, after 225,000 episodes per epoch there is still no convergence. For comparison, with a variable exploration rate it took about 125,000 episodes per epoch for the policies to converge. Likewise for 5 fruits. After 400,000 episodes per epoch there is still no convergence. For compari- son, with a variable exploration rate it took about 225,000 episodes per epoch for convergence.  The above results show that, unlike single-agent RL where having a constant exploration rate is perfectly acceptable, here a constant exploration rate does not work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We used single-agent RL and multi-agent RL for learning dialogue policies in a resource allocation negotiation scenario. Two agents interacted with each other and both learned at the same time. The advantage of this approach is that it does not re- quire SUs to train against or corpora to learn from.</p><p>We compared a traditional single-agent RL al- gorithm (Q-learning) against two multi-agent RL algorithms (PHC and PHC-WoLF) varying the scenario complexity (state space size), the number  of training episodes, and the learning and explo- ration rates. Our results showed that Q-learning is not suitable for concurrent learning given that it is designed for learning against a stationary en- vironment. Q-learning failed to converge in all cases, except for very small state space sizes. On the other hand, both PHC and PHC-WoLF always converged (or in the case of 7 fruits they needed more training episodes) and performed similarly. We also showed that in concurrent learning very high gradually decreasing exploration rates are re- quired for convergence. We conclude that multi- agent RL of dialogue policies is a promising alter- native to using single-agent RL and SUs or learn- ing directly from corpora.</p><p>The focus of this paper is on comparing single- agent RL and multi-agent RL for concurrent learn- ing, and studying the implications for convergence and exploration/learning rates. Our next step is testing with human users. We are particularly in- terested in users whose behavior changes during the interaction and continuous testing against ex- pert repeat users, which has never been done be- fore. Another interesting question is whether cor- pora or SUs may still be required for designing the state and action spaces and the reward func- tions of the interlocutors, bootstrapping the poli- cies, and ensuring that information about the be- havior of human users is encoded in the resulting learned policies. Gaši´ <ref type="bibr" target="#b13">Gaši´c et al. (2013)</ref> showed that it is possible to learn "full" dialogue policies just via interaction with human users (without any boot- strapping using corpora or SUs). Similarly, con- current learning could be used in an on-line fash- ion via live interaction with human users. Or al- ternatively concurrent learning could be used off- line to bootstrap the policies and then these poli- cies could be improved via live interaction with human users (again using concurrent learning to address possible changes in user behavior). These are open research questions for future work. Furthermore, we intend to apply multi-agent RL to more complex negotiation domains, e.g., exper- iment with more than two types of resources (not just apples and oranges) and more types of actions (not just offers and acceptances). We would also like to compare policies learned with multi-agent RL techniques with policies learned with SUs or from corpora both in simulation and with human users. Finally, we aim to experiment with differ- ent feature-based representations of the state and action spaces. Currently all possible deal combi- nations are listed as possible actions and as ele- ments of the state, which can quickly lead to very large state and action spaces as the application be- comes more complex (in our case as the number of fruits increases). However, abstraction is not triv- ial because the agents have no guarantee that the value of a deal is a simple function of the value of its parts, and values may differ for different agents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example interaction between Agents 1 and 2 (A: apples, O: oranges).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 4 fruits and variable exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10.</figDesc><graphic url="image-1.png" coords="8,72.00,62.81,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 5 fruits and variable exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10.</figDesc><graphic url="image-2.png" coords="8,72.00,299.16,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 6 fruits and variable exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10.</figDesc><graphic url="image-3.png" coords="8,307.28,62.81,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: 7 fruits and variable exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10.</figDesc><graphic url="image-4.png" coords="8,307.28,298.53,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 4 fruits and constant exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10.</figDesc><graphic url="image-5.png" coords="9,72.00,62.81,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: 5 fruits and constant exploration rate: Average distance from convergence reward during testing (20 runs). The best possible value is 10.</figDesc><graphic url="image-6.png" coords="9,72.00,301.00,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>State variables. 

</table></figure>

			<note place="foot" n="1"> Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Claire Nelson sadly died in May 2013. We con-tinued and completed this work after her pass-ing away. She is greatly missed. This work was funded by the NSF grant #1117313.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning<address><addrLine>Bannf, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Assessing dialog system user simulation evaluation measures using human judges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics, Columbus</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics, Columbus<address><addrLine>Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiagent learning using a variable learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="250" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-adaptation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Senthilkumar Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Workshop on Spoken Dialogue Systems</title>
		<meeting>of the International Workshop on Spoken Dialogue Systems<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Vanlehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="137" to="180" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The dynamics of reinforcement learning in cooperative multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Conference on Artificial Intelligence</title>
		<meeting>of the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical multiagent reinforcement learning for coordinating verbal and nonverbal actions in robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Dethlefs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ECAI Workshop on Machine Learning for Interactive Systems</title>
		<meeting>of the ECAI Workshop on Machine Learning for Interactive Systems<address><addrLine>Montpellier, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comprehensive reinforcement learning framework for dialogue management optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucie</forename><surname>Daubigney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthilkumar</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="891" to="902" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reinforcement learning with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaakov</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning mixed initiative dialogue strategies by using reinforcement learning on both conversants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">A</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On-line policy optimisation of spoken dialogue systems via live interaction with human subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>of the IEEE Automatic Speech Recognition and Understanding Workshop<address><addrLine>Big Island, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Policy optimisation of POMDP-based dialogue systems without state space compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Workshop on Spoken Language Technology</title>
		<meeting>of the IEEE Workshop on Spoken Language Technology<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On-line policy optimisation of Bayesian spoken dialogue systems via human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>of the International Conference on Acoustics, Speech and Signal essing<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reinforcement learning of argumentation dialogue policies in negotiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">User simulation for spoken dialogue systems: Learning and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning dialogue strategies from older and younger simulated users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">K</forename><surname>Wolters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>of the Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement learning of two-issue negotiation dialogue policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>of the Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Metz, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representing the reinforcement learning state in a negotiation dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>of the IEEE Automatic Speech Recognition and Understanding Workshop<address><addrLine>Merano, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hybrid reinforcement/supervised learning of dialogue policies from fixed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="511" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning: Theoretical framework and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junling</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforcement learning for parameter estimation in statistical spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="168" to="192" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leastsquares policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michail</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating effectiveness and portability of reinforcement learned dialogue strategies with real users: The TALK TownInfo evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Workshop on Spoken Language Technology</title>
		<meeting>of the IEEE Workshop on Spoken Language Technology<address><addrLine>Palm Beach, Aruba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reinforcement learning for dialog management using least-squares policy iteration and fast feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhrid</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning<address><addrLine>New Brunswick, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">User goal change model for spoken dialog state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NAACL-HLT Student Research Workshop</title>
		<meeting>of the NAACL-HLT Student Research Workshop<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling spoken decision making dialogue and optimization of its dialogue strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Komei</forename><surname>Sugiura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyonori</forename><surname>Ohtake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Kashioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>of the Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforcement learning of question-answering dialogue policies for virtual museum guides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Leuski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>of the Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A cultural decision-making model for negotiation based on inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elnaz</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Cognitive Science Conference</title>
		<meeting>of the Cognitive Science Conference<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">POMDP based negotiation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paruchuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sycara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Workshop on Modeling Intercultural Collaboration and Negotiation</title>
		<meeting><address><addrLine>Pasadena, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on metrics for the evaluation of user simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="73" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive information presentation for spoken dialogue systems: Evaluation with human subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Workshop on Natural Language Generation</title>
		<meeting>of the European Workshop on Natural Language Generation<address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Stuttle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="133" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach to evaluating state representations in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Litman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="683" to="696" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="562" to="588" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scaling POMDPs for spoken dialog management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2116" to="2129" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
