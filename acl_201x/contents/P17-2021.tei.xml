<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards String-To-Tree Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
						</author>
						<title level="a" type="main">Towards String-To-Tree Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="132" to="140"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2021</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexical-ized constituency trees. Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Model</head><p>Neural Machine Translation (NMT) <ref type="bibr">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b4">Sutskever et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2014</ref>) has recently became the state-of-the-art approach to machine translation ( <ref type="bibr">Bojar et al., 2016)</ref>, while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches <ref type="bibr">(Koehn, 2010)</ref>. NMT models usually do not make ex- plicit use of syntactic information about the lan- guages at hand. However, a large body of work was dedicated to syntax-based SMT ( <ref type="bibr" target="#b7">Williams et al., 2016)</ref>. One prominent approach to syntax- based SMT is string-to-tree (S2T) translation <ref type="bibr">Knight, 2001, 2002</ref>), in which a source- language string is translated into a target-language tree. S2T approaches to SMT help to ensure the resulting translations have valid syntactic struc- ture, while also mediating flexible reordering be- tween the source and target languages. The main formalism driving current <ref type="bibr">S2T</ref> SMT systems is GHKM rules ( <ref type="bibr">Galley et al., 2004</ref><ref type="bibr">Galley et al., , 2006</ref>), which are synchronous transduction grammar (STSG) frag- ments, extracted from word-aligned sentence pairs with syntactic trees on one side. The GHKM translation rules allow flexible reordering on all levels of the parse-tree. We suggest that NMT can also benefit from the incorporation of syntactic knowledge, and propose a simple method of performing string-to-tree neu- ral machine translation. Our method is inspired by recent works in syntactic parsing, which model trees as sequences ( <ref type="bibr" target="#b6">Vinyals et al., 2015;</ref><ref type="bibr">Choe and Charniak, 2016)</ref>. Namely, we translate a source sentence into a linearized, lexicalized constituency tree, as demonstrated in <ref type="figure">Figure 2</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows a translation from our neural S2T model compared to one from a vanilla NMT model for the same source sentence, as well as the attention-induced word alignments of the two models. Note that the linearized trees we predict are dif- ferent in their structure from those in <ref type="bibr" target="#b6">Vinyals et al. (2015)</ref> as instead of having part of speech tags as terminals, they contain the words of the translated sentence. We intentionally omit the POS informa-Jane hatte eine Katze . → ( ROOT ( S ( N P Jane ) N P ( V P had ( N P a cat ) N P ) V P . ) S ) ROOT <ref type="figure">Figure 2</ref>: An example of a translation from a string to a linearized, lexicalized constituency tree. tion as including it would result in significantly longer sequences. The S2T model is trained on parallel corpora in which the target sentences are automatically parsed. Since this modeling keeps the form of a sequence-to-sequence learning task, we can employ the conventional attention-based sequence to sequence paradigm ( <ref type="bibr">Bahdanau et al., 2014)</ref> as-is, while enriching the output with syn- tactic information. Related Work Some recent works did propose to incorporate syntactic or other linguistic knowl- edge into NMT systems, although mainly on the source side: <ref type="bibr">Eriguchi et al. (2016a,b)</ref> replace the encoder in an attention-based model with a Tree-LSTM ( <ref type="bibr" target="#b5">Tai et al., 2015</ref>) over a constituency parse tree; <ref type="bibr">Bastings et al. (2017)</ref> encoded sen- tences using graph-convolutional networks over dependency trees; <ref type="bibr">Sennrich and Haddow (2016)</ref> proposed a factored NMT approach, where each source word embedding is concatenated to em- beddings of linguistic features of the word; <ref type="bibr">Luong et al. (2015)</ref> incorporated syntactic knowl- edge via multi-task sequence to sequence learning: their system included a single encoder with multi- ple decoders, one of which attempts to predict the parse-tree of the source sentence; <ref type="bibr" target="#b3">Stahlberg et al. (2016)</ref> proposed a hybrid approach in which trans- lations are scored by combining scores from an NMT system with scores from a Hiero <ref type="bibr">(Chiang, 2005</ref><ref type="bibr">(Chiang, , 2007</ref> system. <ref type="bibr" target="#b2">Shi et al. (2016)</ref> explored the syntactic knowledge encoded by an NMT encoder, showing the encoded vector can be used to pre- dict syntactic information like constituency trees, voice and tense with high accuracy.</p><p>In parallel and highly related to our work, <ref type="bibr">Eriguchi et al. (2017)</ref> proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder <ref type="bibr">(Dyer et al., 2016)</ref>, while <ref type="bibr">Nadejde et al. (2017)</ref> incorporated target syntax by predicting CCG tags serialized into the target translation. Our work differs from those by modeling syntax using constituency trees, as was previously common in the "traditional" syntax- based machine translation literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments &amp; Results</head><p>Experimental Setup We first experiment in a resource-rich setting by using the German-English portion of the WMT16 news translation task <ref type="bibr">(Bojar et al., 2016)</ref>, with 4.5 million sentence pairs. We then experiment in a low-resource scenario us- ing the German, Russian and Czech to English training data from the News Commentary v8 cor- pus, following <ref type="bibr">Eriguchi et al. (2017)</ref>. In all cases we parse the English sentences into constituency trees using the BLLIP parser <ref type="bibr">(Charniak and Johnson, 2005</ref>). <ref type="bibr">1</ref> To enable an open vocabulary trans- lation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target. <ref type="bibr">2</ref> In each experiment we train two models. A baseline model (bpe2bpe), trained to trans- late from the source language sentences to En- glish sentences without any syntactic annotation, and a string-to-linearized-tree model (bpe2tree), trained to translate into English linearized con- stituency trees as shown in <ref type="figure">Figure 2</ref>. Words are segmented into sub-word units using the BPE model we learn on the raw parallel data. We use the NEMATUS (Sennrich et al., 2017) 3 implemen- tation of an attention-based NMT model. <ref type="bibr">4</ref> We trained the models until there was no improvement on the development set in 10 consecutive check- points. Note that the only difference between the baseline and the bpe2tree model is the syntactic in- formation, as they have a nearly-identical amount of model parameters (the only additional param- eters to the syntax-aware system are the embed- dings for the brackets of the trees).</p><p>For all models we report results of the best performing single model on the dev-set (new- stest2013+newstest2014 in the resource rich set- ting, newstest2015 in the rest, as measured by BLEU) when translating newstest2015 and new- stest2016, similarly to <ref type="bibr">Sennrich et al. (2016a)</ref>; <ref type="bibr">Eriguchi et al. (2017)</ref>. To evaluate the string-to- tree translations we derive the surface form by re- moving the symbols that stand for non-terminals in the tree, followed by merging the sub-words. We also report the results of an ensemble of the last 5 checkpoints saved during each model training. We compute BLEU scores using the mteval-v13a.pl script from the Moses toolkit ( <ref type="bibr">Koehn et al., 2007</ref> Results As shown in <ref type="table">Table 1</ref>, for the resource-rich setting, the single models (bpe2bpe, bpe2tree) per- form similarly in terms of BLEU on newstest2015. On newstest2016 we witness an advantage to the bpe2tree model. A similar trend is found when evaluating the model ensembles: while they im- prove results for both models, we again see an ad- vantage to the bpe2tree model on newstest2016. <ref type="table" target="#tab_2">Table 2</ref> shows the results in the low-resource set- ting, where the bpe2tree model is consistently bet- ter than the bpe2bpe baseline. We find this in- teresting as the syntax-aware system performs a much harder task (predicting trees on top of the translations, thus handling much longer output se- quences) while having a nearly-identical amount of model parameters. In order to better understand where or how the syntactic information improves translation quality, we perform a closer analysis of the WMT16 experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis</head><p>The Resulting Trees Our model produced valid trees for 5970 out of 6003 sentences in the devel- opment set. While we did not perform an in-depth error-analysis, the trees seem to follow the syntax of English, and most choices seem reasonable.</p><p>Quantifying Reordering English and German differ in word order, requiring a significant amount of reordering to generate a fluent translation. A major benefit of S2T models in SMT is facilitat- ing reordering. Does this also hold for our neural S2T model? We compare the amount of reorder- ing in the bpe2bpe and bpe2tree models using a distortion score based on the alignments derived from the attention weights of the corresponding systems. We first convert the attention weights to hard alignments by taking for each target word the source word with highest attention weight. For an n-word target sentence t and source sentence s let a(i) be the position of the source word aligned to the target word in position i. We define:  </p><formula xml:id="formula_0">d(s, t) = 1 n n i=2 |a(i) − a(i − 1)|</formula><p>For example, for the translations in <ref type="figure" target="#fig_0">Figure 1</ref>, the above score for the bpe2tree model is 2.73, while the score for the bpe2bpe model is 1.27 as the bpe2tree model did more reordering. Note that for the bpe2tree model we compute the score only on tokens which correspond to terminals (words or sub-words) in the tree. We compute this score for each source-target pair on newstest2015 for each model. <ref type="figure">Figure 3</ref> shows a histogram of the binned score counts. The bpe2tree model has more translations with distortion scores in bins 1- onward and significantly less translations in the least-reordering bin (0) when compared to the bpe2bpe model, indicating that the syntactic in- formation encouraged the model to perform more reordering. 5 <ref type="figure" target="#fig_1">Figure 4</ref> tracks the distortion scores throughout the learning process, plotting the av- erage dev-set scores for the model checkpoints saved every 30k updates. Interestingly, both mod- els obey to the following trend: open with a rel- atively high distortion score, followed by a steep decrease, and from there ascend gradually. The bpe2tree model usually has a higher distortion score during training, as we would expect after our previous findings from <ref type="figure">Figure 3</ref>. Tying Reordering and Syntax The bpe2tree model generates translations with their con- stituency tree and their attention-derived align- ments. We can use this information to extract GHKM rules ( <ref type="bibr">Galley et al., 2004</ref>). <ref type="bibr">6</ref> We derive (244) x0 x1 (157) x1 x0 (80) x0 x1 ",/." (56) x1 x0 ",/." (17) x0 "eine" x1 VP(x0:TER PP(x1:TER x2:NP)) (90) x1 x2 x0 (65) x0 x1 x2 (31) x1 x2 x0 ",/." (13) x0 x1 x2 ",/." (7) x1 "der" x2 x0 VP(x0:TER x1:PP) (113) x1 x0 (82) x0 x1 (38) x1 x0 ",/." (18) x0 x1 ",/." (5) ",/." x0 x1 S(x0:NP VP(x1:TER x2:NP)) (69) x0 x1 x2 (51) x0 x2 x1 (35) x0 x1 x2 ",/." (20) x0 x2 x1 ",/." (6) "die" x0 x1 x2 VP(x0:TER x1:NP x2:PP) (52) x0 x1 x2 (38) x1 x2 x0 (20) x1 x2 x0 ",/." (11) x0 x1 x2 ",/." (9) x2 x1 x0 VP(x0:TER x1:NP PP(x2:TER x3:NP)) (40) x0 x1 x2 x3 (32) x1 x2 x3 x0 (18) x1 x2 x3 x0 ",/." (8) x0 x1 x2 x3 ",/." (5) x2 x3 x1 x0 VP(x0:TER NP(x1:NP x2:PP)) (61) x0 x1 x2 (38) x1 x2 x0 (19) x0 x1 x2 ",/." (8) x0 "eine" x1 x2 (8) x1 x2 x0 ",/." NP(x0:NP PP(x1:TER x2:NP)) (728) x0 x1 x2 (110) "die" x0 x1 x2 (107) x0 x1 x2 ",/." (56) x0 x1 "der" x2 (54) "der" x0 x1 x2 S(VP(x0:TER x1:NP)) (41) x1 x0 (26) x0 x1 (14) x1 x0 ",/." (7) "die" x1 x0 (5) x0 x1 ",/." VP(x0:TER x1:VP) (73) x0 x1 (38) x1 x0 (25) x0 x1 ",/." (15) x1 x0 ",/." (9) ",/." x0 x1 Can you say with the "I love you" stop? src Gerade in dieser schweren Phase hat er gezeigt, dass er für uns ein sehr wichtiger Spieler ist", konstatierte Barisic. ref Especially during these difficult times, he showed that he is a very important player for us", Barisic stated. 2tree Especially at this difficult time he has shown that he is a very important player for us," said Barisic. 2bpe It is precisely during this difficult period that he has shown us to be a very important player, "Barisic said. src</p><p>Hopfen und Malz -auch in China eine beliebte Kombination. "Ich weiß jetzt, dass ich das kann -prima!" ref Hops and malt -a popular combination even in China. "I now know that I can do it -brilliant!" 2tree Hops and malt -a popular combination in China.</p><p>"I now know that I can do that! 2bpe Hops and malt -even in China, a popular combination.</p><p>I know now that I can that -prima!" src Die Ukraine hatte gewarnt, Russland könnte auch die Gasversorgung für Europa unterbrechen. ref Ukraine warned that Russia could also suspend the gas supply to Europe. 2tree Ukraine had warned that Russia could also stop the supply of gas to Europe. 2bpe Ukraine had been warned, and Russia could also cut gas supplies to Europe. src Bis dahin gab es in Kollbach im Schulverband Petershausen-Kollbach drei Klassen und in Petershausen fünf. ref Until then, the school district association of Petershausen-Kollbach had three classes in Kollbach and five in Petershausen. 2tree until then, in Kollbach there were three classes and five classes in Petershausen. 2bpe until then there were three classes and in Petershausen five at the school board in Petershausen-Kollbach. <ref type="table">Table 4</ref>: Translation examples from newstest2015. The underlines correspond to the source word at- tended by the first opening bracket (these are consistently the main verbs or structural markers) and the target words this source word was most strongly aligned to. See the supplementary material for an attention weight matrix example when predicting a tree ( <ref type="figure">Figure 6</ref>) and additional output examples. hard alignments for that purpose by treating ev- ery source/target token-pair with attention score above 0.5 as an alignment. Extracting rules from the dev-set predictions resulted in 233,657 rules, where 22,914 of them (9.8%) included reorder- ing, i.e. contained variables ordered differently in the source and the target. We grouped the rules by their LHS (corresponding to a target syntac- tic structure), and sorted them by the total num- ber of RHS (corresponding to a source sequential structure) with reordering. <ref type="table" target="#tab_3">Table 3</ref> shows the top 10 extracted LHS, together with the top-5 RHS, for each rule. The most common rule, VP(x 0 :TER x 1 :NP) → x 1 x 0 , found in 184 sentences in the dev set (8.4%), is indicating that the sequence x 1 x 0 in German was reordered to form a verb phrase in English, in which x 0 is a terminal and x 1 is a noun phrase. The extracted GHKM rules reveal very sensible German-English reordering patterns.</p><p>Relative Constructions Browsing the produced trees hints at a tendency of the syntax-aware model to favor using relative-clause structures and sub- ordination over other syntactic constructions (i.e., "several cameras that are all priced..." vs. "sev- eral cameras, all priced..."). To quantify this, we count the English relative pronouns (who, which, that 7 , whom, whose) found in the newstest2015 translations of each model and in the reference translations, as shown in <ref type="figure">Figure 5</ref>. The bpe2tree model produces more relative constructions com- pared to the bpe2bpe model, and both models pro- duce more such constructions than found in the reference.</p><p>Main Verbs While not discussed until this point, the generated opening and closing brack- ets also have attention weights, providing another opportunity to to peak into the model's behavior. <ref type="figure">Figure 6</ref> in the supplementary material presents an example of a complete attention matrix, including the syntactic brackets. While making full sense of the attention patterns of the syntactic elements re- mains a challenge, one clear trend is that opening the very first bracket of the sentence consistently attends to the main verb or to structural mark- ers (i.e. question marks, hyphens) in the source sentence, suggesting a planning-ahead behavior of the decoder. The underlines in <ref type="table">Table 4</ref> correspond to the source word attended by the first opening bracket, and the target word this source word was most strongly aligned to. In general, we find the alignments from the syntax-based system more sensible (i.e. in <ref type="figure" target="#fig_0">Figure 1</ref> -the bpe2bpe alignments are off-by-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis and Human Evaluations</head><p>The bpe2tree translations read better than their bpe2bpe counterparts, both syntactically and se- mantically, and we highlight some examples which demonstrate this. <ref type="table">Table 4</ref> lists some rep- resentative examples, highlighting improvements that correspond to syntactic phenomena involving reordering or global structure. We also performed a small-scale human-evaluation using mechanical turk on the first 500 sentences in the dev-set. Fur- ther details are available in the supplementary ma- terial. The results are summarized in the following As can be seen, in 186 cases (37.2%) the human evaluators preferred the bpe2tree translations, vs. 154 cases (30.8%) for bpe2bpe, with the rest of the cases (30%) being neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We present a simple string-to-tree neural transla- tion model, and show it produces results which are better than those of a neural string-to-string model. While this work shows syntactic infor- mation about the target side can be beneficial for NMT, this paper only scratches the surface with what can be done on the subject. First, better mod- els can be proposed to alleviate the long sequence problem in the linearized approach or allow a more natural tree decoding scheme (Alvarez-Melis and Jaakkola, 2017). Comparing our approach to other syntax aware NMT models like <ref type="bibr">Eriguchi et al. (2017)</ref> and <ref type="bibr">Nadejde et al. (2017)</ref> may also be of in- terest. A Contrastive evaluation <ref type="bibr">(Sennrich, 2016)</ref> of a syntax-aware system vs. a syntax-agnostic system may also shed light on the benefits of in- corporating syntax into NMT. Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016a. Character-based decoding in tree- to-sequence attention-based neural machine transla- tion. In Proceedings of the 3rd Workshop on Asian Translation (WAT2016). pages 175-183.</p><p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshi- masa Tsuruoka. 2016b. Tree-to-sequence atten- tional neural machine translation. In Proceed- ings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 823-833. http://www.aclweb.org/anthology/P16-1078. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>Data The English side of the corpus was tok- enized (into Penn treebank format) and truecased using the scripts provided in Moses ( <ref type="bibr">Koehn et al., 2007)</ref>. We ran the BPE process on a concatenation of the source and target corpus, with 89500 BPE operations in the WMT experiment and with 45k operations in the other experiments. This resulted in an input vocabulary of 84924 tokens and an out- put vocabulary of 78499 tokens in the WMT16 experiment. The linearized constituency trees are obtained by simply replacing the POS tags in the parse trees with the corresponding word or sub- words. The output vocabulary in the bpe2tree models includes the target subwords and the tree symbols which correspond to an opening or clos- ing of a specific phrase type.</p><p>Hyperparameters The word embedding size was set to 500/256 and the encoder and decoder sizes were set to 1024/256 (WMT16/other ex- periments). For optimization we used Adadelta <ref type="bibr" target="#b10">(Zeiler, 2012</ref>) with minibatch size of 40. For de- coding we used beam search with a beam size of 12. We trained the bpe2tree WMT16 model on sequences with a maximum length of 150 to- kens (the average length for a linearized tree in the training set was about 50 tokens). It was trained for two weeks on a single Nvidia TitanX GPU. The bpe2bpe WMT16 model was trained on se- quences with a maximum length of 50 tokens, and with minibatch size of 80. It was trained for one week on a single Nvidia TitanX GPU. Only in the low-resource experiments we applied dropout as described in <ref type="bibr">Sennrich et al. (2016a)</ref> for Romanian- English.</p><p>Human Evaluation We performed human- evaluation on the Mechnical Turk platform. Each sentence was evaluated using two annotators. For each sentence, we presented the annotators with the English reference sentence, followed by the outputs of the two systems. The German source was not shown, and the two system's outputs were shown in random order. The annotators were in- structed to answer "Which of the two sentences, in your view, is a better portrayal of the the reference sentence." They were then given 6 options: "sent 1 is better", "sent 2 is better", "sent 1 is a little bet- ter", "sent 2 is a little better", "both sentences are equally good", "both sentences are equally bad". We then ignore differences between "better" and "a little better". We count as "strongly better" the cases where both annotators indicated the same sentence as better, as "weakly better" the cases were one annotator chose a sentence and the other indicated they are both good/bad. Other cases are treated as either "both good" / "both bad" or as disagreements.</p><p>Figure 6: The attention weights for the string-to- tree translation in <ref type="figure" target="#fig_0">Figure 1</ref> Additional Output Examples from both mod- els, in the format of <ref type="figure" target="#fig_0">Figure 1</ref>. Notice the improved translation and alignment quality in the tree-based translations, as well as the overall high structural quality of the resulting trees. The few syntactic mistakes in these examples are attachment errors of SBAR and PP phrases, which will also chal- lenge dedicated parsers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top-a lexicalized tree translation predicted by the bpe2tree model. Bottom-a translation for the same sentence from the bpe2bpe model. The blue lines are drawn according to the attention weights predicted by each model.</figDesc><graphic url="image-1.png" coords="1,315.76,472.19,201.30,150.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3: newstest2015 DE-EN translations binned by distortion amount</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Eugene Charniak and Mark Johnson. 2005 .</head><label>2005</label><figDesc>Coarse- to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meet- ing on Association for Computational Linguistics. Association for Computational Linguistics, pages 173-180. David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Pro- ceedings of the 43rd Annual Meeting on Associa- tion for Computational Linguistics. Association for Computational Linguistics, pages 263-270. David Chiang. 2007. Hierarchical phrase-based trans- lation. computational linguistics 33(2):201-228. Do Kook Choe and Eugene Charniak. 2016. Pars- ing as language modeling. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing. Association for Computa- tional Linguistics, Austin, Texas, pages 2331-2336. https://aclweb.org/anthology/D16-1257. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. 2016. Recurrent neural net- work grammars. In Proceedings of the 2016 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies. Association for Computational Linguistics, San Diego, California, pages 199-209. http://www.aclweb.org/anthology/N16-1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU results for the low-resource exper-
iments (News Commentary v8) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Top dev-set GHKM Rules with reordering. Numbers: rule counts. Bolded: reordering rules. 

src 
Dutzende türkischer Polizisten wegen "Verschwörung" gegen die Regierung festgenommen 
ref 
Tens of Turkish Policemen Arrested over 'Plotting' against Gov't 
2tree dozens of Turkish police arrested for "conspiracy" against the government. 
2bpe dozens of turkish policemen on "conspiracy" against the government arrested 
src 
Die Menschen in London weinten, als ich unsere Geschichte erzhlte. Er ging einen Monat nicht zu Arbeit. 
ref 
People in London were crying when I told our story. 
He ended up spending a month off work. 
2tree the people of london wept as I told our story. 
he did not go to work a month. 
2bpe the people of London, when I told our story. 
he went one month to work. 
src 
Achenbach habe für 121 Millionen Euro Wertgegenstände für Albrecht angekauft. 
ref 
Achenbach purchased valuables for Albrecht for 121 million euros. 
2tree Achenbach has bought valuables for Albrecht for 121 million euros. 
2bpe Achenbach have purchased value of 121 million Euros for Albrecht. 
src 
Apollo investierte 2008 1 Milliarde $ in Norwegian Cruise. 
Könntest du mal mit dem "ich liebe dich" aufhören? 
ref 
Apollo made a $1 billion investment in Norwegian Cruise in 2008. 
Could you stop with the "I love you"? 
2tree Apollo invested EUR $1 billion in Norwegian Cruise in 2008. 
Could you stop saying "I love you? 
2bpe Apollo invested 2008 $1 billion in Norwegian Cruise. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>table :</head><label>:</label><figDesc></figDesc><table>2bpe weakly better 
100 
2bpe strongly better 54 
2tree weakly better 122 
2tree strongly better 64 
both good 
26 
both bad 
3 
disagree 
131 

</table></figure>

			<note place="foot" n="1"> https://github.com/BLLIP/bllip-parser 2 https://github.com/rsennrich/ subword-nmt 3 https://github.com/rsennrich/nematus 4 Further technical details of the setup and training are available in the supplementary material.</note>

			<note place="foot" n="5"> We also note that in bins 4-6 the bpe2bpe model had slightly more translations, but this was not consistent among different runs, unlike the gaps in bins 0-3 which were consistent and contain most of the translations. 6 github.com/joshua-decoder/galley-ghkm 134</note>

			<note place="foot" n="7"> &quot;that&quot; also functions as a determiner. We do not distinguish the two cases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Intel Collabora-tive Research Institute for Computational Intelli-gence (ICRI-CI), and The Israeli Science Founda-tion (grant number 1555/15).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<title level="m">Tree-structured decoding with doubly recurrent neural networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Does string-based neural mt learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3215</idno>
		<title level="m">Sequence to sequence learning with neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">SyntaxBased Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan &amp; Claypool publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A syntaxbased statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A decoder for syntax-based statistical mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="303" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
