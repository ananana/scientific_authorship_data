<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
						</author>
						<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1601" to="1611"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1147</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences , and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neu-ral network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that Trivi-aQA is a challenging testbed that is worth significant future study. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading comprehension (RC) systems aim to an- swer any question that could be posed against the facts in some reference text. This goal is challeng- ing for a number of reasons: (1) the questions can be complex (e.g. have highly compositional se- mantics), (2) finding the correct answer can re- quire complex reasoning (e.g. combining facts from multiple sentences or background knowl- edge) and (3) individual facts can be difficult to Question: The Dodecanese Campaign of WWII that was an attempt by the Allied forces to capture islands in the Aegean Sea was the inspiration for which acclaimed 1961 commando film? Answer: The Guns of Navarone Excerpt: The Dodecanese Campaign of World War II was an attempt by Allied forces to capture the Italian- held Dodecanese islands in the Aegean Sea following the surrender of Italy in September 1943, and use them as bases against the German-controlled Balkans. The failed campaign, and in particular the Battle of Leros, inspired the 1957 novel The Guns of Navarone and the successful 1961 movie of the same name.</p><p>Question: American Callan Pinckney's eponymously named system became a best-selling <ref type="bibr">(1980s-2000s)</ref> book/video franchise in what genre? Answer: Fitness Excerpt: Callan Pinckney was an American fitness pro- fessional. She achieved unprecedented success with her Callanetics exercises. Her 9 books all became inter- national best-sellers and the video series that followed went on to sell over 6 million copies. Pinckney's first video release "Callanetics: 10 Years Younger In 10 Hours" outsold every other fitness video in the US. <ref type="figure">Figure 1</ref>: Question-answer pairs with sample ex- cerpts from evidence documents from TriviaQA exhibiting lexical and syntactic variability, and re- quiring reasoning from multiple sentences. recover from text (e.g. due to lexical and syntactic variation). <ref type="figure">Figure 1</ref> shows examples of all these phenomena. This paper presents TriviaQA, a new reading comprehension dataset designed to simul- taneously test all of these challenges.</p><p>Recently, significant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the chal- lenges listed above, for example by crowdsourc- ing the gathering of question answer pairs <ref type="bibr" target="#b23">(Rajpurkar et al., 2016</ref>) or using cloze-style sentences instead of questions ( <ref type="bibr" target="#b11">Hermann et al., 2015;</ref><ref type="bibr" target="#b19">Onishi et al., 2016</ref>) (see <ref type="table">Table 1</ref> for more examples). In general, system performance has improved rapidly as each resource is released. The best models of-   <ref type="table">Table 1</ref>: Comparison of TriviaQA with existing QA datasets. Our dataset is unique in that it is natu- rally occurring, well-formed questions collected independent of the evidences. *NewsQA uses evidence articles indirectly by using only article summaries. ten achieve near-human performance levels within months or a year, fueling a continual need to build ever more difficult datasets. We argue that Triv- iaQA is such a dataset, by demonstrating that a high percentage of its questions require solving these challenges and showing that there is a large gap between state-of-the-art methods and human performance levels.</p><p>TriviaQA contains over 650K question-answer- evidence triples, that are derived by combining 95K Trivia enthusiast authored question-answer pairs with on average six supporting evidence doc- uments per question. To our knowledge, TriviaQA is the first dataset where full-sentence questions are authored organically (i.e. independently of an NLP task) and evidence documents are collected retrospectively from Wikipedia and the Web. This decoupling of question generation from evidence collection allows us to control for potential bias in question style or content, while offering organi- cally generated questions from various topics. De- signed to engage humans, TriviaQA presents a new challenge for RC models. They should be able to deal with large amount of text from var- ious sources such as news articles, encyclopedic entries and blog articles, and should handle infer- ence over multiple sentences. For example, our dataset contains three times as many questions that require inference over multiple sentences than the recently released SQuAD ( <ref type="bibr" target="#b23">Rajpurkar et al., 2016)</ref> dataset. Section 4 present a more detailed discus- sion of these challenges.</p><p>Finally, we present baseline experiments on the TriviaQA dataset, including a linear classifier in- spired by work on <ref type="bibr">CNN Dailymail and MCTest (Chen et al., 2016;</ref><ref type="bibr" target="#b24">Richardson et al., 2013</ref>) and a state-of-the-art neural network baseline ( <ref type="bibr" target="#b27">Seo et al., 2017</ref>). The neural model performs best, but only achieves 40% for TriviaQA in comparison to 68% on SQuAD, perhaps due to the challenges listed above. The baseline results also fall far short of human performance levels, 79.7%, suggesting sig- nificant room for the future work. In summary, we make the following contributions.</p><p>• We collect over 650K question-answer- evidence triples, with questions originat- ing from trivia enthusiasts independent of the evidence documents. A high percent- age of the questions are challenging, with substantial syntactic and lexical variabil- ity and often requiring multi-sentence rea- soning. The dataset and code are avail- able at http://nlp.cs.washington. edu/triviaqa/, offering resources for training new reading-comprehension models.</p><p>• We present a manual analysis quantifying the quality of the dataset and the challenges in- volved in solving the task.</p><p>• We present experiments with two baseline methods, demonstrating that the TriviaQA tasks are not easily solved and are worthy of future study.</p><p>• In addition to the automatically gath- ered large-scale (but noisy) dataset, we present a clean, human-annotated subset of 1975 question-document-answer triples whose documents are certified to contain all facts required to answer the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Problem Formulation We frame reading com- prehension as the problem of answering a ques- tion q given the textual evidence provided by doc- ument set D. We assume access to a dataset of tuples {(q i , a i , D i )|i = 1 . . . n} where a i is a text string that defines the correct answer to question q i . Following recent formulations ( <ref type="bibr" target="#b23">Rajpurkar et al., 2016)</ref>, we further assume that a i appears as a substring for some document in the set D i . 2 However, we differ by setting D i as a set of documents, where previous work assumed a single document ( <ref type="bibr" target="#b11">Hermann et al., 2015</ref>) or even just a short paragraph ( <ref type="bibr" target="#b23">Rajpurkar et al., 2016</ref>).</p><p>Data and Distant Supervision Our evidence documents are automatically gathered from either Wikipedia or more general Web search results (de- tails in Section 3). Because we gather evidence using an automated process, the documents are not guaranteed to contain all facts needed to an- swer the question. Therefore, they are best seen as a source of distant supervision, based on the assumption that the presence of the answer string in an evidence document implies that the docu- ment does answer the question. 3 Section 4 shows that this assumption is valid over 75% of the time, making evidence documents a strong source of distant supervision for training machine reading systems.</p><p>In particular, we consider two types of distant supervision, depending on the source of our doc- uments. For web search results, we expect the documents that contain the correct answer a to be highly redundant, and therefore let each question- answer-document tuple be an independent data point. (|D i | = 1 for all i and q i = q j for many i, j pairs). However, in Wikipedia we generally expect most facts to be stated only once, so we in- stead pool all of the evidence documents and never repeat the same question in the dataset (|D i | = 1.8 on average and q i = q j for all i, j). In other words, each question (paired with the union of all of its evidence documents) is a single data point.</p><p>These are far from the only assumptions that could be made in this distant supervision setup. For example, our data would also support multi- instance learning, which makes the at least once assumption, from relation extraction ( <ref type="bibr" target="#b25">Riedel et al., 2010;</ref><ref type="bibr" target="#b13">Hoffmann et al., 2011</ref>) or many other pos- sibilities. However, the experiments in Section 6 show that these assumptions do present a strong <ref type="bibr">2</ref> The data we will present in Section 3 would further sup- port a task formulation where some documents D do not have the correct answer and the model must learn when to abstain. We leave this to future work. <ref type="bibr">3</ref> An example context for the first question in <ref type="figure">Figure 1</ref> where such an assumption fails would be the following ev- idence string: The Guns of Navarone is a 1961 British- American epic adventure war film directed by J. Lee Thomp- son.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total number of QA pairs 95,956 Number of unique answers 40,478 Number of evidence documents 662,659</head><p>Avg. question length (word) 14 Avg. document length (word) 2,895 signal for learning; we believe the data will fuel significant future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Collection</head><p>We collected a large dataset to support the read- ing comprehension task described above. First we gathered question-answer pairs from 14 trivia and quiz-league websites. We removed questions with less than four tokens, since these were generally either too simple or too vague.</p><p>We then collected textual evidence to answer questions using two sources: documents from Web search results and Wikipedia articles for en- tities in the question. To collect the former, we posed each question 4 as a search query to the Bing Web search API, and collected the top 50 search result URLs. To exclude the trivia websites, we removed from the results all pages from the trivia websites we scraped and any page whose url in- cluded the keywords trivia, question, or answer. We then crawled the top 10 search result Web pages and pruned PDF and other ill formatted doc- uments. The search output includes a diverse set of documents such as blog articles, news articles, and encyclopedic entries.</p><p>Wikipedia pages for entities mentioned in the question often provide useful information. We therefore collected an additional set of evidence documents by applying TAGME, an off-the-shelf entity linker <ref type="bibr" target="#b8">(Ferragina and Scaiella, 2010)</ref>, to find Wikipedia entities mentioned in the question, and added the corresponding pages as evidence docu- ments.</p><p>Finally, to support learning from distant super- vision, we further filtered the evidence documents to exclude those missing the correct answer string and formed evidence document sets as described in Section 2. This left us with 95K question- answer pairs organized into <ref type="formula">(1)</ref>     <ref type="table" target="#tab_3">Table 3</ref>. Answers in TriviaQA belong to a diverse set of types. 92.85% of the answers are titles in Wikipedia, 5 4.17% are numerical expressions (e.g., 9 kilometres) while the rest are open ended noun and verb phrases. A coarse grained type analysis of answers that are Wikipedia entities pre- sented in <ref type="table" target="#tab_4">Table 4</ref>. It should be noted that not all Wikipedia titles are named entities; many are common phrases such as barber or soup. Evidence analysis A qualitative analysis of TriviaQA shows that the evidence contains an- swers for 79.7% and 75.4% of questions from the Wikipedia and Web domains respectively. To analyse the quality of evidence and evaluate base- lines, we asked a human annotator to answer 986 and 1345 (dev and test set) questions from the Wikipedia and Web domains respectively. Trivia <ref type="bibr">Reasoning Lexical variation (synonym)</ref> Major correspondences between the question and the answer sentence are synonyms. Frequency 41% in Wiki documents, 39% in web documents.</p><p>Q   <ref type="table" target="#tab_6">Table 5</ref>.</p><p>On comparing evidence sentences with their corresponding questions, we found that 69% of the questions had a different syntactic structure while 41% were lexically different. For 40% of the questions, we found that the information re-quired to answer them was scattered over multi- ple sentences. Compared to SQuAD, over three times as many questions in TriviaQA require rea- soning over multiple sentences. Moreover, 17% of the examples required some form of world knowledge. Question-evidence pairs in Trivi- aQA display more lexical and syntactic variance than SQuAD. This supports our earlier assertion that decoupling question generation from evidence collection results in a more challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baseline methods</head><p>To quantify the difficulty level of the dataset for current methods, we present results on neural and other models. We used a random entity base- line and a simple classifier inspired from previ- ous work ( <ref type="bibr" target="#b30">Wang et al., 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2016)</ref>, and compare these to <ref type="bibr">BiDAF (Seo et al., 2017)</ref>, one of the best performing models for the SQuAD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Random entity baseline</head><p>We developed the random entity baseline for the Wikipedia domain since the provided documents can be directly mapped to candidate answers. In this heuristic approach, we first construct a candi- date answer set using the entities associated with the provided Wikipedia pages for a given question (on average 1.8 per question). We then randomly pick a candidate that does not occur in the ques- tion. If no such candidate exists, we pick any ran- dom candidate from the candidate set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Entity classifier</head><p>We also frame the task as a ranking problem over candidate answers in the documents. More for- mally, given a question q i , an answer a + i , and a evidence document D i , we want to learn a scoring function score, such that</p><formula xml:id="formula_0">score(a + i |q i , D i ) &gt; score(a − i |q i , D i )</formula><p>where a − i is any candidate other than the answer. The function score is learnt using LambdaMART ( <ref type="bibr" target="#b31">Wu et al., 2010</ref>), 6 a boosted tree based ranking algorithm. This is similar to previous entity-centric classi- fiers for QA ( <ref type="bibr" target="#b4">Chen et al., 2016;</ref><ref type="bibr" target="#b30">Wang et al., 2015)</ref>, and uses context and Wikipedia catalog based fea- tures. To construct the candidate answer set, we consider sentences that contain at least one word in common with the question. We then add every n-gram (n ∈ <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>) that occurs in these sentences and is a title of some Wikipedia article. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Neural model</head><p>Recurrent neural network models (RNNs) <ref type="bibr" target="#b11">(Hermann et al., 2015;</ref><ref type="bibr" target="#b4">Chen et al., 2016</ref>) have been very effective for reading comprehension. For our task, we modified the BiDAF model ( <ref type="bibr" target="#b27">Seo et al., 2017)</ref>, which takes a sequence of context words as input and outputs the start and end positions of the predicted answer in the context. The model uti- lizes an RNN at the character level, token level, and phrase level to encode context and question and uses attention mechanism between question and context.</p><p>Authored independently from the evidence doc- ument, TriviaQA does not contain the exact spans of the answers. We approximate the answer span by finding the first match of answer string in the evidence document. Developed for a dataset where the evidence document is a single paragraph (average 122 words), the BiDAF model does not scale to long documents. To overcome this, we truncate the evidence document to the first 800 words. <ref type="bibr">8</ref> When the data contains more than one evidence document, as in our Wikipedia domain, we predict for each document separately and aggregate the predictions by taking a sum of confidence scores. More specifically, when the model outputs a can- didate answer A i from n documents D i,1 , ...D i,n with confidences c i,1 , ...c i,n , the score of A i is given by</p><formula xml:id="formula_1">score(A i ) = k c i,k</formula><p>We select candidate answer with the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>An evaluation of our baselines shows that both of our tasks are challenging, and that the TriviaQA dataset supports significant future work.  <ref type="table" target="#tab_1">Wikipedia  Questions  61,888  7,993  7,701  Documents 110,648 14,229 13,661   Web  Questions  76,496  9,951  9,509  Documents 528,979 68,621 65,059  Wikipedia  verified   Questions  - 297  584  Documents  - 305  592  Web  Questions  - 322  733  verified  Documents  - 325  769   Table 6</ref>: Data statistics for each task setup. The Wikipedia domain is evaluated over questions while the web domain is evaluated over docu- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Metrics</head><p>We use the same evaluation metrics as SQuAD - exact match (EM) and F1 over words in the an- swer(s). For questions that have Numerical and FreeForm answers, we use a single given answer as ground truth. For questions that have Wikipedia entities as answers, we use Wikipedia aliases as valid answer along with the given answer. Since Wikipedia and the web are vastly differ- ent in terms of style and content, we report per- formance on each source separately. While us- ing Wikipedia, we evaluate at the question level since facts needed to answer a question are gen- erally stated only once. On the other hand, due to high information redundancy in web documents (around 6 documents per question), we report doc- ument level accuracy and F1 when evaluating on web documents. Lastly, in addition to distant su- pervision, we also report evaluation on the clean dev and test questions collection using a human annotator (section 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setup</head><p>We randomly partition QA pairs in the dataset into train (80%), development (10%), and test set (10%). In addition to distant supervision evalua- tion, we also evaluate baselines on verified subsets (see section 4) of the dev and test partitions. <ref type="table">Table  6</ref> contains the number of questions and documents for each task. We trained the entity classifier on a random sample of 50,000 questions from the train- ing set. For training BiDAF on the web domain, we first randomly sampled 80,000 documents. For both domains, we used only those (training) doc- uments where the answer appears in the first 400 tokens to keep training time manageable. Design- ing scalable techniques that can use the entirety of the data is an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>The performance of the proposed models is sum- marized in <ref type="table" target="#tab_8">Table 7</ref>. The poor performance of the random entity baseline shows that the task is not already solved by information retrieval. For both Wikipedia and web documents, BiDAF (40%) out- performs the classifier (23%). The oracle score is the upper bound on the exact match accuracy. <ref type="bibr">9</ref> All models lag significantly behind the human baseline of 79.7% on the Wikipedia domain, and 75.4% on the web domain.</p><p>We analyse the performance of BiDAF on the development set using Wikipedia as the evidence source by question length and answer type. The accuracy of the system steadily decreased as the length of the questions increased -with 50% for questions with 5 or fewer words to 32% for 20 or more words. This suggests that longer composi- tional questions are harder for current methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Error analysis</head><p>Our qualitative error analysis reveals that compo- sitionality in questions and lexical variation and low signal-to-noise ratio in (full) documents is still a challenge for current methods. We randomly sampled 100 incorrect BiDAF predictions from the development set and used Wikipedia evidence documents for manual analysis. We found that 19 examples lacked evidence in any of the provided documents, 3 had incorrect ground truth, and 3 were valid answers that were not included in the answer key. Furthermore, 12 predictions were par- tially correct (Napoleonic vs Napoleonic Wars). This seems to be consistent with human perfor- mance of 79.7%.</p><p>For the rest, we classified each example into one or more categories listed in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Recent interest in question answering has resulted in the creation of several datasets. However, they are either limited in scale or suffer from biases stemming from their construction process. We group existing datasets according to their associ- ated tasks, and compare them against TriviaQA. The analysis is summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Reading comprehension</head><p>Reading comprehension tasks aims to test the abil- ity of a system to understand a document using questions based upon its contents. Researchers have constructed cloze-style datasets ( <ref type="bibr" target="#b12">Hill et al., 2015;</ref><ref type="bibr" target="#b11">Hermann et al., 2015;</ref><ref type="bibr" target="#b20">Paperno et al., 2016;</ref><ref type="bibr" target="#b19">Onishi et al., 2016)</ref>, where the task is to pre- dict missing words, often entities, in a docu- ment. Cloze-style datasets, while easier to con- struct large-scale automatically, do not contain natural language questions.</p><p>Datasets with natural language questions in- clude MCTest ( <ref type="bibr" target="#b24">Richardson et al., 2013</ref>), SQuAD ( <ref type="bibr" target="#b23">Rajpurkar et al., 2016)</ref>, and NewsQA ( <ref type="bibr" target="#b28">Trischler et al., 2016)</ref>. MCTest is limited in scale with only 2640 multiple choice questions. SQuAD con- tains 100K crowdsourced questions and answers paired with short Wikipedia passages. NewsQA uses crowdsourcing to create questions solely from news article summaries in order to control potential bias. The crucial difference between SQuAD/NewsQA and TriviaQA is that TriviaQA questions have not been crowdsourced from pre- selected passages. Additionally, our evidence set consists of web documents, while SQuAD and NewsQA are limited to Wikipedia and news arti- cles respectively. Other recently released datasets include ( <ref type="bibr" target="#b16">Lai et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Open domain question answering</head><p>The recently released MS Marco dataset <ref type="bibr" target="#b18">(Nguyen et al., 2016</ref>) also contains independently authored questions and documents drawn from the search results. However, the questions in the dataset are derived from search logs and the answers are crowdsourced. On the other hand, trivia enthusi- asts provided both questions and answers for our dataset.</p><p>Knowledge base question answering involves converting natural language questions to logical forms that can be executed over a KB. Proposed datasets <ref type="bibr" target="#b3">(Cai and Yates, 2013;</ref><ref type="bibr" target="#b0">Berant et al., 2013;</ref>) are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB.</p><p>A standard task for open domain IR-style QA is the annual TREC competitions <ref type="bibr" target="#b29">(Voorhees and Tice, 2000</ref>), which contains questions from var- ious domains but is limited in size. Many ad- vances from the TREC competitions were used in the IBM Watson system for Jeopardy! <ref type="bibr" target="#b9">(Ferrucci et al., 2010</ref>). Other datasets includes SearchQA ( <ref type="bibr" target="#b6">Dunn et al., 2017)</ref> where Jeopardy! questions are paired with search engine snippets, the Wik- iQA dataset <ref type="bibr" target="#b33">(Yang et al., 2015)</ref> for answer sen- tence selection, and the Chinese language WebQA ( ) dataset, which focuses on the task of answer phrase extraction. TriviaQA contains examples that could be used for both stages of the pipeline, although our focus on this paper is in- stead on using the data for reading comprehension where the answer is always present.</p><p>Other recent approaches attempt to combine structured high precision KBs with semi- structured information sources like OpenIE triples <ref type="bibr" target="#b7">(Fader et al., 2014</ref>), HTML tables ( <ref type="bibr" target="#b21">Pasupat and Liang, 2015)</ref>, and large (and noisy) corpora <ref type="bibr" target="#b26">(Sawant and Chakrabarti, 2013;</ref><ref type="bibr" target="#b15">Joshi et al., 2014;</ref><ref type="bibr" target="#b32">Xu et al., 2015)</ref>. TriviaQA, which has Wikipedia entities as answers, makes it possible to leverage structured KBs like Freebase, which we leave to future work. Furthermore, about 7% of the TriviaQA questions have answers in HTML tables and lists, which could be used to augment these existing resources.</p><p>Trivia questions from quiz bowl have been pre- viously used in other question answering tasks <ref type="bibr" target="#b2">(Boyd-Graber et al., 2012)</ref>. Quiz bowl questions are paragraph length and pyramidal. 10 A num- ber of different aspects of this problem have been carefully studied, typically using classifiers over a pre-defined set of answers ( <ref type="bibr" target="#b14">Iyyer et al., 2014)</ref> and studying incremental answering to answer as quickly as possible <ref type="bibr" target="#b2">(Boyd-Graber et al., 2012</ref>) or using reinforcement learning to model opponent behavior . These competitive chal- lenges are not present in our single-sentence ques- tion setting. Developing joint models for multi- sentence reasoning for questions and answer doc- uments is an important area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We present TriviaQA, a new dataset of 650K question-document-evidence triples.</p><p>To our knowledge, TriviaQA is the first dataset where questions are authored by trivia enthusiasts, inde- pendently of the evidence documents. The evi- dence documents come from two domains -Web search results and Wikipedia pages -with highly differing levels of information redundancy. Re- sults from current state-of-the-art baselines indi-cate that TriviaQA is a challenging testbed that de- serves significant future study.</p><p>While not the focus of this paper, TriviaQA also provides a provides a benchmark for a variety of other tasks such as IR-style question answering, QA over structured KBs and joint modeling of KBs and text, with much more data than previ- ously available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig- ure 2 shows diverse topics indicated by WordNet synsets of answer entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>TriviaQA: Dataset statistics. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>650K training ex- amples for the Web search results, each contain-Property Example annotation Statistics Avg. entities / question Which politician won the Nobel Peace Prize in 2009? 1.77 per question Fine grained answer type What fragrant essential oil is obtained from Damask Rose? 73.5% of questions Coarse grained answer type Who won the Nobel Peace Prize in 2009? 15.5% of questions Time frame What was photographed for the first time in October 1959 34% of questions Comparisons What is the appropriate name of the largest type of frog? 9% of questions</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Properties of questions on 200 annotated examples show that a majority of TriviaQA questions 
contain multiple entities. The boldfaced words hint at the presence of corresponding property. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Distribution of answer types on 200 an-
notated examples. 

sis, we sampled 200 question answer pairs and 
manually analysed their properties. About 73.5% 
of these questions contain phrases that describe a 
fine grained category to which the answer belongs, 
while 15.5% hint at a coarse grained category (one 
of person, organization, location, and miscella-
neous). Questions often involve reasoning over 
time frames, as well as making comparisons. A 
summary of the analysis is presented in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>What is solid CO2 commonly called? Examples S The frozen solid form of CO2, known as dry ice ... Q Who wrote the novel The Eagle Has landed? S The Eagle Has Landed is a book by British writer Jack Higgins</head><label></label><figDesc></figDesc><table>Reasoning Lexical variation and world knowledge 
Major correspondences between the question and the document require common sense or external knowledge. 
Frequency 17% in Wiki documents, 17% in web documents. 
Q What is the first name of Madame Bovary in Flaubert's 1856 novel? 
S Madame Bovary (1856) is the French writer Gustave Flaubert's debut novel. The story focuses on a doctor's 
Examples 
wife, Emma Bovary 
Q Who was the female member of the 1980's pop music duo, Eurythmics? 
S Eurythmics were a British music duo consisting of members Annie Lennox and David A. Stewart. 

Reasoning Syntactic Variation 
After the question is paraphrased into declarative form, its syntactic dependency structure does not match 
that of the answer sentence 
Frequency 69% in Wiki documents, 65% in web documents. 
Q In which country did the Battle of El Alamein take place? 

Examples 
S The 1942 Battle of El Alamein in Egypt was actually two pivotal battles of World War II 
Q Whom was Ronald Reagan referring to when he uttered the famous phrase evil empire in a 1983 speech? 
S The phrase evil empire was first applied to the Soviet Union in 1983 by U.S. President Ronald Reagan. 

Reasoning Multiple sentences 
Requires reasoning over multiple sentences. 
Frequency 40% in Wiki documents, 35% in web documents. 
Q Name the Greek Mythological hero who killed the gorgon Medusa. 
S Perseus asks god to aid him. So the goddess Athena and Hermes helps him out to kill Medusa. 
Examples Q Who starred in and directed the 1993 film A Bronx Tale? 
S Robert De Niro To Make His Broadway Directorial Debut With A Bronx Tale: The Musical. The actor 
starred and directed the 1993 film. 

Reasoning Lists, Table 
Answer found in tables or lists 
Frequency 7% in web documents. 

Examples 
Q In Moh's Scale of hardness, Talc is at number 1, but what is number 2? 
Q What is the collective name for a group of hawks or falcons? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Analysis of reasoning used to answer TriviaQA questions shows that a high proportion of evi-
dence sentence(s) exhibit syntactic and lexical variation with respect to questions. Answers are indicated 
by boldfaced text. 

questions contain multiple clues about the an-
swer(s) not all of which are referenced in the docu-
ments. The annotator was asked to answer a ques-
tion if the minimal set of facts (ignoring temporal 
references like this year) required to answer the 
question are present in the document, and abstain 
otherwise. For example, it is possible to answer 
the question, Who became president of the Mor-
mons in 1844, organised settlement of the Mor-
mons in Utah 1847 and founded Salt Lake City? 
using only the fact that Salt Lake City was founded 
by Brigham Young. We found that the accu-
racy (evaluated using the original answers) for the 
Wikipedia and Web domains was 79.6 and 75.3 
respectively. We use the correctly answered ques-
tions (and documents) as verified sets for evalua-
tion (section 6). 

Challenging problem A comparison of evi-
dence with respect to the questions shows a 
high proportion of questions require reason-
ing over multiple sentences. To compare our 
dataset against previous datasets, we classified 100 
question-evidence pairs each from Wikipedia and 
the Web according to the form of reasoning re-
quired to answer them. We focus the analysis on 
Wikipedia since the analysis on Web documents 
are similar. Categories are not mutually exclusive: 
single example can fall into multiple categories. A 
summary of the analysis is presented in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table>Distractor en-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Performance of all systems on TriviaQA using distantly supervised evaluation. The best per-
forming system is indicated in bold. 

Category 
Proportion 

Insufficient evidence 
19 
Prediction from incorrect document(s) 
7 
Answer not in clipped document 
15 
Paraphrasing 
29 
Distractor entities 
11 
Reasoning over multiple sentences 
18 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Qualitative error analysis of BiDAF on 
Wikipedia evidence documents. 

example, the evidence for the question What was 
Truman Capote's last name before he was adopted 
by his stepfather? consists of the following text 
Truman Garcia Capote born Truman Streckfus 
Persons, was an American ... In 1933, he moved 
to New York City to live with his mother and her 
second husband, Joseph Capote, who adopted him 
as his stepson and renamed him Truman Garca 
Capote. 

</table></figure>

			<note place="foot" n="1"> Data and code available at http://nlp.cs. washington.edu/triviaqa/</note>

			<note place="foot" n="4"> Note that we did not use the answer as a part of the search query to avoid biasing the results.</note>

			<note place="foot" n="5"> This is a very large set since Wikipedia has more than 11 million titles.</note>

			<note place="foot" n="6"> We use the RankLib implementation https:// sourceforge.net/p/lemur/wiki/RankLib/</note>

			<note place="foot" n="7"> Using a named entity recognition system to generate candidate entities is not feasible as answers can be common nouns or phrases. 8 We found that splitting documents into smaller sub documents degrades performance since a majority of sub documents do not contain the answer.</note>

			<note place="foot" n="9"> A question q is considered answerable for the oracle score if the correct answer is found in the evidence D or, in case of the classifier, is a part of the candidate set. Since we truncate documents, the upper bound is not 100%.</note>

			<note place="foot" n="10"> Pyramidal questions consist of a series of clues about the answer arranged in order from most to least difficult.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by DARPA contract FA8750-13-2-0019, the WRF/Cable Professor-ship, gifts from Google and Tencent, and an Allen Distinguished Investigator Award. The authors would like to thank Minjoon Seo for the BiDAF code, and Noah Smith, Srinivasan Iyer, Mark Yatskar, Nicholas FitzGerald, Antoine Bosselut, Dallas Card, and anonymous reviewers for help-ful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D13/D13-1160.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1821-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>CoRR abs/1506.02075</idno>
		<ptr target="https://arxiv.org/abs/1506.02075" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Besting the quiz master: Crowdsourcing incremental classification games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D12-1118" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1042" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>A thorough examination of the</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">cnn/daily mail reading comprehension task</title>
		<ptr target="http://www.aclweb.org/anthology/P16-1223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.05179" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="doi">10.1145/2623330.2623677</idno>
		<ptr target="https://doi.org/10.1145/2623330.2623677" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tagme: On-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
		<idno type="doi">10.1145/1871437.1871689</idno>
		<ptr target="https://doi.org/10.1145/1871437.1871689" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA, CIKM &apos;10</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI MAGAZINE</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Opponent modeling in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1804" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1506.03340" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1511.02301" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1070" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge graph and corpus driven segmentation and answer inference for telegraphic entityseeking queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uma</forename><surname>Sawant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1104" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.04683" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dataset and neural recurrent sequence labeling model for open-domain factoid question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.06275" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1611.09268.pdf" />
	</analytic>
	<monogr>
		<title level="m">Workshop in Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Who did what: A large-scale person-centered cloze dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2230" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The lambada dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernandez</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1144" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Asian Federation of Natural Language Processing, ACL 2015</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part III</title>
		<meeting>the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part III<address><addrLine>Berlin, Heidelberg, ECML PKDD&apos;10</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning joint query interpretation and response ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uma</forename><surname>Sawant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="doi">10.1145/2488388.2488484</idno>
		<ptr target="https://doi.org/10.1145/2488388.2488484" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on World Wide Web</title>
		<meeting>the 22Nd International Conference on World Wide Web<address><addrLine>New York, NY, USA, WWW</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1099" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.01603" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.09830" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
		<idno type="doi">10.1145/345508.345577</idno>
		<ptr target="https://doi.org/10.1145/345508.345577" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA, SIGIR &apos;00</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-2115" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="700" to="706" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adapting boosting for information retrieval measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krysta</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="doi">10.1007/s10791-009-9112-1</idno>
		<ptr target="https://doi.org/10.1007/s10791-009-9112-1" />
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="270" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1502.03044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1237" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
