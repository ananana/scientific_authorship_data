<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-domain Text Classification with Multiple Domains and Disparate Label Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><forename type="middle">S</forename><surname>Bhatt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjira</forename><surname>Sinha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourya</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre India</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-domain Text Classification with Multiple Domains and Disparate Label Sets</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1641" to="1650"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Advances in transfer learning have let go the limitations of traditional supervised machine learning algorithms for being dependent on annotated training data for training new models for every new domain. However, several applications encounter scenarios where models need to transfer/adapt across domains when the label sets vary both in terms of count of labels as well as their connotations. This paper presents first-of-its-kind transfer learning algorithm for cross-domain classification with multiple source domains and dis-parate label sets. It starts with identifying transferable knowledge from across multiple domains that can be useful for learning the target domain task. This knowledge in the form of selective labeled instances from different domains is congregated to form an auxiliary training set which is used for learning the target domain task. Experimental results validate the efficacy of the proposed algorithm against strong baselines on a real world social media and the 20 Newsgroups datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A fundamental assumption in supervised statisti- cal learning is that training and test data are inde- pendently and identically distributed (i.i.d.) sam- ples drawn from a distribution. Otherwise, good performance on test data cannot be guaranteed even if the training error is low. On the other hand, transfer learning techniques allow domains, tasks, and distributions used in training and testing to be different, but related. It works in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains. Pan and Yang, in their survey paper (2010), de- scribed different transfer learning settings depend- ing on if domains and tasks vary as well as labeled data is available in one/more/none of the domains. In this paper, we propose a generic solution for multi-source transfer learning where domains and tasks are different and no labeled data is available in the target domain. This is a relatively less char- tered territory and arguably a more generic setting of transfer learning.</p><p>Motivating example: Consider a social media consulting company helping brands to monitor their social media channels. Two problems typi- cally of interest are: (i) sentiment classification (is a post positive/negative/neutral?) and (ii) subject classification (what was the subject of a post?). While sentiment classification attempts to classify a post based on its polarity, subject classification is towards identifying the subject (or topic) of the post, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The company has been using standard classification techniques from an off-the-shelf machine learning toolbox. While machine learning toolkit helps them to cre- ate and apply statistical models efficiently, the same model can not be applied on a new collec- tion due to variations in data distributions across collections <ref type="bibr">1</ref> . It requires a few hundreds of manu- ally labeled posts for every task on every collec-tion. As social media are extremely high velocity and low retention channels, human labeling efforts act like that proverbial narrow bottleneck. Need of the hour was to reduce, if not eliminate, the human-intensive labeling stage while continue to use machine learning models for new collections.</p><p>Several transfer learning techniques exist in the literature which can reduce labeling efforts re- quired for performing tasks in new collections. Tasks such as sentiment classification, named en- tity recognition (NER), part of speech (POS) tag- ging that have invariant label sets across domains, have shown to be greatly benefited from these works. On the other hand, tasks like subject clas- sification that have disparate label sets across do- mains have not been able to gain at pace with the advances in transfer learning. Towards that we for- mulate the problem of Cross-domain classification with disparate label sets as learning an accurate model for the new unlabeled target domain given labeled data from multiple source domains where all domains have (possibly) different label sets.</p><p>Our contributions: To the best of our knowl- edge, this is the first work to explore the prob- lem of cross-domain text classification with mul- tiple source domains and disparate label sets. The other contributions of this work includes a sim- ple yet efficient algorithm which starts with iden- tifying transferable knowledge from across mul- tiple source domains useful for learning the tar- get domain task. Specifically, it identifies rele- vant class-labels from the source domains such that the instances in those classes can induce class- separability in the target domain. This transferable knowledge is accumulated as an auxiliary training set for an algorithm to learn the target domain clas- sification task followed by suitable transformation of the auxiliary training instances. Organization of the paper is as follows: Section 2 presents the preliminaries and notation, Section 3 summarizes the related work. Section 4 and 5 present the proposed algorithm and experimental results respectively. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Notations</head><p>A domain D = {X , P (X)} is characterized by two components: a feature space X and a marginal probability distribution P (X), where X = {x 1 , x 2 , ...x n } ∈ X . A task T = {Y, f (·)} also consists of two components: a label space Y and an objective predictive function f (·).</p><p>In our settings for cross-domain classification with disparate label sets, we assume M source do- mains, denoted as D S i , where i = {1, 2, ..M }. Each source domain has different marginal distri- bution i.e. P (X S i ) = P (X S j ) and different la- bel space i.e. Y S i = Y S j , ∀i, j ∈ M . The label space across domains vary both in terms of count of class-labels as well as their connotations; how- ever, a finite set of labeled instances are available from each source domain. The target domain (D T ) consists of a finite set of unlabeled instances, de- noted as t i where i = {1, .., N }. Let Y T be the target domain label space with K class-labels. We assume that the number of classes in the target domain i.e. K is known (analogous to clustering where the number of clusters is given). <ref type="table" target="#tab_0">Table 1</ref> summarizes different settings of transfer learning  and how this work differentiates from the existing literature 2 . The first scenario represents the ideal settings of tra- ditional machine learning <ref type="bibr" target="#b24">(Mitchell, 1997</ref>) where a model is trained on a fraction of labeled data and performs well for the same task on the future un- seen instances from the same domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The second scenario where the domains vary while the tasks remain the same is referred to as transductive transfer learning. This is the most ex- tensively studied settings in the transfer learning literature and can be broadly categorized as single and multi-source adaptation. Single source adap- tation ( <ref type="bibr" target="#b7">Chen et al., 2009;</ref><ref type="bibr" target="#b0">Ando and Zhang, 2005;</ref><ref type="bibr" target="#b11">Daumé III, 2009</ref>) primarily aims at minimizing the divergence between the source and target domains either at instance or feature levels. The general idea being identifying a suitable low dimensional space where transformed source and target do- mains data follow similar distributions and hence, a standard supervised learning algorithm can be trained <ref type="bibr" target="#b11">(Daumé III, 2009;</ref><ref type="bibr" target="#b16">Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b27">Pan et al., 2011;</ref><ref type="bibr" target="#b4">Blitzer et al., 2007;</ref><ref type="bibr" target="#b10">Dai et al., 2007;</ref><ref type="bibr" target="#b2">Bhatt et al., 2015)</ref>.</p><p>While several existing single source adaptation techniques can be extended to multi-source adap- tation, the literature in multi-source adaptation can be broadly categorized as: 1) feature representa- tion approaches ( <ref type="bibr" target="#b6">Chattopadhyay et al., 2012;</ref><ref type="bibr" target="#b31">Sun et al., 2011;</ref><ref type="bibr" target="#b13">Duan et al., 2009;</ref><ref type="bibr" target="#b14">Duan et al., 2012</ref>;   <ref type="bibr" target="#b29">Schweikert and Widmer, 2008;</ref><ref type="bibr" target="#b30">Sun and Shi, 2013;</ref><ref type="bibr" target="#b35">Xu and Sun, 2012;</ref>. Our work differentiates in intelli- gently exploiting selective transferable knowledge from multiple sources unlike existing approaches where multiple sources contribute in a brute-force manner.</p><formula xml:id="formula_0">D S = D T , T S = T T Kim et al.<label>(2015</label></formula><p>The third scenario where the tasks differ irre- spective of the relationship among domains is re- ferred to as inductive transfer learning. Self-taught learning ( <ref type="bibr" target="#b28">Raina et al., 2007)</ref> and multi-task <ref type="bibr" target="#b17">(Jiang, 2009;</ref><ref type="bibr" target="#b23">Maurer et al., 2012;</ref><ref type="bibr" target="#b36">Xu et al., 2015;</ref><ref type="bibr" target="#b19">Kumar and Daume III, 2012</ref>) learning are the two main learning paradigms in this scenario and <ref type="table" target="#tab_0">Table 1</ref> differentiates our work from these.</p><p>This work closely relates to the fourth scenario where we allow domains to vary in the marginal probability distributions and the tasks to vary due to different label spaces 3 . The closest prior work by <ref type="bibr" target="#b18">Kim et al. (2015)</ref> address a sequential label- ing problem in NLU where the fine grained label sets across domains differ. However, they assume that there exists a bijective mapping between the coarse and fine-grained label sets across domains. They learn this mapping using labeled instances from the target domain to reduce the problem to a standard domain adaptation problem (Scenario 2). elaborated in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exploiting Multiple Domains</head><p>If we had the mappings between the source and target domain label sets, we could have leveraged existing transfer learning approaches. However, heterogeneity of label sets across domains and the unlabeled data from the target domain exac- erbate the problem. Our objective is to leverage the knowledge from multiple source domains to induce class-separability in the target domain. In- ducing class-separability refers to segregating the target domain into K classes using labeled in- stances from selective K source domain classes.</p><p>Towards this, the proposed algorithm divides each source domain into clusters/groups based on the class-labels such that instances with the same label are grouped in one cluster. All source do- mains are divided into Q clusters where Q = M m=1 ||Y m || represents the count of class-labels across all sources. ||Y m || being the count of class- labels in the m th source domain. C q denotes the q th cluster and µ q denotes its centroid computed as the average of all the members in the cluster. We assert that the target domain instances that have high similarity to a particular source domain clus- ter can be grouped together. Given N target do- main instances and Q source domain clusters, a matrix R (dimension N × Q) is computed based on the similarity of the target domain instances with the source clusters. The i th row of the ma- trix captures the similarity of the i th target domain instance (t i ) with all the source domain clusters. It captures how different source domain class-labels are associated with the target domain instances and hence, can induce class-separability in the tar- get domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extracting Transferable Knowledge</head><p>The similarity matrix R associates target domain instances to the source domain clusters in propor- tion to their similarity. However, the objective is to select the optimal K source domain clusters that fit the maximum number of target domain instances. This problem is similar to the well-known combi- natorial optimization problem of Maximum Cov- erage <ref type="bibr" target="#b33">(Vazirani, 2003)</ref> where given a collection of P sets, we need to select A sets (A &lt; P ) such that the size of the union of the selected sets is maxi- mized. In this paper, we are given Q source do- main clusters and need to select K clusters such that the corresponding number of associated tar- get domain instances is maximized. As the Max- imum Coverage problem is NP-hard, we imple- ment a greedy algorithm for selecting the k source domain clusters, as illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Selecting K Source Clusters</head><p>Input: A matrix R, K = number target domain classes, l= number of selected cluster. Initialize: l = 0, Normalize R such that each row sums up to 1. repeat: 1: Pick the column in R which has maximum sum of similarity scores for uncovered target domain instances. 2: Mark elements in the chosen column as cov- ered.</p><formula xml:id="formula_1">3: l = l + 1 until: l = K Output: K source domain clusters.</formula><p>A source domain contributes partially in terms of zero or more class-labels (clusters) identified using the Algorithm 1. Therefore, we refer to the labeled instances from the selected clusters of a source domain as the partial transferable knowl- edge from that domain. This partial transferable knowledge from across multiple source domains is congregated to form an auxiliary training set, referred to as (AU X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adapting to the Target Domain</head><p>The auxiliary training set comprises labeled in- stances from selected K source domain clusters 4 . Since, the auxiliary set is pulled out from multiple source domains, it follows different data distribu- tion as compared to the target domain. For a clas- sifier, trained on the K-class auxiliary training set, the distributional variations have to be normalized so that it can generalize well on the target domain.</p><p>In this research, we proposed to use an instance weighting technique <ref type="bibr" target="#b16">(Jiang and Zhai, 2007)</ref> to minimize the distributional variations by deferen- tially weighting instances in the auxiliary set. In- tuitively, the auxiliary training instances similar to the target domain are assigned higher weights while training the classifier and vice versa. The weight for the i th instance in the auxiliary set should be proportional to the ratio (Pt(x i )) (Pa(x i )) . How- ever, since the actual probability distributions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Cross-domain Classification with Disparate Label Sets</head><p>Input: M source domains, target domain in- stances (t i ), i = (1, ..., N ), K = number of target domain classes. Process: Divide M sources into Q clusters s.t.</p><formula xml:id="formula_2">Q = M q=1 |Y m |. C q</formula><note type="other">be the q th cluster &amp; µ q be its centroid computed as shown in Eq 1. A: Exploiting Multiple Sources: for i = 0 : till N do for q = 0 : till Q do R[i, q] = Sim(µ q , t i ) end for end for B: Extracting partial knowledge: 1: Pick K columns from R using Algorithm 1. 2: Construct AU X by congregating instances from the selected K source domain class-labels.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C: Adapting to target domain:</head><p>1: Minimize distributional variations using in- stance weighing technique. 2: Train a K-class classifier using AU X.</p><p>Output: K-class target domain classifier.</p><p>(P a (x) and P t (x) for the auxiliary set and target domain respectively) are unknown, the instance difference is approximated as (Pt(x i |d=target)) (Pa(x i |d=auxilliary)) , where d is a random variable used to represent whether x i came from the auxiliary set or the tar- get domain. To calculate this ratio, a binary classi- fier is trained using the auxiliary set and target do- main data with labels {-1} and {+1} respectively. The predicted probabilities from the classifier are used to estimate the ratio as the weight for the i th auxiliary instance x i . Finally, a K-class classifier is trained on the weighted auxiliary training set to perform classification on the target domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Algorithm</head><p>As shown in <ref type="figure">Figure 2</ref>, the step-by-step flow of the proposed algorithm is summarized below:</p><p>1. Divide M source domains into Q clusters, each represented as C q , q = {1, 2, .., Q}.</p><p>2. Compute centroid of each cluster as the aver- age of the cluster members, as shown in Eq.</p><p>1.</p><formula xml:id="formula_3">µ q = 1 ||C q || ||Cq|| (i=1;x i ∈Cq) x i (1)</formula><p>where µ q is the centroid, ||C q || is the mem- bership count and x i is the i th member of C q .</p><p>3. For target instances t i ∀i ∈ N , compute cosine similarity with all the source domain cluster centroids to form the matrix R (di- mensions: N × Q), as shown in Eq. 2</p><formula xml:id="formula_4">R[i, q] = Sim(µ q , t i ) = µ q · t i ||µ q || ||t i ||<label>(2)</label></formula><p>4. Run Algorithm 1 on R to select K optimal source clusters (i.e. columns of R).</p><p>5. Congregate labeled instances from the se- lected source domain clusters to form the K- class auxiliary training set.</p><p>6. Minimize the divergence between the auxil- iary set and target domain using the instance weighing technique, described in Section 4.3.</p><p>7. Finally, train a K-class classifier on deferen- tially weighted auxiliary training instances to perform classification in the target domain.</p><p>The K-class classifier trained on the auxiliary training set is an SVM classifier (Chih-Wei Hsu and Lin, 2003) with L2 − loss from the LIB- LINEAR library <ref type="bibr" target="#b15">(Fan et al., 2008</ref>). The classi- fier used in the instance weighing technique is again an SVM classifier with RBF kernel. The proposed algorithm uses distributional embedding i.e. Doc2Vec ( <ref type="bibr" target="#b21">Le and Mikolov, 2014</ref>) to represent instances from the multiple source and target do- mains. We used an open-source implementation of Doc2Vec ( <ref type="bibr" target="#b21">Le and Mikolov, 2014</ref>) for learning 400 dimensional vector representation using DBoW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>Comprehensive experiments are performed to evaluate the efficacy of the proposed algorithm for cross-domain classification with disparate la- bel sets across domains on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The first dataset is a real-world Online Social Me- dia (OSM) dataset which consists of 74 collec- tions. Each collection comprises comments/tweets that are collected based on user-defined keywords. These keywords are fed to a listening engine which crawls the social media (i.e. Twitter.com) and fetches comments matching the keywords. The task is to classify the comments in a collection  into user-defined categories. These user-defined categories may vary across collections in terms of count as well as their connotations. <ref type="table" target="#tab_2">Table 2</ref> shows an example of the user-defined categories for a few collections related to "Apple" products. In the ex- periments, one collection is used as unlabeled tar- get collection and the remaining collections are used as the labeled source collections. We ran- domly selected 5 target collections to report the performance, as described in <ref type="table" target="#tab_3">Table 3</ref>. The second dataset is the 20 Newsgroups (NG) <ref type="bibr" target="#b20">(Lang, 1995</ref>) dataset which comprises 20, 000 news articles organized into 6 groups with differ- ent sub-groups both in terms of count as well as connotations, as shown in <ref type="figure" target="#fig_1">Figure 3(a)</ref>. Two differ- ent experiments are performed on this dataset. In the first experiment ("Exp-1"), one group is con- sidered as the target domain and the remaining 5 groups as the source domains. In the second ex- periment ("Exp-2"), one sub-group from each of the first five groups 5 is randomly selected to syn- thesize a target domain while all the groups (with the remaining sub-groups) are used as source do- mains. <ref type="figure" target="#fig_1">Figure 3(b)</ref> shows an example on how to synthesize target domains in "Exp-2". There are 720 possible target domains in this experiment and we report the average performance across all pos- sible target domains, referred to as "Grp 7". The task in both the experiments is to categorize the target domain into its K categories (sub-groups) using labeled data from multiple source domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metric</head><p>The performance is reported in terms of classifi- cation accuracy on the target domain. There is no definite mapping between the actual class-labels in the target domain and the K categories (i.e. induced categories) in the auxiliary training set. Therefore, we sequentially evaluate all possible one-to-one mappings between the K categories in the auxiliary training set and target domain to re- port results for the best performing mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Protocol</head><p>The performance of the proposed algorithm is sky- lined by the in-domain performance (Gold), i.e. a classifier trained and tested on the labeled tar- get domain data. We also compared the perfor- mance with spherical K-means clustering <ref type="bibr" target="#b12">(Dhillon and Modha, 2001</ref>) used to group the target domain data into K categories against the ground truth, referred to CL. Spherical K-means clustering is based on cosine similarity and performs better for high-dimensional sparse data such as text.</p><p>To compare with a baseline and an existing adaptation algorithm, we selected the most similar source domain <ref type="bibr">6</ref> with exactly K number of class- labels and report the performance on the best pos- sible mapping, as described in Section 5.2. To compute the baseline (BL), a classifier trained on the source domain is used to categorize the target domain. A widely used domain adaptation algo- rithm, namely structural correspondence learning (SCL) ( <ref type="bibr" target="#b4">Blitzer et al., 2007</ref>) is also applied using the selected source domain.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and Analysis</head><p>Key observations and analysis from the experi- mental evaluations are summarized below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Results on the OSM Dataset</head><p>Results in <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="table" target="#tab_4">Table 4</ref> show the effi- cacy of the proposed algorithm for cross-domain classification with disparate label sets as it out- performs other approaches by at least 15%. Coll ID(#) refers to the target collection and the corre- sponding count of class-labels. Results in <ref type="table" target="#tab_4">Table  4</ref> also compare the performance of the proposed technique without the distributional normalization of the auxiliary training set, referred to as "W/O". Results suggest that suitably weighing instances from the auxiliary training set mitigates the distri- butional variations and enhances the cross-domain performance by at least 3.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Results on the 20Newsgroups Dataset</head><p>Results in <ref type="table" target="#tab_5">Table 5</ref> show that the proposed algo- rithm outperforms other techniques for both the experiments by at least 15 % and 18% respectively on the 20 Newsgroups dataset. In <ref type="table" target="#tab_5">Table 5</ref>, "-" refers to the cases where a single source domain with the same number of class-labels as in the tar- get domain is not available. In "Exp-1" where the source and target categories vary in terms of counts as well as their connotations, the proposed algorithm efficiently induces the classes in the un- labeled target domain using the partial transferable knowledge from multiple sources. For "Exp-2", it is observed that the performance of the proposed algorithm is better than the performance in "Exp- 1" as the target categories have closely related cat- egories (from the same group) in the source do-  mains. <ref type="table" target="#tab_5">Table 5</ref> reports the average performance across all the 720 possible combinations of target domains with a standard deviation of 2.6. <ref type="table">Table 6</ref> validates our assertion that multiple sources are necessary to induce class-separability in the target domain as a single source is not suf- ficient to cater to the heterogeneity of class-labels across domains. It also suggests that the proposed algorithm can learn class-separability in the tar- get domain by using arbitrary diverse class-labels from different sources and does not necessarily re- quire class-labels to follow any sort of coarse-to- fine mapping across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Effect of Multiple Source Domains</head><p>To evaluate the effects of using multiple sources, further experiments were performed by varying the number of available source domains. For the OSM dataset, we varied the number of available source collections from 1 to 73 starting with the most similar source collection and repeat- edly adding the next most similar collection in the pool of available collections. We observe that even the most similar collection was not independently sufficient to induce classes in the target collec- tion and it was favorable to exploit multiple col- lections. Moreover, adding collections based on similarity to the target collection had a better like- lihood of achieving higher performance as com- pared to adding random collections.</p><p>In another experiment, we first identified the source collections which contributed to learning the target task. We removed these collections and applied the proposed algorithm on the remaining source collections. <ref type="figure" target="#fig_3">Figure 5</ref> shows the perfor- <ref type="table">Table 6</ref>: Actual target domain class-labels and the corresponding source domain class clusters used to build the auxiliary training set. mance of the proposed algorithm on 5 such iter- ations of removing the contributing source collec- tions from the previous iteration. We observed a significant drop in the performance with each iter- ation which signifies the effectiveness of the pro- posed algorithm in extracting highly discriminat- ing transferable knowledge from multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Comparing with Domain Adaptation</head><p>We applied domain adaptation techniques con- sidering the auxiliary training set to be a single source domain with the same number of classes as that in the target domain. We applied two of the widely used domain adaptation techniques, namely SCL ( <ref type="bibr" target="#b4">Blitzer et al., 2007</ref>) and SFA ( ) referred to as "AuxSCL" and "AuxSFA" respectively. Results in <ref type="table" target="#tab_7">Table 7</ref> suggest that the proposed algorithm significantly outperforms "AuxSCL" and "AuxSFA" on the two datasets. Generally, existing domain adaptation techniques are built on the co-occurrences of the common fea- tures with the domain specific features and hence, capture how domain specific features in one do- main behaves w.r.t to the domain specific features in the other domain. They assume homogeneous labels and expect the aligned features across do- mains to behave similarly for the prediction task. However, these features are misaligned when the label set across domains vary in terms of their con- notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Effect of Different Representations</head><p>The proposed algorithm uses Doc2Vec ( <ref type="bibr" target="#b21">Le and Mikolov, 2014</ref>) for representing instances from multiple domains. However, the proposed algo- rithm can build on different representations and hence, we compare its performance with tradi- tional TF-IDF representation (including unigrams  and bigrams) and a dense representation using TF- IDF+PCA ( reduced to a dimension such that it covers 90% of the variance). We observe that Doc2Vec representation clearly outperforms the other two representations as it addresses the draw- backs of bag-of n-gram models in terms of implic- itly inheriting the semantics of the words in a doc- ument and offering a more generalizable concise vector representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presented the first study on cross- domain text classification in presence of multiple domains with disparate label sets and proposed a novel algorithm for the same. It proposed to ex- tract partial transferable knowledge from across multiple source domains which was beneficial for inducing class-separability in the target domain. The transferable knowledge was assimilated in terms of selective labeled instances from different source domain to form a K-class auxiliary train- ing set. Finally, a classifier was trained using this auxiliary training set, following a distribution nor- malizing instance weighing technique, to perform the classification task in the target domain. The ef- ficacy of the proposed algorithm for cross-domain classification across disparate label sets will ex- pand the horizon for ML-based algorithms to be more widely applicable in more general and prac- tically observed scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cross-domain (a) sentiment classification and (b) subject classification. Illustrates (a) invariant and (b) disparate label sets.</figDesc><graphic url="image-1.png" coords="1,348.02,204.58,136.71,95.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustrates (a) different groups (b) target domain synthesis ("EXP 2") on the NG dataset.</figDesc><graphic url="image-3.png" coords="6,312.05,63.00,208.57,141.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Compares the performance of different techniques on the OSM dataset. Table 4: Summarizes the performance of the proposed algorithm on the OSM dataset.</figDesc><graphic url="image-4.png" coords="7,101.98,63.04,158.12,88.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of selected source collections on the OSM dataset.</figDesc><graphic url="image-5.png" coords="7,315.66,63.02,201.34,85.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Summarizing the related work and differentiating the novel features of the proposed algorithm.</head><label>1</label><figDesc></figDesc><table>Scenario 
Settings 
Nature of Data 
Learning 
Paradigm 
Main Concepts 
Our Differentiation 

D S = D T , 
T S = T T 

Traditional 
Machine 
learning 

Labelled data in 
source domain(s) 
and unlabeled data in 
target domain 

Source and 
target domains 
are exactly the 
same 

Learn models on training set and 
test on future unseen data 
Allows tasks across domains to be 
different;a more general setting 

Transductive 
Labelled data in 
source domain(s) 
and 

Single source 
domain 
adaptation 

Learning common shared 
representation; instance weighing, 
parameter transfer 

Exploits multiple sources each with 
disparate label sets. 

D S = D T , 
T S = T T 

Transfer 
Learning 
unlabeled data from 
the target domain 
P(X S ) = P(X T ) 
Multi-source 
adaptation 

Classifier combination; efficient 
combination of information from 
multiple sources; Feature 
representation 

Intelligent selection of transferable 
knowledge from multiple sources 
for adaptation. 

Inductive 

Unlabeled data in 
source domain(s) 
and labeled data in 
target domain 

Self-taught 
learning 

Extracts higher level 
representations from unlabeled 
auxiliary data to learn 
instance-to-label mapping with 
labeled target instances 

Learns instance-to-label mapping in 
the unlabeled target domain using 
multiple labeled source domains 
having different data distributions 
and label spaces. 

No 
conditions 
on D S &amp; 
D T , but, 
T S = T T 

Transfer 
Learning 
Labeled data is 
available in all 
domains 

Multi-task 
learning 

Simultaneously learns multiple 
tasks within (or across) domain(s) 
by exploiting the common feature 
subspace shared across the tasks 

Learns the optimal class distribution 
in an unlabeled target domain by 
minimizing the differences with 
multiple labeled source domains. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>)</head><label></label><figDesc></figDesc><table>Labeled data in 
source and target 
domains 

Transfer 
learning with 
disparate label 
set 

Disparate fine grained label sets 
across domains, however, same 
coarse grained labels set can be 
invoked across domains 

No coarse-to-fine label mapping due 
to heterogeneity of label sets, 
Assumes no labelled data in target 
domain. 

Bollegala et al., 2013; Crammer et al., 2008; Man-
sour et al., 2009; Ben-David et al., 2010; Bhatt 
et al., 2016) and 2) combining pre-trained classi-
fiers (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Illustrates variability in label sets across some collections from the OSM dataset.</head><label>2</label><figDesc></figDesc><table>Apple iPhone 6 
Apple iOS 8 
Apple iPad mini3 
Camera 
Locking apps &amp; fea-
tures 
Release date &amp; Fea-
tures 
Design 
Extensibility fea-
tures 
Apple play &amp; NFC 

Review link 
General features re-
lated marketing 
Apple sim card 

Apple Play/NFC 
Camera features 
Touch ID 
Comparison to An-
droid 
Password 
with 
touch integration 
iPad 
mini3 
-
disappoints 
Price 
Health &amp; fitness app 
Apple watch 
Location &amp; Maps 
Firmware updates 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Table illustrates the collections from the EMPATH database used in this research.</head><label>3</label><figDesc></figDesc><table>Collection ID 
Domain 
#Categories 
Coll 1 
Huwaei 
5 
Coll 2 
Healthcare 
9 
Coll 3 
Whattsapp 
8 
Coll 4 
Apple iOS 8 
8 
Coll 5 
Apple iPhone 6 
7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Summarizes the performance of the pro-
posed algorithm on the OSM dataset. 

Coll ID (#) 
BL 
CL 
SCL 
W/O 
Proposed 
Gold 

Coll 1 (5) 
52.6 
43.7 
62.8 
77.4 
81.4 
90.5 
Coll 2 (9) 
38.6 
31.8 
58.8 
72.5 
77.6 
84.4 
Coll 3 (8) 
43.6 
36.4 
60.7 
74.2 
78.5 
87.6 
Coll 4 (8) 
44.7 
38.8 
62.5 
78.8 
82.1 
92.5 
Coll 5 (7) 
50.5 
42.8 
64.4 
76.6 
80.5 
89.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Summarizes the performance of the pro-
posed algorithm on the 20Newsgroups dataset. 

Target(#) 
BL 
CL 
SCL 
W/O 
Proposed 
Gold 
Grp 1 (5) 
-
48.6 
-
79.4 
80.8 
85.6 
Grp 2 (4) 
62.7 
50.2 
62.7 
78.3 
83.6 
89.2 
Grp 3 (4) 
64.3 
54.8 
64.4 
81.6 
85.3 
90.4 
Grp 4 (3) 
69.6 
55.6 
67.3 
82.2 
86.4 
92.5 
Grp 5 (3) 
69.7 
56.4 
70.3 
83.6 
85.3 
91.2 
Grp 7 (5) 
-
52.8 
-
84.6 
88.4 
93.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 7 : Comparing the proposed algorithm with existing domain adaptation algorithms.</head><label>7</label><figDesc></figDesc><table>Dataset 
Target 
SCL 
SFA 
Proposed 

OSM 

Coll 1 
66.2 
64.7 
81.4 
Coll 2 
63.8 
62.6 
77.6 
Coll 3 
64,1 
63.4 
78.5 
Coll 4 
64.2 
65.2 
82.1 
Coll 5 
64.0 
63.7 
80.5 

Grp 1 
65.2 
64.2 
80.8 

NG 
Grp 2 
68.2 
65.3 
83.6 

Exp-1 
Grp 3 
69.4 
68.4 
85.3 
Grp 4 
70.3 
69.2 
86.4 
Grp 5 
69.0 
68.8 
85.3 
NG Exp-2 
Grp 7 
72.6 
70.2 
88.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 8 : Comparing different representations.</head><label>8</label><figDesc></figDesc><table>Dataset 
Target 
TF-IDF 
TF-IDF +PCA 
Doc2Vec 

OSM 

Coll 1 
70.6 
76.8 
81.4 
Coll 2 
69.5 
74.2 
77.6 
Coll 3 
70.2 
75.5 
78.5 
Coll 4 
71.6 
77.9 
82.1 
Coll 5 
70.8 
76.8 
80.5 

Grp 1 
71.8 
75.6 
80.8 

NG 
Grp 2 
73.6 
77.5 
83.6 

Exp-1 
Grp 3 
77.4 
81.1 
85.3 
Grp 4 
76.6 
82.5 
86.4 
Grp 5 
75.5 
81.4 
85.3 
NG Exp-2 
Grp 7 
76.2 
83.6 
88.4 

</table></figure>

			<note place="foot" n="1"> A collection comprises comments/posts pertaining to a particular client/product/services. Domain and collection are used interchangeably.</note>

			<note place="foot" n="2"> This is not the complete view of the transfer learning literature; however, covers relevant work that helps motivate/differentiate the novel features of this paper.</note>

			<note place="foot" n="3"> This work do not consider scenario when domains vary in feature spaces and tasks vary in the objective predictive functions. Figure 2: Illustrates different stages of the proposed algorithm. However, this paper caters to multiple source domains with disparate label sets without assuming availability of any labeled data from the target domain or fine-to-coarse label mappings across domains. 4 Cross-domain Classification with Disparate Label Set The underlying philosophy of the proposed algorithm is to learn the target domain task by using the available information from multiple source domains. To accomplish this, we have developed an algorithm to identify and extract partial transferable knowledge from multiple sources. This knowledge is then suitably transformed to induce classes in the target domain using the class separation from the source domains. Different stages of the proposed algorithm, as shown in Figure 2, are</note>

			<note place="foot" n="4"> The K classes in auxiliary set induce class-separability in the target domain, however, the actual class-labels across these two may not have any sort of coarse-to-fine mapping.</note>

			<note place="foot" n="5"> Group-6 has only 1 sub-group, therefore, it is considered for synthesizing target domain in the experiments.</note>

			<note place="foot" n="6"> The most similar source domain is selected using proxyA distance (Blitzer et al., 2007) which has good correlation with domain adaptation performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A highperformance semi-supervised learning method for text chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An iterative similarity based adaptation technique for cross-domain text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepali</forename><surname>Himanshu Sharad Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourya</forename><surname>Semwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Natural Language Learning</title>
		<meeting>Conference on Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-source iterative adaptation for cross-domain classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Himanshu Sharad Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourya</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification using a sentiment sensitive thesaurus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1719" to="1731" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multisource domain adaptation and its application to early detection of fatigue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting discriminative concepts for domain adaptation in text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Lam</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A practical guide to support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung Chang Chih-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, National Taiwan University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1757" to="1774" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-clustering based classification for out-ofdomain documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<title level="m">Frustratingly easy domain adaptation</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmendra</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="143" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources via auxiliary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources: a domaindependent regularization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor Wai-Hung</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">504518</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LIBLINEAR: a library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task transfer learning for weaklysupervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">New transfer learning techniques for disparate label sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6417</idno>
		<title level="m">Learning task grouping and overlap in multi-task learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NewsWeeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1209.0738</idno>
		<title level="m">Sparse coding for multitask and transfer learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>McGrawHill, Inc</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on World Wide Web</title>
		<meeting>International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-taught learning: Transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An empirical analysis of domain adaptation algorithms for genomic sequence analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Schweikert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian multisource domain adaptations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Shi-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Cybernetics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A two-stage weighting framework for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transfer learning with part-based ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Classifier Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7872</biblScope>
			<biblScope unit="page" from="271" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
		<title level="m">Approximation Algorithms</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer-Verlag</surname></persName>
		</author>
		<imprint>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-source transfer learning with multi-view adaboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing</title>
		<meeting>International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting task-feature co-clusters in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advancement of Artificial Intelligence</title>
		<meeting>Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia</title>
		<meeting>International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
