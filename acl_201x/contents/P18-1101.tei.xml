<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1098" to="1107"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1098</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains. Yet it is limited because it cannot output interpretable actions as in traditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon vari-ational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover inter-pretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classic dialog systems rely on developing a mean- ing representation to represent the utterances from both the machine and human users <ref type="bibr" target="#b20">(Larsson and Traum, 2000;</ref><ref type="bibr" target="#b3">Bohus et al., 2007</ref>). The dialog manager of a conventional dialog system outputs the system's next action in a semantic frame that usually contains hand-crafted dialog acts and slot values <ref type="bibr" target="#b34">(Williams and Young, 2007)</ref>. Then a natu- ral language generation module is used to gener- ate the system's output in natural language based on the given semantic frame. This approach suf- fers from generalization to more complex do- mains because it soon become intractable to man-ually design a frame representation that covers all of the fine-grained system actions. The re- cently developed neural dialog system is one of the most prominent frameworks for developing di- alog agents in complex domains. The basic model is based on encoder-decoder networks ( ) and can learn to generate system responses without the need for hand-crafted meaning repre- sentations and other annotations. Although generative dialog models have ad- vanced rapidly <ref type="bibr" target="#b29">(Serban et al., 2016;</ref><ref type="bibr" target="#b21">Li et al., 2016;</ref>, they cannot provide inter- pretable system actions as in the conventional dia- log systems. This inability limits the effectiveness of generative dialog models in several ways. First, having interpretable system actions enables hu- man to understand the behavior of a dialog system and better interpret the system intentions. Also, modeling the high-level decision-making policy in dialogs enables useful generalization and data- efficient domain adaptation <ref type="bibr" target="#b10">(Gaši´Gaši´c et al., 2010)</ref>. Therefore, the motivation of this paper is to de- velop an unsupervised neural recognition model that can discover interpretable meaning represen- tations of utterances (denoted as latent actions) as a set of discrete latent variables from a large un- labelled corpus as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The dis- covered meaning representations will then be inte- grated with encoder decoder networks to achieve interpretable dialog generation while preserving all the merit of neural dialog systems.</p><p>We focus on learning discrete latent represen- tations instead of dense continuous ones because discrete variables are easier to interpret (van den <ref type="bibr" target="#b31">Oord et al., 2017)</ref> and can naturally correspond to categories in natural languages, e.g. topics, dia- log acts and etc. Despite the difficulty of learn- ing discrete latent variables in neural networks, the recently proposed Gumbel-Softmax offers a reli- able way to back-propagate through discrete vari- ables ( <ref type="bibr" target="#b23">Maddison et al., 2016;</ref><ref type="bibr" target="#b15">Jang et al., 2016)</ref>. However, we found a simple combination of sen- tence variational autoencoders (VAEs) <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> and Gumbel-Softmax fails to learn meaningful discrete representations. We then highlight the anti-information limitation of the ev- idence lowerbound objective (ELBO) in VAEs and improve it by proposing Discrete Information VAE (DI-VAE) that maximizes the mutual infor- mation between data and latent actions. We further enrich the learning signals beyond auto encoding by extending Skip Thought ( <ref type="bibr" target="#b19">Kiros et al., 2015)</ref> to Discrete Information Variational Skip Thought (DI-VST) that learns sentence-level distributional semantics. Finally, an integration mechanism is presented that combines the learned latent actions with encoder decoder models.</p><p>The proposed systems are tested on several real- world dialog datasets. Experiments show that the proposed methods significantly outperform the standard VAEs and can discover meaningful latent actions from these datasets. Also, experiments confirm the effectiveness of the proposed integra- tion mechanism and show that the learned latent actions can control the sentence-level attributes of the generated responses and provide human- interpretable meaning representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is closely related to research in latent variable dialog models. The majority of mod- els are based on Conditional Variational Autoen- coders (CVAEs) ( <ref type="bibr" target="#b29">Serban et al., 2016;</ref><ref type="bibr" target="#b5">Cao and Clark, 2017</ref>) with continuous latent variables to better model the response distribution and encour- age diverse responses.  fur- ther introduced dialog acts to guide the learning of the CVAEs. Discrete latent variables have also been used for task-oriented dialog systems <ref type="bibr" target="#b33">(Wen et al., 2017)</ref>, where the latent space is used to rep- resent intention. The second line of related work is enriching the dialog context encoder with more fine-grained information than the dialog history. <ref type="bibr" target="#b21">Li et al., (2016)</ref> captured speakers' characteristics by encoding background information and speak- ing style into the distributed embeddings. <ref type="bibr" target="#b35">Xing et al., (2016)</ref> maintain topic encoding based on La- tent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> of the conversation to encourage the model to out- put more topic coherent responses.</p><p>The proposed method also relates to sentence representation learning using neural networks. Most work learns continuous distributed repre- sentations of sentences from various learning sig- nals ( <ref type="bibr" target="#b13">Hill et al., 2016</ref>), e.g. the Skip Thought learns representations by predicting the previous and next sentences <ref type="bibr" target="#b19">(Kiros et al., 2015</ref>). An- other area of work focused on learning regular- ized continuous sentence representation, which enables sentence generation by sampling the la- tent space <ref type="bibr" target="#b4">(Bowman et al., 2015;</ref><ref type="bibr" target="#b16">Kim et al., 2017)</ref>. There is less work on discrete sentence repre- sentations due to the difficulty of passing gradi- ents through discrete outputs. The recently devel- oped Gumbel Softmax ( <ref type="bibr" target="#b15">Jang et al., 2016;</ref><ref type="bibr" target="#b23">Maddison et al., 2016</ref>) and vector quantization (van den <ref type="bibr" target="#b31">Oord et al., 2017</ref>) enable us to train discrete vari- ables. Notably, discrete variable models have been proposed to discover document topics ( <ref type="bibr" target="#b26">Miao et al., 2016</ref>) and semi-supervised sequence trans- action ( <ref type="bibr" target="#b39">Zhou and Neubig, 2017)</ref> Our work differs from these as follows: (1) we focus on learning interpretable variables; in prior research the semantics of latent variables are mostly ignored in the dialog generation setting. (2) we improve the learning objective for discrete VAEs and overcome the well-known posterior col- lapsing issue <ref type="bibr" target="#b4">(Bowman et al., 2015;</ref><ref type="bibr" target="#b35">Chen et al., 2016)</ref>. <ref type="formula" target="#formula_3">(3)</ref> we focus on unsupervised learning of salient features in dialog responses instead of hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methods</head><p>Our formulation contains three random variables: the dialog context c, the response x and the la- tent action z. The context often contains the dis- course history in the format of a list of utterances. The response is an utterance that contains a list of word tokens. The latent action is a set of discrete variables that define high-level attributes of x. Be- fore introducing the proposed framework, we first identify two key properties that are essential in or-der for z to be interpretable:</p><p>1. z should capture salient sentence-level fea- tures about the response x.</p><p>2. The meaning of latent symbols z should be independent of the context c.</p><p>The first property is self-evident. The second can be explained: assume z contains a single discrete variable with K classes. Since the context c can be any dialog history, if the meaning of each class changes given a different context, then it is diffi- cult to extract an intuitive interpretation by only looking at all responses with class k ∈ <ref type="bibr">[1, K]</ref>. Therefore, the second property looks for latent ac- tions that have context-independent semantics so that each assignment of z conveys the same mean- ing in all dialog contexts.</p><p>With the above definition of interpretable latent actions, we first introduce a recognition network R : q R (z|x) and a generation network G. The role of R is to map an sentence to the latent vari- able z and the generator G defines the learning signals that will be used to train z's representa- tion. Notably, our recognition network R does not depend on the context c as has been the case in prior work ( <ref type="bibr" target="#b29">Serban et al., 2016)</ref>. The motiva- tion of this design is to encourage z to capture context-independent semantics, which are further elaborated in Section 3.4. With the z learned by R and G, we then introduce an encoder decoder network F : p F (x|z, c) and and a policy net- work π : p π (z|c). At test time, given a context c, the policy network and encoder decoder will work together to generate the next response viãviã x = p F (x|z ∼ p π (z|c), c). In short, R, G, F and π are the four components that comprise our pro- posed framework. The next section will first focus on developing R and G for learning interpretable z and then will move on to integrating R with F and π in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Sentence Representations from</head><p>Auto-Encoding</p><p>Our baseline model is a sentence VAE with dis- crete latent space. We use an RNN as the recog- nition network to encode the response x. Its last hidden state h R |x| is used to represent x. We de- fine z to be a set of K-way categorical variables</p><formula xml:id="formula_0">z = {z 1 ...z m ...z M }, where M is the number of variables. For each z m , its posterior distribution is defined as q R (z m |x) = Softmax(W q h R |x| + b q ).</formula><p>During training, we use the Gumbel-Softmax trick to sample from this distribution and obtain low- variance gradients. To map the latent samples to the initial state of the decoder RNN, we define {e 1 ...e m ...e M } where e m ∈ R K×D and D is the generator cell size. Thus the initial state of the generator is:</p><formula xml:id="formula_1">h G 0 = M m=1 e m (z m ).</formula><p>Finally, the generator RNN is used to reconstruct the response given h G 0 . VAEs is trained to maxmimize the evi- dence lowerbound objective (ELBO) <ref type="bibr" target="#b18">(Kingma and Welling, 2013)</ref>. For simplicity, later discussion drops the subscript m in z m and assumes a sin- gle latent z. Since each z m is independent, we can easily extend the results below to multiple vari- ables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Anti-Information Limitation of ELBO</head><p>It is well-known that sentence VAEs are hard to train because of the posterior collapse issue. Many empirical solutions have been proposed: weaken- ing the decoder, adding auxiliary loss etc. <ref type="bibr" target="#b4">(Bowman et al., 2015;</ref><ref type="bibr" target="#b35">Chen et al., 2016;</ref>. We argue that the posterior collapse issue lies in ELBO and we offer a novel decomposition to understand its behavior. First, instead of writ- ing ELBO for a single data point, we write it as an expectation over a dataset:</p><formula xml:id="formula_2">L VAE = E x [E q R (z|x) [log p G (x|z)] − KL(q R (z|x)p(z))]<label>(1)</label></formula><p>We can expand the KL term as Eq. 2 (derivations in Appendix A.1) and rewrite ELBO as:</p><formula xml:id="formula_3">E x [KL(q R (z|x)p(z))] = (2) I(Z, X)+KL(q(z)p(z)) L VAE = E q(z|x)p(x) [log p(x|z)] − I(Z, X) − KL(q(z)p(z))<label>(3)</label></formula><p>where q(z) = E x [q R (z|x)] and I(Z, X) is the mu- tual information between Z and X. This expan- sion shows that the KL term in ELBO is trying to reduce the mutual information between latent variables and the input data, which explains why VAEs often ignore the latent variable, especially when equipped with powerful decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">VAE with Information Maximization</head><p>and Batch Prior Regularization A natural solution to correct the anti-information issue in Eq. 3 is to maximize both the data likeli-hood lowerbound and the mutual information be- tween z and the input data:</p><formula xml:id="formula_4">L VAE + I(Z, X) = E q R (z|x)p(x) [log p G (x|z)] − KL(q(z)p(z)) (4)</formula><p>Therefore, jointly optimizing ELBO and mutual information simply cancels out the information- discouraging term. Also, we can still sample from the prior distribution for generation because of KL(q(z)p(z)). Eq. 4 is similar to the objec- tives used in adversarial autoencoders ( <ref type="bibr" target="#b24">Makhzani et al., 2015;</ref><ref type="bibr" target="#b16">Kim et al., 2017)</ref>. Our derivation pro- vides a theoretical justification to their superior performance. Notably, Eq. 4 arrives at the same loss function proposed in infoVAE <ref type="bibr" target="#b38">(Zhao S et al., 2017)</ref>. However, our derivation is different, offer- ing a new way to understand ELBO behavior.</p><p>The remaining challenge is how to minimize KL(q(z)p(z)), since q(z) is an expectation over q(z|x). When z is continuous, prior work has used adversarial training ( <ref type="bibr" target="#b24">Makhzani et al., 2015;</ref><ref type="bibr" target="#b16">Kim et al., 2017)</ref> or Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b38">(Zhao S et al., 2017</ref>) to regularize q(z). It turns out that minimizing KL(q(z)p(z)) for dis- crete z is much simpler than its continuous coun- terparts. Let x n be a sample from a batch of N data points. Then we have:</p><formula xml:id="formula_5">q(z) ≈ 1 N N n=1 q(z|x n ) = q (z)<label>(5)</label></formula><p>where q (z) is a mixture of softmax from the pos- teriors q(z|x n ) of each x n . We can approximate KL(q(z)p(z)) by:</p><formula xml:id="formula_6">KL(q (z)p(z)) = K k=1 q (z = k) log q (z = k) p(z = k)<label>(6)</label></formula><p>We refer to Eq. 6 as Batch Prior Regularization (BPR). When N approaches infinity, q (z) ap- proaches the true marginal distribution of q(z).</p><p>In practice, we only need to use the data from each mini-batch assuming that the mini batches are randomized. Last, BPR is fundamentally dif- ferent from multiplying a coefficient &lt; 1 to an- neal the KL term in VAE ( <ref type="bibr" target="#b4">Bowman et al., 2015)</ref>. This is because BPR is a non-linear operation log sum exp. For later discussion, we denote our discrete infoVAE with BPR as DI-VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Sentence Representations from the Context</head><p>DI-VAE infers sentence representations by recon- struction of the input sentence. Past research in distributional semantics has suggested the mean- ing of language can be inferred from the adjacent context <ref type="bibr" target="#b12">(Harris, 1954;</ref><ref type="bibr" target="#b13">Hill et al., 2016</ref>). The dis- tributional hypothesis is especially applicable to dialog since the utterance meaning is highly con- textual. For example, the dialog act is a well- known utterance feature and depends on dialog state <ref type="bibr" target="#b1">(Austin, 1975;</ref><ref type="bibr" target="#b30">Stolcke et al., 2000</ref>). Thus, we introduce a second type of latent action based on sentence-level distributional semantics. Skip thought (ST) is a powerful sentence representation that captures contextual informa- tion ( <ref type="bibr" target="#b19">Kiros et al., 2015)</ref>. ST uses an RNN to en- code a sentence, and then uses the resulting sen- tence representation to predict the previous and next sentences. Inspired by ST's robust perfor- mance across multiple tasks ( <ref type="bibr" target="#b13">Hill et al., 2016)</ref>, we adapt our DI-VAE to Discrete Information Varia- tional Skip Thought (DI-VST) to learn discrete la- tent actions that model distributional semantics of sentences. We use the same recognition network from DI-VAE to output z's posterior distribution q R (z|x). Given the samples from q R (z|x), two RNN generators are used to predict the previous sentence x p and the next sentences x n . Finally, the learning objective is to maximize:</p><formula xml:id="formula_7">L DI-VST = E q R (z|x)p(x)) [log(p n G (x n |z)p p G (x p |z))] − KL(q(z)p(z))<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integration with Encoder Decoders</head><p>We now describe how to integrate a given q R (z|x) with an encoder decoder and a policy network. Let the dialog context c be a sequence of utterances. Then a dialog context encoder network can en- code the dialog context into a distributed represen- </p><formula xml:id="formula_8">tation h e = F e (c</formula><formula xml:id="formula_9">L LAED (θ F , θ π ) = E q R (z|x)p(x,c) [logp π (z|c) + log p F (x|z, c)]<label>(8)</label></formula><p>Also simply augmenting the inputs of the decoders with latent action does not guarantee that the gen- erated response exhibits the attributes of the give action. Thus we use the controllable text gener- ation framework ( <ref type="bibr" target="#b14">Hu et al., 2017</ref>) by introducing L Attr , which reuses the same recognition network q R (z|x) as a fixed discriminator to penalize the de- coder if its generated responses do not reflect the attributes in z.</p><formula xml:id="formula_10">L Attr (θ F ) = E q R (z|x)p(c,x) [log q R (z|F(c, z))]<label>(9</label></formula><p>) Since it is not possible to propagate gradients through the discrete outputs at F d at each word step, we use a deterministic continuous relax- ation ( <ref type="bibr" target="#b14">Hu et al., 2017</ref>) by replacing output of F d with the probability of each word. Let o t be the normalized probability at step t ∈ <ref type="bibr">[1, |x|]</ref>, the inputs to q R at time t are then the sum of word embeddings weighted by o t , i.e. h R t = RNN(h R t−1 , Eo t ) and E is the word embedding matrix. Finally this loss is combined with L LAED and a hyperparameter λ to have Attribute Forcing LAED.</p><formula xml:id="formula_11">L attrLAED = L LAED + λL Attr<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relationship with Conditional VAEs</head><p>It is not hard to see L LAED is closely related to the objective of CVAEs for dialog generation <ref type="bibr" target="#b29">(Serban et al., 2016;</ref>, which is:</p><formula xml:id="formula_12">L CVAE = E q [log p(x|z, c)]−KL(q(z|x, c)p(z|c))<label>(11)</label></formula><p>Despite their similarities, we highlight the key dif- ferences that prohibit CVAE from achieving inter- pretable dialog generation. First L CVAE encour- ages I(x, z|c) (Agakov, 2005), which learns z that capture context-dependent semantics. More in- tuitively, z in CVAE is trained to generate x via p(x|z, c) so the meaning of learned z can only be interpreted along with its context c. Therefore this violates our goal of learning context-independent semantics. Our methods learn q R (z|x) that only depends on x and trains q R separately to ensure the semantics of z are interpretable standalone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>The proposed methods are evaluated on four datasets. The first corpus is Penn Treebank (PTB) ( <ref type="bibr" target="#b25">Marcus et al., 1993</ref>) used to evaluate sen- tence VAEs ( <ref type="bibr" target="#b4">Bowman et al., 2015)</ref>. We used the version pre-processed by <ref type="bibr" target="#b27">Mikolov (Mikolov et al., 2010)</ref>. The second dataset is the Stanford Multi-Domain Dialog (SMD) dataset that contains 3,031 human-Woz, task-oriented dialogs collected from 3 different domains (navigation, weather and scheduling) <ref type="bibr" target="#b9">(Eric and Manning, 2017)</ref>. The other two datasets are chat-oriented data: Daily Dialog (DD) and Switchboard (SW) (Godfrey and Hol- liman, 1997), which are used to test whether our methods can generalize beyond task-oriented di- alogs but also to to open-domain chatting. DD contains 13,118 multi-turn human-human dialogs annotated with dialog acts and emotions. ( <ref type="bibr" target="#b22">Li et al., 2017)</ref>. SW has 2,400 human-human telephone conversations that are annotated with topics and dialog acts. SW is a more challenging dataset be- cause it is transcribed from speech which contains complex spoken language phenomenon, e.g. hesi- tation, self-repair etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparing Discrete Sentence Representation Models</head><p>The first experiment used PTB and DD to eval- uate the performance of the proposed methods in learning discrete sentence representations. We implemented DI-VAE and DI-VST using GRU- RNN ( <ref type="bibr" target="#b8">Chung et al., 2014</ref>) and trained them using Adam ( <ref type="bibr" target="#b17">Kingma and Ba, 2014</ref>). Besides the pro- posed methods, the following baselines are com- pared. Unregularized models: removing the KL(q|p) term from DI-VAE and DI-VST leads to a simple discrete autoencoder (DAE) and dis- crete skip thought (DST) with stochastic discrete hidden units. ELBO models: the basic discrete sentence VAE (DVAE) or variational skip thought (DVST) that optimizes ELBO with regularization term KL(q(z|x)p(z)). We found that standard training failed to learn informative latent actions for either DVAE or DVST because of the poste- rior collapse. Therefore, KL-annealing ( <ref type="bibr" target="#b4">Bowman et al., 2015)</ref> and bag-of-word loss ( ) are used to force these two models learn meaningful representations. We also include the results for VAE with continuous latent variables reported on the same PTB ( . Ad- ditionally, we report the perplexity from a stan- dard GRU-RNN language model ( <ref type="bibr">Zaremba et al., 2014</ref>). The evaluation metrics include reconstruction perplexity (PPL), KL(q(z)p(z)) and the mutual information between input data and latent vari-ables I <ref type="bibr">(x, z)</ref>. Intuitively a good model should achieve low perplexity and KL distance, and si- multaneously achieve high I(x, z). The discrete latent space for all models are M =20 and K=10. Mini-batch size is 30.  <ref type="table">Table 1</ref> shows that all models achieve better per- plexity than an RNNLM, which shows they man- age to learn meaningful q(z|x). First, for auto- encoding models, DI-VAE is able to achieve the best results in all metrics compared other meth- ods. We found DAEs quickly learn to reconstruct the input but they are prone to overfitting dur- ing training, which leads to lower performance on the test data compared to DI-VAE. Also, since there is no regularization term in the latent space, q(z) is very different from the p(z) which pro- hibits us from generating sentences from the la- tent space. In fact, DI-VAE enjoys the same lin- ear interpolation properties reported in <ref type="bibr" target="#b4">(Bowman et al., 2015</ref>) (See Appendix A.2). As for DVAEs, it achieves zero I(x, z) in standard training and only manages to learn some information when train- ing with KL-annealing and bag-of-word loss. On the other hand, our methods achieve robust per- formance without the need for additional process- ing. Similarly, the proposed DI-VST is able to achieve the lowest PPL and similar KL compared to the strongly regularized DVST. Interestingly, al- though DST is able to achieve the highest I(x, z), but PPL is not further improved. These results confirm the effectiveness of the proposed BPR in terms of regularizing q(z) while learning mean- ingful posterior q(z|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dom</head><p>In order to understand BPR's sensitivity to batch size N , a follow-up experiment varied the batch size from 2 to 60 (If N =1, DI-VAE is equiv- alent to DVAE). <ref type="figure">Figure 2</ref> show that as N increases, <ref type="figure">Figure 2</ref>: Perplexity and I(x, z) on PTB by vary- ing batch size N . BPR works better for larger N . perplexity, I(x, z) monotonically improves, while KL(qp) only increases from 0 to 0.159. After N &gt; 30, the performance plateaus. Therefore, using mini-batch is an efficient trade-off between q(z) estimation and computation speed.</p><p>The last experiment in this section investigates the relation between representation learning and the dimension of the latent space. We set a fixed budget by restricting the maximum number of modes to be about 1000, i.e. K M ≈ 1000. We then vary the latent space size and report the same evaluation metrics. <ref type="table">Table 2</ref> shows that models with multiple small latent variables perform sig- nificantly better than those with large and few la- tent variables.  <ref type="table">Table 2</ref>: DI-VAE on PTB with different latent di- mensions under the same budget.</p><formula xml:id="formula_13">K, M K M PPL KL(qp) I(x, z) 1000</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interpreting Latent Actions</head><p>The next question is to interpret the meaning of the learned latent action symbols. To achieve this, the latent action of an utterance x n is obtained from a greedy mapping: a n = argmax k q R (z = k|x n ).</p><p>We set M =3 and K=5, so that there are at most 125 different latent actions, and each x n can now be represented by a 1 -a 2 -a 3 , e.g. "How are you?" → 1-4-2. Assuming that we have access to man- ually clustered data according to certain classes (e.g. dialog acts), it is unfair to use classic clus- ter measures <ref type="bibr" target="#b32">(Vinh et al., 2010)</ref> to evaluate the clusters from latent actions. This is because the uniform prior p(z) evenly distribute the data to all possible latent actions, so that it is expected that frequent classes will be assigned to several latent actions. Thus we utilize the homogeneity met- ric ( <ref type="bibr" target="#b28">Rosenberg and Hirschberg, 2007</ref>) that mea- sures if each latent action contains only members of a single class. We tested this on the SW and DD, which contain human annotated features and we report the latent actions' homogeneity w.r.t these features in  works better than DI-VAE in terms of creating ac- tions that are more coherent for emotion and dia- log acts. The results are interesting on SW since DI-VST performs worse on dialog acts than DI- VAE. One reason is that the dialog acts in SW are more fine-grained (42 acts) than the ones in DD (5 acts) so that distinguishing utterances based on words in x is more important than the information in the neighbouring utterances. We then apply the proposed methods to SMD which has no manual annotation and contains task- oriented dialogs. Two experts are shown 5 ran- domly selected utterances from each latent action and are asked to give an action name that can de- scribe as many of the utterances as possible. Then an Amazon Mechanical Turk study is conducted to evaluate whether other utterances from the same latent action match these titles. 5 workers see the action name and a different group of 5 utterances from that latent action. They are asked to select all utterances that belong to the given actions, which tests the homogeneity of the utterances falling in the same cluster. Negative samples are included to prevent random selection. <ref type="table" target="#tab_5">Table 4</ref> shows that both methods work well and DI-VST achieved better homogeneity than DI-VAE.</p><p>Since DI-VAE is trained to reconstruct its input and DI-VST is trained to model the context, they group utterances in different ways. For example, DI-VST would group "Can I get a restaurant", "I am looking for a restaurant" into one action where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Exp Agree Worker κ Match Rate DI-VAE 85.6% 0.52 71.3% DI-VST 93.3% 0.48 74.9%  <ref type="table">Table 5</ref>: Example latent actions discovered in SMD using our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dialog Response Generation with Latent Actions</head><p>Finally we implement an LAED as follows. The encoder is a hierarchical recurrent encoder <ref type="bibr" target="#b29">(Serban et al., 2016</ref>) with bi-directional GRU-RNNs as the utterance encoder and a second GRU-RNN as the discourse encoder. The discourse encoder output its last hidden state h e |x| . The decoder is another GRU-RNN and its initial state of the de- coder is obtained by</p><formula xml:id="formula_14">h d 0 = h e |x| + M m=1 e m (z m )</formula><p>, where z comes from the recognition network of the proposed methods. The policy network π is a 2-layer multi-layer perceptron (MLP) that models p π (z|h e |x| ). We use up to the previous 10 utter- ances as the dialog context and denote the LAED using DI-VAE latent actions as AE-ED and the one uses DI-VST as ST-ED.</p><p>First we need to confirm whether an LAED can generate responses that are consistent with the semantics of a given z. To answer this, we use a pre-trained recognition network R to check if a generated response carries the attributes in the given action. We generate dialog responses on a test dataset viã x = F(z ∼ π(c), c) with greedy RNN decoding. The generated re- sponses are passed into the R and we measure at- tribute accuracy by counting˜xcounting˜ counting˜x as correct if z = argmax k q R (k|˜xk|˜x).  <ref type="table">Table 6</ref>: Results for attribute accuracy with and without attribute loss.</p><p>responses are highly consistent with the given la- tent actions. Also, latent actions from DI-VAE achieve higher attribute accuracy than the ones from DI-VST, because z from auto-encoding is ex- plicitly trained for x reconstruction. Adding L attr is effective in forcing the decoder to take z into ac- count during its generation, which helps the most in more challenging open-domain chatting data, e.g. SW and DD. The accuracy of ST-ED on SW is worse than the other two datasets. The reason is that SW contains many short utterances that can be either a continuation of the same speaker or a new turn from the other speaker, whereas the re- sponses in the other two domains are always fol- lowed by a different speaker. The more complex context pattern in SW may require special treat- ment. We leave it for future work. The second experiment checks if the policy net- work π is able to predict the right latent action given just the dialog context. We report both accu- racy, i.e. argmax k q R (k|x) = argmax k p π (k |c) and perplexity of p π (z|c). The perplexity mea- sure is more useful for open domain dialogs be- cause decision-making in complex dialogs is often one-to-many given a similar context ( ).  the three dialog datasets. These scores provide useful insights to understand the complexity of a dialog dataset. For example, accuracy on open- domain chatting is harder than the task-oriented SMD data. Also, it is intuitive that predicting sys- tem actions is easier than predicting user actions on SMD. Also, in general the prediction scores for ST-ED are higher the ones for AE-ED. The rea- son is related to our previous discussion about the granularity of the latent actions. Since latent ac- tions from DI-VST mainly model the the type of utterances used in certain types of context, it is easier for the policy network to predict latent ac- tions from DI-VST. Therefore, choosing the type of latent actions is a design choice and depends on the type of interpretability that is needed. We fin- ish with an example generated from the two vari- ants of LAED on SMD as shown in <ref type="table">Table 8</ref>. Given a dialog context, our systems are able to output a probability distribution over different latent ac- tions that have interpretable meaning along with their natural language realizations.  <ref type="table">Table 8</ref>: Interpretable dialog generation on SMD with top probable latent actions. AE-ED predicts more fine-grained but more error-prone actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper presents a novel unsupervised frame- work that enables the discovery of discrete latent actions and interpretable dialog response genera- tion. Our main contributions reside in the two sentence representation models DI-VAE and DI- VST, and their integration with the encoder de- coder models. Experiments show the proposed methods outperform strong baselines in learning discrete latent variables and showcase the effec- tiveness of interpretable dialog response gener- ation. Our findings also suggest promising fu- ture research directions, including learning better context-based latent actions and using reinforce-ment learning to adapt policy networks. We be- lieve that this work is an important step forward towards creating generative dialog models that can not only generalize to large unlabelled datasets in complex domains but also be explainable to hu- man users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed models learn a set of discrete variables to represent sentences by either autoencoding or context prediction.</figDesc><graphic url="image-1.png" coords="1,312.09,355.33,208.62,56.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 . On DD, results show DI-VST</head><label>3</label><figDesc></figDesc><table>SW 
DD 
Act Topic Act Emotion 
DI-VAE 0.48 0.08 
0.18 0.09 
DI-VST 0.33 0.13 
0.34 0.12 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Homogeneity results (bounded [0, 1]). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Human evaluation results on judging the homogeneity of latent actions in SMD.</head><label>4</label><figDesc></figDesc><table>DI-VAE may denote two actions for them. Finally, 
Table 4.2 shows sample annotation results, which 
show cases of the different types of latent actions 
discovered by our models. 

Model 
Action 
Sample utterance 
DI-VAE 
scheduling 
-sys: okay, scheduling a yoga 
activity with Tom for the 8th at 
2pm. 
-sys: okay, scheduling a meet-
ing for 6 pm on Tuesday with 
your boss to go over the quar-
terly report. 
requests 
-usr: find out if it 's supposed 
to rain 
-usr: find nearest coffee shop 
DI-VST 
ask sched-
ule info 

-usr: when is my football ac-
tivity and who is going with 
me? 
-usr: tell me when my dentist 
appointment is? 
requests 
-usr: how about other coffee? 
-usr: 11 am please 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 .3 shows our generated</head><label>4</label><figDesc></figDesc><table>Domain AE-ED +L attr ST-ED +L attr 
SMD 
93.5% 
94.8% 91.9% 93.8% 
DD 
88.4% 
93.6% 78.5% 86.1% 
SW 
84.7% 
94.6% 57.3% 61.3% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 7 shows the prediction scores on</head><label>7</label><figDesc></figDesc><table>SMD 
AE-ED 3.045 (51.5% sys 52.4% usr 50.5%) 
ST-ED 1.695 (75.5% sys 82.1% usr 69.2%) 
DD 
SW 
AE-ED 4.47 (35.8%) 4.46 (31.68%) 
ST-ED 3.89 (47.5%) 3.68 (33.2%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Performance of policy network. L attr is 
included in training. 

</table></figure>

			<note place="foot" n="1"> Data and code are available at https://github. com/snakeztc/NeuralDialog-LAED.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Variational Information Maximization in Stochastic Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Vsevolodovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agakov</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How to do things with words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Langshaw</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Olympus: an open-source framework for conversational spoken language interface research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander I</forename><surname>Eskenazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on bridging the gap: Academic and industrial research in dialog technologies. Association for Computational Linguistics</title>
		<meeting>the workshop on bridging the gap: Academic and industrial research in dialog technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05962</idno>
		<title level="m">Latent variable dialogue models and their diversity</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. 2016. Variational lossy autoencoder</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Keyvalue retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05414</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaussian processes for fast policy optimisation of pomdp-based dialogue managers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">Young</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</title>
		<meeting>the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Switchboard-1 release 2. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holliman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributional structure. Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03483</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04223</idno>
		<title level="m">Adversarially regularized autoencoders for generating discrete structures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Information state and dialogue management in the trindi dialogue move engine toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Staffan</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David R Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="323" to="340" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03957</idno>
		<title level="m">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2007 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06069</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Nguyen Xuan Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10229</idno>
		<title level="m">Latent intention dialogue models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Topic augmented neural response generation with a joint attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08340</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10960</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
		<title level="m">Infovae: Information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multispace variational encoder-decoders for semisupervised labeled sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01691</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
