<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How much do word embeddings encode about syntax?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How much do word embeddings encode about syntax?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="822" to="827"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Do continuous word embeddings encode any useful information for constituency parsing? We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser: by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon. We test each of these hypotheses with a targeted change to a state-of-the-art base-line. Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data. Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from tree-banks in other ways.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper investigates a variety of ways in which word embeddings might augment a con- stituency parser with a discrete state space. Word embeddings-representations of lexical items as points in a real vector space-have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval <ref type="bibr" target="#b3">(Deerwester et al., 1990)</ref>. While word embeddings can be constructed di- rectly from surface distributional statistics, as in LSA, more sophisticated tools for unsupervised extraction of word representations have recently gained popularity <ref type="bibr" target="#b2">(Collobert et al., 2011;</ref><ref type="bibr" target="#b9">Mikolov et al., 2013a</ref>). Semi-supervised and unsupervised models for a variety of core NLP tasks, includ- ing named-entity recognition <ref type="bibr" target="#b4">(Freitag, 2004)</ref>, part- of-speech tagging <ref type="bibr" target="#b12">(Schütze, 1995)</ref>, and chunking <ref type="bibr" target="#b14">(Turian et al., 2010</ref>) have been shown to benefit from the inclusion of word embeddings as fea- tures. In the other direction, access to a syntac- tic parse has been shown to be useful for con- structing word embeddings for phrases composi- tionally ( <ref type="bibr">Hermann and Blunsom, 2013;</ref><ref type="bibr" target="#b0">Andreas and Ghahramani, 2013)</ref>. Dependency parsers have seen gains from distributional statistics in the form of discrete word clusters ( <ref type="bibr" target="#b8">Koo et al., 2008)</ref>, and re- cent work ( <ref type="bibr" target="#b1">Bansal et al., 2014)</ref> suggests that simi- lar gains can be derived from embeddings like the ones used in this paper.</p><p>It has been less clear how (and indeed whether) word embeddings in and of themselves are use- ful for constituency parsing. There certainly exist competitive parsers that internally represent lexi- cal items as real-valued vectors, such as the neural network-based parser of <ref type="bibr" target="#b5">Henderson (2004)</ref>, and even parsers which use pre-trained word embed- dings to represent the lexicon, such as <ref type="bibr" target="#b13">Socher et al. (2013)</ref>. In these parsers, however, use of word vectors is a structural choice, rather than an added feature, and it is difficult to disentangle whether vector-space lexicons are actually more powerful than their discrete analogs-perhaps the perfor- mance of neural network parsers comes entirely from the model's extra-lexical syntactic structure. In order to isolate the contribution from word em- beddings, it is useful to demonstrate improvement over a parser that already achieves state-of-the-art performance without vector representations.</p><p>The fundamental question we want to explore is whether embeddings provide any information beyond what a conventional parser is able to in- duce from labeled parse trees. It could be that the distinctions between lexical items that embed- dings capture are already modeled by parsers in other ways and therefore provide no further bene- fit. In this paper, we investigate this question em- pirically, by isolating three potential mechanisms for improvement from pre-trained word embed-  dings. Our result is mostly negative. With ex- tremely limited training data, parser extensions us- ing word embeddings give modest improvements in accuracy (relative error reduction on the order of 1.5%). However, with reasonably-sized training corpora, performance does not improve even when a wide variety of embedding methods, parser mod- ifications, and parameter settings are considered.</p><p>The fact that word embedding features result in nontrivial gains for discriminative dependency parsing ( <ref type="bibr" target="#b1">Bansal et al., 2014</ref>), but do not appear to be effective for constituency parsing, points to an interesting structural difference between the two tasks. We hypothesize that dependency parsers benefit from the introduction of features (like clus- ters and embeddings) that provide syntactic ab- stractions; but that constituency parsers already have access to such abstractions in the form of su- pervised preterminal tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Three possible benefits of word embeddings</head><p>We are interested in the question of whether a state-of-the-art discrete-variable constituency parser can be improved with word embeddings, and, more precisely, what aspect (or aspects) of the parser can be altered to make effective use of embeddings.</p><p>It seems clear that word embeddings exhibit some syntactic structure. Consider <ref type="figure" target="#fig_1">Figure 1</ref>, which shows embeddings for a variety of English determiners, projected onto their first two princi- pal components. We can see that the quantifiers each and every cluster together, as do few and most. These are precisely the kinds of distinc- tions between determiners that state-splitting in the Berkeley parser has shown to be useful <ref type="bibr" target="#b11">(Petrov and Klein, 2007)</ref>, and existing work ( <ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>) has observed that such regular em- bedding structure extends to many other parts of speech. But we don't know how prevalent or important such "syntactic axes" are in practice. Thus we have two questions: Are such groupings (learned on large data sets but from less syntacti- cally rich models) better than the ones the parser finds on its own? How much data is needed to learn them without word embeddings?</p><p>We consider three general hypotheses about how embeddings might interact with a parser:</p><p>1. Vocabulary expansion hypothesis: Word embeddings are useful for handling out-of- vocabulary words, because they automati- cally ensure that unknown words are treated the same way as known words with similar representations. Example: the infrequently- occurring treebank tag UH dominates greet- ings (among other interjections). Upon en- countering the unknown word hey, the parser assigns a low posterior probability of hav- ing been generated from UH. But its distri- butional representation is very close to the known word hello, and a model capable of mapping hey to its neighbor should be able to assign the right tag.</p><p>2. Statistic sharing hypothesis: Word embed- dings are useful for handling in-vocabulary words, by making it possible to pool statistics for related words. Example: individual first names are also rare in the treebank, but tend to cluster together in distributional represen- tations. A parser which exploited this effect could use this to acquire a robust model of name behavior by sharing statistics from all first names together, preventing low counts from producing noisy models of names.</p><p>seems to group words by definiteness. We would expect a feature corresponding to a word's position along this axis to be a useful feature in a feature-based lexicon.</p><p>Note that these hypotheses are not all mutually exclusive, and two or all of them might provide in- dependent gains. Our first task is thus to design a set of orthogonal experiments which make it pos- sible to test each of the three hypotheses in isola- tion. It is also possible that other mechanisms are at play that are not covered by these three hypothe- ses, but we consider these three to be likely central effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parser extensions</head><p>For the experiments in this paper, we will use the Berkeley parser <ref type="bibr" target="#b11">(Petrov and Klein, 2007)</ref> and the related Maryland parser ( <ref type="bibr" target="#b7">Huang and Harper, 2011</ref>). The Berkeley parser induces a latent, state- split PCFG in which each symbol V of the (ob- served) X-bar grammar is refined into a set of more specific symbols {V 1 , V 2 , . . .} which cap- ture more detailed grammatical behavior. This allows the parser to distinguish between words which share the same tag but exhibit very differ- ent syntactic behavior-for example, between ar- ticles and demonstrative pronouns. The Maryland parser builds on the state-splitting parser, replac- ing its basic word emission model with a feature- rich, log-linear representation of the lexicon.</p><p>The choice of this parser family has two moti- vations. First, these parsers are among the best in the literature, with a test performance of 90.  <ref type="formula">(2004)</ref>). Second, and more importantly, the fact that they use no continuous state representations internally makes it easy to design experiments that isolate the con- tributions of word vectors, without worrying about effects from real-valued operators higher up in the model. We consider the following extensions:</p><p>Vocabulary expansion → OOV model To evaluate the vocabulary expansion hypothe- sis, we introduce a simple but targeted out-of- vocabulary (OOV) model in which every unknown word is simply replaced by its nearest neighbor in the training set. For OOV words which are not in the dictionary of embeddings, we back off to the unknown word model for the underlying parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistic sharing → Lexicon pooling model</head><p>To evaluate the statistic sharing hypothesis, we propose a novel smoothing technique. The Berke- ley lexicon stores, for each latent (tag, word) pair, the probability p(w|t) directly in a lookup ta- ble. If we want to encourage similarly-embedded words to exhibit similar behavior in the generative model, we need to ensure that the are preferen- tially mapped onto the same latent preterminal tag. In order to do this, we replace this direct lookup with a smoothed, kernelized lexicon, where:</p><formula xml:id="formula_0">p(w|t) = 1 Z ∑ w ′ α t,w ′ e −β||ϕ(w)−ϕ(w ′ )|| 2<label>(1)</label></formula><p>with Z a normalizing constant to ensure that p(·|t) sums to one over the entire vocabulary. ϕ(w) is the vector representation of the word w, α t,w are per- basis weights, and β is an inverse radius parame- ter which determines the strength of the smooth- ing. Each α t,w is learned in the same way as its corresponding probability in the original parser model-during each M step of the training proce- dure, α w,t is set to the expected number of times the word w appears under the refined tag t. Intu- itively, as β grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where β = 0, Equation 1 is a uniform distribu- tion over the entire vocabulary). As β grows large words become more independent (and in the limit where β = ∞, each summand in Equation 1 is zero except where w ′ = w, and we recover the original direct-lookup model). There are computational concerns associated with this approach: the original scoring procedure for a (word, tag) pair was a single (constant-time) lookup; here it might take time linear in the size of the vocabulary. This causes parsing to become unacceptably slow, so an approximation is neces- sary. Luckily, the exponential decay of the kernel ensures that each word shares most of its weight with a small number of close neighbors, and al- most none with words farther away. To exploit this, we pre-compute the k-nearest-neighbor graph of points in the embedding space, and take the sum in Equation 1 only over this set of nearest neigh- bors. Empirically, taking k = 20 gives adequate performance, and increasing it does not seem to alter the behavior of the parser.</p><p>As in the OOV model, we also need to worry about how to handle words for which we have no vector representation. In these cases, we simply treat the words as if their vectors were so far away from everything else they had no influence, and report their weights as p(w|t) = α w . This ensures that our model continues to include the original Berkeley parser model as a limiting case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding structure → embedding features</head><p>To evaluate the embedding structure hypothesis, we take the Maryland featured parser, and replace the set of lexical template features used by that parser with a set of indicator features on a dis- cretized version of the embedding. For each di- mension i, we create an indicator feature corre- sponding to the linearly-bucketed value of the fea- ture at that index. In order to focus specifically on the effect of word embeddings, we remove the morphological features from the parser, but retain indicators on the identity of each lexical item.</p><p>The extensions we propose are certainly not the only way to target the hypotheses described above, but they have the advantage of being min- imal and straightforwardly interpretable, and each can be reasonably expected to improve parser per- formance if its corresponding hypothesis is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>We use the Maryland implementation of the Berkeley parser as our baseline for the kernel- smoothed lexicon, and the Maryland featured parser as our baseline for the embedding-featured lexicon. <ref type="bibr">1</ref> For all experiments, we use 50- dimensional word embeddings. Embeddings la- beled C&amp;W are from <ref type="bibr" target="#b2">Collobert et al. (2011)</ref>; em- beddings labeled CBOW are from <ref type="bibr" target="#b9">Mikolov et al. (2013a)</ref>, trained with a context window of size 2.</p><p>Experiments are conducted on the Wall Street Journal portion of the English Penn Treebank. We prepare three training sets: the complete training set of 39,832 sentences from the treebank (sec- tions 2 through 21), a smaller training set, consist- ing of the first 3000 sentences, and an even smaller set of the first 300.</p><p>Per-corpus-size settings of the parameter β are set by searching over several possible settings on the development set. For each training corpus size we also choose a different setting of the number of splitting iterations over which the Berkeley parser is run; for 300 sentences this is two splits, and for  <ref type="table">Table 2</ref>: Test set experiments with the best com- bination of models (based on development exper- iments). Again, we observe small gains with re- stricted training sets but no gains on the full train- ing set. Entries marked * are statistically signifi- cant (p &lt; 0.05) under a paired bootstrap resam- pling test. 3000 four splits. This is necessary to avoid over- fitting on smaller training sets. Consistent with the existing literature, we stop at six splits when using the full training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Various model-specific experiments are shown in <ref type="table">Table 1</ref>. We begin by investigating the OOV model. As can be seen, this model alone achieves small gains over the baseline for a 300-word train- ing corpus, but these gains become statistically in- significant with more training data. This behavior is almost completely insensitive to the choice of embedding.</p><p>Next we consider the lexicon pooling model. We began by searching over exponentially-spaced values of β to determine an optimal setting for  <ref type="table">Table 3</ref>: Experiments for other corpora, using the same combined model (lexicon pooling and OOV) as in <ref type="table">Table 2</ref>. Again, we observe no significant gains over the baseline.</p><p>each training set size; as expected, for small set- tings of β (corresponding to aggressive smooth- ing) performance decreased; as we increased the parameter, performance increased slightly before tapering off to baseline parser performance. The first block in <ref type="table">Table 1</ref> shows the best settings of β for each corpus size; as can be seen, this also gives a small improvement on the 300-sentence training corpus, but no discernible once the system has ac- cess to a few thousand labeled sentences.</p><p>Last we consider a model with a featured lex- icon, as described in <ref type="bibr" target="#b7">Huang and Harper (2011)</ref>. A baseline featured model ("ident") contains only indicator features on word identity (and performs considerably worse than its generative counter- part on small data sets). As described above, the full featured model adds indicator features on the bucketed value of each dimension of the word em- bedding. Here, the trend observed in the other two models is even more prominent-embedding fea- tures lead to improvements over the featured base- line, but in no case outperform the standard base- line with a generative lexicon.</p><p>We take the best-performing combination of all of these models (based on development experi- ments, a combination of the lexical pooling model with β = 0.3, and OOV, both using C&amp;W word embeddings), and evaluate this on the WSJ test set ( <ref type="table">Table 2</ref>). We observe very small (but statis- tically significant) gains with 300 and 3000 train sentences, but a decrease in performance on the full corpus.</p><p>To investigate the possibility that improvements from embeddings are exceptionally difficult to achieve on the Wall Street Journal corpus, or on English generally, we perform (1) a domain adap- tation experiment, in which we use the OOV and lexicon pooling models to train on WSJ and test on the first 4000 sentences of the Brown corpus (the "WSJ → Brown" column in <ref type="table">Table 3</ref>), and (2) a multilingual experiment, in which we train and test on the French treebank (the "French" column). Apparent gains from the OOV and lexicon pooling models remain so small as to be statistically indis- tinguishable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>With the goal of exploring how much useful syn- tactic information is provided by unsupervised word embeddings, we have presented three vari- ations on a state-of-the-art parsing model, with extensions to the out-of-vocabulary model, lexi- con, and feature set. Evaluation of these modi- fied parsers revealed modest gains on extremely small training sets, which quickly vanish as train- ing set size increases. Thus, at least restricted to phenomena which can be explained by the exper- iments described here, our results are consistent with two claims: (1) unsupervised word embed- dings do contain some syntactically useful infor- mation, but (2) this information is redundant with what the model is able to determine for itself from only a small amount of labeled training data.</p><p>It is important to emphasize that these results do not argue against the use of continuous repre- sentations in a parser's state space, nor argue more generally that constituency parsers cannot possi- bly benefit from word embeddings. However, the failure to uncover gains when searching across a variety of possible mechanisms for improvement, training procedures for embeddings, hyperparam- eter settings, tasks, and resource scenarios sug- gests that these gains (if they do exist) are ex- tremely sensitive to these training conditions, and not nearly as accessible as they seem to be in de- pendency parsers. Indeed, our results suggest a hypothesis that word embeddings are useful for dependency parsing (and perhaps other tasks) be- cause they provide a level of syntactic abstrac- tion which is explicitly annotated in constituency parses. We leave explicit investigation of this hy- pothesis for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Word representations of English determiners, projected onto their first two principal components. Embeddings from Collobert et al. (2011).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al. (2013) and 90.1 for Henderson</figDesc></figure>

			<note place="foot" n="3">. Embedding structure hypothesis: The structure of the space used for the embeddings directly encodes syntactic information in its coordinate axes. Example: with the exception of a, the vertical axis in Figure 1</note>

			<note place="foot" n="1"> Both downloaded from https://code.google. com/p/umd-featured-parser/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by BBN under DARPA contract HR0011-12-C-0014. The first author is supported by a National Science Foun-dation Graduate Research Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generative model of vector space semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the ACL Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trained named entity recognition using distributional clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature-rich log-linear lexical model for latent variable pcfg grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">P</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing</title>
		<meeting>the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="219" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. Assocation for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. Assocation for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributional part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Association for Computational Linguistics</title>
		<meeting>the European Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Proceedings of the Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
