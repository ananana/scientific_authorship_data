<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1006" to="1017"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1093</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Implicit discourse relation classification is of great challenge due to the lack of con-nectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neu-ral network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminabil-ity of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse relations connect linguistic units such as clauses and sentences to form coherent seman- tics. Identification of discourse relations can ben- efit a variety of downstream applications includ- ing question answering ( <ref type="bibr" target="#b24">Liakata et al., 2013)</ref>, ma- chine translation ( <ref type="bibr" target="#b21">Li et al., 2014</ref>), text summariza- tion ( <ref type="bibr" target="#b11">Gerani et al., 2014</ref>), opinion spam detection , and so forth.</p><p>Connectives (e.g., but, so, etc) are one of the most critical linguistic cues for identifying dis- course relations. When explicit connectives are present in the text, a simple frequency-based map- ping is sufficient to achieve over 85% classifica- tion accuracy <ref type="bibr" target="#b42">(Xue et al., 2016;</ref>. In contrast, implicit discourse relation recognition has long been seen as a challenging problem, with the best accuracy so far still lower than 50% ). In the implicit case, discourse rela- tions are not lexicalized by connectives, but to be inferred from relevant sentences (i.e., arguments). For example, the following two adjacent sentences Arg1 and Arg2 imply relation Cause (i.e., Arg2 is the cause of Arg1).</p><p>[Arg1]: Never mind.</p><p>[Arg2]: You already know the answer.</p><p>[Implicit connective]: Because</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Discourse relation]: Cause</head><p>Various attempts have been made to directly in- fer underlying relations by modeling the seman- tics of the arguments, ranging from feature-based methods ( <ref type="bibr" target="#b26">Lin et al., 2009;</ref><ref type="bibr" target="#b31">Pitler et al., 2009</ref>) to the very recent end-to-end neural models <ref type="bibr" target="#b6">(Chen et al., 2016a;</ref><ref type="bibr" target="#b35">Qin et al., 2016c</ref>). Despite impressive per- formance, the absence of strong explicit connec- tive cues has made the inference extremely hard and hindered further improvement. In fact, even the human annotators would make use of connec- tives to aid relation annotation. For instance, the popular Penn Discourse Treebank (PDTB) bench- mark data <ref type="bibr" target="#b32">(Prasad et al., 2008</ref>) was annotated by first inserting a connective expression (i.e., im- plicit connective, as shown in the above example) manually, and determining the abstract relation by combining both the implicit connective and con- textual semantics. Therefore, the huge performance gap between explicit and implicit parsing (namely, 85% vs 50%), as well as the human annotation practice, strongly motivates to incorporate connective infor- mation to guide the reasoning process. This paper aims to advance implicit parsing by making use of annotated implicit connectives available in train- ing data. Few recent work has explored such com- bination. <ref type="bibr" target="#b46">Zhou et al. (2010)</ref> developed a two-step approach by first predicting implicit connectives whose sense is then disambiguated to obtain the relation. However, the pipeline approach usually suffers from error propagation, and the method it- self has relied on hand-crafted features which do not necessarily generalize well. Other research leveraged explicit connective examples for data augmentation <ref type="bibr" target="#b1">Braud and Denis, 2015;</ref><ref type="bibr" target="#b2">Braud and Denis, 2016)</ref>. Our work is orthogonal and complemen- tary to this line.</p><p>In this paper, we propose a novel neural method that incorporates implicit connectives in a princi- pled adversarial framework. We use deep neu- ral models for relation classification, and take the intuition that, sentence arguments integrated with connectives would enable highly discriminative neural features for accurate relation inference, and an ideal implicit relation classifier, even though without access to connectives, should mimic the connective-augmented reasoning behavior by ex- tracting similarly salient features. We therefore setup a secondary network in addition to the im- plicit relation classifier, building upon connective- augmented inputs and serving as a feature learning model for the implicit classifier to emulate.</p><p>Methodologically, however, feature imitation in our problem is challenging due to the semantic gap induced by adding the connective cues. It is nec- essary to develop an adaptive scheme to flexibly drive learning and transfer discriminability. We devise a novel adversarial approach which enables a self-calibrated imitation mechanism. Specifi- cally, we build a discriminator which distinguishes between the features by the two counterpart net- works. The implicit relation network is then trained to correctly classify relations and simulta- neously to fool the discriminator, resulting in an adversarial framework. The adversarial mecha- nism has been an emerging method in different context, especially for image generation <ref type="bibr" target="#b12">(Goodfellow et al., 2014</ref>) and domain adaptation <ref type="bibr" target="#b10">(Ganin et al., 2016;</ref><ref type="bibr" target="#b8">Chen et al., 2016c</ref>). Our adversar- ial framework is unique to address neural fea- ture emulation between two models. Besides, to the best of our knowledge, this is the first adver- sarial approach in the context of discourse pars- ing. Compared to previous connective exploit- ing work ( <ref type="bibr" target="#b46">Zhou et al., 2010;</ref><ref type="bibr" target="#b40">Xu et al., 2012)</ref>, our method provides a new integration paradigm and an end-to-end procedure that avoids inefficient feature engineering and error propagation.</p><p>Our method is evaluated on the PDTB 2.0 benchmark in a variety of experimental settings. The proposed adversarial model greatly improves over standalone neural models and previous best- performing approaches. We also demonstrate that our implicit recognition network successfully imi- tates and extracts crucial hidden representations.</p><p>We begin by briefly reviewing related work in section 2. Section 3 presents the proposed adver- sarial model. Section 4 shows substantially im- proved experimental results over previous meth- ods. Section 5 discusses extensions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Implicit Discourse Relation Recognition</head><p>There has been a surge of interest in implicit dis- course parsing since the release of PDTB ( <ref type="bibr" target="#b32">Prasad et al., 2008)</ref>, the first large discourse corpus distin- guishing implicit examples from explicit ones. A large set of work has focused on direct classifica- tion based on observed sentences, including struc- tured methods with linguistically-informed fea- tures ( <ref type="bibr" target="#b26">Lin et al., 2009;</ref><ref type="bibr" target="#b31">Pitler et al., 2009;</ref><ref type="bibr" target="#b46">Zhou et al., 2010)</ref>, end-to-end neural models ( <ref type="bibr">Qin et al., 2016b,c;</ref><ref type="bibr" target="#b6">Chen et al., 2016a;</ref>, and combined approaches ( <ref type="bibr" target="#b17">Ji et al., 2016)</ref>. However, the lacking of connec- tive cues makes learning purely from contextual semantics full of challenges.</p><p>Prior work has attempted to leverage connec- tive information. <ref type="bibr" target="#b46">Zhou et al. (2010)</ref> also incorpo- rate implicit connectives, but in a pipeline man- ner by first predicting the implicit connective with a language model and determining discourse rela- tion accordingly. Instead of treating implicit con- nectives as intermediate prediction targets which can suffer from error propagation, we use the con- nectives to induce highly discriminative features to guide the learning of an implicit network, serving as an adaptive regularization mechanism for en-hanced robustness and generalization. Our frame- work is also end-to-end, avoiding costly feature engineering. Another notable line aims at adapt- ing explicit examples for data synthesis <ref type="bibr" target="#b0">(Biran and McKeown, 2013;</ref><ref type="bibr" target="#b1">Braud and Denis, 2015;</ref>, multi-task learning ( <ref type="bibr" target="#b20">Lan et al., 2013;</ref>, and word representation <ref type="bibr" target="#b2">(Braud and Denis, 2016)</ref>. Our work is orthogonal and complementary to these methods, as we use implicit connectives which have been annotated for implicit examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Networks</head><p>Deep neural networks have gained impressive suc- cess in various natural language processing tasks ( <ref type="bibr" target="#b3">Cai et al., 2017)</ref>, in which adversarial networks have been shown especially effective in deep genera- tive modeling ( <ref type="bibr" target="#b12">Goodfellow et al., 2014</ref>) and do- main adaptation ( <ref type="bibr" target="#b10">Ganin et al., 2016)</ref>. Generative adversarial nets ( <ref type="bibr" target="#b12">Goodfellow et al., 2014</ref>) learn to produce realistic samples through competition between a generator and a real/fake discrimina- tor. Professor forcing ( <ref type="bibr" target="#b19">Lamb et al., 2016</ref>) ap- plies a similar idea to improve long-term gener- ation of a recurrent neural language model. Other approaches ( <ref type="bibr" target="#b7">Chen et al., 2016b;</ref>) extend the framework for con- trollable image/text generation. <ref type="bibr" target="#b22">Li et al. (2015)</ref>; <ref type="bibr" target="#b37">Salimans et al. (2016)</ref> propose feature matching which trains generators to match the statistics of real/fake examples. Their features are extracted by the discriminator rather than the classifier net- works as in our case. Our work differs from the above since we consider the context of discrimi- native modeling. Adversarial domain adaptation forces a neural network to learn domain-invariant features using a classifier that distinguishes the domain of the network's input data based on the hidden feature. Our adversarial framework is dis- tinct in that besides the implicit relation network we construct a second neural network serving as a teacher model for feature emulation.</p><p>To the best of our knowledge, this is the first to employ the idea of adversarial learning in the context of discourse parsing. We propose a novel connective exploiting scheme based on feature im- itation, and to this end derive a new adversar- ial framework, achieving substantial performance gain over existing methods. The proposed ap- proach is generally applicable to other tasks for utilizing any indicative side information. We give more discussions in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial Method</head><p>Discourse connectives are key indicators for dis- course relation. In the annotation procedure of the PDTB implicit relation benchmark, annotators inserted implicit connective expressions between adjacent sentences to lexicalize abstract relations and help with final decisions. Our model aims at making full use of the provided implicit connec- tives at training time to regulate learning of im- plicit relation recognizer, encouraging extraction of highly discriminative semantics from raw argu- ments, and improving generalization at test time.</p><p>Our method provides a novel adversarial frame- work that leverages connective information in a flexible adaptive manner, and is efficiently trained end-to-end through standard back-propagation.</p><p>The basic idea of the proposed approach is sim- ple. We want our implicit relation recognizer, which predicts the underlying relation of sen- tence arguments without discourse connective, to have prediction behaviors close to a connective- augmented relation recognizer which is provided with a discourse connective in addition to the ar- guments. The connective-augmented recognizer is in analogy to an annotator with the help of connec- tives as in the human annotation process, and the implicit recognizer would be improved by learn- ing from such an "informed" annotator. Specif- ically, we want the latent features extracted by the two models to match as closely as possible, which explicitly transfers the discriminability of the connective-augmented representations to im- plicit ones.</p><p>To this end, instead of manually selecting a closeness metric, we take advantage of the ad- versarial framework by constructing a two-player zero-sum game between the implicit recognizer and a rival discriminator. The discriminator at- tempts to distinguish between the features ex- tracted by the two relation models, while the im- plicit relation model is trained to maximize the ac- curacy on implicit data, and at the same time to confuse the discriminator.</p><p>In the next we first present the overall architec- ture of the proposed approach (section 3.1), then develop the training procedure (section 3.2). The components are realized as deep (convolutional) neural networks, with detailed modeling choices   discussed in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Let (x, y) be a pair of input and output of implicit relation classification, where x = (x 1 , x 2 ) is a pair of sentence arguments, and y is the underlying discourse relation. Each training example also in- cludes an annotated implicit connective c that best expresses the relation. <ref type="figure" target="#fig_1">Figure 1</ref> shows the archi- tecture of our framework. The neural model for implicit relation clas- sification (i-CNN in the figure) extracts latent representation from the arguments, denoted as H I (x 1 , x 2 ), and feeds the feature into a classifier C for final prediction C(H I (x 1 , x 2 )). For ease of notation, we will also use H I (x) to denote the latent feature on data x.</p><p>The second relation network (a-CNN) takes as inputs the sentence arguments along with an implicit connective, to induce the connective- augmented representation H A (x 1 , x 2 , c), and ob- tains relation prediction C(H A (x 1 , x 2 , c)). Note that the same final classifier C is used for both net- works, so that the feature representations by the two networks are ensured to be within the same semantic space, enabling feature emulation as pre- sented shortly.</p><p>We further pair the implicit network with a ri- val discriminator D to form our adversarial game. The discriminator is to differentiate between the reasoning behaviors of the implicit network i-CNN and the augmented network a-CNN. Specifically, D is a binary classifier that takes as inputs a la- tent feature H derived from either i-CNN or a- CNN given appropriate data (where implicit con- nectives is either missing or present, respectively). The output D(H) estimates the probability that H comes from the connective-augmented a-CNN rather than i-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Procedure</head><p>The system is trained through an alternating op- timization procedure that updates the components in an interleaved manner. In this section, we first present the training objective for each component, and then give the overall training algorithm.</p><p>Let θ D denote the parameters of the discrimina- tor. The training objective of D is straightforward, i.e., to maximize the probability of correctly dis- tinguishing the input features:</p><formula xml:id="formula_0">max θ D LD = E (x,c,y)∼data log D(HA(x, c); θD)+ log(1 − D(HI (x); θD)) ,<label>(1)</label></formula><p>where E (x,c,y)∼data [·] denotes the expectation in terms of the data distribution.</p><p>We denote the parameters of the implicit net- work i-CNN and the classifier C as θ I and θ C , respectively. The model is then trained to (a) correctly classify relations in training data and (b) produce salient features close to connective- augmented ones. The first objective can be ful- filled by minimizing the usual cross-entropy loss:</p><formula xml:id="formula_1">LI,C (θI , θC ) = E (x,y)∼data J C(HI (x; θI ); θC ), y ,<label>(2)</label></formula><p>Algorithm 1 Adversarial Model for Implicit Recognition Input: Training data {(x, c, y) n } Parameters: λ 1 , λ 2 -balancing parameters 1: Initialize {θ I , θ C } and {θ A } by minimizing</p><p>Eq. <ref type="formula" target="#formula_1">(2)</ref> and Eq.(4), respectively 2: repeat 3:</p><p>Train the discriminator through Eq. <ref type="formula" target="#formula_0">(1)</ref> 4:</p><p>Train the relation models through Eq. <ref type="formula" target="#formula_4">(5)</ref> 5: until convergence Output: Adversarially enhanced implicit relation network i-CNN with classifier C for prediction where J(p, y) = − k I(y = k) log p k is the cross-entropy loss between predictive distribution p and ground-truth label y. We achieve objective (b) by minimizing the discriminator's chance of correctly telling apart the features:</p><formula xml:id="formula_2">LI (θI ) = Ex∼data log 1 − D(HI (x; θI )) .<label>(3)</label></formula><p>The parameters of the augmented network a- CNN, denoted as θ A , can be learned by simply fit- ting to the data, i.e., minimizing the cross-entropy loss as follows:</p><formula xml:id="formula_3">LA(θA) = E (x,c,y)∼data J C(HA(x, c; θA)), y .<label>(4)</label></formula><p>As mentioned above, here we use the same classi- fier C as for the implicit network, forcing a unified feature space of both networks. We combine the above objectives Eqs.(2)-(4) of the relation classi- fiers and minimize the joint loss:</p><formula xml:id="formula_4">min θ I ,θ A ,θ C LI,A,C = LI,C (θI , θC ) + λ1LI (θI ) + λ2LA(θA),<label>(5)</label></formula><p>where λ 1 and λ 2 are two balancing parameters cal- ibrating the weights of the classification losses and the feature-regulating loss. In practice, we pre- train the implicit and augmented networks inde- pendently by minimizing Eq.(2) and Eq.(4), re- spectively. In the adversarial training process, we found setting λ 2 = 0 gives stable conver- gence. That is, the connective-augmented features are fixed after the pre-training stage. Algorithm 1 summarizes the training procedure, where we interleave the optimization of Eq.(1) and Eq.(5) at each iteration. More practical details are provided in section 4. We instantiate all modules as neural networks (section 3.3) which are differ- entiable, and perform the optimization efficiently through standard stochastic gradient descent and back-propagation.   Through Eq.</p><p>(1) and Eq. <ref type="formula" target="#formula_2">(3)</ref>, the discriminator and the implicit relation network follow a min- imax competition, which drives both to improve until the implicit feature representations are close to the connective-augmented latent representa- tions, encouraging the implicit network to ex- tract highly discriminative features from raw sen- tence arguments for relation classification. Alter- natively, we can see Eq.(3) as an adaptive regu- larization on the implicit model, which, compared to pre-fixed regularizors such as 2 -regularization, provides a more flexible, self-calibrated mecha- nism to improve generalization ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Component Structures</head><p>We have presented our adversarial framework for implicit relation classification. We now discuss the model realization of each component. All com- ponents of the framework are parameterized with neural networks. Distinct roles of the modules in the framework lead to different modeling choices. <ref type="figure" target="#fig_4">Figure 2</ref> il- lustrates the structure of the implicit relation net- work i-CNN. We use a convolutional network as it is a common architectural choice for discourse parsing. The network takes as inputs the word vectors of the tokens in each sentence argument, and maps each argument to intermediate features through a shared convolutional layer. The result- ing representations are then concatenated and fed into a max pooling layer to select most salient fea- tures as the final representation. The final classi- fier C is a simple fully-connected layer followed by a softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Classification Networks</head><p>The connective-augmented network a-CNN has a similar structure as i-CNN, wherein implicit con- nective is appended to the second sentence as in- put. The key difference from i-CNN is that here we adopt average k-max pooling, which takes the av- erage of the top-k maximum values in each pool- ing window. The reason is to prevent the net- work from solely selecting the connective induced features (which are typically the most salient fea- tures) which would be the case when using max pooling, but instead force it to also attend to con- textual features derived from the arguments. This facilitates more homogeneous output features of the two networks, and thus facilitates feature imi- tation. In all the experiments we fixed k = 2.</p><p>Discriminator The discriminator is a binary classifier to identify the correct source of an in- put feature vector. To make it a strong rival to the feature imitating network (i-CNN), we model the discriminator as a multi-layer perceptron (MLP) enhanced with gated mechanism for efficient in- formation flow <ref type="bibr" target="#b38">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b35">Qin et al., 2016c</ref>), as shown in <ref type="figure" target="#fig_5">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate the effectiveness of our approach both quantitatively and qualitatively with exten- sive experiments. We evaluate prediction perfor- mance on the PDTB benchmark in different set- tings. Our method substantially improves over a diverse set of previous models, especially in the practical multi-class classification task. We per- form in-depth analysis of the model behaviors, and show our adversarial framework successfully en- ables the implicit relation model to imitate and learn discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We use PDTB 2.0 1 , one of the largest manually annotated discourse relation corpus. The dataset contains 16,224 implicit relation instances in total, with three levels of senses: Level-1 Class, Level-2 Type, and Level-3 Subtypes. The 1st level con- sists of four major relation Classes: COMPARI- SON, CONTINGENCY, EXPANSION and TEMPO- RAL. The 2nd level contains 16 Types.</p><p>To make extensive comparison with prior work of implicit discourse relation classification, we evaluate on two popular experimental settings: 1) multi-class classification for 2nd-level types ( <ref type="bibr" target="#b26">Lin et al., 2009;</ref>, and 2) one- versus-others binary classifications for 1st-level classes <ref type="bibr" target="#b31">(Pitler et al., 2009</ref>). We describe the de- tailed configurations in the following respective sections. We will focus our analysis on the multi- class classification setting, which is most realis- tic in practice and serves as a building block for a complete discourse parser such as that for the shared tasks of <ref type="bibr">CoNLL-2015</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>Here we provide the detailed architecture configurations of each component we used in the experiments.</p><p>• Throughout the experiments i-CNN and a- CNN contain 3 sets of convolutional filters with the filter sizes selected on the dev set. <ref type="table">Table 1</ref> lists the filter configurations of the convolutional layer in i-CNN and a-CNN in different tasks. As described in section 3.3, following the convolutional layer is a max pooling layer in i-CNN, and an average k- max pooling layer with k = 2 in a-CNN.</p><p>• The final single-layer classifier C contains 512 neurons with tanh activation function.</p><p>• The discriminator D consists of 4 fully- connected layers, with 2 gated pathways from layer 1 to layer 3 and layer 4 (see <ref type="figure" target="#fig_5">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Filter sizes Filter number</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDTB-Lin 2, 4, 8 3×256</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDTB-Ji 2, 5, 10 3×256</head><p>One-vs-all 2, 5, 10 3×1024 <ref type="table">Table 1</ref>: The convolutional architectures of i-CNN and a-CNN in different tasks (section 4). For ex- ample, in PDTB-Lin, we use 3 sets of filters, each of which is of size 2, 4, and 8, respectively; and each set has 256 filters.</p><p>The size of each layer is set to 1024 and is fixed in all the experiments.</p><p>• We set the dimension of the input word vec- tors to 300 and initialize with pre-trained word2vec ( <ref type="bibr" target="#b30">Mikolov et al., 2013</ref>). The max- imum length of sentence argument is set to 80. Truncation and zero-padding are applied when necessary.</p><p>All experiments were performed on a TITAN-X GPU and 128GB RAM, with neural implementa- tion based on Tensorflow 2 .</p><p>For adversarial model training, it is critical to keep balance between the progress of the two play- ers. We use a simple strategy which at each itera- tion optimizes the discriminator and the implicit relation network on a randomly-sampled mini- batch. We found this is enough to stabilize the training. The neural parameters are trained using AdaGrad <ref type="bibr" target="#b9">(Duchi et al., 2011</ref>) with an initial learn- ing rate of 0.001. For the balancing parameters in Eq. <ref type="formula" target="#formula_4">(5)</ref>, we set λ 1 = 0.1, while λ 2 = 0. That is, after the initialization stage the weights of the connective-augmented network a-CNN are fixed. This has been shown capable of giving stable and good predictive performance for our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implicit Relation Classification</head><p>We will mainly focus on the general multi-class classification problem in two alternative settings adopted in prior work, showing the superiority of our model over previous state of the arts. We perform in-depth comparison with carefully de- signed baselines, providing empirical insights into the working mechanism of the proposed frame- work. For broader comparisons we also report the performance in the one-versus-all setting.   <ref type="table">Table 2</ref>: Accuracy (%) on the test sets of the PDTB-Lin and PDTB-Ji settings for multi-class classification. Please see the text for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-class Classifications</head><p>We first adopt the standard PDTB splitting con- vention following <ref type="bibr" target="#b26">(Lin et al., 2009</ref>), denoted as PDTB-Lin, where sections 2-21, 22, and 23 are used as training, dev, and test sets, respectively. The most frequent 11 types of relations are se- lected in the task. During training, instances with more than one annotated relation types are con- sidered as multiple instances, each of which has one of the annotations. At test time, a prediction that matches one of the gold types is considered as correct. The test set contains 766 examples. More details are in ( <ref type="bibr" target="#b26">Lin et al., 2009</ref>). An alternative, slightly different multi-class setting is used in , denoted as PDTB-Ji, where sections 2-20, 0-1, and 21-22 are used as training, dev, and test sets, respectively. The resulting test set contains 1039 examples. We also evaluate in this setting for thorough comparisons. <ref type="table">Table 2</ref> shows the classification accuracy in both of the settings. We see that our model (Row 10) achieves state-of-the-art performance, greatly outperforming previous methods (Rows 6- 9) with various modeling paradigms, including the linguistic feature-based model ( <ref type="bibr" target="#b26">Lin et al., 2009)</ref>, pure neural methods ( <ref type="bibr" target="#b35">Qin et al., 2016c)</ref>, and com- bined approach .</p><p>To obtain better insights into the working mech- anism of our method, we further compare with a set of carefully selected baselines as shown in Rows 1-5. 1) "Word-vector" sums over the word vectors for sentence representation, show- ing the base effect of word embeddings. 2) "CNN" is a standalone convolutional net having the exact same architecture with our implicit rela-tion network. Our model trained within the pro- posed framework provides significant improve- ment, showing the benefits of utilizing implicit connectives at training time. 3) "Ensemble" has the same neural architecture with the proposed framework except that the input of a-CNN is not augmented with implicit connectives. This essen- tially is an ensemble of two implicit recognition networks. We see that the method performs even inferior to the single CNN model. This further confirms the necessity of exploiting connective in- formation. 4) "Multi-task" is the convolutional net augmented with an additional task of simultane- ously predicting the implicit connectives based on the network features. As a straightforward way of incorporating connectives, we see that the method slightly improves over the stand-alone CNN, while falling behind our approach with a large margin. This indicates that our proposed feature imitation is a more effective scheme for making use of im- plicit connectives. 5) At last, " 2 -reg" also imple- ments feature mimicking by imposing an 2 dis- tance penalty between the implicit relation fea- tures and connective-augmented features. We see that the simple model has obtained improvement over previous best-performing systems in both settings, further validating the idea of imitation. However, in contrast to the fixed 2 regularization, our adversarial framework provides an adaptive mechanism, which is more flexible and performs better as shown in the   scores. Our method outperforms most of the prior systems in all the tasks. We achieve state-of-the- art performance in recognition of the Expansion relation, and obtain comparable scores with the best-performing methods in each of the other rela- tions, respectively. Notably, our feature imitation scheme greatly improves over ( <ref type="bibr" target="#b46">Zhou et al., 2010</ref>) which leverages implicit connectives as an inter- mediate prediction task. This provides additional evidence for the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>We now take a closer look into the modeling be- havior of our framework, by investigating the pro- cess of the adversarial game during training, as well as the feature imitation effects. <ref type="figure">Figure 4</ref> demonstrates the training progress of different components. The a-CNN network keeps high predictive accuracy as implicit connectives are given, showing the importance of connective cues. The rise-and-fall patterns in the accuracy of the discriminator clearly show its competition with the implicit relation network i-CNN as train- ing goes. At first few iterations the accuracy of the discriminator increases quickly to over 0.9, while at late stage the accuracy drops to around 0.6, showing that the discriminator is getting confused by i-CNN (an accuracy of 0.5 indicates full con- fusion). The i-CNN network keeps improving in terms of implicit relation classification accuracy, as it is gradually fitting to the data and simultane- ously learning increasingly discriminative features by mimicking a-CNN. The system exhibits simi- lar learning patterns in the two different settings, showing the stability of the training strategy.</p><p>We finally visualize the output feature vec- tors of i-CNN and a-CNN using the t-SNE method <ref type="bibr" target="#b29">(Maaten and Hinton, 2008</ref>) in <ref type="figure">Figure 5</ref>. Without feature imitation, the extracted features by the two networks are clearly separated <ref type="figure">(Fig- ure 5(a)</ref>). In contrast, as shown in <ref type="figure">Figures 5(b)</ref>- (c), the feature vectors are increasingly mixed as training proceeds. Thus our framework has suc- cessfully driven i-CNN to induce similar represen- tations with a-CNN, even though connectives are not present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head><p>We have developed an adversarial neural frame- work that facilitates an implicit relation network to extract highly discriminative features by mimick- ing a connective-augmented network. Our method achieved state-of-the-art performance for implicit discourse relation classification. Besides implicit connective examples, our model can naturally ex- ploit enormous explicit connective data to further improve discourse parsing. The proposed adversarial feature imitation scheme is also generally applicable to other con- text to incorporate indicative side information available at training time for enhanced infer- ence. Our framework shares a similar spirit of the iterative knowledge distillation method (Hu et al., 2016a,b) which train a "student" network to mimic the classification behavior of a knowledge- informed "teacher" network. Our approach en- courages imitation on the feature level instead of the final prediction level. This allows our ap- proach to apply to regression tasks, and more in- terestingly, the context in which the student and teacher networks have different prediction out- puts, e.g., performing different tasks, while trans- ferring knowledge between each other can be ben- eficial. Besides, our adversarial mechanism pro- vides an adaptive metric to measure and drive the imitation procedure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the proposed method. The framework contains three main components: 1) an implicit relation network i-CNN over raw sentence arguments, 2) a connective-augmented relation network a-CNN whose inputs are augmented with implicit connectives, and 3) a discriminator distinguishing between the features by the two networks. The features are fed to the final classifier for relation classification. The discriminator and i-CNN form an adversarial pair for feature imitation. At test time, the implicit network i-CNN with the classifier is used for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Never mind. Arg2: You know the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Neural structure of i-CNN. Two sets of convolutional filters are shown, with the corresponding features in red and blue, respectively. The weights of the filters on two input arguments are tied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Neural structure of the discriminator D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: (Best viewed in colors.) Test-set performance of three components over training epochs. Relation networks a-CNN and i-CNN are measured with multi-class classification accuracy (with or without implicit connectives, respectively), while the discriminator is evaluated with binary classification accuracy. Top: the PDTB-Lin setting (Lin et al., 2009), where first 8 epochs are for initialization stage (thus the discriminator is fixed and not shown); Bottom: the PDTB-Ji setting (Ji and Eisenstein, 2015), where first 3 epochs are for initialization.</figDesc><graphic url="image-3.png" coords="9,375.08,284.80,130.95,130.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 : Never mind. x 2 : You Know the answer. i a +implicit connective c: Because D C x 1 :</head><label></label><figDesc></figDesc><table>Never mind. 
x 2 : Because You Know the answer. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>table .</head><label>.</label><figDesc></figDesc><table>Model 
COMP. CONT. 
EXP. 
TEMP. 

Pitler et al. (2009) 
21.96 
47.13 
-
16.76 
Qin et al. (2016c) 
41.55 
57.32 
71.50 
35.43 
Zhang et al. (2016a) 
35.88 
50.56 
71.48 
29.54 
Zhou et al. (2010) 
31.79 
47.16 
70.11 
20.30 
Liu and Li (2016) 
36.70 
54.48 
70.43 
38.84 
Chen et al. (2016a) 
40.17 
54.76 
-
31.32 

Ours 
40.87 
54.56 
72.38 
36.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparisons of F 1 scores (%) for binary 
classification. 

One-versus-all Classifications 
We also report the results of four one-versus-all 
binary classifications for more comparisons with 
prior work. We follow the conventional experi-
mental setting (Pitler et al., 2009) by selecting sec-
tions 2-20, 21-22, and 0-1 as training, dev, and test 
sets. Table 4 lists the statistics of the data. 
Following previous work, Table 3 reports the F1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Distributions of positive and negative in-
stances from the train/dev/test sets in four binary 
relation classification tasks. 

</table></figure>

			<note place="foot" n="1"> http://www.seas.upenn.edu/∼pdtb/</note>

			<note place="foot" n="2"> https://www.tensorflow.org</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aggregated word pair features for implicit discourse relation disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
	<note>Short Papers). Sofia, Bulgaria</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing word representations for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2201" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning connective-based word representations for implicit discourse relation identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and accurate neural word segmentation for Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL). Vancouver</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL). Vancouver</meeting>
		<imprint>
			<publisher>Canada</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shallow discourse parsing using constituent parsing tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning-Shared Task (CONLL)</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning-Shared Task (CONLL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deceptive opinion spam detection using deep level linguistic feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 4th CCF Conference on Natural Language Processing and Chinese Computing (NLPCC 2015)</title>
		<meeting><address><addrLine>Nanchang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9362</biblScope>
			<biblScope unit="page" from="465" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implicit discourse relation detection via a deep architecture with gated relevance network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1726" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
	<note>John Schulman, Ilya Sutskever, and Pieter Abbeel</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1602" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2410" to="2420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00955</idno>
		<title level="m">Controllable text generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks with massive learned knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP<address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One vector is not enough: Entity-augmented distributed semantics for discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="329" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A latent variable recurrent neural network for discourse-driven language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="332" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Closing the gap: Domain adaptation from explicit to implicit discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2219" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4601" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="476" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Assessing the discourse factors that influence the quality of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi Jessy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="283" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A constituent syntactic parse tree based discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Conference on Computational Natural Language Learning-Shared Task (CONLL)</title>
		<meeting>the Twentieth Conference on Computational Natural Language Learning-Shared Task (CONLL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="60" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discourse-driven content model for summarising scientific articles evaluated in a complex question answering task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dobnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamasree</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Batchelor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Rebholz-Schuhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="747" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Recurrent topictransition GAN for visual paragraph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07022</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the Penn Discourse Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations via repeated reading: Neural networks with multi-level attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1224" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Implicit discourse relation classification via multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02776</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>South Lake Tahoe, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of he Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACL-IJCNLP). Suntec</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of he Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACL-IJCNLP). Suntec<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The sixth international conference on Language Resources and Evaluation (LREC). Marrakech</title>
		<meeting><address><addrLine>Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Implicit discourse relation recognition with contextaware character-enhanced embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shallow discourse parsing using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL-16 shared task</title>
		<meeting>the CoNLL-16 shared task<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A stacking gated neural architecture for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2263" to="2270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving the inference of implicit discourse relations via classifying explicit discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL: HLT)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL: HLT)<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="799" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning distributed word representations for bidirectional LSTM recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Connective prediction using machine learning for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><forename type="middle">Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2012 International Joint Conference on Neural Networks (IJCNN). Brisbane, Australia</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The CoNLL-2015 shared task on shallow discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning-Shared Task (CoNLL)</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning-Shared Task (CoNLL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The CoNLL-2016 shared task on shallow discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Conference on Computational Natural Language Learning-Shared Task (CoNLL)</title>
		<meeting>the Twentieth Conference on Computational Natural Language Learning-Shared Task (CoNLL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variational neural discourse relation recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Texas</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Probabilistic graph-based dependency parsing with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1382" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Predicting discourse connectives for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1507" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
