<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a Lexical Simplifier Using Wikipedia</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colby</forename><surname>Horn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Middlebury College</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathryn</forename><surname>Manduca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Middlebury College</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Middlebury College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a Lexical Simplifier Using Wikipedia</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="458" to="463"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we introduce a new lexical simplification approach. We extract over 30K candidate lexical simplifications by identifying aligned words in a sentence-aligned corpus of English Wikipedia with Simple English Wikipedia. To apply these rules, we learn a feature-based ranker using SVM rank trained on a set of labeled simplifications collected using Amazon&apos;s Mechanical Turk. Using human simplifications for evaluation, we achieve a precision of 76% with changes in 86% of the examples.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text simplification is aimed at reducing the read- ing and grammatical complexity of text while re- taining the meaning <ref type="bibr" target="#b4">(Chandrasekar and Srinivas, 1997)</ref>. Text simplification techniques have a broad range of applications centered around increasing data availability to both targeted audiences, such as children, language learners, and people with cognitive disabilities, as well as to general readers in technical domains such as health and medicine <ref type="bibr" target="#b6">(Feng, 2008)</ref>.</p><p>Simplifying a text can require a wide range of transformation operations including lexical changes, syntactic changes, sentence splitting, deletion and elaboration ( <ref type="bibr" target="#b5">Coster and Kauchak, 2011;</ref><ref type="bibr" target="#b17">Zhu et al., 2010)</ref>. In this paper, we ex- amine a restricted version of the text simplifica- tion problem, lexical simplification, where text is simplified by substituting words or phrases with simpler variants. Even with this restriction, lexi- cal simplification techniques have been shown to positively impact the simplicity of text and to im- prove reader understanding and information reten- tion ( . Additionally, restrict- ing the set of transformation operations allows for more straightforward evaluation than the general simplification problem ( <ref type="bibr" target="#b13">Specia et al., 2012</ref>).</p><p>Most lexical simplification techniques rely on transformation rules that change a word or phrase into a simpler variant with similar meaning <ref type="bibr" target="#b0">(Biran et al., 2011;</ref><ref type="bibr" target="#b13">Specia et al., 2012;</ref><ref type="bibr" target="#b15">Yatskar et al., 2010)</ref>. Two main challenges exist for this type of approach. First, the lexical focus of the trans- formation rules makes generalization difficult; a large number of transformation rules is required to achieve reasonable coverage and impact. Second, rules do not apply in all contexts and care must be taken when performing lexical transformations to ensure local cohesion, grammaticality and, most importantly, the preservation of the original mean- ing.</p><p>In this paper, we address both of these issues. We leverage a data set of 137K aligned sentence pairs between English Wikipedia and Simple En- glish Wikipedia to learn simplification rules. Pre- vious approaches have used unaligned versions of Simple English Wikipedia to learn rules <ref type="bibr" target="#b0">(Biran et al., 2011;</ref><ref type="bibr" target="#b15">Yatskar et al., 2010)</ref>, however, by using the aligned version we are able to learn a much larger rule set.</p><p>To apply lexical simplification rules to a new sentence, a decision must be made about which, if any, transformations should be applied. Previous approaches have used similarity measures (Biran et al., 2011) and feature-based approaches ( <ref type="bibr" target="#b13">Specia et al., 2012)</ref> to make this decision. We take the lat- ter approach and train a supervised model to rank candidate transformations.</p><p>The first school was established in 1857. The first school was started in 1857. The district was established in 1993 by merging the former districts of Bernau and Eberswalde. The district was made in 1993 by joining the old districts of Bernau and Eberswalde. <ref type="table">Table 1</ref>: Two aligned sentence pairs. The bottom sentence is a human simplified version of the top sentence. Bold words are candidate lexical simpli- fications.</p><p>1. The bottom sentence of each pair is a simpli- fied variant of the top sentence. By identifying aligned words within the aligned sentences, can- didate lexical simplifications can be learned. The bold words show two such examples, though other candidates exist in the bottom pair. By examining aligned sentence pairs we can learn a simplifica- tion rule. For example, we might learn:</p><p>established → began, made, settled, started Given a sentence s 1 , s 2 , ..., s n , a simplification rule applies if the left hand side of the rule can be found in the sentence (s i = w, for some i). If a rule applies, then a decision must be made about which, if any, of the candidate simplifications should be substituted for the word w to simplify the sentence. For example, if we were attempting to simplify the sentence</p><p>The ACL was established in 1962.</p><p>using the simplification rule above, some of the simplification options would not apply because of grammatical constraints, e.g. began, while others would not apply for semantic reasons, e.g. settled. This does not mean that these are not good simplifications for established since in other contexts, they might be appropriate. For example, in the sentence began is a reasonable option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning a Lexical Simplifier</head><p>We break the learning problem into two steps: 1) learn a set of simplification rules and 2) learn a ranking function for determining the best simpli- fication candidate when a rule applies. Each of these steps are outlined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rule Extraction</head><p>To extract the set of simplification rules, we use a sentence-aligned data set of English Wikipedia sentences (referred to as normal) aligned to Sim- ple English Wikipedia sentences (referred to as simple) <ref type="bibr" target="#b5">(Coster and Kauchak, 2011</ref>). The data set contains 137K such aligned sentence pairs. Given a normal sentence and the corresponding aligned simple sentence, candidate simplifications are extracted by identifying a word in the simple sentence that corresponds to a different word in the normal sentence. To identify such pairs, we au- tomatically induce a word alignment between the normal and simple sentence pairs using GIZA++ <ref type="bibr" target="#b11">(Och and Ney, 2000</ref>). Words that are aligned are considered as possible candidates for extraction. Due to errors in the sentence and word alignment processes, not all words that are aligned are actu- ally equivalent lexical variants. We apply the fol- lowing filters to reduce such spurious alignments:</p><p>• We remove any pairs where the normal word occurs in a stoplist. Stoplist words tend to be simple already and stoplist words that are be- ing changed are likely either bad alignments or are not simplifications.</p><p>• We require that the part of speeches (POS) of the two words be the same. The parts of speech were calculated based on a full parse of the sentences using the Berkeley parser <ref type="bibr" target="#b12">(Petrov and Klein, 2007</ref>).</p><p>• We remove any candidates where the POS is labeled as a proper noun. In most cases, proper nouns should not be simplified.</p><p>All other aligned word pairs are extracted. To generate the simplification rules, we collect all candidate simplifications (simple words) that are aligned to the same normal word.</p><p>As mentioned before, one of the biggest chal- lenges for lexical simplification systems is gen- eralizability. To improve the generalizability of the extracted rules, we add morphological variants of the words in the rules. For nouns, we include both singular and plural variants. For verbs, we expand to all inflection variants. The morpholog- ical changes are generated using MorphAdorner <ref type="bibr" target="#b2">(Burns, 2013</ref>) and are applied symmetrically: any change to the normal word is also applied to the corresponding simplification candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lexical Simplification as a Ranking Problem</head><p>A lexical simplification example consists of three parts: 1) a sentence, s 1 , s 2 , ..., s n , 2), a word in that sentence, s i , and 3) a list of candidate sim- plifications for s i , c 1 , c 2 , ..., c m . A labeled exam- ple is an example where the rank of the candidate simplifications has been specified. Given a set of labeled examples, the goal is to learn a ranking function that, given an unlabeled example (exam- ple without the candidate simplifications ranked), specifies a ranking of the candidates.</p><p>To learn this function, features are extracted from a set of labeled lexical simplification exam- ples. These labeled examples are then used to train a ranking function. We use SVM rank <ref type="bibr" target="#b7">(Joachims, 2006</ref>), which uses a linear support vector machine.</p><p>Besides deciding which of the candidates is most applicable in the context of the sentence, even if a rule applies, we must also decide if any simplification should occur. For example, there may be an instance where none of the can- didate simplifications are appropriate in this con- text. Rather than viewing this as a separate prob- lem, we incorporate this decision into the ranking problem by adding w as a candidate simplifica- tion. For each rule, w → c 1 , c 2 , ..., c m we add one additional candidate simplification which does not change the sentence, w → c 1 , c 2 , ..., c m , w. If w is ranked as the most likely candidate by the ranking algorithm, then the word is not simplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Features</head><p>The role of the features is to capture information about the applicability of the word in the context of the sentence as well as the simplicity of the word. Many features have been suggested previ- ously for use in determining the simplicity of a word (Specia et al., 2012) and for determining if a word is contextually relevant <ref type="bibr" target="#b0">(Biran et al., 2011;</ref><ref type="bibr" target="#b10">McCarthy and Navigli, 2007)</ref>. Our goal for this paper is not feature exploration, but to examine the usefulness of a general framework for feature- based ranking for lexical simplification. The fea- tures below represent a first pass at candidate fea- tures, but many others could be explored.</p><p>Candidate Probability p(c i |w): in the sentence-aligned Wikipedia data, when w is aligned to some candidate simplifica- tion, what proportion of the time is that candidate c i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequency</head><p>The frequency of a word has been shown to cor- relate with the word's simplicity and with peo- ple's knowledge of that word ( . We measured a candidate simplification's frequency in two corpora: 1) Simple English Wikipedia and 2) the web, as measured by the un- igram frequency from the Google n-gram corpus <ref type="bibr" target="#b1">(Brants and Franz, 2006</ref>).</p><p>Language Models n-gram language models capture how likely a par- ticular sequence is and can help identify candidate simplifications that are not appropriate in the con- text of the sentence. We included features from four different language models trained on four dif- ferent corpora: 1) Simple English Wikipedia, 2) English Wikipedia, 3) Google n-gram corpus and 4) a linearly interpolated model between 1) and 2) with λ = 0.5, i.e. an even blending. We used the SRI language modeling toolkit <ref type="bibr" target="#b14">(Stolcke, 2002</ref>) with Kneser-Kney smoothing. All models were trigram language models except the Google n-gram model, which was a 5-gram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Frequency</head><p>As another measure of the applicability of a can- didate in the context of the sentence, we also cal- culate the frequency in the Google n-grams of the candidate simplification in the context of the sen- tence with context windows of one and two words. If the word to be substituted is at position i in the sentence (w = s i ), then the one word window frequency for simplification c j is the trigram fre- quency of s i−1 c j s i+1 and the two word window the 5-gram frequency of s i−2 s i−1 c j s i+1 s i+2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>For training and evaluation of the models, we col- lected human labelings of 500 lexical simplifica- tion examples using Amazon's Mechanical Turk (MTurk) <ref type="bibr">1</ref> . MTurk has been used extensively for annotating and evaluating NLP tasks and has been shown to provide data that is as reliable as other forms of human annotation <ref type="bibr" target="#b3">(Callison-Burch and Dredze, 2010;</ref><ref type="bibr" target="#b16">Zaidan and Callison-Burch, 2011</ref>). <ref type="figure" target="#fig_1">Figure 1</ref> shows an example of the task we asked annotators to do. Given a sentence and a word to be simplified, the task is to suggest a simpler variant of that word that is appropriate in the con- text of the sentence. Candidate sentences were se-Enter a simpler word that could be substituted for the red, bold word in the sentence. A simpler word is one that would be understood by more people or people with a lower reading level (e.g. children).</p><p>Food is procured with its suckers and then crushed using its tough "beak" of chitin. lected from the sentence-aligned Wikipedia cor- pus where a word in the normal sentence is be- ing simplified to a different word in the simple sentence, as identified by the automatically in- duced word alignment. The normal sentence and the aligned word were then selected for annota- tion. These examples represent words that other people (those that wrote/edited the Simple En- glish Wikipedia page) decided were difficult and required simplification.</p><p>We randomly selected 500 such sentences and collected candidate simplifications from 50 people per sentence, for a total of 25,000 annotations. To participate in the annotation process, we required that the MTurk workers live in the U.S. (for En- glish proficiency) and had at least a 95% accep- tance rate on previous tasks.</p><p>The simplifications suggested by the annotators were then tallied and the resulting list of simpli- fications with frequencies provides a ranking for training the candidate ranker. <ref type="table">Table 2</ref> shows the ranked list of annotations collected for the exam- ple in <ref type="figure" target="#fig_1">Figure 1</ref>. This data set is available online. <ref type="bibr">2</ref> Since these examples were selected from En- glish Wikipedia they, and the corresponding aligned Simple English Wikipedia sentences, were removed from all resources used during both the rule extraction and the training of the ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Other Approaches</head><p>We compared our lexical simplification approach (rank-simplify) to two other approaches. To un- derstand the benefit of the feature-based ranking algorithm, we compared against a simplifier that uses the same rule set, but ranks the candidates only based on their frequency in Simple English Wikipedia (frequency). This is similar to base- lines used in previous work <ref type="bibr" target="#b0">(Biran et al., 2011</ref>).</p><p>To understand how our extracted rules com- pared to the rules extracted by Biran et al., we 2 http://www.cs.middlebury.edu/ ˜ dkauchak/simplification/ used their rules with our ranking approach (rank- Biran). Their approach also extracts rules from a corpus of English Wikipedia and Simple En- glish Wikipedia, however, they do not utilize a sentence-aligned version and instead rely on con- text similarity measures to extract their rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>We used the 500 ranked simplification examples to train and evaluate our approach. We employed 10- fold cross validation for all experiments, training on 450 examples and testing on 50.</p><p>We evaluated the models with four different metrics:</p><p>precision: Of the words that the system changed, what percentage were found in any of the human annotations.</p><p>precision@k: Of the words that the system changed, what percentage were found in the top k human annotations, where the annotations were ranked by response frequency. For example, if we were calculating the precision@1 for the example in <ref type="table">Table 2</ref>, only "obtained" would be considered correct.</p><p>accuracy: The percentage of the test examples where the system made a change to one of the annotations suggested by the human annotators. Note that unlike precision, if the system does not suggest a change to a word that was simplified it still gets penalized.</p><p>changed: The percentage of the test examples where the system suggested some change (even if it wasn't a "correct" change). <ref type="table">Table 3</ref> shows the precision, accuracy and percent changed for the three systems. Based on all three metrics, our system achieves the best results. Al- though the rules generated by Biran et al. have rea- sonable precision, they suffer from a lack of cov- erage, only making changes on about 5% of the <ref type="table">word   frequency  word  frequency  word  frequency  obtained  17  made  2  secured  1  gathered  9  created  1  found  1  gotten  8  processed  1  attained  1  grabbed  4  received  1  procured  1  acquired  2  collected  1  aquired  1   Table 2</ref>: Candidate simplifications generated using MTurk for the examples in <ref type="figure" target="#fig_1">Figure 1</ref>  <ref type="table">Table 3</ref>: Precision, accuracy and percent changed for the three systems, averaged over the 10 folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>examples. For our approach, the extracted rules had very good coverage, applying in over 85% of the examples. This difference in coverage can be partially at- tributed to the number of rules learned. We learned simplifications for 14,478 words with an average of 2.25 candidate simplifications per word. In con- trast, the rules from Biran et al. only had simpli- fications for 3,598 words with an average of 1.18 simplifications per word.</p><p>The precision of both of the approaches that utilized the SVM candidate ranking were sig- nificantly better than the frequency-based ap- proach. To better understand the types of sug- gestions made by the systems, <ref type="figure" target="#fig_2">Figure 2</ref> shows the precision@k for increasing k. On average, over the 500 examples we collected, people suggested 12 different simplifications, though this varied de- pending on the word in question and the sentence. As such, at around k=12, the precision@k of most of the systems has almost reached the final preci- sion. However, even at k = 5, which only counts correct an answer in the top 5 human suggested results, our system still achieved a precision of around 67%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>In this paper we have introduced a new rule ex- traction algorithm and a new feature-based rank- ing approach for applying these rules in the con- text of different sentences. The number of rules learned is an order of magnitude larger than any previous lexical simplification approach and the quality of the resulting simplifications after apply- ing these rules is better than previous approaches.</p><p>Many avenues exist for improvement and for better understanding how well the current ap- proach works. First, we have only explored a small set of possible features in the ranking algo- rithm. Additional improvements could be seen by incorporating a broader feature set. Second, more analysis needs to be done to understand the quality of the produced simplifications and their impact on the simplicity of the resulting sentences. Third, the experiments above assume that the word to be sim- plified has already been identified in the sentence. This identification step also needs to be explored to implement a sentence-level simplifier using our approach. Fourth, the ranking algorithm can be applied to most simplification rules (e.g. we ap- plied the ranking approach to the rules obtained by <ref type="bibr" target="#b0">Biran et al. (2011)</ref>). We hope to explore other approaches for increasing the rule set by incorpo- rating other rule sources and other rule extraction techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The researcher established a new paper writing routine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example task setup on MTurk soliciting lexical simplifications from annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision@k for varying k for the three different approaches averaged over the 10 folds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>The frequency 
is the number of annotators that suggested that simplification. 

precision accuracy changed 
frequency 
53.9% 
46.1% 
84.9% 
rank-Biran 
71.4% 
3.4% 
5.2% 
rank-simplify 
76.1% 
66.3% 
86.3% 

</table></figure>

			<note place="foot" n="1"> https://www.mturk.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Putting it simply: A context-aware approach to lexical simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noé</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Web 1T 5-gram version 1. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Franz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Morphadorner v2: A Java library for the morphological adornment of english language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Creating speech and language data with Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic induction of rules for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Based Systems</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple English Wikipedia: A new text simplification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Text simplification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">CUNY Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training linear svms in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effect of word familiarity on actual and perceived text difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gondy</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">User evaluation of the effects of a text simplification algorithm using term familiarity on perception, understanding, learning, and information retention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gondy</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">E</forename><surname>Endicott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obay</forename><surname>Mouradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Internet Research</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>JMIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semeval2007 task 10: English lexical substitution task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SEMEVAL</title>
		<meeting>SEMEVAL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HTLNAACL</title>
		<meeting>HTLNAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 1: English lexical simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Lexical and Computerational Semantics (*SEM)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Statistical Language Processing</title>
		<meeting>the International Conference on Statistical Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescumizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<editor>NAACL/HLT</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crowdsourcing translation: Professional quality from non-professionals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCL</title>
		<meeting>ICCL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
