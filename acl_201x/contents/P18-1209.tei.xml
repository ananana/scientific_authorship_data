<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Low-rank Multimodal Fusion with Modality-Specific Factors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><forename type="middle">Bharadhwaj</forename><surname>Lakshminarasimhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Low-rank Multimodal Fusion with Modality-Specific Factors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2247" to="2256"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is mul-timodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the ex-pressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: mul-timodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal research has shown great progress in a variety of tasks as an emerging research field of arti- ficial intelligence. Tasks such as speech recognition ( <ref type="bibr" target="#b25">Yuhas et al., 1989)</ref>, emotion recognition, <ref type="bibr" target="#b4">(De Silva et al., 1997)</ref>, <ref type="bibr" target="#b1">(Chen et al., 1998)</ref>, <ref type="bibr">(Wöllmer et al., 2013)</ref>, sentiment analysis, <ref type="bibr" target="#b11">(Morency et al., 2011</ref>) * equal contributions as well as speaker trait analysis and media descrip- tion ( <ref type="bibr" target="#b13">Park et al., 2014a</ref>) have seen a great boost in performance with developments in multimodal research.</p><p>However, a core research challenge yet to be solved in this domain is multimodal fusion. The goal of fusion is to combine multiple modalities to leverage the complementarity of heterogeneous data and provide more robust predictions. In this regard, an important challenge has been on scaling up fusion to multiple modalities while maintaining reasonable model complexity. Some of the recent attempts ( <ref type="bibr" target="#b6">Fukui et al., 2016)</ref>,  at multimodal fusion investigate the use of tensors for multimodal representation and show significant improvement in performance. Unfortunately, they are often constrained by the exponential increase of cost in computation and memory introduced by using tensor representations. This heavily restricts the applicability of these models, especially when we have more than two views of modalities in the dataset.</p><p>In this paper, we propose the Low-rank Mul- timodal Fusion, a method leveraging low-rank weight tensors to make multimodal fusion efficient without compromising on performance. The over- all architecture is shown in <ref type="figure">Figure 1</ref>. We evalu- ated our approach with experiments on three mul- timodal tasks using public datasets and compare its performance with state-of-the-art models. We also study how different low-rank settings impact the performance of our model and show that our model performs robustly within a wide range of rank settings. Finally, we perform an analysis of the impact of our method on the number of param- eters and run-time with comparison to other fusion methods. Through theoretical analysis, we show that our model can scale linearly in the number of modalities, and our experiments also show a corre- sponding speedup in training when compared with  <ref type="figure">Figure 1</ref>: Overview of our Low-rank Multimodal Fusion model structure: LMF first obtains the unimodal representation z a , z v , z l by passing the unimodal inputs x a , x v , x l into three sub-embedding networks f v , f a , f l respectively. LMF produces the multimodal output representation by performing low-rank multimodal fusion with modality-specific factors. The multimodal representation can be then used for generating prediction tasks.</p><p>other tensor-based models.</p><p>The main contributions of our paper are as fol- lows:</p><p>• We propose the Low-rank Multimodal Fusion method for multimodal fusion that can scale linearly in the number of modalities.</p><p>• We show that our model compares to state-of- the-art models in performance on three multi- modal tasks evaluated on public datasets.</p><p>• We show that our model is computationally efficient and has fewer parameters in compari- son to previous tensor-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multimodal fusion enables us to leverage comple- mentary information present in multimodal data, thus discovering the dependency of information on multiple modalities. Previous studies have shown that more effective fusion methods translate to bet- ter performance in models, and there's been a wide range of fusion methods. Early fusion is a technique that uses feature concatenation as the method of fusion of differ- ent views. Several works that use this method of fusion ( <ref type="bibr" target="#b17">Poria et al., 2016)</ref> , ( <ref type="bibr" target="#b20">Wang et al., 2016)</ref> use input-level feature concatenation and use the concatenated features as input, sometimes even re- moving the temporal dependency present in the modalities <ref type="bibr" target="#b11">(Morency et al., 2011</ref>). The drawback of this class of method is that although it achieves fusion at an early stage, intra-modal interactions are potentially suppressed, thus losing out on the context and temporal dependencies within each modality.</p><p>On the other hand, late fusion builds sepa- rate models for each modality and then integrates the outputs together using a method such as ma- jority voting or weighted averaging <ref type="bibr" target="#b23">(Wortwein and Scherer, 2017)</ref>, <ref type="bibr" target="#b12">(Nojavanasghari et al., 2016)</ref>. Since separate models are built for each modality, inter-modal interactions are usually not modeled effectively.</p><p>Given these shortcomings, more recent work focuses on intermediate approaches that model both intra-and inter-modal dynamics. <ref type="bibr" target="#b6">Fukui et al. (2016)</ref> proposes to use Compact Bilinear Pooling over the outer product of visual and linguistic repre- sentations to exploit the interactions between vision and language for visual question answering. Sim- ilar to the idea of exploiting interactions,  proposes Tensor Fusion Network, which computes the outer product between uni- modal representations from three different modal- ities to compute a tensor representation. These methods exploit tensor representations to model inter-modality interactions and have shown a great success. However, such methods suffer from expo- nentially increasing computational complexity, as the outer product over multiple modalities results in extremely high dimensional tensor representations.</p><p>For unimodal data, the method of low-rank ten- sor approximation has been used in a variety of applications to implement more efficient tensor op- erations. <ref type="bibr" target="#b19">Razenshteyn et al. (2016)</ref> proposes a mod- ified weighted version of low-rank approximation, and <ref type="bibr" target="#b9">Koch and Lubich (2010)</ref> applies the method towards temporally dependent data to obtain low- rank approximations. As for applications, <ref type="bibr" target="#b10">Lei et al. (2014)</ref> proposes a low-rank tensor technique for dependency parsing while <ref type="bibr" target="#b21">Wang and Ahuja (2008)</ref> uses the method of low-rank approximation applied directly on multidimensional image data (Datum- as-is representation) to enhance computer vision applications. <ref type="bibr" target="#b8">Hu et al. (2017)</ref> proposes a low-rank tensor-based fusion framework to improve the face recognition performance using the fusion of facial attribute information. However, none of these previ- ous work aims to apply low-rank tensor techniques for multimodal fusion.</p><p>Our Low-rank Multimodal Fusion method pro- vides a much more efficient method to com- pute tensor-based multimodal representations with much fewer parameters and computational com- plexity. The efficiency and performance of our ap- proach are evaluated on different downstream tasks, namely sentiment analysis, speaker-trait recogni- tion and emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Low-rank Multimodal Fusion</head><p>In this section, we start by formulating the problem of multimodal fusion and introducing fusion meth- ods based on tensor representations. Tensors are powerful in their expressiveness but do not scale well to a large number of modalities. Our proposed model decomposes the weights into low-rank fac- tors, which reduces the number of parameters in the model. This decomposition can be performed efficiently by exploiting the parallel decomposition of low-rank weight tensor and input tensor to com- pute tensor-based fusion. Our method is able to scale linearly with the number of modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multimodal Fusion using Tensor Representations</head><p>In this paper, we formulate multimodal fusion as</p><formula xml:id="formula_0">a multilinear function f V 1 × V 2 × ... × V M → H where V 1 , V 2 , .</formula><p>.., V M are the vector spaces of input modalities and H is the output vector space. Given a set of vector representations, {z m } M m=1</p><p>which are encoding unimodal information of the M different modalities, the goal of multimodal fusion is to integrate the unimodal representations into one compact multimodal representation for downstream tasks.</p><p>Tensor representation is one successful approach for multimodal fusion. It first requires a transfor- mation of the input representations into a high- dimensional tensor and then mapping it back to a lower-dimensional output vector space. Previous works have shown that this method is more effec- tive than simple concatenation or pooling in terms of capturing multimodal interactions ( , <ref type="bibr" target="#b6">(Fukui et al., 2016)</ref>. Tensors are usually created by taking the outer product over the input modalities. In addition, in order to be able to model the interactions between any subset of modalities using one tensor,  proposed a simple extension to append 1s to the unimodal rep- resentations before taking the outer product. The input tensor Z formed by the unimodal representa- tion is computed by:</p><formula xml:id="formula_1">Z = M m=1 z m , z m ∈ R dm (1)</formula><p>where M m=1 denotes the tensor outer product over a set of vectors indexed by m, and z m is the input representation with appended 1s.</p><p>The input tensor Z ∈ R d 1 ×d 2 ×...d M is then passed through a linear layer g(⋅) to to produce a vector representation:</p><formula xml:id="formula_2">h = g(Z; W, b) = W ⋅ Z + b, h, b ∈ R dy (2)</formula><p>where W is the weight of this layer and b is the bias. With Z being an order-M tensor (where M is the number of input modalities), the weight W will naturally be a tensor of order-(M + 1) in</p><formula xml:id="formula_3">R d 1 ×d 2 ×...×d M ×d h .</formula><p>The extra (M + 1)-th dimension corresponds to the size of the output representation d h . In the tensor dot product W ⋅ Z, the weight ten- sor W can be then viewed as d h order-M tensors. In other words, the weight W can be partitioned into</p><formula xml:id="formula_4">W k ∈ R d 1 ×...×d M , k = 1, ..., d h . Each W k con-</formula><p>tributes to one dimension in the output vector h, i.e.</p><formula xml:id="formula_5">h k = W k ⋅ Z.</formula><p>This interpretation of tensor fusion is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> for the bi-modal case.</p><p>One of the main drawbacks of tensor fusion is that we have to explicitly create the high- dimensional tensor Z. The dimensionality of Z The number of parameters to learn in the weight tensor W will also increase exponentially. This not only introduces a lot of computation but also exposes the model to risks of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low-rank Multimodal Fusion with Modality-Specific Factors</head><p>As a solution to the problems of tensor-based fu- sion, we propose Low-rank Multimodal Fusion (LMF). LMF parameterizes g(⋅) from Equation 2 with a set of modality-specific low-rank factors that can be used to recover a low-rank weight ten- sor, in contrast to the full tensor W. Moreover, we show that by decomposing the weight into a set of low-rank factors, we can exploit the fact that the tensor Z actually decomposes into {z m } M m=1 , which allows us to directly compute the output h without explicitly tensorizing the unimodal repre- sentations. LMF reduces the number of parameters as well as the computation complexity involved in tensorization from being exponential in M to linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Low-rank Weight Decomposition</head><p>The idea of LMF is to decompose the weight tensor W into M sets of modality-specific factors. How- ever, since W itself is an order-(M + 1) tensor, commonly used methods for decomposition will result in M + 1 parts. Hence, we still adopt the view introduced in Section 3.1 that W is formed by</p><formula xml:id="formula_6">d h order-M tensors W k ∈ R d 1 ×...×d M , k = 1, ..., d h stacked together. We can then decompose each W k separately.</formula><p>For an order-M tensor</p><formula xml:id="formula_7">W k ∈ R d 1 ×...×d M , there</formula><p>always exists an exact decomposition into vectors in the form of:</p><formula xml:id="formula_8">W k = R i=1 M m=1 w (i) m,k , w (i) m,k ∈ R d m<label>(3)</label></formula><p>The minimal R that makes the decomposition valid is called the rank of the tensor. The vector sets</p><formula xml:id="formula_9">{{w (i) m,k } M m=1 } R i=1</formula><p>are called the rank R decomposi- tion factors of the original tensor.</p><p>In LMF, we start with a fixed rank r, and pa- rameterize the model with r decomposition factors {{w</p><formula xml:id="formula_10">(i) m,k } M m=1 } r i=1</formula><p>, k = 1, ..., d h that can be used to reconstruct a low-rank version of these W k .</p><p>We can regroup and concatenate these vectors into M modality-specific low-rank factors. Let w</p><formula xml:id="formula_11">(i) m = [w (i) m,1 , w (i) m,2 , ..., w (i) m,d h ], then for modality m, {w (i) m } r i=1</formula><p>is its corresponding low-rank factors. And we can recover a low-rank weight tensor by:</p><formula xml:id="formula_12">W = r i=1 M m=1 w (i) m (4)</formula><p>Hence equation 2 can be computed by</p><formula xml:id="formula_13">h = r i=1 M m=1 w (i) m ⋅ Z<label>(5)</label></formula><p>Note that for all m, w</p><p>m ∈ R dm×d h shares the same size for the second dimension. We define their outer product to be over only the dimensions that are not shared:</p><formula xml:id="formula_15">w (i) m ⊗ w (i)</formula><p>n ∈ R dm×dn×d h . A bimodal example of this procedure is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Nevertheless, by introducing the low-rank fac- tors, we now have to compute the reconstruction of</p><formula xml:id="formula_16">W = ∑ r i=1 M m=1 w (i)</formula><p>m for the forward computa- tion. Yet this introduces even more computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Efficient Low-rank Fusion Exploiting</head><p>Parallel Decomposition In this section, we will introduce an efficient pro- cedure for computing h, exploiting the fact that tensor Z naturally decomposes into the original input {z m } M m=1 , which is parallel to the modality- specific low-rank factors. In fact, that is the main reason why we want to decompose the weight ten- sor into M modality-specific factors.</p><p>Using the fact that Z = M m=1 z m , we can sim- plify equation 5: where Λ M m=1 denotes the element-wise product over a sequence of tensors: Λ 3</p><formula xml:id="formula_17">h = r i=1 M m=1 w (i) m ⋅ Z = r i=1 M m=1 w (i) m ⋅ Z = r i=1 M m=1 w (i) m ⋅ M m=1 z m = M Λ m=1 r i=1 w (i) m ⋅ z m (6) 5 &gt; (S) ⨂ + + ⋯ 5 # (S) 5 &gt; (T) ⨂ 5 # (T) V ⨂ E E ! &gt; ! # g E = ℎ</formula><formula xml:id="formula_18">t=1 x t = x 1 ○ x 2 ○ x 3 .</formula><p>An illustration of the trimodal case of equation 6 is shown in <ref type="figure">Figure 1</ref>. We can also derive equation 6 for a bimodal case to clarify what it does:</p><formula xml:id="formula_19">h = r i=1 w (i) a ⊗ w (i) v ⋅ Z = r i=1 w (i) a ⋅ z a ○ r i=1 w (i) v ⋅ z v<label>(7)</label></formula><p>An important aspect of this simplification is that it exploits the parallel decomposition of both Z and W, so that we can compute h without actually creating the tensor Z from the input representa- tions z m . In addition, different modalities are de- coupled in the simplified computation of h, which allows for easy generalization of our approach to an arbitrary number of modalities. Adding a new modality can be simply done by adding another set of modality-specific factors and extend Equa- tion 7. Last but not least, Equation 6 consists of fully differentiable operations, which enables the parameters {w (i) m } r i=1 m = 1, ..., M to be learned end-to-end via back-propagation.</p><p>Using Equation 6, we can compute h directly from input unimodal representations and their modal-specific decomposition factors, avoiding the weight-lifting of computing the large input ten- sor Z and W, as well as the r linear transfor- mation. Instead, the input tensor and subsequent linear projection are computed implicitly together in Equation 6, and this is far more efficient than the original method described in Section 3.1. In- deed, LMF reduces the computation complexity of tensorization and fusion from</p><formula xml:id="formula_20">O(d y ∏ M m=1 d m ) to O(d y × r × ∑ M m=1 d m )</formula><p>. In practice, we use a slightly different form of Equation 6, where we concatenate the low-rank factors into M order-3 tensors and swap the or- der in which we do the element-wise product and summation:</p><formula xml:id="formula_21">h = r i=1 M Λ m=1 w (1) m , w (2) m , ..., w (r) m ⋅ ˆ z m i,<label>(8)</label></formula><p>and now the summation is done along the first di- mension of the bracketed matrix.</p><p>[⋅] i, indicates the i-th slice of a matrix. In this way, we can parame- terize the model with M order-3 tensors, instead of parameterizing with sets of vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Methodology</head><p>We compare LMF with previous state-of-the-art baselines, and we use the Tensor Fusion Networks (TFN) (Zadeh et al., 2017) as a baseline for tensor- based approaches, which has the most similar struc- ture with us except that it explicitly forms the large multi-dimensional tensor for fusion across different modalities. We design our experiments to better understand the characteristics of LMF. Our goal is to answer the following four research questions: (1) Impact of Multimodal Low-rank Fusion: Di- rect comparison between our proposed LMF model and the previous TFN model. The results of these experiments are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We perform our experiments on the following multi- modal datasets, CMU-MOSI ( <ref type="bibr" target="#b29">Zadeh et al., 2016a</ref> To evaluate model generalization, all datasets are split into training, validation, and test sets such that the splits are speaker independent, i.e., no identical speakers from the training set are present in the test sets. <ref type="table">Table 1</ref> illustrates the data splits for all datasets in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Features</head><p>Each dataset consists of three modalities, namely language, visual, and acoustic modalities. To reach the same time alignment across modalities, we perform word alignment using P2FA <ref type="bibr" target="#b24">(Yuan and Liberman, 2008)</ref> which allows us to align the three modalities at the word granularity. We calculate the visual and acoustic features by taking the average of their feature values over the word time interval . Language We use pre-trained 300-dimensional Glove word embeddings ( <ref type="bibr" target="#b15">Pennington et al., 2014)</ref> to encode a sequence of transcribed words into a sequence of word vectors.</p><p>Visual The library Facet 1 is used to extract a set of visual features for each frame (sampled at 30Hz) in- cluding 20 facial action units, 68 facial landmarks, head pose, gaze tracking and HOG features ( <ref type="bibr" target="#b31">Zhu et al., 2006</ref>). Acoustic We use COVAREP acoustic analysis framework <ref type="bibr" target="#b5">(Degottex et al., 2014</ref>) to extract a set of low-level acoustic features, including 12 Mel frequency cepstral coefficients (MFCCs), pitch, voiced/unvoiced segmentation, glottal source, peak slope, and maxima dispersion quotient features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Architecture</head><p>In order to compare our fusion method with previ- ous work, we adopt a simple and straightforward model architecture 2 for extracting unimodal rep- resentations. Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as f a , f v , f l , to extract unimodal representations z a , z v , z l from uni- modal input features x a , x v , x l . For acoustic and visual modality, the sub-embedding network is a simple 2-layer feed-forward neural network, and for language modality, we used an LSTM (Hochre- iter and Schmidhuber, 1997) to extract represen- tations. The model architecture is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baseline Models</head><p>We compare the performance of LMF to the follow- ing baselines and state-of-the-art models in multi- modal sentiment analysis, speaker trait recognition, and emotion recognition. Support Vector Machines Support Vector Ma- chines (SVM) <ref type="bibr" target="#b3">(Cortes and Vapnik, 1995</ref>) is a widely used non-neural classifier. This baseline is trained on the concatenated multimodal features for classification or regression task (Pérez-Rosas  ) explicitly models view-specific and cross-view dynamics by creat- ing a multi-dimensional tensor that captures uni-modal, bimodal and trimodal interactions across three modalities. Memory Fusion Network The Memory Fusion Network (MFN) ( <ref type="bibr">Zadeh et al., 2018a</ref>) accounts for view-specific and cross-view interactions and con- tinuously models them through time with a special attention mechanism and summarized through time with a Multi-view Gated Memory. Bidirectional Contextual LSTM The Bidirec- tional Contextual LSTM (BC-LSTM) ( , <ref type="bibr" target="#b6">(Fukui et al., 2016</ref>) performs context- dependent fusion of multimodal data. Multi-View LSTM The Multi-View LSTM (MV- LSTM) <ref type="bibr" target="#b18">(Rajagopalan et al., 2016</ref>) aims to capture both modality-specific and cross-modality interac- tions from multiple modalities by partitioning the memory cell and the gates corresponding to multi- ple modalities. Multi-attention Recurrent Network The Multi- attention Recurrent Network (MARN) ( <ref type="bibr" target="#b28">Zadeh et al., 2018b</ref>) explicitly models interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory called the Long-short Term Hybrid Memory (LSTHM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Metrics</head><p>Multiple evaluation tasks are performed during our evaluation: multi-class classification and regres- sion. The multi-class classification task is applied to all three multimodal datasets, and the regres- sion task is applied to the CMU-MOSI and the POM dataset. For binary classification and multi- class classification, we report F1 score and accu- racy Acc−k where k denotes the number of classes. Specifically, Acc−2 stands for the binary classifica- tion. For regression, we report Mean Absolute Er- ror (MAE) and Pearson correlation <ref type="bibr">(Corr)</ref>. Higher values denote better performance for all metrics except for MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>In this section, we present and discuss the results from the experiments designed to study the re- search questions introduced in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Impact of Low-rank Multimodal Fusion</head><p>In this experiment, we compare our model directly with the TFN model since it has the most similar structure to our model, except that TFN explic- itly forms the multimodal tensor fusion. The com- parison reported in the last two rows of <ref type="table" target="#tab_3">Table 2</ref> demonstrates that our model significantly outper- forms TFN across all datasets and metrics. This competitive performance of LMF compared to TFN emphasizes the advantage of Low-rank Multimodal Fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with the State-of-the-art</head><p>We compare our model with the baselines and state- of-the-art models for sentiment analysis, speaker traits recognition and emotion recognition. Results are shown in <ref type="table" target="#tab_3">Table 2</ref>. LMF is able to achieve com- petitive and consistent results across all datasets.</p><p>On the multimodal sentiment regression task, LMF outperforms the previous state-of-the-art model on MAE and Corr. Note the multiclass accuracy is calculated by mapping the range of continuous sentiment values into a set of intervals that are used as discrete classes.</p><p>On the multimodal speaker traits Recognition task, we report the average evaluation score over 16 speaker traits and shows that our model achieves the state-of-the-art performance over all three eval- uation metrics on the POM dataset.</p><p>On the multimodal emotion recognition task, our model achieves better results compared to the state- of-the-art models across all emotions on the F1 score. F1-emotion in the evaluation metrics indi- cates the F1 score for a certain emotion class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Complexity Analysis</head><p>Theoretically, the model complexity of our fu- sion method is</p><formula xml:id="formula_22">O(d y × r × ∑ M m=1 d m ) compared to O(d y ∏ M m=1 d m )</formula><p>of TFN from Section 3.1. In practice, we calculate the total number of parame- ters used in each model, where we choose M = 3,</p><formula xml:id="formula_23">d 1 = 32, d 2 = 32, d 3 = 64, r = 4, d y = 1.</formula><p>Under this hyper-parameter setting, our model contains about 1.1e6 parameters while TFN contains about 12.5e6 parameters, which is nearly 11 times more. Note that, the number of parameters above counts not only the parameters in the multimodal fusion stage but also the parameters in the subnetworks.</p><p>Furthermore, we evaluate the computational complexity of LMF by measuring the training and testing speeds between LMF and TFN. <ref type="table" target="#tab_4">Table 3</ref> illustrates the impact of Low-rank Multimodal Fu- sion on the training and testing speeds compared with TFN model. Here we set rank to be 4 since it can generally achieve fairly competent perfor- mance.    Based on these results, performing a low-rank multimodal fusion with modality-specific low-rank factors significantly reduces the amount of time needed for training and testing the model. On an NVIDIA Quadro K4200 GPU, LMF trains with an average frequency of 1134.82 IPS (data point inferences per second) while the TFN model trains at an average of 340.74 IPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Rank Settings</head><p>To evaluate the impact of different rank settings for our LMF model, we measure the change in perfor- mance on the CMU-MOSI dataset while varying <ref type="figure">Figure 4</ref>: The Impact of different rank settings on Model Performance: As the rank increases, the results become unstable and low rank is enough in terms of the mean absolute error. the number of rank. The results are presented in <ref type="figure">Figure 4</ref>. We observed that as the rank increases, the training results become more and more unstable and that using a very low rank is enough to achieve fairly competent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduce a Low-rank Multi- modal Fusion method that performs multimodal fu- sion with modality-specific low-rank factors. LMF scales linearly in the number of modalities. LMF achieves competitive results across different mul- timodal tasks. Furthermore, LMF demonstrates a significant decrease in computational complex- ity from exponential to linear time. In practice, LMF effectively improves the training and testing efficiency compared to TFN which performs multi- modal fusion with tensor representations.</p><p>Future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tensor fusion via tensor outer product will increase exponentially with the number of modalities as ∏ M m=1 d m. The number of parameters to learn in the weight tensor W will also increase exponentially. This not only introduces a lot of computation but also exposes the model to risks of overfitting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Decomposing weight tensor into low-rank factors (See Section 3.2.1 for details.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )</head><label>2</label><figDesc>Comparison with the State-of-the-art: We evaluate the performance of LMF and state-of-the- art baselines on three different tasks and datasets. (3) Complexity Analysis: We study the modal complexity of LMF and compare it with the TFN model. (4) Rank Settings: We explore performance of LMF with different rank settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>et al., 2013), (Park et al., 2014a), (Zadeh et al., 2016b). Deep Fusion The Deep Fusion model (DF) (No- javanasghari et al., 2016) trains one deep neural model for each modality and then combine the out- put of each modality network with a joint neural network. Tensor Fusion Network The Tensor Fusion Net- work (TFN) (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for sentiment analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality 
trait recognition on POM. Best results are highlighted in bold. 

Model 
Training Speed (IPS) 
Testing Speed (IPS) 
TFN 
340.74 
1177.17 
LMF 
1134.82 
2249.90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of the training and testing 
speeds between TFN and LMF. The second and 
the third columns indicate the number of data point 
inferences per second (IPS) during training and 
testing time respectively. Both models are imple-
mented in the same framework with equivalent run-
ning environment. 

</table></figure>

			<note place="foot" n="1"> goo.gl/1rh1JN 2 The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based upon work partially sup-ported by the National Science Foundation (Award # 1833355) and Oculus VR. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or Oculus VR, and no official endorse-ment should be inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jean-nette Chang, Sungbok Lee, and Shrikanth S. Narayanan. 2008. Iemocap: Interactive emotional dyadic motion capture database. Journal of Lan-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno type="doi">10.1007/s10579-008-9076-6</idno>
		<ptr target="https://doi.org/10.1007/s10579-008-9076-6" />
	</analytic>
	<monogr>
		<title level="j">guage Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal human emotion/expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Lawrence S Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third IEEE International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="doi">10.1145/3136755.3136801</idno>
		<ptr target="https://doi.org/10.1145/3136755.3136801" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supportvector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facial emotion recognition using multi-modal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liyanage C De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information, Communications and Signal Processing</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Covarepa collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
	<note>Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attribute-enhanced face recognition with neural tensor fusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Neil M Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamical tensor approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Othmar</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Lubich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2360" to="2375" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Multimodal Interactions</title>
		<meeting>the 13th International Conference on Multimodal Interactions</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for persuasiveness prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnaz</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayanth</forename><surname>Koushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="284" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><forename type="middle">Suk</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moitreya</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
		<meeting>the 16th International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><forename type="middle">Suk</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moitreya</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="doi">10.1145/2663204.2663260</idno>
		<ptr target="https://doi.org/10.1145/2663204.2663260" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
		<meeting>the 16th International Conference on Multimodal Interaction<address><addrLine>New York, NY, USA, ICMI &apos;14</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Utterance-level multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="973" to="982" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional mkl based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extending long short-term memory for multi-view structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Shyam Sundar Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weighted low rank approximations with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Ilya Razenshteyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-eighth annual ACM symposium on Theory of Computing</title>
		<meeting>the forty-eighth annual ACM symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaksha</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tensor approximation approach to dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="229" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What really mattersan information gain analysis of questions and reactions in automated ptsd screenings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Wortwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speaker identification on the scotus corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3878</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integration of acoustic and visual speech signals using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moise</forename><forename type="middle">H</forename><surname>Ben P Yuhas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="65" to="71" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00927</idno>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00923</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Prateek Vij, Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Chen</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society Conference on. IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
