<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Comprehension using Rich Semantic Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Comprehension using Rich Semantic Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="486" to="492"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine comprehension tests the sys-tem&apos;s ability to understand a piece of text through a reading comprehension task. For this task, we propose an approach using the Abstract Meaning Representation (AMR) formalism. We construct meaning representation graphs for the given text and for each question-answer pair by merging the AMRs of comprising sentences using cross-sentential phenomena such as coreference and rhetorical structures. Then, we reduce machine comprehension to a graph containment problem. We posit that there is a latent mapping of the question-answer meaning representation graph onto the text meaning representation graph that explains the answer. We present a unified max-margin framework that learns to find this mapping (given a corpus of texts and question-answer pairs), and uses what it learns to answer questions on novel texts. We show that this approach leads to state of the art results on the task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning to efficiently represent and reason with natural language is a fundamental yet long- standing goal in NLP. This has led to a series of efforts in broad-coverage semantic representation (or "sembanking"). Recently, AMR, a new seman- tic representation in standard neo-Davidsonian <ref type="bibr" target="#b5">(Davidson, 1969;</ref><ref type="bibr" target="#b17">Parsons, 1990</ref>) framework has been proposed. AMRs are rooted, labeled graphs which incorporate PropBank style semantic roles, within-sentence coreference, named entities and the notion of types, modality, negation, quantifi- cation, etc. in one framework.</p><p>In this paper, we describe an approach to use</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elaboration</head><p>Text:</p><p>Snippet Graph:</p><p>Alignments:</p><p>Hypothesis Graph: The question and answer candidate are combined to generate a hy- pothesis. This hypothesis is AMR parsed to construct a hypothesis meaning representation graph after some post-processing ( § 2.1). Similar processing is done for each sentence in the passage as well. Then, a subset (not neces- sarily contiguous) of these sentence meaning representation graphs is found. These representation subgraphs are further merged using coreference informa- tion, resulting into a structure called the relevant text snippet graph. Finally, the hypothesis meaning representation graph is aligned to the snippet graph. The dashed red lines show node alignments, solid red lines show edge alignments, and thick solid black arrow shows the rhetorical structure label (elaboration).</p><p>AMR for the task of machine comprehension. Ma- chine comprehension ( <ref type="bibr" target="#b18">Richardson et al., 2013</ref>) evaluates a machine's understanding by posing a series of multiple choice reading comprehension tests. The tests are unique as the answer to each question can be found only in its associated texts, requiring us to go beyond simple lexical solutions. Our approach models machine comprehension as an extension to textual entailment, learning to out- put an answer that is best entailed by the pas- sage. It works in two stages. First, we construct a meaning representation graph for the entire pas- sage ( § 2.1) from the AMR graphs of compris- ing sentences. To do this, we account for cross- sentence linguistic phenomena such as entity and  <ref type="table">person  name  Sammy  op1  name  do m ai n   dog  person  name  Katy  op1  name  poss  pr ep -o f  arg1</ref> Figure 2: The AMR parse for the hypothesis in <ref type="figure">Figure 1</ref>. The person nodes are merged to achieve the hypothesis meaning representation graph. event coreference, and rhetorical structures. A similar meaning representation graph is also con- structed for each question-answer pair. Once we have these graphs, the comprehension task hence- forth can be reduced to a graph containment prob- lem. We posit that there is a latent subgraph of the text meaning representation graph (called snip- pet graph) and a latent alignment of the question- answer graph onto this snippet graph that entails the answer (see <ref type="figure">Figure 1</ref> for an example). Then, we propose a unified max-margin approach ( § 2.2) that jointly learns the latent structure (subgraph selection and alignment) and the QA model. We evaluate our approach on the MCTest dataset and achieve competitive or better results than a number of previous proposals for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Meaning Representation Graphs</head><p>We construct the meaning representation graph us- ing individual sentences AMR graphs and merging identical concepts (using entity and event corefer- ence). First, for each sentence AMR, we merge nodes corresponding to multi-word expressions and nodes headed by a date entity ("date-entity"), or a named entity ("name") or a person entity ("person"). For example, the hypothesis meaning representation graph in <ref type="figure">Figure 1</ref> was achieved by merging the AMR parse shown in <ref type="figure">Figure 2</ref>.</p><p>Next, we select the subset of sentence AMRs corresponding to sentences needed to answer the question. This step uses cross-sentential phe- nomena such as rhetorical structures 1 and en- tities/event coreference. The coreferent enti- ties/event mentions are further merged into one node resulting in a graph called the relevant text snippet graph. A similar process is also per- 1 Rhetorical structure theory ( <ref type="bibr" target="#b13">Mann and Thompson, 1988)</ref> tells us that sentences with discourse relations are related to each other. Previous works in QA ( <ref type="bibr" target="#b11">Jansen et al., 2014</ref>) have shown that these relations can help us answer certain kinds of questions. As an example, the "cause" relation between sen- tences in the text can often give cues that can help us answer "why" or "how" questions. Hence, the passage meaning rep- resentation also remembers RST relations between sentences. formed with the hypothesis sentences (generated by combining the question and answer candidate) as shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Max-Margin Solution</head><p>For each question q i ∈ Q, let t i be the corre- sponding passage text and A i = {a i1 , . . . , a im } be the set of candidate answers to the question. Our solution casts the machine comprehension task as a textual entailment task by converting each question-answer candidate pair (q i , a ij ) into a hypothesis statement h ij . We use the question matching/rewriting rules described in <ref type="bibr" target="#b4">Cucerzan and Agichtein (2005)</ref> to get the hypothesis state- ments. For each question q i , the machine com- prehension task reduces to picking the hypothe- sisˆhsisˆ sisˆh i that has the highest likelihood of being en- tailed by the text t i among the set of hypotheses h i = {h i1 , . . . , h im } generated for the question q i . Let h * i ∈ h i be the hypothesis corresponding to the correct answer.</p><p>As described, we use subgraph matching to help us model the inference. We assume that the se- lection of sentences to generate the relevant text snippet graph and the mapping of the hypothe- sis meaning representation graph onto the passage meaning representation graph is latent and infer it jointly along with the answer. We treat it as a structured prediction problem of ranking the hy- pothesis set h i such that the correct hypothesis h * i is at the top of this ranking. We learn a scoring function S w (t, h, z) with parameter w such that the score of the correct hypothesis h * i and corre- sponding best latent structure z * i is higher than the score of the other hypotheses and corresponding best latent structures. In a max-margin fashion, we want that</p><formula xml:id="formula_0">S w (t i , h * i , z * i ) &gt; S(t i , h ij , z ij ) + 1 − ξ i for all h j ∈ h \ h * for some slack ξ i .</formula><p>Writing the relaxed max margin formulation:</p><formula xml:id="formula_1">min ||w|| 1 2 ||w|| 2 2 + C i max z ij ,h ij ∈h i \h * i Sw(ti, hij, zij) + ∆(h * i , hij) −C i Sw(ti, h * i , z * i ) (1) We use 0-1 cost, i.e. ∆(h * i , h ij ) = 1(h * i = h ij ).</formula><p>If the scoring function is convex then this objective is in concave-convex form and hence can be solved by the concave-convex program- ming procedure (CCCP) <ref type="bibr" target="#b25">(Yuille and Rangarajan, 2003)</ref>. We assume the scoring function to be linear:S w (t, h, z) = w T ψ(t, h, z). Here, ψ(t, h, z) is a feature map discussed later. The CCCP algorithm essentially alternates between solving for z * i , z ij ∀j s.t. h ij ∈ h i \ h * i and w to achieve a local minima. In the absence of in- formation regarding the latent structure z we pick the structure that gives the best score for a given hypothesis i.e. arg max z S w (t, h, z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scoring Function and Inference</head><p>Now, we define the scoring function S w (t, h, z). Let the hypothesis meaning representation graph be G = (V , E ). Our latent structure z decom- poses into the selection (z s ) of relevant sentences that lead to the text snippet graph G, and the map- ping (z m ) of every node and edge in G onto G. We define the score such that it factorizes over the nodes and edges in G . The weight vector w also has three components w s , w v and w e corre- sponding to the relevant sentences selection, node matches and edge matches respectively. An edge in the graph is represented as a triple (v 1 , r, v 2 ) consisting of the enpoint vertices and relation r.</p><formula xml:id="formula_2">Sw(t, h, z) = w T s f (G , G, t, h, zs) + v ∈V w T v f (v , zm(v )) + e ∈E w T e f (e , zm(e ))</formula><p>Here, t is the text corresponding to the hypoth- esis h, and f are parts of the feature map ψ to be described later. z(v ) maps a node v ∈ V to a node in V . Similarly, z(e ) maps an edge e ∈ E to an edge in E.</p><p>Next, we describe the inference procedure i.e. how to select the structure that gives the best score for a given hypothesis. The inference is per- formed in two steps: The first step selects the relevant sentences from the text. This is done by simply maximizing the first part of the score:</p><formula xml:id="formula_3">z s = arg max zs w T s f (G , G, t, h, z s ).</formula><p>Here, we only consider subsets of 1, 2 and 3 sentences as most questions can be answered by 3 sentences in the passage. The second step is formulated as an integer linear program by rewriting the scoring function. The ILP objective is:</p><formula xml:id="formula_4">v ∈V v∈V z v ,v w T v f (v , v) + e ∈E e∈E z e ,e w T e f (e , e)</formula><p>Here, with some abuse of notation, z v ,v and z e ,e are binary integers such that z v ,v = 1 iff z maps v onto v else z v ,v = 0. Similarly, z e ,e = 1 iff z maps e onto e else z e ,e = 0. Additionally, we have the following constrains to our ILP:</p><p>• Each node v ∈ V (or each edge e ∈ E ) is mapped to exactly one node v ∈ V (or one edge e ∈ E). Hence: v∈V z v ,v = 1 ∀v and e∈E z e ,e = 1 ∀e</p><p>• If an edge e ∈ E is mapped to an edge e ∈ E, then vertices (v 1</p><note type="other">e , v 2 e ) that form the end points of e must also be aligned to ver- tices (v 1 e , v 2 e ) that form the end points of e. Here, we note that AMR parses also have in- verse relations such as "arg0-of". Hence, we resolve this with a slight modification. If nei- ther or both relations (corresponding to edges e and e) are inverse relations (case 1), we en- force that v 1 e align with v 1 e and v 2 e align with v 2 e . If exactly one of the relations is an in- verse relation (case 2), we enforce that v 1 e align with v 2 e and v 2 e align with v 1 e . Hence, we introduce the following constraints: z e e ≤ z v 1 e v 1 e and z e e ≤ z v 2 e v 2 e ∀e .e in case 1 z e e ≤ z v 1 e v 2 e and z e e ≤ z v 2 e v 1 e ∀e .e in case 2</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Features</head><p>Our feature function ψ(t, h, z) decomposes into three parts, each corresponding to a part of the la- tent structure.</p><p>The first part corresponds to relevant sentence selection. Here, we include features for match- ing local neighborhoods in the sentence subset and the hypothesis: features for matching bigrams, tri- grams, dependencies, semantic roles, predicate- argument structure as well as the global syntac- tic structure: a graph kernel for matching AMR graphs of entire sentences <ref type="bibr" target="#b21">(Srivastava and Hovy, 2013)</ref>. Before computing the graph kernel, we re- verse all inverse relation edges in the AMR graph. Note that if a sentence subset contains the answer to the question, it should intuitively be similar to the question as well as to the answer. Hence, we add features that are the element-wise prod- uct of features for the subset-question match and subset-answer match. In addition to features for the exact word/phrase match of the snippet and the hypothesis, we also add features using two para- phrase databases: ParaPara (Chan et al., 2011) and DIRT ( <ref type="bibr" target="#b12">Lin and Pantel, 2001</ref>). These databases contain paraphrase rules of the form string 1 → string 2 . ParaPara rules were extracted through bilingual pivoting and DIRT rules were extracted using the distributional hypothesis. Whenever we have a substring in the text snippet that can be transformed into another using any of these two databases, we keep match features for the sub- string with a higher score (according to the cur- rent w) and ignore the other substring. Finally, we also have features corresponding to the RST ( <ref type="bibr" target="#b13">Mann and Thompson, 1988</ref>) links to enable infer- ence across sentences. RST tells us that sentences with discourse relations are related to each other and can help us answer certain kinds of questions ( <ref type="bibr" target="#b11">Jansen et al., 2014</ref>). For example, the "cause" relation between sentences in the text can often give cues that can help us answer "why" or "how" questions. Hence, we have additional features - conjunction of the rhetorical structure label from a RST parser and the question word as well.</p><p>The second part corresponds to node matches. Here, we have features for (a) Surface-form match (Edit-distance), and (b) Semantic word match (cosine similarity using SENNA word vectors <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>) and "Antonymy" 'Class- Inclusion' or 'Is-A' relations using Wordnet).</p><p>The third part corresponds to edge matches. Let the edges be e = (v 1 , r, v 2 ) and e = (v 1 , r , v 2 ) for notational convenience. Here, we introduce two features based on the relations -indicator that the two relations are the same or inverse of each other, indicator that the two relations are in the same relation category -categories as described in <ref type="bibr" target="#b0">Banarescu et al. (2013)</ref>. Then, we introduce a number of features based on distributional rep- resentation of the node pairs. We compute three vertex vector compositions (sum, difference and product) of the nodes for each edge proposed in recent representation learning literature in NLP ( <ref type="bibr" target="#b15">Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013)</ref> i.e. v 1 v 2 and v 1 v 2 for = {+, −, ×}. Then, we compute the cosine similarities of the resulting compositions producing three features. Finally we introduce features based on the struc- tured distributional semantic representation <ref type="bibr" target="#b6">(Erk and Padó, 2008;</ref><ref type="bibr" target="#b1">Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b9">Goyal et al., 2013</ref>) which takes the relations into account while performing the composition. Here, we use a large text corpora (in our experiments, the English Wikipedia) and construct a representation matrix M (r) ⊂ V × V for every relation r (V is the vocabulary) where, the ij th element M (r) ij has the value log(1+x) where x is the frequency for the i th and j th vocabulary items being in relation r in the corpora. This allows us to compose the node and relation representations and compare them. Here we compute the cosine similarity of the compo- sitions (v 1 ) T M (r) and (v 1 ) T M (r ) , the compo- sitions M (r) v 2 and M (r ) v 2 and their repective sums (v 1 ) T M (r) + M (r) v 2 and (v 1 ) T M (r ) + M (r ) v 2 to get three more features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Negation and Multi-task Learning</head><p>Next, we borrow two ideas from <ref type="bibr" target="#b19">Sachan et al. (2015)</ref> namely, negation and multi-task learning, treating different question types in the machine comprehension setup as different tasks.</p><p>Handling negation is important for our model as facts align well with their negated versions. We use a simple heuristic. During training, if we detect negation (using a set of simple rules that test for presence of negation words ("not", "n't", etc.)), we flip the corresponding constraint, now requiring that the correct hypothesis to be ranked below all the incorrect ones. During test phase if we detect negation, we predict the answer corre- sponding to the hypothesis with the lowest score.</p><p>QA systems often include a question classifica- tion component that divides the questions into se- mantic categories based on the type of the ques- tion or answers expected. This allows the model to learn question type specific parameters when needed. We experiment with three task classifi- cations proposed by <ref type="bibr" target="#b19">Sachan et al. (2015)</ref>. First is QClassification, which classifies the question, based on the question word (what, why, what, etc.). Next is the QAClassification scheme, which classifies questions into different semantic classes based on the possible semantic types of the an- swers sought. The third scheme, TaskClassifica- tion classifies the questions into one of 20 subtasks for Machine Comprehension proposed in <ref type="bibr" target="#b23">Weston et al. (2015)</ref>. We point the reader to <ref type="bibr" target="#b19">Sachan et al. (2015)</ref> for details on the multi-task model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets: We use MCTest-500 dataset ( <ref type="bibr" target="#b18">Richardson et al., 2013</ref>), a freely available set of 500 sto- ries (300 train, 50 dev and 150 test) and associated questions to evaluate our model. Each story in MCTest has four multiple-choice questions, each with four answer choices. Each question has ex- actly one correct answer. Each question is also annotated as 'single' or 'multiple'. The questions annotated 'single' require just one sentence in the passage to answer them. For 'multiple' questions it should not be possible to find the answer to the question with just one sentence of the passage. In a sense, 'multiple' questions are harder than 'single' questions as they require more complex inference. We will present the results breakdown for 'single' or 'multiple' category questions as well. Baselines: We compare our approach to the fol- lowing baselines: (1-3) The first three baselines are taken from <ref type="bibr" target="#b18">Richardson et al. (2013)</ref>. SW and SW+D use a sliding window and match a bag of words constructed from the question and the can- didate answer to the text. RTE uses textual en- tailment by selecting the hypothesis that has the highest likelihood of being entailed by the pas- sage. (4) LEX++, taken from Smith et al. <ref type="formula">(2015)</ref> is another lexical matching method that takes into account multiple context windows, question types and coreference. <ref type="formula">(5)</ref> JACANA uses an off the shelf aligner and aligns the hypothesis state- ment with the passage. (6-7) LSTM and QANTA, taken from <ref type="bibr" target="#b19">Sachan et al. (2015)</ref>, use neural net- works (LTSMs and Recursive NNs, respectively). (8) ATTENTION, taken from <ref type="bibr" target="#b24">Yin et al. (2016)</ref>, uses an attention-based convolutional neural net- work. (9) DISCOURSE, taken from Narasimhan and Barzilay (2015), proposes a discourse based model.</p><p>(10-14) LSSVM, LSSVM+Negation, LSSVM+Negation (MultiTask), taken from <ref type="bibr" target="#b19">Sachan et al. (2015)</ref> are all discourse aware latent struc- tural svm models. LSSVM+Negation accounts for negation. LSSVM+Negation+MTL further in- coporates multi-task learning based on question types. Here, we have three variants of multitask learners based on the three question classification strategies. (15) Finally, SYN+FRM+SEM, taken from <ref type="bibr" target="#b22">Wang et al. (2015)</ref> proposes a framework with features based on syntax, frame semantics, coreference and word embeddings. Results: We compare our AMR subgraph contain- ment approach 2 where we consider our modifica- tions for negation and multi-task learning as well in <ref type="table">Table 1</ref>. We can observe that our models have a comparable performance to all the baselines in- cluding the neural network approaches and all pre- vious approaches proposed for this task. Further, when we incorporate multi-task learning, our ap- proach achieves the state of the art. Also, our ap- proaches have a considerable improvement over the baselines for 'multiple' questions. This shows the MCTest-500 dataset. The table shows accuracy on the test set of MCTest- 500. All differences between the baselines (except SYN+FRM+SEM) and our approaches, and the improvements due to negation and multi-task learning are significant (p &lt; 0.05) using the two-tailed paired T-test.</p><p>the benefit of our latent structure that allows us to combine evidence from multiple sentences. The negation heuristic helps significantly, especially for 'single' questions (majority of negation cases in the MCTest dataset are for the "single" ques- tions). The multi-task method which performs a classification based on the subtasks for machine comprehension defined in <ref type="bibr" target="#b23">Weston et al. (2015)</ref> does better than QAClassification that learns the question answer classification. QAClassification in turn performs better than QClassification that learns the question classification only. These results, together, provide validation for our approach of subgraph matching over mean- ing representation graphs, and the incorporation of negation and multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed a solution for reading comprehen- sion tests using AMR. Our solution builds inter- mediate meaning representations for passage and question-answers. Then it poses the comprehen- sion task as a subgraph matching task by learn- ing latent alignments from one meaning represen- tation to another. Our approach achieves compet- itive or better performance than other approaches proposed for this task. Incorporation of negation and multi-task learning leads to further improve- ments establishing it as the new state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Example latent answer-entailing structure from the MCTest dataset. The question and answer candidate are combined to generate a hypothesis. This hypothesis is AMR parsed to construct a hypothesis meaning representation graph after some post-processing ( § 2.1). Similar processing is done for each sentence in the passage as well. Then, a subset (not necessarily contiguous) of these sentence meaning representation graphs is found. These representation subgraphs are further merged using coreference information, resulting into a structure called the relevant text snippet graph. Finally, the hypothesis meaning representation graph is aligned to the snippet graph. The dashed red lines show node alignments, solid red lines show edge alignments, and thick solid black arrow shows the rhetorical structure label (elaboration).</figDesc></figure>

			<note place="foot" n="2"> We tune the SVM parameter C on the dev set. We use Stanford CoreNLP, HILDA parser (Feng and Hirst, 2014) and JAMR (Flanigan et al., 2014) for preprocessing.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reranking bilingually extracted paraphrases using monolingual distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Factoid question answering over unstructured and structured content on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agichtein2005] S</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agichtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The individuation of events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Davidson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A structured vector space model for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padó2008] Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A linear-time bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirst2014] Vanessa</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flanigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Shashank Srivastava, and Eduard H. Hovy. 2013. A structured distributional semantic model for event co-reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goyal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="467" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discourse complements lexical semantics for non-factoid answer reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dirt@ sbt@ discovery of inference rules from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pantel2001] Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<title level="m">{Rhetorical Structure Theory: Toward a functional theory of text organisation}. Text</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="234" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapata2008] Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-15" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine comprehension with discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barzilay2015] Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1253" to="1262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Events in the Semantics of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Parsons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning answer-entailing structures for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A strong lexical matching method for the machine comprehension test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellery</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1721" />
			<biblScope unit="page" from="1693" to="1698" />
		</imprint>
	</monogr>
	<note>Smith et al.2015</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A walk-based semantically enriched tree kernel over distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hovy2013] Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1411" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="700" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention-based convolutional neural network for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.04341</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">A L</forename><surname>Rangarajan2003</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
