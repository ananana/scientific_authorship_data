<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TextFlow: A Text Similarity Measure based on Continuous Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Mrabet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halil</forename><surname>Kilicoglu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
						</author>
						<title level="a" type="main">TextFlow: A Text Similarity Measure based on Continuous Sequences</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="763" to="772"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1071</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include n-grams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>The number of pages required to print the content of the World Wide Web was estimated to 305 bil- lion in a 2015 article <ref type="bibr">1</ref> . While a big part of this content consists of visual information such as pic- tures and videos, texts also continue growing at a very high pace. A recent study shows that the av- erage webpage weights 1,200 KB with plain text accounting for up to 16% of that size 2 .</p><p>While efficient distribution of textual data and computations are the key to deal with the increas- 1 http://goo.gl/p9lt7V 2 http://goo.gl/c41wpa ing scale of textual search, similarity measures still play an important role in refining search re- sults to more specific needs such as the recognition of paraphrases and textual entailment, plagiarism detection and fine-grained ranking of information. These tasks are also often performed on dedicated document collections for domain-specific applica- tions where text similarity measures can be di- rectly applied.</p><p>Finding relevant approaches to compute text similarity motivated a lot of research in the last decades ( <ref type="bibr" target="#b14">Sahami and Heilman, 2006;</ref><ref type="bibr" target="#b5">Hatzivassiloglou et al., 1999)</ref>, and more recently with deep learning methods <ref type="bibr" target="#b17">(Socher et al., 2011;</ref><ref type="bibr" target="#b19">Yih et al., 2011;</ref><ref type="bibr" target="#b16">Severyn and Moschitti, 2015)</ref>. However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be ap- plied (i) regardless of the application domain and (ii) without requiring training corpora.</p><p>For instance, <ref type="bibr" target="#b18">Yih and Meek (2007)</ref> presented an approach to improve text similarity measures for web search queries with a length ranging from one word to short sequences of words. The pro- posed method is tailored to this specific task, and the training models are not expected to perform well on different kinds of data such as sentences, questions or paragraphs. In a more general study, <ref type="bibr" target="#b0">Achananuparp et al. (2008)</ref> compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question vari- ants task. In their experiments the best perfor- mance was obtained with a linear combination of semantic and lexical similarities, including a word order similarity proposed in ( <ref type="bibr" target="#b8">Li et al., 2006</ref>). This word order similarity is computed by con- structing first two vectors representing the com- mon words between two given sentences and using their respective positions in the sentences as term weights. The similarity value is then obtained by subtracting the two vectors and taking the absolute value. While such representation takes into ac- count the actual positions of the words, it does not allow detecting sub-sequence matches and takes into account missing words only by omission.</p><p>More generally, existing standalone (or tradi- tional) text similarity measures rely on the inter- sections between token sets and/or text sizes and frequency, including measures such as the Co- sine similarity, Euclidean distance, <ref type="bibr">Levenshtein (Sankoff and Kruskal, 1983)</ref>, Jaccard (Jain and <ref type="bibr" target="#b6">Dubes, 1988)</ref> and Jaro <ref type="bibr" target="#b7">(Jaro, 1989)</ref>. The se- quential nature of natural language is taken into account mostly through word n-grams and skip- grams which capture distinct slices of the analysed texts but do not preserve the order in which they appear.</p><p>In this paper, we use intuitions from a common representation in DNA sequence alignment to de- sign a new standalone similarity measure called TextFlow (XF). The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches. Our contributions can be detailed further as fol- lows:</p><p>• A novel standalone similarity measure which:</p><p>-exploits the full sequence of words in the compared texts. -is asymmetric in a way that allows it to provide the best performance on dif- ferent tasks (e.g., paraphrase detection, textual entailment and ranking). -when required, it can be trained with a small set of parameters controlling the impact of sub-sequence matching, posi- tion gaps and unmatched words. -provides consistent high performance across tasks and datasets compared to traditional similarity measures.</p><p>• A neural network architecture to train TextFlow parameters for specific tasks.</p><p>• An empirical study on both performance con- sistency and standard evaluation measures, performed with eight datasets from three dif- ferent tasks. • A new evaluation measure, called CORE, used to better show the consistency of a sys- tem at high performance using both its rank average and rank variance when compared to competing systems over a set of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The TextFlow Similarity</head><p>XF is inspired from a dot matrix representation commonly used in pairwise DNA sequence align- ment (cf. <ref type="figure" target="#fig_0">figure 1</ref>). We use a similar dot matrix representation for text pairs and draw a curve os- cillating around the diagonal (cf. <ref type="figure" target="#fig_1">figure 2</ref>). The area under the curve is considered to be the dis- tance between the two text pairs which is then normalized with the matrix surface. For practical computation, we transform this first intuitive rep- resentation using the delta of positions as in <ref type="figure">figure  3</ref>. In this setting, the Y axis is the delta of posi- tions of a word occurring in the two texts being compared. If the word does not occur in the tar- get text, the delta is considered to be a maximum reference value (l in <ref type="figure" target="#fig_1">figure 2</ref>). The semantics are: the bigger the area un- der curve is, the lower the similarity between the compared texts. XF values are real numbers in the <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> interval, with 1 indicating a perfect match, and 0 indicating that the compared texts do not have any common tokens. With this rep- resentation, we are able to take into account all matched words and sub-sequences at the same time. The exact value for the XF similarity be- tween two texts X = {x 1 , x 2 , .., x n } and Y = {y 1 , y 2 , .., y m } is therefore computed as: </p><formula xml:id="formula_0">XF (X, Y ) = 1 − 1 nm n i=2 1 S i T i,i−1 (X, Y ) − 1 nm n i=2 1 S i R i,i−1 (X, Y )<label>(1)</label></formula><p>With <ref type="figure">figure 3</ref>) and R i,i−1 (X, Y ) corresponding to the rectangu- lar component. They are expressed as:</p><formula xml:id="formula_1">T i,i−1 (X, Y ) corresponding to the triangu- lar area in the [i − 1, i] step (cf.</formula><formula xml:id="formula_2">Ti,i−1(X, Y ) = |∆P (xi, X, Y ) − ∆P (xi−1, X, Y )| 2<label>(2)</label></formula><p>and:</p><formula xml:id="formula_3">Ri,i−1(X, Y ) = M in(∆P (xi, X, Y ), ∆P (xi−1, X, Y ))<label>(3)</label></formula><p>With:</p><p>• ∆P (x i , X, Y ) the minimum difference be- tween x i positions in X and Y . x i position in X is multiplied by the factor |Y | |X| for nor- malization. If</p><formula xml:id="formula_4">x i / ∈ X ∩ Y , ∆P (x i , X, Y )</formula><p>is set to the same reference value equal to m, (i.e., the cost of a missing word is set by de- fault to the length of the target text), and:</p><p>• S i is the length of the longest matching se- quence between X and Y including the word</p><formula xml:id="formula_5">x i , if x i ∈ X ∩ Y , or 1 otherwise.</formula><p>XF computation is performed in O(nm) in the worst case where we have to check all tokens in the target text Y for all tokens in the input text X. XF is an asymmetric similarity measure. Its asymmet- ric aspect has interesting semantic applications as we show in the example below (cf. <ref type="figure" target="#fig_1">figure 2</ref>). The minimum value of XF provided the best differ- entiation between positive and negative text pairs when looking for semantic equivalence (i.e., para- phrases), the maximum value was among the top three for the textual entailment example. We con- duct this comparison at a larger scale in the evalu- ation section.</p><p>We add 3 parameters to XF in order to repre- sent the importance that should be given to posi- tion deltas (Position factor α), missing words (sen- sitivity factor β), and sub-sequence matching (se- quence factor γ), such that:</p><formula xml:id="formula_6">XF α,β,γ (X, Y ) = 1 − 1 βnm n i=2 α S γ i T β i,i−1 (X, Y ) − 1 βnm n i=2 α S γ i R β i,i−1 (X, Y )<label>(4)</label></formula><p>With:</p><formula xml:id="formula_7">T β i,i−1 (X, Y ) = |∆ β P (xi, X, Y ) − ∆ β P (xi−1, X, Y )| 2 (5) R β i,i−1 (X, Y ) = M in(∆ β P (xi, X, Y ), ∆ β P (xi−1, X, Y ))<label>(6)</label></formula><p>and:</p><formula xml:id="formula_8">• ∆ β P (x i , X, Y ) = βm, if x i / ∈ X ∩ Y</formula><p>• α &lt; β: forces missing words to always cost more than matched words.</p><p>•</p><formula xml:id="formula_9">S γ i = 1if S i = 1orx i / ∈ X ∩ Y γ S i f orS i &gt; 1</formula><p>The γ factor increases or decreases the impact of sub-sequence matching, α applies to individ- ual token matches whether inside or outside a se- quence, and β increases or decreases the impact of Positive Entailment E1 Under a blue sky with white clouds, a child reaches up to touch the propeller of a plane standing parked on a field of grass.</p><p>E2 A child is reaching to touch the propeller of a plane.</p><p>Negative Entailment E3 Two men on bicycles competing in a race. E4 Men are riding bicycles on the street.</p><p>Positive Paraphrase P1 The most serious breach of royal security in recent years occurred in 1982 when 30- year-old Michael Fagan broke into the queen's bedroom at Buckingham Palace.</p><p>P2 It was the most serious breach of royal security since 1982 when an intruder, Michael Fagan, found his way into the Queen's bedroom at Buckingham Palace.</p><p>Negative Paraphrase P3 "Americans don't cut and run, we have to see this misadventure through," she said. P4 She also pledged to bring peace to Iraq: "Americans don't cut and run, we have to see this misadventure through."  missing tokens as well as the normalization quan- tity βnm in equation 4 to keep the similarity val- ues in the [0,1] range.</p><formula xml:id="formula_10">Task Entailment Recognition Paraphrase Detection Sentence Pair (E1, E2) (E3, E4) (E1, E2) -(E3, E4) (P1, P2) (P3, P4) (P1, P2) -(P3, P4) Example class (Pos/Neg) (Pos) (Neg) (Gap) (Pos) (Neg)<label>(Gap</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parameter Training</head><p>By default XF has canonical parameters set to 1. However, when needed, α, β, and γ can be learned on training data for a specific task. We designed a neural network to perform this task, with a hidden layer dedicated to compute the exact XF value. To do so we compute, for each input text pair, the co- efficients vector that would lead exactly to the XF value when multiplied by the vector &lt; α β , α βγ , 1 &gt;. <ref type="figure" target="#fig_3">Figure 5</ref>) presents the training neural network con- sidering several types of sequences (or transla- tions) of the input text pairs (e.g., lemmas, words, synsets).</p><p>We use identity as activation function in the dedicated XF layer in order to have a correct com- parison with the other similarity measures, includ- ing canonical XF where the similarity value is pro- vided in the input layer (cf. <ref type="figure">figure 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Datasets. This evaluation was performed on 8 datasets from 3 different classification tasks: Tex- tual Entailment Recognition, Paraphrase Detec- tion, and ranking relevance. The datasets are as follows:</p><p>• RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge ( <ref type="bibr" target="#b2">Dagan et al., 2006</ref>). Each dataset consists of sentence pairs which are anno- tated with 2 labels: entailment, and non- entailment. They contain respectively (200, 800), (800, 800), and (800, 800) (train, test) pairs.</p><p>• Guardian: an RTE dataset collected from 78,696 Guardian articles 5 published from January 2004 onwards and consisting of 32K pairs which we split randomly in 90%/10% training/test sets. Positive examples were collected from the titles and first sentences. Negative examples were collected from the same source by selecting consecutive sen- tences and random sentences.</p><p>• SNLI: a recent RTE dataset consisting of 560K training sentence pairs annotated with • MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.</p><p>• Semeval-16-3B: a dataset of question- question similarity collected from Stack- Overflow ( <ref type="bibr" target="#b11">Nakov et al., 2016</ref>). The dataset contains 3,169 training pairs and 700 test pairs. Three labels are considered: "Perfect Match", "Relevant" or "Irrelevant". We com- bined the first two into the same positive cat- egory for our evaluation.</p><p>• Semeval-14-1: a corpus of Sentences Involv- ing Compositional Knowledge ( <ref type="bibr" target="#b9">Marelli et al., 2014</ref>) consisting of 10,000 English sentence pairs annotated with both similarity scores and relevance labels.</p><p>Features. After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences con- structed, respectively, with the following value from each token:</p><p>• Word (plain text value)</p><p>• Lemma</p><p>• Part-Of-Speech (POS) tag</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• WordNet Synset 6 OR Lemma</head><p>• WordNet Synset OR Lemma for Nouns</p><p>• WordNet Synset OR Lemma for Verbs</p><p>• WordNet Synset OR Lemma for Nouns and Verbs.</p><p>In the last 4 types of sequences the lemma is used when there is no corresponding WordNet synset. In a first experiment we compare differ- ent aggregation functions on top of XF (minimum, maximum and average) in table 1. We used the Li- bLinear 7 SVM classifier for this task.</p><p>In the second part of the evaluation, we use neu- ral networks to compare the efficiency of XF c , XF t and other similarity measures with in the same setting. We use the neural net described in <ref type="figure" target="#fig_3">figure 5</ref> for the trained version XF t and the equiv- alent architecture presented in <ref type="figure">figure 6</ref> for all other similarity measures. For canonical XF c we use by default the best aggregation for the task as ob- served in positive class (i.e., entailment, paraphrase, and ranking relevance). We also study the relative rank in performance of each similarity measure across all datasets using the average rank, the rank vari- ance and a new evaluation measure called Con- sistent peRformancE (CORE), computed as fol- lows for a system m, a set of datasets D, a set of systems S, and an evaluation measure E ∈ {F 1, P recision, Recall, Accuracy}:</p><formula xml:id="formula_11">CORE D,S,E (m) = M IN p∈S AV G d∈D (R S (E d (p)) + V d∈D (R S (E d (p))) AV G d∈D R S (E d (m)) + V d∈D R S (E d (m))<label>(7)</label></formula><p>With R S (E d (m)) the rank of m according to the evaluation measure E on dataset d w.r.t. com- peting systems S. V d∈D (R S (E d (m))) is the rank variance of m over datasets. The results in tables 2, 3, and 4 demonstrate the intuition. Basically, CORE tells us how consistent a system/method is in having high performance, relatively to the set of competing systems S. The maximum value of CORE is 1 for the best performing system ac- cording to its rank. It also allows quantifying how consistently a system achieves high performance for the remaining systems.</p><p>TextFlow outperformed the results obtained with a combination of word order similarity and semantic similarities tested in ( <ref type="bibr" target="#b0">Achananuparp et al., 2008)</ref>, with gaps of +1.0 in F1 and +6.1 ac- curacy on MSRP and +4.2 F1 and +2.7% accuracy on RTE 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Canonical Text Flow</head><p>T F c had the best average and micro-average accu- racy on the 8 classification datasets, with a gap of +0.4 to +6.3 when compared to the state-of-the-art measures. It also reached the best precision aver- age with a gap of +1.8 to +6.3. On the F1 score level XF c achieved the second best performance with a gap of -1.7, mainly caused by its under- performance in recall, where it had the third best performance with a gap of -6.3 (cf.     <ref type="table">Table 5</ref>: Recall values. The best result is highlighted, the second best is underlined.</p><note type="other">Guardian .748 .750 .820 .778 .780 .847 .726 .847 .848 .867 .876 SNLI .621 .599 .665 .612 .608 .631 .556 .630 .619 .641 .656 MSRP .719 .689 .720 .729 .731 .687 .699 .685 .717 .724 .732 Semeval-16-3B .756 .734 .769 .781 .780 .759 .751 .759 .737 .777 .782 Semeval-14-1 .790 .756 .779 .783 .786 .749 .719 .749 .757 .783 .798 AVG .678 .651 .692 .674 .675 .668 .633 .669 .670 .696 .710 Micro Avg .699 .675 .725 .700 .700 .701 .646 .701 .701 .726 .739 RANK Avg 5.1 8.2 4.5 5.6 5.5 6.9 10.1 6.7 6.7 4.1 1.2 RANK Var. 9.0 5.9 4.3 10.0 8.6 5.3 1.6 6.2 8.2 2.7 0.2 CORE 0.104 0.103 0.167 0.094 0.104 0.121 0.125 0.113 0.098 0.215 1.000</note><p>accuracy, F1 and precision, and the second best for recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Trained Text Flow</head><p>When compared to state-of-the-art measures and to canonical XF, the trained version, XF t , obtained the best accuracy with a gap ranging from +1.4 to +7.8. XF t also obtained the second best F1 average with a -1.0 gap, but with clear inconsistencies according to the dataset. XF t obtained the best precision with a gap ranging from +0.8 to +7.1. XF t did not perform well on recall with 64.5% micro-average compared to WordOverlap with 72%. Both its recall and F1 performance can be explained by the fact that the measure was trained to optimize accuracy, and not the F1 score for the positive class; which also suggests that the approach could be adapted to F1 optimization if needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Synthesis</head><p>Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XF t was optimized. We argue that this consis- tency was made possible through the asymmetry of XF which allowed it to adapt to different kinds of similarities (i.e., equivalence/paraphrase, in- ference/entailment, and mutual distance/ranking). These results also show that the actual position difference is a relevant factor for text similarity. We explain it mainly by the natural flow of lan- guage where the important entities and relations are often expressed first, in contrast with a purely logical-driven approach which has to consider, for instance, that active forms and passive forms are equivalent in meaning. The difference in positions is also not read literally, both because of the higher impact associated to missed words and to the α parameter which allows leveraging their impact in the trained version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Experiments</head><p>In additional experiments, we compared T F c and T F t with the other similarity measures when ap- plied to bi-grams and tri-grams instead of individ- ual tokens.</p><p>The results were significantly lower on all datasets (between 3 and 10 points loss in accu- racy) for both the soa measures and TextFlow vari- ants. This result could be explained by the fact that n-grams are too rigid when a sub-sequence varies even slightly, e.g., the insertion of a new word in- side a 3-words sequence leads to a tri-gram mis- match and reduces bi-gram overlap from 100% to 50% for the considered sub-sequence. This issue is not encountered with TextFlow as it relies on the token level, and such an insertion will not cancel or reduce significantly the gains from the correct ordering of the words. It must be noted here that not all languages grant the same level of impor- tance to sequences and that additional multilingual tests have to be carried out. In addition to binary classification output such as textual entailment and paraphrase recognition, text similarity measures can be evaluated more precisely when we consider the correlation of their values for ranking purposes.</p><p>We conducted ranking correlation experiments on three test datasets provided at the semantic text similarity task at Semeval 2012, with gold score values for their text pairs. The datasets have 750 sentence pairs each, and are extracted from the Microsoft Research video descriptions corpus, MSRP and the SMTeuroparl <ref type="bibr">11</ref> . When compared to the traditional similarity measures, TextFlow had the best correlation on the first two datasets with, for instance, 0.54 and 0.46 pearson correla- tion on the lemmas sequences and the second best on the MSRP extract where the Cosine similarity had the best performance with 0.71 vs 0.68, not- ing that the Cosine similarity uses word frequen- cies when the evaluated version of TextFlow did not use word-level weights.</p><p>Including word weights is one of the promis- ing perspectives in line with this work as it could be done simply by making the deltas vary accord- ing to the weight/importance of the (un)matched word. Also, in such a setting, the impact of a sequence of N words will naturally increase or decrease according to the word weights (cf. <ref type="figure">fig- ure 3)</ref>. We conducted a preliminary test using the inverse document frequency of the words as ex- tracted from Wikipedia with Gensim <ref type="bibr">12</ref> , which led to an improvement of up to 2% for most datasets, with performance decreasing slightly on two of them. Other kinds of weights could also be in- cluded just as easily, such as contextual word re- latedness using embeddings or other semantic re- latedness factors such as WordNet distances <ref type="bibr" target="#b12">(Pedersen et al., 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a novel standalone similarity mea- sure that takes into account continuous word se- quences. An evaluation on eight datasets show promising results for textual entailment recogni- tion, paraphrase detection and ranking. Among the potential extensions of this work are the inclu- sion of different kinds of weights such as TF-IDF, embedding relatedness and semantic relatedness. We also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dot matrix example for 2 DNA sequences (Mount, 2004)</figDesc><graphic url="image-1.png" coords="2,324.34,62.81,181.42,171.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of TextFlow Intuition</figDesc><graphic url="image-3.png" coords="3,89.06,288.41,181.42,125.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example sentences and similarity values. The best value per column is highlighted. The second best is underlined. Worst and second worst values are followed by one and two stars. Entailment examples are taken from SNLI (Bowman et al., 2015). Paraphrase examples are taken from MSRP 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: NN architecture A1 for XF Parameter Training</figDesc><graphic url="image-4.png" coords="5,115.99,62.80,362.84,218.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>table 3 .Table 1 : Accuracy evaluation with different aggregations of XF using an SVM classifier.</head><label>31</label><figDesc></figDesc><table>Task 

Entailment Recognition 
Paraphrase Detection 
Ranking Relevance 
Datasets 
RTE 1 RTE 2 RTE 3 Guardian SNLI 
MSRP 
Semeval16-t3B Semeval12-t17 
XF MIN 
55.3 
53.8 
60.0 
77.3 
58.0 
72.1 
77.4 
77.8 
XF AVG 
51.4 1 
57.2 
62.5 
84.9 
62.0 
72.0 
77.6 
79.5 
XF MAX 
53.9 
61.3 
64.7 
86.7 
64.3 
71.4 
76.7 
77.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>table 3 ). On a rank level, XF c had the best consistent rank for</head><label>3</label><figDesc></figDesc><table>Cosine 

Euc 
Overlap 
Dice 
Jaccard Damerau 
JW 
LEV 
LCS 
XFC XFT 
RTE 1 
.561 
.564 
.550 
.504 
.511 
.557 
.532 
.561 
.568 
.550 
.575 
RTE 2 
.575 
.555 
.598 
.566 
.572 
.548 
.541 
.551 
.548 
.597 
.612 
RTE 3 
.652 
.562 
.636 
.637 
.630 
.567 
.538 
.567 
.562 
.627 
.647 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Accuracy values using. The best result is highlighted, the second best is underlined.</head><label>2</label><figDesc></figDesc><table>Cosine 
Euc 
Overlap 
Dice 
Jaccard Damerau 
JW 
LEV 
LCS 
XFC XFT 
RTE 1 
.612 
.564 
.636 
.512 
.523 
.578 
.513 
.583 
.494 
.565 
.599 
RTE 2 
.579 
.590 
.662 
.565 
.558 
.549 
.516 
.551 
.555 
.616 
.646 
RTE 3 
.705 
.598 
.682 
.695 
.682 
.608 
.556 
.607 
.603 
.665 
.690 
Guardian 
.742 
.749 
.816 
.774 
.776 
.849 
.713 
.849 
.850 
.862 
.873 
SNLI 
.582 
.576 
.641 
.562 
.564 
.627 
.479 
.627 
.611 
.594 
.585 
MSRP 
.808 
.797 
.812 
.814 
.813 
.784 
.802 
.783 
.804 
.804 
.810 
Semeval-16-3B 
.632 
.462 
.625 
.648 
.644 
.544 
.545 
.547 
.508 
.633 
.662 
Semeval-14-1 
.764 
.707 
.748 
.753 
.746 
.706 
.680 
.706 
.714 
.744 
.673 
AVG 
.678 
.630 
.702 
.665 
.663 
.655 
.600 
.656 
.642 
.685 
.692 
Micro Avg 
.684 
.656 
.716 
.679 
.677 
.691 
.608 
.692 
.688 
.702 
.687 
RANK Avg 
4.5 
8.12 
3.12 
5.12 
5.5 
6.89 
9.88 
6.62 
7.12 
4.62 
3.88 
RANK Var. 
9.7 
4.7 
4.4 
14.7 
6.6 
8.7 
1.8 
9.1 
8.1 
2.3 
11.0 
CORE 
0.485 
0.538 
0.915 
0.348 
0.571 
0.443 
0.588 0.438 0.452 1.000 0.464 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : F1 scores. The best result is highlighted, the second best is underlined.</head><label>3</label><figDesc></figDesc><table>Cosine 
Euc 
Overlap 
Dice 
Jaccard Damerau 
JW 
LEV 
LCS 
XFC XFT 
RTE 1 
.548 
.564 
.534 
.503 
.510 
.552 
.535 
.555 
.596 
.546 
.566 
RTE 2 
.574 
.547 
.571 
.567 
.578 
.547 
.546 
.551 
.546 
.588 
.594 
RTE 3 
.624 
.565 
.618 
.611 
.610 
.568 
.547 
.568 
.564 
.616 
.627 
Guardian 
.759 
.753 
.836 
.789 
.789 
.839 
.749 
.840 
.839 
.891 
.894 
SNLI 
.644 
.608 
.690 
.642 
.632 
.631 
.577 
.630 
.621 
.679 
.735 
MSRP 
.740 
.705 
.732 
.749 
.755 
.723 
.713 
.722 
.743 
.760 
.765 
Semeval-16-3B 
.634 
.708 
.678 
.698 
.698 
.732 
.698 
.729 
.674 
.700 
.686 
Semeval-14-1 
.745 
.738 
.738 
.743 
.769 
.716 
.672 
.716 
.727 
.762 
.740 
AVG 
.659 
.649 
.675 
.663 
.668 
.664 
.630 
.664 
.664 
.693 
.701 
Micro Avg 
.693 
.674 
.721 
.699 
.704 
.694 
.645 
.693 
.693 
.737 
.752 
RANK Avg. 
5.6 
7.5 
5.9 
5.9 
5.1 
6.1 
9.6 
6.1 
7.1 
3.2 
2.5 
RANK Var. 
9.4 
10.0 
6.4 
5.3 
7.8 
7.0 
4.6 
7.6 
11.6 
3.1 
6.9 
CORE 
0.420 
0.361 
0.515 
0.567 
0.488 
0.482 
0.446 0.462 0.338 1.000 0.676 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Precision values. The best result is highlighted, the second best is underlined. 

769 

</table></figure>

			<note place="foot" n="5"> https://github.com/daoudclarke/ rte-experiment</note>

			<note place="foot" n="6"> https://wordnet.princeton.edu/ 7 https://www.csie.ntu.edu.tw/ ˜ cjlin/ liblinear/</note>

			<note place="foot" n="12"> https://radimrehurek.com/gensim/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the Intramural Research Program of the NIH, National Library of Medicine.</p><p>11 goo.gl/NVnybD</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The evaluation of sentence similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palakorn</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiajiong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data warehousing and knowledge discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee R Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tolerating spelling errors during patient validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Sideli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Biomedical Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="486" to="509" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">L</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 joint sigdat conference on empirical methods in natural language processing and very large corpora. Citeseer</title>
		<meeting>the 1999 joint sigdat conference on empirical methods in natural language processing and very large corpora. Citeseer</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Algorithms for clustering data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew A Jaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">406</biblScope>
			<biblScope unit="page" from="414" to="420" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sentence similarity based on semantic nets and corpus statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zuhair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeley</forename><surname>O&amp;apos;shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1138" to="1150" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>SemEval-2014</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bioinformatics: sequence and genome analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David W Mount</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cold Spring Harbor Laboratory Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abed Alhakim Freihat, Jim Glass, and Bilal Randeree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="525" to="545" />
		</imprint>
	</monogr>
	<note>Semeval-2016 task 3: Community question answering</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordnet:: Similarity: measuring the relatedness of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Demonstration papers at HLT-NAACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="38" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximate string comparison and its effect on an advanced record linkage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced record linkage system. US Bureau of the Census</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Research Report. Citeseer</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A web-based kernel function for measuring the similarity of short text snippets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Heilman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Time warps, string edits, and macromolecules: the theory and practice of sequence comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sankoff</surname></persName>
		</author>
		<editor>Sankoff, David</editor>
		<editor>Kruskal, Joseph B. 1</editor>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Addison-Wesley Publication</publisher>
		</imprint>
	</monogr>
<note type="report_type">Reading</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving similarity measures for short segments of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1489" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
