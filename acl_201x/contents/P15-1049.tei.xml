<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yiyang@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<email>minchang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Interactive Computing</orgName>
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="504" to="513"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features. However, tree-based models are seldom studied in the context of structured learning despite their recent success on various classification and ranking tasks. In this paper, we propose SMART , a tree-based structured learning framework based on multiple additive regression trees. SMART is especially suitable for handling tasks with dense features , and can be used to learn many different structures under various loss functions. We apply SMART to the task of tweet entity linking-a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base. A novel inference algorithm is proposed to handle the special structure of the task. The experimental results show that SMART significantly outperforms state-of-the-art tweet entity linking systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language processing (NLP) prob- lems can be formalized as structured prediction tasks. Standard algorithms for structured learning include Conditional Random Field (CRF) <ref type="bibr" target="#b19">(Lafferty et al., 2001</ref>) and Structured Supported Vec- tor Machine (SSVM) ( <ref type="bibr" target="#b35">Tsochantaridis et al., 2004</ref>). These algorithms, usually equipped with a linear model and sparse lexical features, achieve state- of-the-art performances in many NLP applica- tions such as part-of-speech tagging, named entity recognition and dependency parsing.</p><p>This classical combination of linear models and sparse features is challenged by the recent emerg- ing usage of dense features such as statistical and embedding features. Tasks with these low dimen- sional dense features require models to be more sophisticated to capture the relationships between features. Therefore, non-linear models start to re- ceive more attention as they are often more expres- sive than linear models.</p><p>Tree-based models such as boosted trees <ref type="bibr" target="#b15">(Friedman, 2001</ref>) are flexible non-linear models. They can handle categorical features and count data bet- ter than other non-linear models like Neural Net- works. Unfortunately, to the best of our knowl- edge, little work has utilized tree-based methods for structured prediction, with the exception of TreeCRF ( <ref type="bibr" target="#b12">Dietterich et al., 2004)</ref>.</p><p>In this paper, we propose a novel structured learning framework called S-MART (Structured Multiple Additive Regression Trees). Unlike TreeCRF, S-MART is very versatile, as it can be applied to tasks beyond sequence tagging and can be trained under various objective functions. S- MART is also powerful, as the high order relation- ships between features can be captured by non- linear regression trees.</p><p>We further demonstrate how S-MART can be applied to tweet entity linking, an important and challenging task underlying many applications in- cluding product feedback <ref type="bibr" target="#b0">(Asur and Huberman, 2010)</ref> and topic detection and tracking <ref type="bibr" target="#b22">(Mathioudakis and Koudas, 2010)</ref>. We apply S-MART to entity linking using a simple logistic function as the loss function and propose a novel inference al- gorithm to prevent overlaps between entities.</p><p>Our contributions are summarized as follows:</p><p>• We propose a novel structured learning framework called S-MART. S-MART com- bines non-linearity and efficiency of tree- based models with structured prediction, leading to a family of new algorithms. (Sec- tion 2)</p><p>• We apply S-MART to tweet entity link- ing. Building on top of S-MART, we pro- pose a novel inference algorithm for non- overlapping structure with the goal of pre- venting conflicting entity assignments. (Sec- tion 3)</p><p>• We provide a systematic study of evaluation criteria in tweet entity linking by conduct- ing extensive experiments over major data sets. The results show that S-MART sig- nificantly outperforms state-of-the-art entity linking systems, including the system that is used to win the NEEL 2014 challenge <ref type="bibr">(Cano and others, 2014)</ref>. (Section 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structured Multiple Additive Regression Trees</head><p>The goal of a structured learning algorithm is to learn a joint scoring function S between an input x and an output structure y, S : (x, y) → R. The structured output y often contains many interde- pendent variables, and the number of the possible structures can be exponentially large with respect to the size of x. At test time, the prediction y for x is obtained by arg max</p><formula xml:id="formula_0">y∈Gen(x) S(x, y),</formula><p>where Gen(x) represents the set of all valid output structures for x. Standard learning algorithms often directly op- timize the model parameters. For example, as- sume that the joint scoring function S is param- eterized by θ. Then, gradient descent algorithms can be used to optimize the model parameters θ iteratively. More specifically,</p><formula xml:id="formula_1">θ m = θ m−1 − η m ∂L(y * , S(x, y; θ)) ∂θ m−1 ,<label>(1)</label></formula><p>where y * is the gold structure, L(y * , S(x, y; θ)) is a loss function and η m is the learning rate of the m-th iteration.</p><p>In this paper, we propose a framework called Structured Multiple Additive Regression Trees (S-MART), which generalizes Multiple Additive Regression Trees (MART) for structured learn- ing problems. Different from Equation (1), S- MART does not directly optimize the model pa- rameters; instead, it approximates the optimal scoring function that minimize the loss by adding (weighted) regression tree models iteratively.</p><p>Due to the fact that there are exponentially many input-output pairs in the training data, S-MART assumes that the joint scoring function can be decomposed as</p><formula xml:id="formula_2">S(x, y) = k∈Ω(x) F (x, y k ),</formula><p>where Ω(x) contains the set of the all factors for input x and y k is the sub-structure of y that cor- responds to the k-th factor in Ω(x). For instance, in the task of word alignment, each factor can be defined as a pair of words from source and target languages respectively. Note that we can recover y from the union of {y k } K 1 . The factor scoring function F (x, y k ) can be optimized by performing gradient descent in the function space in the following manner:</p><formula xml:id="formula_3">F m (x, y k ) = F m−1 (x, y k ) − η m g m (x, y k ) (2)</formula><p>where function g m (x, y k ) is the functional gradi- ent.</p><p>Note that g m is a function rather than a vector. Therefore, modeling g m theoretically requires an infinite number of data points. We can address this difficulty by approximating g m with a finite num- ber of point-wise functional gradients</p><formula xml:id="formula_4">g m (x, y k = u k ) = (3) ∂L(y * , S(x, y k = u k )) ∂F (x, y k = u k ) F (x,y k )=F m−1 (x,y k )</formula><p>where u k index a valid sub-structure for the k-th factor of x.</p><p>The key point of S-MART is that it approximates −g m by modeling the point-wise negative func- tional gradients using a regression tree h m . Then the factor scoring function can be obtained by</p><formula xml:id="formula_5">F (x, y k ) = M m=1 η m h m (x, y k ),</formula><p>where h m (x, y k ) is also called a basis function and η m can be simply set to 1 <ref type="bibr" target="#b26">(Murphy, 2012)</ref>.</p><p>The detailed S-MART algorithm is presented in Algorithm 1. The factor scoring function F (x, y k ) is simply initialized to zero at first (line 1). After this, we iteratively update the function by adding regression trees. Note that the scoring function is shared by all the factors. Specifically, given the current decision function F m−1 , we can consider line 3 to line 9 a process of generating the pseudo Algorithm 1 S-MART: A family of structured learning algorithms with multiple additive regres- sion trees 1: F0(x, y k ) = 0 2: for m = 1 to M do:</p><p>going over all trees 3:</p><formula xml:id="formula_6">D ← ∅ 4:</formula><p>for all examples do: going over all examples 5:</p><p>for y k ∈ Ω(x) do: going over all factors 6:</p><p>For all u k , obtain g ku by Equation <ref type="formula" target="#formula_14">(3)  7</ref>:</p><formula xml:id="formula_7">D ← D ∪ {(Φ(x, y k = u k ), −g ku )} 8:</formula><p>end for 9:</p><p>end for 10:</p><p>hm(x, y k ) ← TrainRegressionTree(D) 11:</p><p>Fm(x, y k ) = Fm−1(x, y k ) + hm(x, y k ) 12: end for training data D for modeling the regression tree. For each training example, S-MART first computes the point-wise functional gradients according to Equation (3) (line 6). Here we use g ku as the ab- breviation for g m (x, y k = u k ). In line 7, for each sub-structure u k , we create a new training exam- ple for the regression problem by the feature vec- tor Φ(x, y k = u k ) and the negative gradient −g ku . In line 10, a regression tree is constructed by min- imizing differences between the prediction values and the point-wise negative gradients. Then a new basis function (modeled by a regression tree) will be added into the overall F (line 11).</p><p>It is crucial to note that S-MART is a fam- ily of algorithms rather than a single algorithm. S-MART is flexible in the choice of the loss functions. For example, we can use either lo- gistic loss or hinge loss, which means that S- MART can train probabilistic models as well as non-probabilistic ones. Depending on the choice of factors, S-MART can handle various structures such as linear chains, trees, and even the semi- Markov chain ( <ref type="bibr" target="#b30">Sarawagi and Cohen, 2004</ref>).</p><p>S-MART versus MART There are two key differences between S-MART and MART. First, S-MART decomposes the joint scoring function S(x, y) into factors to address the problem of the exploding number of input-output pairs for struc- tured learning problems. Second, S-MART mod- els a single scoring function F (x, y k ) over inputs and output variables directly rather than O differ- ent functions F o (x), each of which corresponds to a label class.</p><p>S-MART versus TreeCRF TreeCRF can be viewed as a special case of S-MART, and there are two points where S-MART improves upon TreeCRF. First, the model designed in <ref type="bibr" target="#b12">(Dietterich et al., 2004</ref>) is tailored for sequence tagging prob- lems. Similar to MART, for a tagging task with O tags, they choose to model O functions F o (x, o ) instead of directly modeling the joint score of the factor. This imposes limitations on the feature functions, and TreeCRF is consequently unsuit- able for many tasks such as entity linking. 1 Second, S-MART is more general in terms of the objective functions and applicable structures. In the next section, we will see how S-MART can be applied to a non-linear-chain structure and various loss func- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">S-MART for Tweet Entity Linking</head><p>We first formally define the task of tweet entity linking. As input, we are given a tweet, an entity database (e.g., Wikipedia where each article is an entity), and a lexicon 2 which maps a surface form into a set of entity candidates. For each incoming tweet, all n-grams of this tweet will be used to find matches in the lexicon, and each match will form a mention candidate. As output, we map every men- tion candidate (e.g., "new york giants") in the mes- sage to an entity (e.g., NEW YORK GIANTS) or to Nil (i.e., a non-entity). A mention candidate can often potentially link to multiple entities, which we call possible entity assignments.</p><p>This task is a structured learning problem, as the final entity assignments of a tweet should not over- lap with each other. <ref type="bibr">3</ref> We decompose this learn- ing problem as follows: we make each mention candidate a factor, and the score of the entity as- signments of a tweet is the sum of the score of each entity and mention candidate pair. Although all mention candidates are decomposed, the non- overlapping constraint requires the system to per- form global inference.</p><p>Consider the example tweet in <ref type="figure">Figure 1</ref>, where we show the tweet with the mention candidates in brackets. To link the mention candidate "new york giants" to a non-Nil entity, the system has to link previous overlapping mention candidates to Nil. It is important to note that this is not a lin- ear chain problem because of the non-overlapping constraint, and the inference algorithm needs to be <ref type="figure">Figure 1</ref>: Example tweet and its mention candidates. Each mention candidate is marked as a pair of brackets in the original tweet and forms a column in the graph. The graph demonstrates the non-overlapping constraint. To link the mention candidate "new york giants" to a non-Nil entity, the system has to link previous four overlapping mention candidates to Nil. The mention candidate "eli manning" is not affected by "new york giants". Note that this is not a standard linear chain problem.</p><p>carefully designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Applying S-MART</head><p>We derive specific model for tweet entity linking task with S-MART and use logistic loss as our run- ning example. The hinge loss version of the model can be derived in a similar way.</p><p>Note that the tweet and the mention candidates are given. Let x be the tweet, u k be the entity as- signment of the k-th mention candidate. We use function F (x, y k = u k ) to model the score of the k-th mention candidate choosing entity u k . <ref type="bibr">4</ref> The overall scoring function can be decomposed as fol- lows:</p><formula xml:id="formula_8">S(x, y = {u k } K k=1 ) = K k=1 F (x, y k = u k )</formula><p>S-MART utilizes regression trees to model the scoring function F (x, y k = u k ), which requires point-wise functional gradient for each entity of every mention candidate. Let's first write down the logistic loss function as</p><formula xml:id="formula_9">L(y * , S(x, y)) = − log P (y * |x) = log Z(x) − S(x, y * )</formula><p>where Z(x) = y exp(S(x, y)) is the potential function. Then the point-wise gradients can be computed as</p><formula xml:id="formula_10">g ku = ∂L ∂F (x, y k = u k ) = P (y k = u k |x) − 1[y * k = u k ], where 1[·]</formula><p>represents an indicator function. The conditional probability P (y k = u k |x) can be com- puted by a variant of the forward-backward algo- rithm, which we will detail in the next subsection. <ref type="bibr">4</ref> Note that each mention candidate has different own en- tity sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>The non-overlapping structure is distinct from lin- ear chain and semi-Markov chain ( <ref type="bibr" target="#b30">Sarawagi and Cohen, 2004</ref>) structures. Hence, we propose a carefully designed forward-backward algorithm to calculate P (y k = u k |x) based on current scor- ing function F (x, y k = u k ) given by the re- gression trees. The non-overlapping constraint distinguishes our inference algorithm from other forward-backward variants.</p><p>To compute the forward probability, we sort 5 the mention candidates by their end indices and define forward recursion by</p><formula xml:id="formula_11">α(u 1 , 1) = exp(F (x, y 1 = u 1 )) α(u k , k) = exp(F (x, y k = u k )) · P −1 p=1 exp(F (x, y k−p = Nil)) · u k−P α(u k−P , k − P )<label>(4)</label></formula><p>where k − P is the index of the previous non- overlapping mention candidate. Intuitively, for the k-th mention candidate, we need to identify its nearest non-overlapping fellow and recursively compute the probability. The overlapping mention candidates can only take the Nil entity.</p><p>Similarly, we can sort the mention candidates by their start indices and define backward recur-sion by</p><formula xml:id="formula_12">β(u K , K) =1 β(u k , k) = u k+Q exp(F (x, y k+Q = u k+Q )) · Q−1 q=1 exp(F (x, y k+q = Nil)) · β(u k+Q , k + Q)<label>(5)</label></formula><p>where k + Q is the index of the next non- overlapping mention candidate. Note that the third terms of equation <ref type="formula" target="#formula_11">(4)</ref> or <ref type="formula" target="#formula_12">(5)</ref> will vanish if there are no corresponding non-overlapping mention candi- dates.</p><p>Given the potential function can be computed by</p><formula xml:id="formula_13">Z(x) = u k α(u k , k)β(u k , k), for entities that are not Nil, P (y k = u k |x) = exp(F (x, y k = u k )) · β(u k , k) Z(x) · P −1 p=1 exp(F (x, y k−p = Nil)) · u k−P α(u k−P , k − P )<label>(6)</label></formula><p>The probability for the special token Nil can be obtained by</p><formula xml:id="formula_14">P (y k = Nil|x) = 1 − u k =Nil P (y k = u k |x)<label>(7)</label></formula><p>In the worst case, the total cost of the forward- backward algorithm is O(max{T K, K 2 }), where T is the number of entities of a mention candi- date. <ref type="bibr">6</ref> Finally, at test time, the decoding problem arg max y S(x, y) can be solved by a variant of the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Beyond S-MART: Modeling entity-entity relationships</head><p>It is important for entity linking systems to take advantage of the entity-to-entity information while making local decisions. For instance, the identi- fication of entity "eli manning" leads to a strong clue for linking "new york giants" to the NFL team.</p><p>Instead of defining a more complicated struc- ture and learning everything jointly, we employ a two-stage approach as the solution for modeling entity-entity relationships after we found that S- MART achieves high precision and reasonable re- call. Specifically, in the first stage, the system identifies all possible entities with basic features, which enables the extraction of entity-entity fea- tures. In the second stage, we re-train S-MART on a union of basic features and entity-entity features. We define entity-entity features based on the Jac- card distance introduced by <ref type="bibr" target="#b16">Guo et al. (2013)</ref>.</p><p>Let Γ(e i ) denotes the set of Wikipedia pages that contain a hyperlink to an entity e i and Γ(t −i ) denotes the set of pages that contain a hyperlink to any identified entity e j of the tweet t in the first stage excluding e i . The Jaccard distance between e i and t is</p><formula xml:id="formula_15">Jac(e i , t) = |Γ(e i ) ∩ Γ(t −i )| |Γ(e i ) ∪ Γ(t −i )| .</formula><p>In addition to the Jaccard distance, we add one ad- ditional binary feature to indicate if the current en- tity has the highest Jaccard distance among all en- tities for this mention candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are designed to answer the fol- lowing three research questions in the context of tweet entity linking:</p><p>• Do non-linear learning algorithms perform better than linear learning algorithms?</p><p>• Do structured entity linking models perform better than non-structured ones?</p><p>• How can we best capture the relationships be- tween entities?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Methodology and Data</head><p>We evaluate each entity linking system using two evaluation policies: Information Extraction (IE) driven evaluation and Information Retrieval (IR) driven evaluation. For both evaluation settings, precision, recall and F1 scores are reported. Our data is constructed from two publicly available sources: Named Entity Extraction &amp; Linking (NEEL) Challenge ( ) datasets, and the datasets released by <ref type="bibr" target="#b13">Fang and Chang (2014)</ref>. Note that we gather two datasets from Fang and Chang (2014) and they are used in two different evaluation settings. We refer to these two datasets as TACL-IE and TACL-IR, respectively. We perform some data cleaning and unification on these sets. <ref type="bibr">7</ref> The statistics of the datasets are pre- sented in <ref type="table">Table 1</ref>.</p><p>IE-driven evaluation The IE-driven evaluation is the standard evaluation for an end-to-end entity linking system. We follow <ref type="bibr" target="#b6">Carmel et al. (2014)</ref> and relax the definition of the correct mention boundaries, as they are often ambiguous. A men- tion boundary is considered to be correct if it over- laps (instead of being the same) with the gold men- tion boundary. Please see ( <ref type="bibr" target="#b6">Carmel et al., 2014</ref>) for more details on the procedure of calculating the precision, recall and F1 score. The NEEL and TACL-IE datasets have differ- ent annotation guidelines and different choices of knowledge bases, so we perform the following procedure to clean the data and unify the annota- tions. We first filter out the annotations that link to entities excluded by our knowledge base. We use the same knowledge base as the ERD 2014 com- petition ( <ref type="bibr" target="#b6">Carmel et al., 2014</ref>), which includes the union of entities in Wikipedia and Freebase. Sec- ond, we follow NEEL annotation guideline and re-annotate TACL-IE dataset. For instance, in or- der to be consistent with NEEL, all the user tags (e.g. @BarackObama) are re-labeled as entities in TACL-IE.</p><p>We train all the models with NEEL Train dataset and evaluate different systems on NEEL Test and TACL-IE datasets. In addition, we sam- ple 800 tweets from NEEL Train dataset as our development set to perform parameter tuning.</p><p>IR-driven evaluation The IR-driven evaluation is proposed by <ref type="bibr" target="#b13">Fang and Chang (2014)</ref>. It is motivated by a key application of entity linking -retrieval of relevant tweets for target entities, which is crucial for downstream applications such as product research and sentiment analysis. In particular, given a query entity we can search for tweets based on the match with some potential sur- face forms of the query entity. Then, an entity linking system is evaluated by its ability to cor- rectly identify the presence or absence of the query entity in every tweet. Our IR-driven evaluation is based on the TACL-IR set, which includes 980 tweets sampled for ten query entities of five entity types (roughly 100 tweets per entity). About 37% of the sampled tweets did not mention the query entity due to the anchor ambiguity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Features We employ a total number of 37 dense features as our basic feature set. Most of the fea- tures are adopted from <ref type="bibr" target="#b16">(Guo et al., 2013)</ref>   <ref type="bibr">et al., 2004</ref>). For non-linear models, we consider polynomial SSVM, which employs polynomial kernel inside the structured SVM algorithm. We also include LambdaRank ( <ref type="bibr" target="#b27">Quoc and Le, 2007)</ref>, a neural- based learning to rank algorithm, which is widely used in the information retrieval literature. We further compare with MART, which is designed for performing multiclass classification using log loss without considering the structured informa- tion. Finally, we have our proposed log-loss S- MART algorithm, as described in Section 3. <ref type="bibr">9</ref> Note that our baseline systems are quite strong. Linear SSVM has been used in one of the state- of-the-art tweet entity linking systems ( <ref type="bibr" target="#b16">Guo et al., 2013</ref>), and the system based on MART is the win- ning system of the 2014 NEEL Challenge (Cano and others, 2014) <ref type="bibr">10</ref> . <ref type="table">Table 2</ref> summarizes several properties of the al- gorithms. For example, most algorithms are struc- <ref type="table">Table 2</ref>: Included algorithms and their properties.</p><note type="other">Model Structured Non-linear Tree-based Structured Perceptron Linear SSVM Polynomial SSVM LambdaRank MART S-MART</note><p>tured (e.g. they perform dynamic programming at test time) except for MART and LambdaRank, which treat mention candidates independently.</p><p>Parameter tuning All the hyper-parameters are tuned on the development set. Then, we re-train our models on full training data (including the dev set) with the best parameters. We choose the soft margin parameter C from {0.5, 1, 5, 10} for two structured SVM methods. After a prelimi- nary parameter search, we fixed the number of trees to 300 and the minimum number of docu- ments in a leaf to 30 for all tree-based models. For LambdaRank, we use a two layer feed for- ward network. We select the number of hidden units from {10, 20, 30, 40} and learning rate from {0.1, 0.01, 0.001}.</p><p>It is widely known that F1 score can be affected by the trade-off between precision and recall. In order to make the comparisons between all algo- rithms fairer in terms of F1 score, we include a post-processing step to balance precision and re- call for all the systems. Note the tuning is only conducted for the purpose of robust evaluation. In particular, we adopt a simple tuning strategy that works well for all the algorithms, in which we add a bias term b to the scoring function value of Nil:</p><formula xml:id="formula_16">F (x, y k = Nil) ← F (x, y k = Nil) + b.</formula><p>We choose the bias term b from values between −3.0 to 3.0 on the dev set and apply the same bias term at test time. <ref type="table" target="#tab_3">Table 3</ref> presents the empirical findings for S- MART and competitive methods on tweet entity linking task in both IE and IR settings. In the fol- lowing, we analyze the empirical results in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Linear models vs. non-linear models <ref type="table" target="#tab_3">Table 3</ref> clearly shows that linear models perform worse than non-linear models when they are restricted to the IE setting of the tweet entity linking task. The story is similar in IR-driven evaluation, with the exception of LambdaRank. Among the lin- ear models, linear SSVM demonstrates its supe- riority over Structured Perceptron on all datasets, which aligns with the results of ( <ref type="bibr" target="#b36">Tsochantaridis et al., 2005</ref>) on the named entity recognition task.</p><p>We have many interesting observations on the non-linear models side. First, by adopting a polynomial kernel, the non-linear SSVM further improves the entity linking performances on the NEEL datasets and TACL-IR dataset. Second, LambdaRank, a neural network based model, achieves better results than linear models in IE- driven evaluation, but the results in IR-driven eval- uation are worse than all the other methods. We believe the reason for this dismal performance is that the neural-based method tends to overfit the IR setting given the small number of training ex- amples. Third, both MART and S-MART signifi- cantly outperform alternative linear and non-linear methods in IE-driven evaluation and performs bet- ter or similar to other methods in IR-driven eval- uation. This suggests that tree-based non-linear models are suitable for tweet entity linking task. Finally, S-MART outperforms previous state-of- the-art method Structured SVM by a surprisingly large margin. In the NEEL Test dataset, the dif- ference is more than 10% F1. Overall, the results show that the shallow linear models are not ex- pressive enough to capture the complex patterns in the data, which are represented by a few dense features.</p><p>Structured learning models To showcase structured learning technique is crucial for entity linking with non-linear models, we compare S-MART against MART directly. As shown in Model NEEL Dev <ref type="table">NEEL Test  TACL-IE  TACL-IR  P  R  F1  P  R  F1  P  R  F1  P  R</ref>    <ref type="table" target="#tab_3">Table 3</ref>, S-MART can achieve higher precision and recall points compared to MART on all datasets in terms of IE-driven evaluation, and can improve F1 by 4 points on NEEL Test and TACL-IR datasets. The task of entity linking is to produce non-overlapping entity assignments that match the gold mentions. By adopting structured learning technique, S-MART is able to automatically take into account the non-overlapping constraint during learning and inference, and produce global optimal entity assignments for mention candidates of a tweet. One effect is that S-MART can easily eliminate some common errors caused by popular entities (e.g. new york in <ref type="figure">Figure 1</ref>).</p><p>Modeling entity-entity relationships Entity- entity relationships provide strong clues for entity disambiguation. In this paper, we use the sim- ple two-stage approach described in Section 3.3 to capture the relationships between entities. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the significant improvement in IR-driven evaluation indicates the importance of incorporating entity-entity information. Interestingly, while IR-driven results are signif- icantly improved, IE-driven results are similar or even worse given entity-entity features. We be- lieve the reason is that IE-driven and IR-driven evaluations focus on different aspects of tweet en- tity linking task. As <ref type="bibr" target="#b16">Guo et al. (2013)</ref> shows that most mentions in tweets should be linked to the most popular entities, IE setting actually pays more attention on mention detection sub-problem. In contrast to IE setting, IR setting focuses on en- tity disambiguation, since we only need to decide whether the tweet is relevant to the query entity. Therefore, we believe that both evaluation policies are needed for tweet entity linking. <ref type="figure" target="#fig_0">Figure 2</ref> shows the results of tuning the bias term for balancing precision and recall on the dev set. The results show that S-MART outperforms competitive ap- proaches without any tuning, with similar margins to the results after tuning. Balancing precision and recall improves F1 scores for all the systems, which suggests that the simple tuning method per- forms quite well. Finally, we have an interest- ing observation that different methods have vari- ous scales of model scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Balance Precision and Recall</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Linear structured learning methods have been pro- posed and widely used in the literature. Popu- lar models include Structured Perceptron <ref type="bibr" target="#b8">(Collins, 2002</ref>), Conditional Random Field ( <ref type="bibr" target="#b19">Lafferty et al., 2001</ref>) and Structured SVM ( <ref type="bibr" target="#b34">Taskar et al., 2004;</ref><ref type="bibr" target="#b36">Tsochantaridis et al., 2005)</ref>. Recently, many struc- tured learning models based on neural networks have been proposed and are widely used in lan- guage modeling ( <ref type="bibr" target="#b2">Bengio et al., 2006;</ref><ref type="bibr" target="#b24">Mikolov et al., 2010)</ref>, sentiment classification ( <ref type="bibr" target="#b33">Socher et al., 2013)</ref>, as well as parsing <ref type="bibr" target="#b32">(Socher et al., 2011</ref>). <ref type="bibr" target="#b9">Cortes et al. (2014)</ref> recently proposed a boosting framework which treats different struc- tured learning algorithms as base learners to en- semble structured prediction results.</p><p>Tree-based models have been shown to pro- vide more robust and accurate performances than neural networks in some tasks of computer vi- sion ( <ref type="bibr" target="#b29">Roe et al., 2005;</ref><ref type="bibr" target="#b1">Babenko et al., 2011</ref>) and information retrieval ( <ref type="bibr" target="#b20">Li et al., 2007;</ref><ref type="bibr" target="#b37">Wu et al., 2010)</ref>, suggesting that it is worth to investi- gate tree-based non-linear models for structured learning problems. To the best of our knowl- edge, <ref type="bibr">TreeCRF (Dietterich et al., 2004</ref>) is the only work that explores tree-based methods for struc- tured learning problems. The relationships be- tween TreeCRF and our work have been discussed in Section 2.</p><p>Early research on entity linking has focused on well written documents ( <ref type="bibr" target="#b3">Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b10">Cucerzan, 2007;</ref><ref type="bibr" target="#b25">Milne and Witten, 2008)</ref>. Due to the raise of social media, many techniques have been proposed or tailored to short texts in- cluding tweets, for the problem of entity linking <ref type="bibr" target="#b14">(Ferragina and Scaiella, 2010;</ref><ref type="bibr" target="#b23">Meij et al., 2012;</ref><ref type="bibr" target="#b16">Guo et al., 2013</ref>) as well as the related problem of named entity recognition (NER) ( <ref type="bibr" target="#b28">Ritter et al., 2011)</ref>. Recently, non-textual information such as spatial and temporal signals have also been used to improve entity linking systems <ref type="bibr" target="#b13">(Fang and Chang, 2014</ref>). The task of entity linking has attracted a lot of attention, and many shared tasks have been hosted to promote entity linking research ( <ref type="bibr" target="#b18">Ji et al., 2010;</ref><ref type="bibr" target="#b17">Ji and Grishman, 2011;</ref><ref type="bibr">Cano and others, 2014;</ref><ref type="bibr" target="#b6">Carmel et al., 2014)</ref>.</p><p>Building an end-to-end entity linking system in- volves in solving two interrelated sub-problems: mention detection and entity disambiguation. Ear- lier research on entity linking has been largely fo- cused on the entity disambiguation problem, in- cluding most work on entity linking for well- written documents such as news and encyclope- dia articles <ref type="bibr" target="#b10">(Cucerzan, 2007)</ref> and also few for tweets ( <ref type="bibr" target="#b21">Liu et al., 2013)</ref>. Recently, people have focused on building systems that consider mention detection and entity disambiguation jointly. For example, Cucerzan (2012) delays the mention de- tection decision and consider the mention detec- tion and entity linking problem jointly. Similarly, <ref type="bibr" target="#b31">Sil and Yates (2013)</ref> proposed to use a reranking approach to obtain overall better results on men- tion detection and entity disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose S-MART, a family of structured learning algorithms which is flexible on the choices of the loss functions and structures. We demonstrate the power of S-MART by applying it to tweet entity linking, and it significantly out- performs the current state-of-the-art entity linking systems. In the future, we would like to investigate the advantages and disadvantages between tree- based models and other non-linear models such as deep neural networks or recurrent neural net- works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Balance precisions and recalls. X-axis corresponds to values of the bias terms for the special token Nil. Note that S-MART is still the overall winning system without tuning the threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>IE-driven and IR-driven evaluation results for different models. The best results with basic features are in bold. The 
results are underlined if adding entity-entity features gives the overall best results. 

</table></figure>

			<note place="foot" n="1"> For example, entity linking systems need to model the similarity between an entity and the document. The TreeCRF formulation does not support such features. 2 We use the standard techniques to construct the lexicon from anchor texts, redirect pages and other information resources. 3 We follow the common practice and do not allow embedded entities.</note>

			<note place="foot" n="5"> Sorting helps the algorithms find non-overlapping candidates.</note>

			<note place="foot" n="6"> The cost is O(K 2 ) only if every mention candidate of the tweet overlaps other mention candidates. In practice, the algorithm is nearly linear w.r.t K.</note>

			<note place="foot" n="7"> We plan to release the cleaned data and evaluation code if license permitted.</note>

			<note place="foot" n="8"> We consider features of Base, Capitalization Rate, Popularity, Context Capitalization and Entity Type categories. 9 Our pilot experiments show that the log-loss SMART consistently outperforms the hinge-loss S-MART. 10 Note that the numbers we reported here are different from the results in NEEL challenge due to the fact that we have cleaned the datasets and the evaluation metrics are slightly different in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Asur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1003.5699</idno>
		<title level="m">Predicting the future with social media</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the ACL (EACL)</title>
		<meeting>the European Chapter of the ACL (EACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microposts2014 neel challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Cano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microposts2014 NEEL Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making sense of microposts (# microposts2014) named entity extraction &amp; linking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amparo</forename><forename type="middle">E</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Stankovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aba-Sah</forename><surname>Dadzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Making Sense of Microposts (# Microposts2014)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">14: entity recognition and disambiguation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on Empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning ensembles of structured prediction rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The msr system for entity linking at tac 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Analysis Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training conditional random fields via gradient tree boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Thomas G Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ashenfelter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bulatov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning (ICML)</title>
		<meeting>the twenty-first international conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entity linking on microblogs with spatial and temporal signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TAGME: on-thefly annotation of short text fragments (by Wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jerome H Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">To link or not to link? a study on end-to-end tweet entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1020" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge base population: Successful approaches and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overview of the tac 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><forename type="middle">Ellis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Text Analysis Conference</title>
		<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on Machine learning (ICML)</title>
		<meeting>the 18th international conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mcrank: Learning to rank using multiple classification and gradient boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Entity linking for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1304" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Twittermonitor: trend detection over the twitter stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathioudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Koudas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGMOD International Conference on Management of data (SIGMOD)</title>
		<meeting>the 2010 ACM SIGMOD International Conference on Management of data (SIGMOD)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1155" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adding semantics to microblog posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Web Search and Web Data Mining (WSDM)</title>
		<meeting>International Conference on Web Search and Web Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to link with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosted decision trees as an alternative to artificial neural networks for particle identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Jun</forename><surname>Byron P Roe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Stancu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</title>
		<imprint>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Re-ranking for joint named-entity recognition and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2369" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Roller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentyfirst international conference on Machine learning (ICML)</title>
		<meeting>the twentyfirst international conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adapting boosting for information retrieval measures. Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krysta</forename><forename type="middle">M</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="254" to="270" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
