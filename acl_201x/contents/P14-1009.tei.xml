<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nghia</forename><forename type="middle">The</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="90" to="99"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences. We present here a new model that, like those of Co-ecke et al. (2010) and Baroni and Zam-parelli (2010), closely mimics the standard Montagovian semantic treatment of composition in distributional terms. However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences. We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals. 1 Compositional distributional semantics The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used to represent word meaning in a variety of tasks (Turney and Pantel, 2010). If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors. Developing a practical model of compositionality is still an open issue, which we address in this paper. One approach is to use simple, parameter-free models that perform operations such as point-wise multiplication or summing (Mitchell and La-pata, 2008). Such models turn out to be surprisingly effective in practice (Blacoe and Lap-ata, 2012), but they have obvious limitations. For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order are lost in composition: pandas eat bamboo is identical to bamboo eats pandas. Guevara (2010), Mitchell and Lapata (2010), Socher et al. (2011) and Zanzotto et al. (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. A related approach (Socher et al., 2012) assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister. The training proposed in this model estimates the parameters in a supervised setting. Despite positive empirical evaluation , this approach is hardly practical for general-purpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Compositional distributional semantics</head><p>The research of the last two decades has estab- lished empirically that distributional vectors for words obtained from corpus statistics can be used to represent word meaning in a variety of tasks <ref type="bibr">(Turney and Pantel, 2010)</ref>. If distributional vec- tors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors. De- veloping a practical model of compositionality is still an open issue, which we address in this pa- per. One approach is to use simple, parameter- free models that perform operations such as point- wise multiplication or summing <ref type="bibr" target="#b21">(Mitchell and Lapata, 2008</ref>). Such models turn out to be sur- prisingly effective in practice <ref type="bibr" target="#b6">(Blacoe and Lapata, 2012</ref>), but they have obvious limitations. For instance, symmetric operations like vector addi- tion are insensitive to syntactic structure, there- fore meaning differences encoded in word order are lost in composition: pandas eat bamboo is identical to bamboo eats pandas. <ref type="bibr" target="#b15">Guevara (2010)</ref>, <ref type="bibr" target="#b22">Mitchell and Lapata (2010)</ref>, <ref type="bibr">Socher et al. (2011)</ref> and <ref type="bibr">Zanzotto et al. (2010)</ref> generalize the simple additive model by applying structure-encoding op- erators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. A related approach <ref type="bibr">(Socher et al., 2012</ref>) assumes richer lexical rep- resentations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister. The training proposed in this model estimates the parameters in a super- vised setting. Despite positive empirical evalua- tion, this approach is hardly practical for general- purpose semantic language processing, since it re- quires computationally expensive approximate pa- rameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The lexical function model</head><p>None of the proposals mentioned above, from sim- ple to elaborate, incorporates in its architecture the intuitive idea (standard in theoretical linguistics) that semantic composition is more than a weighted combination of words. Generally one of the com- ponents of a phrase, e.g., an adjective, acts as a function affecting the other component (e.g., a noun). This underlying intuition, adopted from formal semantics of natural language, motivated the creation of the lexical function model of com- position (lf ) ( <ref type="bibr" target="#b3">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b8">Coecke et al., 2010</ref>). The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them ( ). In lf, arguments are vectors and functions taking arguments (e.g., adjectives that combine with nouns) are tensors, with the number of arguments (n) determining the order of tensor (n+1). For example, adjectives, as unary functors, are modeled with 2-way tensors, or matrices. Tensor by vector multiplication formal- izes function application and serves as the general composition method. <ref type="bibr" target="#b3">Baroni and Zamparelli (2010)</ref> propose a practi- cal and empirically effective way to estimate ma- trices representing adjectival modifiers of nouns by linear regression from corpus-extracted exam- ples of noun and adjective-noun vectors. Un- like the neural network approach of <ref type="bibr">Socher et al. (2011;</ref>, the Baroni and Zamparelli method does not require manually labeled data nor costly iterative estimation procedures, as it relies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem.</p><p>The same method was later applied to matrix representations of intransitive verbs and determin- ers ( , al- ways with good empirical results.</p><p>The full range of semantic types required for natural language processing, including those of adverbs and transitive verbs, has to include, how- ever, tensors of greater rank. The estimation method originally proposed by Baroni and Zam- parelli has been extended to 3-way tensors rep- resenting transitive verbs by  with preliminary success. Grefenstette et al.'s method works in two steps. First, one esti- mates matrices of verb-object phrases from sub- ject and subject-verb-object vectors; next, transi- tive verb tensors are estimated from verb-object matrices and object vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Problems with the extension of the lexical function model to sentences</head><p>With all the advantages of lf, scaling it up to ar- bitrary sentences, however, leads to several issues. In particular, it is desirable for all practical pur- poses to limit representation size. For example, if noun meanings are encoded in vectors of 300 dimensions, adjectives become matrices of 300 2 cells, and transitive verbs are represented as ten- sors with 300 3 =27, 000, 000 dimensions. Estimating tensors of this size runs into data sparseness issues already for less common tran- sitive verbs. Indeed, in order to train a transitive verb tensor (e.g., eat), the method of  requires a sufficient number of dis- tinct verb object phrases with that verb (e.g., eat cake, eat fruits), each attested in combination with a certain number of subject nouns with sufficient frequency to extract sensible vectors. It is not fea- sible to obtain enough data points for all verbs in such a training design.</p><p>Things get even worse for other categories. Adverbs like quickly that modify intransitive verbs have to be represented with 300 2 2 = 8, 100, 000, 000 dimensions. Modifiers of transi- tive verbs would have even greater representation size, which may not be possible to store and learn efficiently.</p><p>Another issue is that the same or similar items that occur in different syntactic contexts are as- signed different semantic types with incompara- ble representations. For example, verbs like eat can be used in transitive or intransitive construc- tions (children eat meat/children eat), or in passive (meat is eaten). Since predicate arity is encoded in the order of the corresponding tensor, eat and the like have to be assigned different representa- tions (matrix or tensor) depending on the context. Deverbal nouns like demolition, often used with- out mention of who demolished what, would have to get vector representations while the correspond- ing verbs (demolish) would become tensors, which makes immediately related verbs and nouns in- comparable. Nouns in general would oscillate be- tween vector and matrix representations depend- ing on argument vs. predicate vs. modifier posi- tion (an animal runs vs. this is an animal vs. an- imal shelter). Prepositions are the hardest, as the syntactic positions in which they occur are most diverse (park in the dark vs. play in the dark vs. be in the dark vs. a light glowing in the dark).</p><p>In all those cases, the same word has to be mapped to tensors of different orders. Since each of these tensors must be learned from examples individually, their obvious relation is missed. Be- sides losing the comparability of the semantic con- tribution of a word across syntactic contexts, we also worsen the data sparseness issues.</p><p>The last, and related, point is that for the ten- sor calculus to work, one needs to model, for each word, each of the constructions in the corpus that the word is attested in. In its pure form lf does not include an emergency backoff strategy when unknown words or constructions are encountered. For example, if we only observe transitive usages of to eat in the training corpus, and encounter an intransitive or passive example of it in testing data, the system would not be able to compose a sen- tence vector at all. This issue is unavoidable since we don't expect to find all words in all possible constructions even in the largest corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The practical lexical function model</head><p>As follows from section 1.2, it would be desirable to have a compositional distributional model that encodes function-argument relations but avoids the troublesome high-order tensor representations of the pure lexical function model, with all the practical problems that come with them. We may still want to represent word meanings in differ- ent syntactic contexts differently, but at the same time we need to incorporate a formal connection between those representations, e.g., between the transitive and the intransitive instantiations of the verb to eat. Last but not least, all items need to include a common aspect of their representation (e.g., a vector) to allow comparison across cate- gories (the case of demolish and demolition).</p><p>To this end, we propose a new model of compo- sition that maintains the idea of function applica- tion, while avoiding the complications and rigidity of lf. We call our proposal practical lexical func- tion model, or plf. In plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matri- ces, with one matrix for each argument the func- tion takes. After applying the matrices to the cor- responding argument vectors, a single representa- tion is obtained by summing across all resulting vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word meaning representation</head><p>In plf, all words are represented by a vector, and functional words, such as predicates and modi- fiers, are also assigned one or more matrices. The general form of a semantic representation for a linguistic unit is an ordered tuple of a vector and n ∈ N matrices: <ref type="bibr">1</ref> x,</p><formula xml:id="formula_0">2 1 x , . . . , 2n x</formula><p>The number of matrices in the representation encodes the arity of a linguistic unit, i.e., the num- ber of other units to which it applies as a function. Each matrix corresponds to a function-argument relation, and words have as many matrices as many arguments they take: none for (most) nouns, <ref type="bibr">1</ref> Matrices associated with term x are symbolized  one for adjectives and intransitive verbs, two for transitives, etc. The matrices formalize argument slot saturation, operating on an argument vector representation through matrix by vector multipli- cation, as described in the next section. Modifiers of n-ary functors are represented by n+1-ary structures. For instance, we treat adjec- tives that modify nouns (0-ary) as unary functions, encoded in a vector-matrix pair. Adverbs have dif- ferent semantic types depending on their syntac- tic role. Sentential adverbs are unary, while ad- verbs that modify adjectives (very) or verb phrases (quickly) are encoded as binary functions, repre- sented by a vector and two matrices. The form of semantic representations we are using is shown in <ref type="table">Table 1</ref>. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic composition</head><p>Our system incorporates semantic composition via two composition rules, one for combining struc- tures of different arity and the other for symmet- ric composition of structures with the same ar- ity. These rules incorporate insights of two em- pirically successful models, lexical function and the simple additive approach, used as the default structure merging strategy.</p><p>The first rule is function application, illustrated in <ref type="figure">Figure 1</ref>. <ref type="table" target="#tab_2">Table 2</ref> illustrates simple cases of function application. For transitive verbs seman- tic composition applies iteratively as shown in the derivation of <ref type="figure">Figure 2</ref>. For ternary predicates such x + 2 n+k x × y,  as give in a ditransitive construction, the first step in the derivation absorbs the innermost argument by multiplying its vector by the third give matrix, and then composition proceeds like for transitives. The second composition rule, symmetric com- position applies when two syntactic sisters are of the same arity (e.g., two vectors, or two vector- matrix pairs). Symmetric composition simply sums the objects in the two tuples: vector with vector, n-th matrix with n-th matrix.</p><formula xml:id="formula_1">2 1 x + 2 1 y , . . . , 2n x + 2n y , . . . x</formula><p>Symmetric composition is reserved for struc- tures in which the function-argument distinction is problematic. Some candidates for such treat- ment are coordination and nominal compounds, although <ref type="bibr">we</ref>    <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Note that the sing and dance composition in Ta- ble 3 skips the conjunction. Our current plf im- plementation treats most grammatical words, in- cluding conjunctions, as "empty" elements, that do not project into semantics. This choice leads to some interesting "serendipitous" treatments of various constructions. For example, since the cop- ula is empty, a sentence with a predicative adjec- tive (cars are red) is treated in the same way as a phrase with the same adjective in attributive posi- tion (red cars) -although the latter, being a phrase and not a full sentence, will later be embedded as argument in a larger construction. Similarly, leav- ing the relative pronoun empty makes cars that run identical to cars run, although, again, the for- mer will be embedded in a larger construction later in the derivation.</p><p>We conclude our brief exposition of plf with an alternative intuition for it: the plf model is also a more sophisticated version of the additive ap- proach, where argument words are adapted by ma- trices that encode the relation to their functors be- fore the sentence vector is derived by summing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Satisfying the desiderata</head><p>Let us now outline how plf addresses the short- comings of lf listed in Section 1.2. First, all is- sues caused by representation size disappear. An n-ary predicate is no longer encoded as an n+1- way tensor; instead we have a sequence of n ma- trices. The representation size grows linearly, not exponentially, for higher semantic types, allowing for simpler and more efficient parameter estima- tion, storage, and computation.</p><p>As a consequence of our architecture, we no longer need to perform the complicated step-by- step estimation for elements of higher arity. In- deed, one can estimate each matrix of a com- plex representation individually using the simple method of <ref type="bibr" target="#b3">Baroni and Zamparelli (2010)</ref>. For in- stance, for transitive verbs we estimate the verb- subject combination matrix from subject and verb-  subject vectors, the verb-object combination ma- trix from object and verb-object vectors. We ex- pect a reasonably large corpus to feature many oc- currences of a verb with a variety of subjects and a variety of objects (but not necessarily a variety of subjects with each of the objects as required by Grefenstette et al.'s training), allowing us to avoid the data sparseness issue. The semantic representations we propose in- clude a semantic vector for constituents of any se- mantic type, thus enabling semantic comparison for words of different parts of speech (the case of demolition vs. demolish).</p><p>Finally, the fact that we represent the predicate interaction with each of its arguments in a sepa- rate matrix allows for a natural and intuitive treat- ment of argument alternations. For instance, as shown in <ref type="table" target="#tab_6">Table 4</ref>, one can distinguish the transi- tive and intransitive usages of the verb to eat by the presence of the object-oriented matrix of the verb while keeping the rest of the representation intact. To model passive usages, we insert the ob- ject matrix of the verb only, which will be multi- plied by the syntactic subject vector, capturing the similarity between eat meat and meat is eaten.</p><p>So keeping the verb's interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbi- trary semantic types, but also provides a sensible built-in strategy for handling a word's occurrence in multiple constructions. Indeed, if we encounter a verb used intransitively which was only attested as transitive in the training corpus, we can simply omit the object matrix to obtain a type-appropriate representation. On the other hand, if the verb oc- curs with more arguments than usual in testing materials, we can add a default diagonal identity matrix to its representation, signaling agnosticism about how the verb relates to the unexpected argu- ment. This flexibility makes our model suitable to compute vector representations of sentences with- out stumbling at unseen syntactic usages of words.</p><p>To summarize, plf is an extension of the lexi- cal function model that inherits its strengths and overcomes its weaknesses. We still employ a linguistically-motivated notion of semantic com- position as function application and use distinct kinds of representations for different semantic types. At the same time, we avoid high order ten- sor representations, produce semantic vectors for all syntactic constituents, and allow for an elegant and transparent correspondence between different syntactic usages of a lexeme, such as the transi- tive, the intransitive, and the passive usages of the verb to eat. Last but not least, our implementation is suitable for realistic language processing since it allows to produce vectors for sentences of arbi- trary size, including those containing novel syn- tactic configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation materials</head><p>We consider 5 different benchmarks that focus on different aspects of sentence-level semantic com- position. The first data set, created by Edward Grefenstette and Mehrnoosh Sadrzadeh and in- troduced in , features 200 sentence pairs that were rated for similarity by 43 annotators. In this data set, sentences have fixed adjective-noun-verb-adjective-noun (anvan) structure, and they were built in order to cru- cially require context-based verb disambiguation (e.g., young woman filed long nails is paired with both young woman smoothed long nails and young woman registered long nails). We also consider a similar data set introduced by <ref type="bibr" target="#b13">Grefenstette (2013)</ref>, comprising 200 sentence pairs rated by 50 anno- tators. We will call these benchmarks anvan1 and anvan2, respectively. Evaluation is carried out by computing the Spearman correlation between the annotator similarity ratings for the sentence pairs and the cosines of the vectors produced by the var- ious systems for the same sentence pairs.</p><p>The benchmark introduced by The <ref type="bibr" target="#b9">Pham et al. (2013)</ref> at the TFDS workshop (tfds below) was specifically designed to test compositional meth- ods for their sensitivity to word order and the se- mantic effect of determiners. The tfds benchmark contains 157 target sentences that are matched with a set of (approximate) paraphrases (8 on av-erage), and a set of "foils" (17 on average). The foils have high lexical overlap with the targets but very different meanings, due to different determin- ers and/or word order. For example, the target A man plays an acoustic guitar is matched with paraphrases such as A man plays guitar and The man plays the guitar, and foils such as The man plays no guitar and A guitar plays a man. A good system should return higher similarities for the comparison with the paraphrases with respect to that with the foils. Performance is assessed through the t-standardized cross-target average of the difference between mean cosine with para- phrases and mean cosine with foils (Pham and col- leagues, equivalently, reported non-standardized average and standard deviations).</p><p>The two remaining data sets are larger and more 'natural', as they were not constructed by linguists under controlled conditions to focus on specific phenomena. They are aimed at evaluating sys- tems on the sort of free-form sentences one en- counters in real-life applications. The msrvid data set from the SemEval-2012 Semantic Textual Sim- ilarity (STS) task <ref type="bibr" target="#b0">(Agirre et al., 2012)</ref> consists of 750 sentence pairs that describe brief videos. Sen- tence pairs were scored for similarity by 5 subjects each. Following standard practice in paraphrase detection studies (e.g., <ref type="bibr" target="#b6">Blacoe and Lapata (2012)</ref>), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. We obtain a final similarity score by weighted ad- dition of the 3 cues, with the optimal weights de- termined by linear regression on separate msrvid train data that were also provided by the SemEval task organizers (before combining, we checked that the collinearity between cues was low). Sys- tem scores are evaluated by their Pearson correla- tion with the human ratings.</p><p>The final set we use is onwn, from the *SEM- 2013 STS shared task ( <ref type="bibr" target="#b1">Agirre et al., 2013</ref>). This set contains 561 pairs of glosses (from the Word- Net and OntoNotes databases), rated by 5 judges for similarity. Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences (consider, e.g., cause something to pass or lead somewhere; coerce by violence, fill with terror). For this reason, they are very chal- lenging for standard parsers. Indeed, we estimated from a sample of 40 onwn glosses that the C&amp;C parser (see below) has only 45% accuracy on this set. Since plf needs syntactic information to con- struct sentence vectors compositionally, we test it on onwn to make sure that it is not overly sensi- tive to parser noise. Evaluation proceeds as with msrvid (cue weights are determined by 10-fold cross-validation). <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic space construction and composition model implementation</head><p>Our source corpus was given by the concatena- tion of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English Wikipedia (en. wikipedia.org) and the British National Cor- pus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words.</p><p>We collected a 30K-by-30K matrix by counting co-occurrence of the 30K most frequent content lemmas (nouns, adjectives and verbs) within a 3- word window. The raw count vectors were trans- formed into positive Pointwise Mutual Informa- tion scores and reduced to 300 dimensions by the Singular Value Decomposition. All vectors were normalized to length 1. This setup was picked without tuning, as we found it effective in previ- ous, unrelated experiments. <ref type="bibr">4</ref> We consider four composition models. The add (additive) model produces the vector of a sentence by summing the vectors of all content words in it. Similarly, mult uses component-wise multiplica- tion of vectors for composition. While these mod- els are very simple, a long experimental tradition has proven their effectiveness <ref type="bibr" target="#b20">(Landauer and Dumais, 1997;</ref><ref type="bibr" target="#b21">Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b22">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b6">Blacoe and Lapata, 2012)</ref>.</p><p>For the lf (lexical function) model, we construct functional matrix representations of adjectives, de- terminers and intransitive verbs. These are trained using Ridge regression with generalized cross- validation from corpus-extracted vectors of nouns, as input, and phrases including those nouns as out- put (e.g., the matrix for red is trained from corpus- extracted noun, red-noun vector pairs). Transi- tive verb tensors are estimated using the two-step regression procedure outlined by <ref type="bibr" target="#b13">Grefenstette et al. (2013)</ref>. We did not attempt to train a lf model for the larger and more varied msrvid and onwn data sets, as this would have been extremely time consuming and impractical for all the reasons we discussed in Section 1.2 above.</p><p>Training plf (practical lexical function) pro- ceeds similarly, but we also build preposition matrices (from noun, preposition-noun vector pairs), and for verbs we prepare separate subject and object matrices.</p><p>Since syntax guides lf and plf composition, we supplied all test sentences with categorial gram- mar parses. Every sentence in the anvan1 and anvan2 datasets has the form (subject) Adjective + Noun + Transitive Verb + (object) Adjective + Noun, so parsing them is trivial. All sentences in tfds have a predictable structure that allows per- fect parsing with simple finite state rules. In all these cases, applying a general-purpose parser to the data would have, at best, had no impact and, at worst, introduced parsing errors. For msrvid and onwn, we used the output of the C&amp;C parser (Clark and Curran, 2007). <ref type="table">Table 5</ref> summarizes the performance of our mod- els on the chosen tasks, and compares it to the state of the art reported in previous work, as well as to various strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The plf model performs very well on both an- van benchmarks, outperforming not only add and mult, but also the full-fledged lf model. Given that these data sets contain, systematically, transi- tive verbs, the major difference between plf and lf lies in their representation of the latter. Evidently, the separately-trained subject and object matrices of plf, being less affected by data sparseness than the 3-way tensors of lf, are better able to capture how verbs interact with their arguments. For an- van1, plf is just below the state of the art, which is based on disambiguating the verb vector in con- text , and lf out- performs the baseline, which consists in using the verb vector only as a proxy to sentence similar- ity. <ref type="bibr">5</ref> On anvan2, plf outperforms the best model <ref type="bibr">5</ref> We report state of the art from Kartsaklis and Sadrzadeh models anvan anvan tfds msr <ref type="table" target="#tab_2">onwn  1  2  vid  add  8  22  -0.2 78  66  mult  8  -4  -2.3 77  55  lf  15</ref>    reported by <ref type="bibr" target="#b13">Grefenstette (2013)</ref> (an implementa- tion of the lexical function ideas along the lines of <ref type="bibr" target="#b10">Grefenstette and Sadrzadeh (2011a;</ref><ref type="bibr" target="#b11">2011b)</ref>). And lf is, again, the only model, besides plf, that per- forms better than the baseline. In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish be- tween true paraphrases and foils (indeed, for the mult model foils are significantly closer to the tar- gets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determin- ers only). Our plf approach is able to handle deter- miners and word order correctly, as demonstrated by a highly significant (p &lt; 0.01) difference be- tween paraphrase and foil similarity (average dif- ference in cosine .017, standard deviation .077). In this case, however, the traditional lf model (aver- age difference .044, standard deviation .092) out- performs plf. Since determiners are handled iden- tically under the two approaches, the culprit must be word order. We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with in- (2013) rather than , since only the for- mer used a source corpus that is comparable to ours. verted arguments, and thus makes this model par- ticularly sensitive to word order differences. In- deed, if we limit evaluation to those foils charac- terized by word order changes only, lf discrim- inates between paraphrases and foils even more clearly, whereas the plf difference, while still sig- nificant, decreases slightly.</p><p>The state-of-the-art row for tfds reports the lf implementation by The <ref type="bibr" target="#b9">Pham et al. (2013)</ref>, which outperforms ours. The main difference is that Pham and colleagues do not normalize vectors like we do. If we don't normalize, we do get larger dif- ferences for our models as well, but consistently lower performance in all other tasks. More wor- ryingly, the simple word overlap baseline reported in the table sports a larger difference than our best model. Clearly, this baseline is exploiting the sys- tematic determiner differences in the foils and, in- deed, when it is evaluated on foils where only word order changes its performance is no longer significant.</p><p>On msrvid, the plf approach outperforms add and mult, although the difference between the three is not big. Our result stands in contrast with <ref type="bibr" target="#b6">Blacoe and Lapata (2012)</ref>, the only study we are aware of that compared a sophisticated composi- tion model <ref type="bibr">(Socher et al.'s 2011 model)</ref> to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. <ref type="bibr">6</ref> The best 2012 STS system <ref type="bibr">(Bär et al., 2012)</ref>, obtained 0.87 correla- tion, but with many more and considerably more complex features than the ones we used here. In- deed, our simple system would have obtained a re- spectable 25/89 ranking in the STS 2012 msrvid task. Still, we must also stress the impressive per- formance of our baseline, given by the combina- tion of the word overlap and sentence length cues. This suggests that the msrvid benchmark lacks the lexical and syntactic variety we would like to test our systems on.</p><p>Our plf model is again the best on the onwn set (albeit by a small margin over add). This is a very positive result, in the light of the fact that the parser has very low performance on the onwn glosses, thus suggesting that plf can pro- duce sensible semantic vectors from noisy syntac- <ref type="bibr">6</ref> We refer here to the results reported in the er- ratum available at http://homepages.inf.ed.ac. uk/s1066731/pdf/emnlp2012erratum.pdf. The add/mult advantage was even more marked in the original pa- per. tic representations. Here the overlap+length base- line does not perform so well, and again the best STS 2013 system ( <ref type="bibr">Han et al., 2013</ref>) uses consider- ably richer knowledge sources and algorithms than ours. Our plf-based method would have reached a respectable 20/90 rank in the STS 2013 onwn task.</p><p>As a final remark, in all experiments the running time of plf was only slightly larger than for the simpler models, but orders of magnitude smaller than lf, confirming another practical side of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduced an approach to compositional dis- tributional semantics based on a linguistically- motivated syntax-to-semantics type mapping, but simple and flexible enough that it can produce rep- resentations of English sentences of arbitrary size and structure.</p><p>We showed that our approach is competitive against the more complex lexical function model when evaluated on the simple constructions the latter can be applied to, and it outperforms the ad- ditive and multiplicative compositionality models when tested on more realistic benchmarks (where the full-fledged lexical function approach is dif- ficult or impossible to use), even in presence of strong noise in its syntactic input. While our re- sults are encouraging, no current benchmark com- bines large-scale, real-life data with the syntactic variety on which a syntax-driven approach to se- mantics such as ours could truly prove its worth. The recently announced SemEval 2014 Task 1 7 is filling exactly this gap, and we look forward to ap- ply our method to this new benchmark, as soon as it becomes available.</p><p>One of the strengths of our framework is that it allows for incremental improvement focused on specific constructions. For example, one could add representations for different conjunctions <ref type="bibr">(and vs. or)</ref>, train matrices for verb arguments other than subject and direct object, or include new types of modifiers into the model, etc.</p><p>While there is potential for local improvements, our framework, which extends and improves on existing compositional semantic vector models, has demonstrated its ability to account for full sen- tences in a principled and elegant way. Our imple- mentation of the model relies on simple and effi-cient training, works fast, and shows good empiri- cal results. <ref type="bibr">Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng, and Christopher Manning. 2011</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 :</head><label>5</label><figDesc>Performance of composition models on all evaluation sets. Figures of merit follow previ- ous art on each set and are: percentage Spearman coefficients for anvan1 and anvan2, t-standardized average difference between mean cosines with paraphrases and with foils for tfds, percentage Pearson coefficients for msrvid and onwn. State- of-the-art (soa) references: anvan1: Kartsaklis and Sadrzadeh (2013); anvan2: Grefenstette (2013); tfds: The Pham et al. (2013); msrvid: Bär et al. (2012); onwn: Han et al. (2013). Baselines: anvan1/anvan2: verb vectors only; tfds: word overlap; msrvid/onwn: word overlap + sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Examples of function application.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Examples of symmetric composition.</head><label>3</label><figDesc></figDesc><table>not the only possible one here. See two examples 
of Symmetric Composition application in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The verb to eat associated to different sets 
of matrices in different syntactic contexts. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> To determine the number and ordering of matrices representing the word in the current syntactic context, our plf implementation relies on the syntactic type assigned to the word in the categorial grammar parse of the sentence.</note>

			<note place="foot" n="3"> We did not evaluate on other STS benchmarks since they have characteristics, such as high density of named entities, that would require embedding our compositional models into more complex systems, obfuscating their impact on the overall performance. 4 With the multiplicative composition model we also tried Nonnegative Matrix Factorization instead of Singular Value Decomposition, because the negative values produced by SVD are potentially problematic for mult. In addition, we repeated the evaluation for the multiplicative and additive models without any form of dimensionality reduction. The overall pattern of results did not change significantly, and thus for consistency we report all models&apos; performance only for the SVD-reduced space.</note>

			<note place="foot" n="7"> http://alt.qcri.org/semeval2014/ task1/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Roberto Zamparelli and the COM-POSES team for helpful discussions. This re-search was supported by the ERC 2011 Starting Independent Research Grant n. 283554 (COM-POSES).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 6: a pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM</title>
		<meeting>*SEM<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">SEM 2013 shared task: Semantic Textual Similarity</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Proceedings of *SEM</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UKP: Computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM</title>
		<meeting>*SEM<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<ptr target="http://clic.cimec.unitn.it/composes/materials/frege-in-space.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A relatedness benchmark to test the role of determiners in compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL (Short Papers)</title>
		<meeting>ACL (Short Papers)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="53" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Widecoverage efficient statistical parsing with CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="345" to="384" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">General estimation and evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>ACL Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experimenting with transitive verbs in a DisCoCat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GEMS</title>
		<meeting>GEMS<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-step regression learning for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Zhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWCS</title>
		<meeting>IWCS<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="131" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Category-Theoretic</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Quantitative Compositional Distributional Models of Natural Language Semantics</title>
		<imprint/>
		<respStmt>
			<orgName>University of Oxford Essex</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A regression model of adjective-noun compositionality in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano</forename><surname>Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GEMS</title>
		<meeting>GEMS<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lushan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic textual similarity systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umbc</forename><surname>Ebiquity-Core</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM</title>
		<meeting>*SEM<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prior disambiguation of word tensors for constructing sentence vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1590" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Separating disambiguation from composition in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A solution to Plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
