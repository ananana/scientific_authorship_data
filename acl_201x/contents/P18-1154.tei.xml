<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gangal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1661" to="1671"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1661</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A variety of work in NLP has sought to produce fluent natural language descriptions conditioned on a contextual grounding. For example, several lines of work explore methods for describing im- ages of scenes and videos <ref type="bibr" target="#b5">(Karpathy and Fei-Fei, 2015)</ref>, while others have conditioned on structured sources like Wikipedia infoboxes (Lebret et al., * HJ and VG contributed equally for this paper <ref type="bibr">1</ref> We will make the code-base (including data collection and processing) publicly available at https://github. com/harsh19/ChessCommentaryGeneration 2016). In most cases, progress has been driven by the availability of large training corpora that pair natural language with examples from the ground- ing ( <ref type="bibr" target="#b12">Lin et al., 2014)</ref>. One line of work has in- vestigated methods for producing and interpreting language in the context of a game, a space that has rich pragmatic structure, but where training data has been hard to come by. In this paper, we in- troduce a new large-scale resource for learning to correlate natural language with individual moves in the game of chess. We collect a dataset of more than 298K chess move/commentary pairs across â‰ˆ 11K chess games from online chess forums. To the best of our knowledge, this is the first such dataset of this scale for a game commentary generation task. We provide an analysis of the dataset and highlight the large variety in commentary texts by categorizing them into six different aspects of the game that they respectively discuss. Automated game commentary generation can be a useful learning aid. Novices and experts alike can learn more about the game by hearing expla-nations of the motivations behind moves, or their quality. In fact, on sites for game aficionados, these commentaries are standard features, speak- ing to their interestingness and utility as comple- ments to concrete descriptions of the game boards themselves.</p><p>Game commentary generation poses a number of interesting challenges for existing approaches to language generation. First, modeling human commentary is challenging because human com- mentators rely both on their prior knowledge of game rules as well as their knowledge of effec- tive strategy when interpreting and referring to the game state. Secondly, there are multiple aspects of the game state that can be talked about for a given move -the commentator's choice depends on the pragmatic context of the game. For example, for the move shown in <ref type="figure" target="#fig_0">Figure 1</ref>, one can comment simply that the pawn was moved, or one may com- ment on how the check was blocked by that move. Both descriptions are true, but the latter is most salient given the player's goal. However, some- times, none of the aspects may stand out as being most salient, and the most salient aspect may even change from commentator to commentator. More- over, a human commentator may introduce varia- tions in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary. This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language ( <ref type="bibr" target="#b13">Liu et al., 2016</ref>).</p><p>Prior work has explored game commentary gen- eration. <ref type="bibr" target="#b11">Liao and Chang (1990)</ref>; <ref type="bibr" target="#b27">Sadikov et al. (2006)</ref> have explored chess commentary genera- tion, but for lack of large-scale training data their methods have been mainly rule-based. <ref type="bibr" target="#b4">Kameko et al. (2015)</ref> have explored commentary gener- ation for the game of Shogi, proposing a two- step process where salient terms are generated from the game state and then composed in a language model. In contrast, given the larger amount of training data available to us, our pro- posed model uses an end-to-end trainable neu- ral architecture to predict commentaries given the game state. Our model conditions on semantic and pragmatic information about the current state and explicitly learns to compose, conjoin, and se- lect these features in a recurrent decoder module. We perform an experimental evaluation compar- ing against baselines and variants of our model that ablate various aspects of our proposed archi-  tecture. Outputs on the 'Move Description' subset of data from our final model were judged by hu- mans to be as good as human written ground truth commentaries on measures of fluency and correct- ness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Chess Commentary Dataset</head><p>In this section we introduce our new large-scale Chess Commentary dataset, share some statistics about the data, and discuss the variety in type of commentaries. The data is collected from the online chess discussion forum gameknot.com, which features multiple games self-annotated with move-by-move commentary. The dataset consists of 298K aligned game move/commentary pairs. Some commentaries are written for a sequence of few moves <ref type="figure" target="#fig_1">(Figure 2</ref>) while others correspond to a single move. For the purpose of initial analysis and modeling, we limit ourselves to only those data points where com- mentary text corresponds to a single move. Addi- tionally, we split the multi-sentence commentary texts to create multiple data points with the same chess board and move inputs.</p><p>What are commentaries about? We observe that there is a large variety in the commentary  texts. To analyze this variety, we consider la- belling the commentary texts in the data with a predefined set of categories. The choice of these categories is made based on a manual inspection of a sub-sample of data. We consider the follow- ing set of commentary categories (Also shown in <ref type="table" target="#tab_2">Table 2</ref>):</p><p>â€¢ Direct move description (MoveDesc 3 ): Ex- plicitly or implicitly describe the current move.</p><p>â€¢ Quality of move (Quality 4 ): Describe the quality of the current move.</p><p>â€¢ Comparative: Compare multiple possible moves.</p><p>â€¢ Move Rationale or Planning (Planning): Describe the rationale for the current move, in terms of the future gameplay, advantage over other potential moves etc.</p><p>â€¢ Contextual game information: Describe not the current move alone, but the overall game state -such as possibility of win/loss, overall aggression/defence, etc.</p><p>â€¢ General information: General idioms &amp; ad- vice about chess, information about play- ers/tournament, emotional remarks, retorts, etc.</p><p>The examples in <ref type="table" target="#tab_2">Table 2</ref> illustrate these classes. Note that the commentary texts are not necessar- ily limited to one tag, though that is true for most <ref type="bibr">3</ref> MoveDesc &amp; 'Move Description' used interchangeably 4 Quality and 'Move Quality' used interchangeably of the data. A total of 1K comments are anno- tated by two annotators. A SVM classifier <ref type="bibr" target="#b19">(Pedregosa et al., 2011a</ref>) is trained for each comment class, considering the annotation as ground truth and using word unigrams as features. This classi- fier is then used to predict tags for the train, valida- tion and test sets. For "Comparative" category, we found that a classifier with manually defined rules such as presence of word "better" performs better than the classifier, perhaps due to the paucity of data, and thus we use this instead . As can be ob- served in <ref type="table" target="#tab_2">Table 2</ref>, the classifiers used are able to generalize well on the held out dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Game Aware Neural Commentary</head><p>Generations (GAC)</p><p>Our dataset D consists of data points of the form</p><formula xml:id="formula_0">(S i , M i , G i ), i âˆˆ {1, 2, .</formula><p>., |D|}, where S i is the commentary text for move M i and G i is the corresponding chess game. S i is a se- quence of m tokens S i1 , S i2 , ..., S im We want to model P (S i |M i , G i ). For simplicity, we use only current board (C i ) and previous board (R i ) information from the game.</p><formula xml:id="formula_1">P (S i |M i , G i ) = P (S i |M i , C i , R i ).</formula><p>We model this using an end-to-end trainable neural model, which models conjunctions of fea- tures using feature encoders. Our model employs a selection mechanism to select the salient fea- tures for a given chess move. Finally a LSTM recurrent neural network <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997</ref>) is used to generate the commentary text based on selected features from encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Incorporating Domain Knowledge</head><p>Past work shows that acquiring domain knowledge is critical for NLG systems <ref type="bibr" target="#b26">(Reiter et al., 2003b;</ref><ref type="bibr" target="#b14">Mahamood and Reiter, 2012)</ref>. Commentary texts cover a range of perspectives, including criticism or goodness of current move, possible alternate moves, quality of alternate moves, etc. To be able to make such comments, the model must learn about the quality of moves, as well as the set of valid moves for a given chess board state. We con- sider the following features to provide our model with necessary information to generate commen- tary texts ( <ref type="figure">Figure 3</ref>):</p><p>Move features f move (M i , C i , R i ) encode the current move information such as which piece moved, the position of the moved piece before and after the move was made, the type and position <ref type="figure">Figure 3</ref>: The figure shows some features extracted using the chess board states before (left) and after (right) a chess move. Our method uses various semantic and pragmatic features of the move, including the location and type of piece being moved, which opposing team pieces attack the piece being moved before as well as after the move, the change in score by Stockfish UCI engine, etc.</p><p>of the captured piece (if any), whether the current move is castling or not, and whether there was a check or not.</p><p>Threat features f threat (M i , C i , R i ) encode in- formation about pieces of opposite player attack- ing the moved piece before and after the move, and the pieces of opposite player being attacked by the piece being moved. To extract this information, we use the python-chess library <ref type="bibr">5</ref> Score features f score (M i , C i , R i ) capture the quality of move and general progress of the game. This is done using the game evaluation score be- fore and after the move, and average rank of pawns of both the players. We use Stockfish evaluation engine to obtain the game evaluation scores. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Representation</head><p>In our simplest conditioned language gen- eration model GAC-sparse, we repre- sent the above described features using sparse representations through binary- valued features.</p><formula xml:id="formula_2">g sparse (M i , C i , R i ) = SparseRep(f move , f threat , f score )</formula><p>For our full GAC model we consider repre- senting features through embeddings. This has the advantage of allowing for a shared embed- ding space, which is pertinent for our problem since attribute values can be shared, e.g. the same piece type can occur as the moved piece as well as the captured piece. For categorical fea- tures, such as those indicating which piece was moved, we directly look up the embedding us- ing corresponding token. For real valued features such as game scores, we first bin them and then use corresponding number for embedding lookup. Let E represent the embedding matrix. Then E[f j move ] represents embeddings of j th move fea- ture, or in general E[f move ] represents the con- catenated embeddings of all move features. Simi- larly, E(f move , f threat , f score ) represents concate- nated embeddings of all the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Conjunctions</head><p>We conjecture that explicitly modeling feature conjunctions might improve the performance. So we need an encoder which can handle input sets of features of variable length (features such as pieces attacking the moved piece can be of vari- able length). One way to handle this is by picking up a canonical ordering of the features and con- sider a bidirectional LSTM encoder over the fea- ture embeddings. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, this gen- erates conjunctions of features.</p><formula xml:id="formula_3">g enc = BiLSTM * ({E(f move , f threat , f score ))})</formula><p>Here E() represents the embedding matrix as described earlier and BiLST M * represents a se- quential application of the BiLST M function. Thus, if there a total of m feature keys and em- bedding dimension is d, E(f move , f threat , f score ) is matrix of m * d. If hidden size of BILSTM is of size x, then g enc is of dimensionality m * x. We observe that different orderings gave similar performance. We also experimented with running k encoders, each on different ordering of features, and then letting the decoder access to each of the k encodings. This did not yield any significant gain in performance.</p><p>The GAC model, unlike GAC-sparse, has some advantages as it uses a shared, continuous space We observe that feeding in feature conjunctions helps a lot. We consider a selection mechanism for the model to choose salient attributes from the input at every decoder step.</p><p>to embed attribute values of different features, and can perform arbitrary feature conjunctions before passing a representation to the decoder, thereby sharing the burden of learning the necessary fea- ture conjunctions. Our experiments confirm this intuition -GAC produces commentaries with higher BLEU as well as more diversity compared to GAC-sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoder</head><p>We use a LSTM decoder to generate the sentence given the chess move and the features g. At every output step t, the LSTM decoder predicts a distri- bution over vocabulary words taking into account the current hidden state h t , the input token i t , and additional selection vector c t . For GAC-sparse, the selection vector is simply an affine transfor- mation of the features g. For GAC model selection vector is derived via a selection mechanism.</p><formula xml:id="formula_4">o t , h dec t = LST M (h dec tâˆ’1 , [concat(E dec (i t ), c t )]) p t = softmax(W o [concat(o t , c t )] + b s )</formula><p>where p t represents th probability distribution over the vocabulary, E dec () represents the decoder word embedding matrix and elements of W o ma- trix are trainable parameters.</p><p>Selection/Attention Mechanism: As there are different salient attributes across the different chess moves, we also equip the GAC model with a mechanism to select and identify these attributes. We first transform h dec t by multiplying it with a trainable matrix W c , and then take dot product of the result with each g i .</p><formula xml:id="formula_5">a (i) t = dot(W c * h dec t , g enc i ) Î± t = softmax(a t ) c t = i=|g| i=1 Î± (i) t g enc i</formula><p>We use cross-entropy loss over the decoding outputs to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We split each of the data subsets in a 70:10:20 ra- tio into train, validation and test. All our models are implemented in Pytorch version 0.3.1 ( <ref type="bibr" target="#b18">Paszke et al., 2017</ref>). We use the ADAM optimizer ( <ref type="bibr" target="#b8">Kingma and Ba, 2014</ref>) with its default parame- ters and a mini-batch size of 32. Validation set perplexity is used for early-stopping. At test-time, we use greedy search to generate the model output. We observed that beam decoding does not lead to any significant improvement in terms of validation BLEU score.</p><p>We observe the BLEU ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>) and BLEU-2 ( <ref type="bibr" target="#b30">Vedantam et al., 2015</ref>) scores to measure the performance of the models. Addi-tionally, we consider a measure to quantify the di- versity in the generated outputs. Finally, we also conduct a human evaluation study. In the remain- der of this section, we discuss baselines along with various experiments and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>In this subsection we discuss the various baseline methods.</p><p>Manually-defined template (TEMP) We devise manually defined templates <ref type="bibr" target="#b21">(Reiter, 1995)</ref> for 'Move Description' and 'Move Quality' cate- gories. Note that template-based outputs tend to be repetitive as they lack diversity -drawing from a small, fixed vocabulary and using a largely static sentence structure. We define templates for a fixed set of cases which cover our data (For exact template specifications, refer to Appendix B).</p><p>Nearest Neighbor (NN): We observe that the same move on similar board states often leads to similar commentary texts. To construct a simple baseline, we find the most similar move N M CR from among training data points for a given previ- ous (R) and current (C) board states and move M . The commentary text corresponding to N M CR is selected as the output. Thus, we need to consider a scoring function to find the closest matching data point in training set. We use the Move, Threat and Score features to compute similarity to do so. By using a sparse representation, we consider total of 148 Move features, 18 Threat features, and 19 Score features. We use sklearn's ( <ref type="bibr" target="#b20">Pedregosa et al., 2011b</ref>) NearestNeighbor module to find the closest matching game move.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Board Information Only (RAW):</head><p>The RAW baseline ablates to assess the importance of our pragmatic feature functions. This archi- tecture is similar to GAC, except that instead of our custom features A(f (R i , C i )), the encoder en- codes raw board information of current and previ- ous board states.</p><formula xml:id="formula_6">A RAW (R i , C i ) = [Lin(R i ), Lin(C i )]</formula><p>Lin() for a board denotes it's representation in a row-linear fashion. Each element of Lin() is a piece name (e.g pawn) denoting the piece at that square with special symbols for empty squares.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comment Category Models</head><p>As shown earlier, we categorize comments into six different categories. Among these, in this paper  we consider only the first three as the amount of variance in the last three categories indicates that it would be extremely difficult for a model to learn to reproduce them accurately. The number of data points, as tagged by the trained classifiers, in the subsets 'Move Description', 'Move Quality' and 'Comparative' are 28,228, 793 and 5397 respectively. We consider separate commentary generation models for each of the three categories. Each model is tuned separately on the correspond- ing validation sets. <ref type="table" target="#tab_4">Table 3</ref> shows the BLEU and BLEU-2 scores for the proposed model under different subsets of features. Overall BLEU scores are low, likely due to the inherent variance in the language generation task ( <ref type="bibr" target="#b16">Novikova et al., 2017)</ref> , although a precursory examination of the outputs for data points selected randomly from test set indicated that they were reasonable. <ref type="figure">Figure 5</ref> illustrates commentaries generated by our models through an example (a larger list of qualitative examples can be found in Appendix C).  features directly encode proxies for move quality as per a chess evaluation engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Single Model For All Categories</head><p>In this experiment, we merge the training and val- idation data of the first three categories and tune a single model for this merged data. We then com- pare its performance on all test sentences in our data. COMB denotes using the best GAC model for a test example based on its original class (e.g Desc) and computing the BLEU of the sentences so generated with the ground truth. GAC-all rep- resents the GAC model learnt on the merged train- ing data. As can be seen from <ref type="table" target="#tab_8">Table 5</ref>, this does not lead to any performance improvements. We investigate this issue further by analyzing whether the board states are predictive of the type of category or not. To achieve this, we construct a multi-class classifier using all the Move, Threat and Score features to predict the three categories under consideration. However, we observe accuracy of around 33.4%, which is very close to the performance of a random prediction model. This partially explains why a single model did not fare better even though it had the opportunity to learn  from a larger dataset.</p><p>Category-aware model (CAT) We observed above that with the considered features, it is not possible to predict the type of comment to be made, and the GAC-all model results are better than COMB results. Hence, we extend the GAC- all model to explicitly provide with the informa- tion about the comment category. We achieve this by adding a one-hot representation of the category of the comment to the input of the RNN decoder at every time step. As can be seen in the <ref type="table" target="#tab_8">Table  5</ref>, CAT(M) performs better than GAC-all(M) in terms of BLEU-4, while performing slightly worse on BLEU-2. This demonstrates that explicitly pro- viding information about the comment category can help the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Diversity In Generated Commentaries</head><p>Humans use some variety in the choice of words and sentence structure. As such, outputs from rule based templates, which demonstrate low variety, may seem repetitive and boring. To capture this quantitatively, and to demonstrate the variety in texts from our method, we calculate the entropy <ref type="bibr" target="#b28">(Shannon, 1951)</ref> of the distribution of unigrams, bigrams and trigrams of words in the predicted outputs, and report the geometric mean of these values. Using only a small set of words in similar counts will lead to lower entropy and is undesir- able. As can be observed from <ref type="table" target="#tab_4">Table 3</ref>, template baseline performs worse on the said measure com- pared to our methods for the 'MoveDesc' subset of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Evaluation Study</head><p>As discussed in the qualitative examples above, we often found the outputs to be good -though BLEU scores are low. BLEU is known to cor- relate poorly <ref type="bibr" target="#b23">(Reiter and Belz, 2009;</ref><ref type="bibr" target="#b31">Wiseman et al., 2017;</ref><ref type="bibr" target="#b16">Novikova et al., 2017</ref>) with human relevance scores for NLG tasks. Hence, we conduct a human evaluation study for the best 2 neural (GAC,GAC-sparse) and best 2 non-neural methods (TEMP,NN).</p><p>Setup: Specifically, annotators are shown a chess move through previous board and resulting board snapshots, along with information on which piece moved (a snapshot of a HIT 7 is provided in the Ap- pendix D). With this context, they were shown text commentary based on this move and were asked to judge the commentary via three questions, short- ened versions of which can be seen in the first col- umn of <ref type="table" target="#tab_10">Table 6</ref>. We randomly select 100 data points from the test split of 'Move Description' category and collect the predictions from each of the methods under consideration. We hired two Anglophone (Lifetime HIT acceptance % &gt; 80) annotators for every human-evaluated test example. We addi- tionally assess chess proficiency of the annotators using questions from the chess-QA dataset by <ref type="bibr" target="#b1">(Cirik et al., 2015)</ref>. Within each HIT, we ask two randomly selected questions from the chess-QA dataset. Finally we consider only those HITs wherein the annotator was able to answer the proficiency questions correctly.</p><p>Results: We conducted a human evaluation study for the MoveDesc subset of the data. As can be observed from <ref type="table" target="#tab_10">Table 6</ref>, outputs from our method attain slightly more favorable scores compared to the ground truth commentaries. This shows that the predicted outputs from our model are not worse than ground truth on the said measures. This is in spite of the fact that the BLEU-4 score for the predicted outputs is only âˆ¼ 2 w.r.t. the ground truth outputs. One reason for slightly lower performance of the ground truth outputs on the said measures is that some of the human writ- 7 Human Intelligence Task ten commentaries are either very ungrammatical or too concise. A more surprising observation is that around 30% of human written ground truth outputs were also marked as not valid for given board move. On inspection, it seems that com- mentary often contains extraneous game informa- tion beyond that of move alone, which indicates that an ideal comparison should be over commen- tary for an entire game, although this is beyond the scope of the current work.</p><p>The inter-annotator agreement for our experi- ments <ref type="bibr">(Cohens Îº (Cohen, 1968)</ref>) is 0.45 for Q1 and 0.32 for Q2. We notice some variation in Îº coefficients across different systems. While TEMP and GAC responses had a 0.5-0.7 coeffi- cient range, the responses for CLM had a much lower coefficient. In our setup, each HIT consists of 7 comments, one from each system. For Q3 (fluency), which is on an ordinal scale, we mea- sure rank-order consistency between the responses of the two annotators of a HIT. Mean Kendall Ï„ ( <ref type="bibr" target="#b6">Kendall, 1938)</ref> across all HITs was found to be 0.39.</p><p>To measures significance of results, we per- form bootstrap tests on 1000 subsets of size 50 with a significance threshold of p = 0.05 for each pair of systems. For Q1, we observe that GAC(M), GAC(M+T) and GAC(M+T+S) meth- ods are significantly better than baselines NN and GAC-sparse. We find that neither of GAC(M+T) and GT significantly outperform each other on Q1 as well as Q2. But we do find that GAC(M+T) does better than GAC(M) on both Q1 and Q2. For fluency scores, we find that GAC(M+T) is more fluent than GT, NN , GAC-sparse, GAC(M). Neither of GAC(M) and GAC(M+T+S) is signifi- cantly more fluent than the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>NLG research has a long history, with systems ranging from completely rule-based to learning- based ones <ref type="bibr" target="#b25">(Reiter et al., 2005</ref><ref type="bibr" target="#b24">(Reiter et al., , 2003a</ref>, which have had both practical successes <ref type="bibr" target="#b25">(Reiter et al., 2005</ref>) and failures <ref type="bibr" target="#b24">(Reiter et al., 2003a)</ref>. Recently, there have been numerous works which propose text generation given structured records, biogra- phies ( <ref type="bibr" target="#b9">Lebret et al., 2016</ref>), recipes ( <ref type="bibr" target="#b32">Yang et al., 2016;</ref><ref type="bibr" target="#b7">Kiddon et al., 2016)</ref>, etc. A key difference between generation given a game state compared to these inputs is that the game state is an evolv- ing description at a point in a process, as opposed  to recipes (which are independent of each other), records (which are static) and biographies (which are one per person, and again independent). More- over, our proposed method effectively uses vari- ous types of semantic and pragmatic information about the game state. In this paper we have introduced a new large- scale data for game commentary generation. The commentaries cover a variety of aspects like move description, quality of move, and alternative moves. This leads to a content selection challenge, similar to that noted in <ref type="bibr" target="#b31">Wiseman et al. (2017)</ref>. Un- like <ref type="bibr" target="#b31">Wiseman et al. (2017)</ref>, our focus is on gener- ating commentary for individual moves in a game, as opposed to game summaries from aggregate statistics as in their task.</p><p>One of the first NLG datasets was the SUMTIME-METEO ( <ref type="bibr" target="#b25">Reiter et al., 2005</ref>) corpus with â‰ˆ 500 record-text pairs for technical weather forecast generation. <ref type="bibr" target="#b10">Liang et al (2009)</ref> worked on common weather forecast generation using the WEATHERGOV dataset, which has â‰ˆ 10K record-text pairs. A criticism of WEATHER- GOV dataset <ref type="bibr" target="#b22">(Reiter, 2017)</ref> is that weather records themselves may have used templates and rules with optional human post-editing. There have been prior works on generating commentary for ROBOCUP matches <ref type="bibr" target="#b0">(Chen and Mooney, 2008;</ref><ref type="bibr" target="#b15">Mei et al., 2015</ref>). The ROBOCUP dataset, how- ever, is collected from 4 games and contains about 1K events in total. Our dataset is two orders of magnitude larger than the ROBOCUP dataset, and we hope that it provides a promising setting for future NLG research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we curate a dataset for the task of chess commentary generation and propose meth- ods to perform generation on this dataset. Our proposed method effectively utilizes information related to the rules and pragmatics of the game. A human evaluation study judges outputs from the proposed methods to be as good as human written commentary texts for 'Move Description' subset of the data.</p><p>Our dataset also contains multi-move-single commentary pairs in addition to single move- single commentary pairs. Generating commentary for such multi-moves is a potential direction for future work. We anticipate this task to require even deeper understanding of the game pragmat- ics than the single move-single commentary case.</p><p>Recent work <ref type="bibr" target="#b29">(Silver et al., 2016</ref>) has proposed reinforcement learning based game-playing agents which learn to play board games from scratch, learning end-to-end from both recorded games and self-play. An interesting point to explore is whether such pragmatically trained game state representations can be leveraged for the task of game commentary generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Move commentary generated from our method (Game-aware neural commentary generation (GAC)) and some baseline methods for a sample move.</figDesc><graphic url="image-1.png" coords="1,318.34,496.42,200.11,171.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A multi-move, single commentary example from our data. Here, the sequence of moves Ba4 â†’ b5 â†’ Nd6 â†’ bxa4 â†’ e5 is commented upon.</figDesc><graphic url="image-2.png" coords="2,307.28,62.81,222.24,134.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The figure shows a model overview. We first extract various semantic and pragmatic features from the previous and current chess board states. We represent features through embedding in a shared space. We observe that feeding in feature conjunctions helps a lot. We consider a selection mechanism for the model to choose salient attributes from the input at every decoder step.</figDesc><graphic url="image-5.png" coords="5,107.14,200.95,67.51,67.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Commentary texts have a large variety making the 
problem of content selection an important challenge in our 
dataset. We classify the commentaries into 6 different cate-
gories using a classifier trained on some hand-labelled data, 
a fraction of which is kept for validation. % data refers to 
the percentage of commentary sentences in the tagged data 
belonging to the respective category. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of baselines and our model with differ-
ent subsets of features as per various quantitative measures. 
( S = Score, M= Move, T = Threat features; ) On all data sub-
sets, our model outperforms the TEMP and NN baselines. 
Among proposed models, GAC performs better than GAC-
sparse &amp; RAW in general. For NN, GAC-sparse and GAC 
methods, we experiment with multiple feature combinations 
and report only the best as per BLEU scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of the GAC model with different fea-
ture sets. ( S = Score, M= Move, T = Threat features; ) Dif-
ferent subset of features work best for different subsets. For 
instance, Score features seem to help only in the Quality cat-
egory. Note that the results for Quality are from 5-fold cross-
validation, since the number of datapoints in the category is 
much lesser than the other two. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The COMB approaches show the combined per-
formance of separately trained models on the respective test 
subsets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Human study results on MoveDesc data category. Outputs from GAC are in general better than ground truth, NN and 
GAC-sparse. TEMP outperforms other methods, though as shown earlier, outputs from TEMP lack diversity. 

</table></figure>

			<note place="foot" n="5"> https://pypi.org/project/ python-chess/ 6 https://stockfishchess.org/about/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Volkan Cirik, Daniel Clothiaux, Hiroaki Hayashi and anonymous reviewers for providing valuable comments and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to sportscast: a test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chess q&amp;a: Question Answering on Chess Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reasoning, Attention, Memory (RAM) Workshop, Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">213</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a game commentary generator with grounded move expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Kameko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Games (CIG), 2015 IEEE Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Globally Coherent Text Generation with Neural Checklist Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ChloÃ©</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07771</idno>
		<title level="m">Neural text generation from structured data with application to the biography domain</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computer Generation of Chinese Commentary on Othello Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Wen</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Rocling III Computational Linguistics Conference III</title>
		<meeting>Rocling III Computational Linguistics Conference III</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="393" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Working with clinicians to improve a patient-information NLG system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saad</forename><surname>Mahamood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Natural Language Generation Conference. Association for Computational Linguistics</title>
		<meeting>the Seventh International Natural Language Generation Conference. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="100" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00838</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why we need new evaluation metrics for nlg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">OndÅ™ej</forename><surname>DuÅ¡ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">Cercas</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D17-1238" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2241" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Soumith Chintala, and Gregory Chanan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Pytorch</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GaÃ«l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<title level="m">NLG vs. templates. arXiv preprint cmp-lg/9504013</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">You Need to Understand Your Corpora-the Weathergov Example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<ptr target="https://ehudreiter.com/2017/05/09/weathergov/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An investigation into the validity of some metrics for automatically evaluating natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="529" to="558" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lessons from a failure: Generating tailored smoking cessation letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="41" to="58" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Choosing words in computergenerated weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayajulu</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Davy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="137" to="169" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acquiring correct knowledge for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Somayajulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="491" to="516" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated chess tutor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Sadikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Moina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Guid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Krivec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bratko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prediction and entropy of printed English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Labs Technical Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Challenges in Data-to-Document Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08052</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01628</idno>
		<title level="m">Reference-Aware Language Models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
