<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Bilingual Sentiment Word Embeddings for Cross-language Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Bilingual Sentiment Word Embeddings for Cross-language Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="430" to="440"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The sentiment classification performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classification (CLSC) can leverage the rich resources in one language (source language) for sentiment classification in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the sentiment information of text. This paper proposes an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed B-SWE incorporate sentiment information of text into bilingual embeddings. Furthermore , we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on large-scale parallel corpora. Experiments on NLP&amp;CC 2013 CLSC dataset show that our approach outperforms the state-of-the-art systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification is a task of predicting sen- timent polarity of text, which has attracted consid- erable interest in the NLP field. To date, a num- ber of corpus-based approaches ( <ref type="bibr" target="#b19">Pang et al., 2002;</ref><ref type="bibr" target="#b20">Pang and Lee, 2004;</ref><ref type="bibr" target="#b15">Kennedy and Inkpen, 2006</ref>) have been developed for sentiment classification. The approaches heavily rely on quality and quan- tity of the labeled corpora, which are considered as the most valuable resources in sentiment classi- fication task. However, such sentiment resources are imbalanced in different languages. To leverage resources in the source language to improve the sentiment classification performance in the target language, cross-language sentiment classification (CLSC) approaches have been investigated.</p><p>The traditional CLSC approaches employ ma- chine translation (MT) systems to translate corpo- ra in the source language into the target language, and train the sentiment classifiers in the target lan- guage ( <ref type="bibr" target="#b0">Banea et al., 2008)</ref>. Directly employing the translated resources for sentiment classifica- tion in the target language is simple and could get acceptable results. However, the gap between the source language and target language inevitably impacts the performance of sentiment classifica- tion. To improve the classification accuracy, multi- view approaches have been proposed. In these ap- proaches, the resources in the source language and their translations in the target language are both used to train sentiment classifiers in two indepen- dent views <ref type="bibr" target="#b27">(Wan, 2009;</ref><ref type="bibr" target="#b12">Gui et al., 2013;</ref><ref type="bibr" target="#b31">Zhou et al., 2014a</ref>). The final results are determined by en- semble classifiers in these two views to overcome the weakness of monolingual classifiers. However, learning language-specific classifiers in each view fails to capture the common sentiment information of two languages during training process.</p><p>With the revival of interest in deep learning ( <ref type="bibr" target="#b9">Hinton and Salakhutdinov, 2006</ref>), shared deep representations (or embeddings) ( <ref type="bibr" target="#b3">Bengio et al., 2013</ref>) are employed for CLSC <ref type="bibr" target="#b6">(Chandar A P et al., 2013)</ref>. Usually, paired sentences from par- allel corpora are used to learn word embeddings across languages <ref type="bibr" target="#b6">(Chandar A P et al., 2013;</ref><ref type="bibr" target="#b7">Chandar A P et al., 2014</ref>), eliminating the need of MT systems. The learned bilingual embeddings could easily project the training data and test data into a common space, where training and testing are per- formed. However, high-quality bilingual embed- dings rely on the large-scale task-related parallel corpora, which are not always readily available. Meanwhile, though semantic similarities across languages are captured during bilingual embed- ding learning process, sentiment information of text is ignored. That is, bilingual embeddings learned from unlabeled parallel corpora are not effective enough for CLSC because of a lack of explicit sentiment information. <ref type="bibr" target="#b24">Tang and Wan (2014)</ref> first proposed a bilingual sentiment embed- ding model using the original training data and the corresponding translations through a linear map- ping rather than deep learning technique.</p><p>This paper proposes a denoising autoencoder based approach to learning bilingual sentimen- t word embeddings (BSWE) for CLSC, which incorporates sentiment polarities of text into the bilingual embeddings. The proposed approach learns BSWE with the original labeled documents and their translations instead of parallel corpo- ra. The BSWE learning process consists of two phases: the unsupervised phase of semantic learn- ing and the supervised phase of sentiment learn- ing. In the unsupervised phase, sentiment words and their negation features are extracted from the source training data and their translations to rep- resent paired documents. These features are used as inputs for a denoising autoencoder to learn the bilingual embeddings. In the supervised phase, sentiment polarity labels of documents are used to guide BSWE learning for incorporating sentiment information into the bilingual embeddings.</p><p>The learned BSWE are applied to project En- glish training data and Chinese test data into a common space. In this space, a linear support vec- tor machine (SVM) is used to perform training and testing. The experiments are carried on NLP&amp;CC 2013 CLSC dataset, including book, DVD and music categories. Experimental results show that our approach achieves 80.68% average accuracy, which outperforms the state-of-the-art systems on this dataset. Although the BSWE are only evaluat- ed on English-Chinese CLSC here, it can be pop- ularized to many other languages.</p><p>The major contributions of this work can be summarized as follows:</p><p>• We propose bilingual sentiment word em- beddings (BSWE) for CLSC based on deep learning technique. Experimental results show that the proposed BSWE significantly outperform the bilingual embeddings by in- corporating sentiment information.</p><p>• Instead of large-scale parallel corpora, on- ly the labeled English corpora and English- to-Chinese translations are required for B- SWE learning. It is proved that in spite of the small-scale of training set, our approach outperforms the state-of-the-art systems in NLP&amp;CC 2013 CLSC share task.</p><p>• We employ sentiment words and their nega- tion features rather than all words in doc- uments to learn sentiment-specific embed- dings, which significantly reduces the dimen- sion of input vectors as well as improves sen- timent classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the literature related to this paper from two perspectives: cross-language sentiment classification and embedding learning for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cross-language Sentiment Classification (CLSC)</head><p>The critical problem of CLSC is how to bridge the gap between the source language and target lan- guage. Machine translations or parallel corpora are usually employed to solve this problem. We present a brief review of CLSC from two aspects: machine translation based approaches and parallel corpora based approaches.</p><p>Machine translation based approaches use MT systems to project training data into the target lan- guage or test data into the source language. <ref type="bibr" target="#b27">Wan (2009)</ref> proposed a co-training approach for CLSC. The approach first translated Chinese test data in- to English, and English training data into Chinese. Then, they performed training and testing in t- wo independent views: English view and Chinese view. <ref type="bibr" target="#b12">Gui et al. (2013)</ref> combined self-training approach with co-training approach by estimating the confidence of each monolingual system. <ref type="bibr" target="#b12">Li et al. (2013)</ref> selected the samples in the source language that were similar to those in the target language to decrease the gap between two lan- guages. <ref type="bibr" target="#b31">Zhou et al. (2014a)</ref> proposed a combi- nation CLSC model, which adopted denoising au- toencoders ( <ref type="bibr" target="#b26">Vincent et al., 2008)</ref> to enhance the robustness to translation errors of the input.</p><p>Most recently, a number of studies adopt deep learning technique to learn bilingual representa- tions with parallel corpora. Bilingual represen- tations have been successfully applied in many NLP tasks, such as machine translation ( <ref type="bibr" target="#b32">Zou et al., 2013)</ref>, sentiment classification (Chan- dar A P et al., <ref type="bibr" target="#b30">Zhou et al., 2014b</ref>), tex- t classification <ref type="bibr" target="#b7">(Chandar A P et al., 2014</ref>), etc. <ref type="bibr" target="#b6">Chandar A P et al. (2013)</ref> learned bilingual representations with aligned sentences through- out two phases: the language-specific represen- tation learning phase and the shared representa- tion learning phase. In the language-specific rep- resentation learning phase, they applied autoen- coders to obtain a language-specific representa- tion for each entity in two languages respective- ly. In shared representation learning phase, pairs of parallel language-specific representations were passed to an autoencoder to learn bilingual repre- sentations. To joint language-specific representa- tions and bilingual representations, <ref type="bibr" target="#b7">Chandar A P et al. (2014)</ref> integrated the two learning phases in- to a unified process to learn bilingual embeddings. <ref type="bibr" target="#b30">Zhou et al. (2014b)</ref> employed bilingual represen- tations for English-Chinese CLSC. The work men- tioned above employed aligned sentences in bilin- gual embedding learning process. However, in the sentiment classification process, only representa- tions in the source language are used for training, and representations in the target language are used for testing, which ignores the interactions of se- mantic information between the source language and target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding Learning for Sentiment Classification</head><p>Bilingual embedding learning algorithms focus on capturing syntactic and semantic similarities across languages, but ignore sentiment informa- tion. To date, many embedding learning algo- rithms have been developed for sentiment classi- fication problem by incorporating sentiment in- formation into word embeddings. Maas et al. (2011) presented a probabilistic model that com- bined unsupervised and supervised techniques to learn word vectors, capturing semantic informa- tion as well as sentiment information.  introduced sentiment labels into Neural Network Language Models ( <ref type="bibr" target="#b1">Bengio et al., 2003)</ref> to enhance sentiment expression ability of word vectors.  theoretically and em- pirically analyzed the effects of the syntactic con- text and sentiment information in word vectors, and showed that the syntactic context and senti- ment information were equally important to senti- ment classification. Recent years have seen a surge of interest in word embeddings with deep learning technique <ref type="bibr" target="#b5">(Bespalov et al., 2011;</ref><ref type="bibr" target="#b11">Glorot et al., 2011;</ref><ref type="bibr" target="#b21">Socher et al., 2011;</ref><ref type="bibr" target="#b22">Socher et al., 2012</ref>), which have been empirically shown to preserve linguistic regulari- ties ( <ref type="bibr" target="#b18">Mikolov et al., 2013)</ref>. Our work focuses on learning bilingual sentiment word embeddings (B- SWE) with deep learning technique. Unlike the work of <ref type="bibr" target="#b7">Chandar A P et al. (2014)</ref> that adopt- ed parallel corpora to learn bilingual embeddings, we only use training data and their translations to learn BSWE. More importantly, sentiment infor- mation is integrated into bilingual embeddings to improve their performance in CLSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bilingual Sentiment Word Embeddings</head><p>(BSWE) for Cross-language Sentiment Classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Denoising Autoencoder</head><p>It has been demonstrated that the denoising au- toencoder could decrease the effects of translation errors on the performance of CLSC ( <ref type="bibr" target="#b31">Zhou et al., 2014a)</ref>. This paper proposes a deep learning based approach, which employs the denoising autoen- coder to learn the bilingual embeddings for CLSC. A denoising autoencoder is the modification of an autoencoder. The autoencoder ( <ref type="bibr" target="#b2">Bengio et al., 2007</ref>) includes an encoder f θ and a decoder g θ . Through the process of encoding and decod- ing, the parameters θ and θ of the autoencoder will be trained by gradient descent to minimize the loss function. The sum of reconstruction cross- entropies across the training set is usually used as the loss function:</p><formula xml:id="formula_0">l(x) = − d i=1 [x i logˆxlogˆ logˆx i +(1−x i ) log(1−ˆ x i )] (1)</formula><p>A denoising autoencoder enhances robustness to noises by corrupting the input x to a partially destroyed versioñ x. The desired noise level of the input x can be changed by adjusting the destruc- tion fraction ν. For each input x, a fixed number νd (d is the dimension of x) of components are selected randomly, and their values are set to 0, while the others are left untouched. Like an au- toencoder, the destroyed input˜xinput˜ input˜x is mapped to a latent representation y = f θ (˜ x) = σ <ref type="bibr">(W˜xW˜x + b)</ref>. Then y is mapped back to a reconstructed vectorˆx vectorˆ vectorˆx throughˆxthroughˆ throughˆx = g θ (y) = σ(W T y + c). The loss function of a denoising autoencoder is the same as that of an autoencoder. Minimizing the loss makesˆx makesˆ makesˆx close to the input x rather thañ x. Our BSWE learning process can be divided in- to two phases: the unsupervised phase of seman- tic learning and the supervised phase of sentiment learning. In the unsupervised phase, a denoising autoencoder is employed to learn the bilingual em- beddings. In the supervised phase, the sentiment information is incorporated into the bilingual em- beddings based on sentiment labels of documents to obtain BSWE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Phase of the Bilingual Embedding Learning</head><p>In the unsupervised phase, the English training documents and their Chinese translations are em- ployed to learn the bilingual embeddings (Sen- timent polarity labels of documents are not em- ployed in this phase). Based on the English docu- ments, 2,000 English sentiment words in MPQA subjectivity lexicon 1 are extracted by the Chi- square method ( <ref type="bibr" target="#b10">Galavotti et al., 2000</ref>). Their cor- responding Chinese translations are used as Chi- nese sentiment words. Besides, some sentimen- t words are often modified by negation words, which lead to inversion of their polarities. There- fore, negation features are introduced to each sen- timent word to represent its negative form. We take into account 14 frequently-used nega- tion words in English such as not and none; 5 negation words in Chinese such as Ø (no/not) and vk (without). A sentiment word modified by these negation words in the window [-2, 2] is con- sidered as its negative form in this paper, while sentiment word features remain the initial mean- ing. Negation features use binary expressions. If a sentiment word is not modified by negation words, the value of its negation features is set to 0. Thus, the sentiment words and their corresponding nega- tion features in English and Chinese are adopted to represent the document pairs (x E , x C ).</p><p>We expect that pairs of documents could be forced to capture the common semantic informa- tion of two languages. To achieve this, a denoising autoencoder is used to perform the reconstructions of paired documents in both English and Chinese. <ref type="figure">Figure 1</ref> shows the framework of bilingual embed- ding learning. <ref type="figure">Figure 1</ref>: The framework of bilingual embedding learning.</p><formula xml:id="formula_1">E W E x  E y C x ˆ [ ] T E C W W ， C x ) , ( C E l x x E x ˆ E x ) ( E l x C W C x  C y C x ˆ C x ) , ( E C l x x E x ˆ E x ) ( C l x [ ] T E C W W ， E x C x (a) reconstruction from x E (b) reconstruction from x C</formula><p>For the corrupted versions˜xversions˜ versions˜x E (˜ x C ) of the initial input vector x E (x C ), we use the sigmoid function as the activation function to extract latent repre- sentations:</p><formula xml:id="formula_2">y E = f θ (˜ x E ) = σ(W E ˜ x E + b)<label>(2)</label></formula><formula xml:id="formula_3">y C = f θ (˜ x C ) = σ(W C ˜ x C + b)<label>(3)</label></formula><p>where W E and W C are the language-specific word representation matrices, corresponding to English and Chinese respectively. Notice that the bias b is shared to ensure that the produced repre- sentations in two languages are on the same scale.</p><p>For the latent representations in either language, we would like two decoders to perform recon- structions in English and Chinese respectively. As shown in <ref type="figure">Figure 1(a</ref></p><note type="other">), for the latent representation y E in English, one decoder is used to map y E back to a reconstructionˆxreconstructionˆ reconstructionˆx E in English, and the other is used to map y E back to a reconstructionˆx reconstructionˆ reconstructionˆx C in Chinese such that:</note><formula xml:id="formula_4">ˆ x E = g θ (y E ) = σ(W T E y E + c E )<label>(4)</label></formula><formula xml:id="formula_5">ˆ x C = g θ (y E ) = σ(W T C y E + c C )<label>(5)</label></formula><p>where c E and c C are the biases of the decoders in English and Chinese, respectively. Similarly, the same steps repeat for the latent representation y C in Chinese, which are shown in <ref type="figure">Figure 1(b)</ref>. The encoder and decoder structures allow us to learn a mapping within and across languages. Specifically, for a given document pair (x E , x C ), we can learn bilingual embeddings to recon- struct x E from itself (loss l(x E )), reconstruc- t x C from itself (loss l(x C )), construct x C from x E (loss l(x E , x C )), construct x E from x C (loss l(x C , x E )) and reconstruct the concatena- tion of x E and</p><formula xml:id="formula_6">x C ([x E , x C ]) from itself (loss l([x E , x C ], [ˆ x E , ˆ x C ])</formula><p>). The sum of 5 losses is used as the loss function of bilingual embeddings:</p><formula xml:id="formula_7">L =l(x E ) + l(x C ) + l(x E , x C ) + l(x C , x E ) + l([x E , x C ], [ˆ x E , ˆ x C ])<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Supervised Phase of Sentiment Learning</head><p>In the unsupervised phase, we have learned the bilingual embeddings, which could capture the se- mantic information within and across languages. However, the sentiment polarities of text are ig- nored in the unsupervised phase. Bilingual em- beddings without sentiment information are not effective enough for sentiment classification task. This paper proposes an approach to learning B- SWE for CLSC, which introduces a supervised learning phase to incorporate sentiment informa- tion into the bilingual embeddings. The process of supervised phase is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>[ , ] </p><formula xml:id="formula_8">E C W W b y  label max ( | ; ) p s d   sentiment ( | ; ) p s d  ] , [ C E x x</formula><formula xml:id="formula_9">= σ([W E , W C ][x E , x C ] + b), where [W E , W C ] is the concatenation of W E and W C .</formula><p>The latent bilingual representation y b is used to obtain the positive polarity probability p(s = 1|d; ξ) of a document through a sigmoid function:</p><formula xml:id="formula_10">p(s = 1|d; ξ) = σ(ϕ T y b + b l )<label>(7)</label></formula><p>where ϕ is the logistic regression weight vector and b l is the bias of logistic regression. The senti- ment label s is a Boolean value representing sen- timent polarity of a document: s = 0 represents negative polarity and s = 1 represents positive po- larity. Parameter</p><formula xml:id="formula_11">ξ * = {[W E , W C ] * , b * , ϕ * , b * l } is learned</formula><p>by maximizing the objective function according to the sentiment polarity label s i of doc- ument d i :</p><formula xml:id="formula_12">ξ * = arg max ξ i=1 log p(s i |d i ; ξ)<label>(8)</label></formula><p>Through the supervised learning phase, [W E , W C ] is optimized by maximizing senti- ment polarity probability. Thus, rich sentiment information is encoded into the bilingual embed- dings.</p><p>The following experiments will prove that the proposed BSWE outperform the traditional bilin- gual embeddings significantly in CLSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bilingual Document Representation</head><p>Method (BDR)</p><p>Once we have learned BSWE [W E , W C ], whose columns are representations for sentiment words, we can use them to represent documents in two languages.</p><p>Given an English training document d E containing 2,000 sentiment word features s 1 , s 2 , · · · , s 2,000 and 2,000 corresponding nega- tion features, we represent it as the TF-IDF weighted sum of BSWE:</p><formula xml:id="formula_13">φ d E = 4,000 i=1 T F − IDF (s i )W E.,s i<label>(9)</label></formula><p>Similarly, for its Chinese translation d C containing 2,000 sentiment word features t 1 , t 2 , · · · , t 2,000 and 2,000 corresponding negation features, we represent it as:</p><formula xml:id="formula_14">φ d C = 4,000 j=1 T F − IDF (t j )W C.,t j<label>(10)</label></formula><p>We propose a bilingual document representa- tion method (BDR) in this paper, which represents each document d i with the concatenation of its En- glish and Chinese representations</p><formula xml:id="formula_15">[φ d E , φ d C ]</formula><p>. B- DR is expected to enhance the ability of sentiment expression for further improving the classification performance. Such bilingual document represen- tations are fed to a linear SVM to perform senti- ment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Data Set. The proposed approach is evaluated on NLP&amp;CC 2013 CLSC dataset 2 3 . The dataset con-sists of product reviews on three categories: book, DVD, and music. Each category contains 4,000 English labeled data as training data (the ratio of the number of positive and negative samples is 1:1) and 4,000 Chinese unlabeled data as test data.</p><p>Tools. In our experiments, Google Translate 4 is adopted for both English-to-Chinese and Chinese- to-English translation. ICTCLAS ( <ref type="bibr" target="#b29">Zhang et al., 2003</ref>) is used as Chinese word segmentation tool. A denoising autoencoder is developed based on Theano system <ref type="figure" target="#fig_1">(Bergstra et al., 2010)</ref>. BSWE are trained for 50 and 30 epochs in unsuper- vised phase and supervised phases respectively. SV M light <ref type="bibr" target="#b14">(Joachims, 1999</ref>) is used to train lin- ear SVM sentiment classifiers Evaluation Metric. The performance is evalu- ated by the classification accuracy for each cate- gory, and the average accuracy of three categories, respectively. The category accuracy is defined as:</p><formula xml:id="formula_16">Accuracy c = #system correct c #system total c (11)</formula><p>where c is one of the three categories, and #system correct c and #system total c stand for the number of being correctly classified re- views and the number of total reviews in the cate- gory c, respectively. The average accuracy is shown as:</p><formula xml:id="formula_17">Average = 1 3 c Accuracy c<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluations on BSWE</head><p>In this section, we evaluate the quality of BSWE for CLSC. The dimension of bilingual embeddings d is set to 50, and destruction fraction ν is set to 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Bilingual Embedding Learning Methods</head><p>We first compare our unsupervised bilingual em- bedding learning method with the parallel cor- pora based method. The parallel corpora based method uses the paired documents in the parallel corpus <ref type="bibr">5</ref> to learn bilingual embeddings, while our method only uses the English training documents and their Chinese translations (Sentiment polari- ty labels of documents are not employed here). The Boolean feature weight calculation method is adopted to represent documents for bilingual em- bedding learning and BDR is employed to rep- resent training data and test data for sentimen- t classification. To represent the paired docu- ments in the parallel corpus, 27,597 English word- s and 31,786 Chinese words are extracted for bilingual embedding learning. Our method only needs 2,000 English sentiment words, 2,000 Chi- nese sentiment words, and their negation features, which significantly reduces the dimension of input vectors. The average accuracies on NLP&amp;CC 2013 test data of the two bilingual embedding learning methods are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. As can be seen from <ref type="figure" target="#fig_2">Figure 3</ref>, when the corpus scales of the two methods are the same (4,000 paired documents), our method (75.09% average accuracy) surpasses the parallel corpora method (54.82% average ac- curacy) by about 20%. With the scale of the par- allel corpora increasing, the performance of par- allel corpora based method is steadily improved. However, the performance is not as good as our bilingual embedding learning method. Though the document number of the parallel corpus is up to 70,000 , the average accuracy is only 70.05%. It is proved that our method is more suitable for learn- ing bilingual embeddings for cross-language senti- ment classification than the parallel corpora based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Feature Weight in Bilingual Embeddings</head><p>In this part, we compare the Boolean and TF- IDF feature weight calculation methods in bilin- gual embedding learning process. <ref type="table">Table 1 shows the classification accuracy with   435   Category   book  DVD  music  Average  Boolean</ref> 76.22% 74.30% 74.75% 75.09% TF-IDF 76.65% 77.60% 74.50% 76.25% <ref type="table">Table 1</ref>: The classification accuracy with the Boolean and TF-IDF methods.</p><p>the Boolean and TF-IDF methods. Generally, the TF-IDF method performs better than the Boolean method. The average accuracy of the TF-IDF method is 1.16% higher than the Boolean method, which illustrates that the TF-IDF method could re- flect the latent contribution of sentiment words to each document effectively. The TF-IDF weight calculation method is exploited in the following experiments. Notice that sentiment information</p><p>is not yet introduced in the bilingual embeddings here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Sentiment Information in BSWE</head><p>Incorporating sentiment information in the bilin- gual embeddings, the performance of bilingual embeddings (without sentiment information) and BSWE (with sentiment information) is compared in <ref type="figure" target="#fig_3">Figure 4</ref>. As can be seen from <ref type="figure" target="#fig_3">Figure 4</ref>, by encoding sen- timent information in the bilingual embeddings, the performance in book, DVD and music cate- gories significantly improves to 79.47%, 78.72% and 76.58% respectively (2.82% increase in book, 1.12% in DVD, and 2.08% in music). The av- erage accuracy reaches 78.26%, which is 2.01% higher than that of the bilingual embeddings. The experimental results indicate the effectiveness of sentiment information in the bilingual embedding learning. The BSWE learning approach is em- ployed for CLSC in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Bilingual Document Representation Method</head><p>In this experiment, our bilingual document rep- resentation method (BDR) is compared with the following monolingual document representation methods.</p><p>En-En: This method represents training and test documents in English only with W E . English training documents and Chinese-to-English trans- lations of test documents are both represented with W E .</p><p>Cn-Cn: This method represents training and test documents in Chinese only with W C . English-to-Chinese translations of training docu- ments and Chinese test documents are both repre- sented with W C .</p><p>En-Cn: This method represents English train- ing documents with W E , while represents Chi- nese test documents with W C . <ref type="bibr" target="#b7">Chandar A P et al. (2014)</ref> employed this method in their work.</p><p>BDR: This method adopts our bilingual doc- ument representation method, which represents training and test documents with both W E and W C .  From <ref type="figure" target="#fig_4">Figure 5</ref> we can see that En-En, Cn-Cn, and En-Cn get similar results. BDR performs con- stantly better than the other representation meth- ods throughout the interval [0, 0.9]. The absolute superiority of BDR benefits from the enhanced a- bility of sentiment expression.</p><p>Meanwhile, when the input x is partially de-stroyed (ν varies from 0.1 to 0.9), the perfor- mance of En-En, Cn-Cn and En-Cn remains sta- ble, which illustrates the robustness of the denois- ing autoencoder to corrupting noises. In addi- tion, the average accuracies of BDR in the inter- val ν ∈ [0.1, 0.9] are all higher than the average accuracy under the condition ν = 0 (78.23%). Therefore, adding noises properly to the training data could improve the performance of BSWE for CLSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Influences of Dimension d and</head><p>Destruction Fraction ν <ref type="figure" target="#fig_6">Figure 6</ref> shows the relationship between accura- cies and dimension d of BSWE as well as that be- tween accuracies and destruction fraction ν in au- toencoders in different categories. Dimension of embeddings d varies from 50 to 500, and destruc- tion fraction ν varies from 0.1 to 0.9. As shown in <ref type="figure" target="#fig_6">Figure 6</ref>, the average accuracies generally move upward as dimension of BSWE in- creasing. Generally, the average accuracies keep higher than 80% with ν varying from 0.1 to 0.5 as well as dimension varying from 300 to 500. When ν = 0.1 and d = 400, the average accu- racy reaches the peak value 80.68% (category ac- curacy of 81.05% in book, 81.60% in DVD, and 79.40% in music). The experimental results show that in BSWE learning process, increasing the di- mension of embeddings or properly adding noises to the training data helps improve the performance of CLSC. In this paper, we only evaluate BSWE when dimension d varies from 50 to 500. Howev- er, there is still space for further improvement if d continues to increase. <ref type="table" target="#tab_1">Table 2</ref> shows comparisons of the performance between our approach and some state-of-the-art systems on NLP&amp;CC 2013 CLSC dataset. Our approach achieves the best performance with an 80.68% average accuracy. Compared with the re- cent related work, our approach is more effective and suitable for eliminating the language gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Related Work</head><p>Chen   <ref type="bibr" target="#b12">Gui et al. (2013;</ref> and Zhou et al. (2014a) adopted the multi-view approach to bridge the lan- guage gap. <ref type="bibr" target="#b12">Gui et al. (2013)</ref> proposed a mixed CLSC model by combining co-training and trans- fer learning strategies. They achieved the high- est accuracy of 78.89% in NLP&amp;CC CLSC share task. <ref type="bibr" target="#b13">Gui et al. (2014)</ref> further improved the accu- racy to 80.10% by removing noise from the trans- ferred samples to avoid negative transfers. <ref type="bibr" target="#b31">Zhou et al. (2014a)</ref> built denoising autoencoders in t- wo independent views to enhance the robustness to translation errors in the inputs and achieved 80.02% accuracy. The multi-view approach learn- s language-specific classifiers in each view dur- ing training process, which is difficult to capture the common sentiment information of the two lan- guages. Our approach integrates the bilingual em- bedding learning into a unified process, and out- performs <ref type="bibr" target="#b8">Chen et al. (2014)</ref>, <ref type="bibr" target="#b12">Gui et al. (2013)</ref>, <ref type="bibr" target="#b13">Gui et al. (2014)</ref> and <ref type="bibr" target="#b31">Zhou et al. (2014a)</ref> by 3.59%, 1.79%, 0.58%, and 0.66% respectively. The su- periority of our approach benefits from the unified bilingual embedding learning process and the in- tegration of semantic and sentiment information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper proposes an approach to learning B- SWE by incorporating sentiment information in- to the bilingual embeddings for CLSC. The pro- posed approach learns BSWE with the labeled documents and their translations rather than par- allel corpora. In addition, BDR is proposed to en- hance the sentiment expression ability which com- bines English and Chinese representations. Exper- iments on the NLP&amp;CC 2013 CLSC dataset show that our approach outperforms the previous state- of-the-art systems as well as traditional bilingual embedding systems. The proposed BSWE are on- ly evaluated on English-Chinese CLSC in this pa- per, but it can be popularized to other languages. Both semantic and sentiment information play an important role in sentiment classification. In the following work, we will further investigate the relationship between semantic and sentiment in- formation for CLSC, and balance their functions to optimize their combination for CLSC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The encoder maps a d-dimensional input vector x ∈ [0, 1] d to a hidden representation y ∈ [0, 1] d through a deterministic mapping y = f θ (x) = σ(Wx + b), parameterized by θ = {W, b}. W is a weight matrix, b is a bias term, and σ(x) is the activation function. The decoder maps y back to a reconstructed vectorˆxvectorˆ vectorˆx = g θ (y) = σ(W T y + c), parameterized by θ = {W T , c}, where c is the bias term for reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The supervised learning process. For paired documents [x E , x C ], the sigmoid function is adopted as the activation function to extract latent bilingual representations y b = σ([W E , W C ][x E , x C ] + b), where [W E , W C ] is the concatenation of W E and W C. The latent bilingual representation y b is used to obtain the positive polarity probability p(s = 1|d; ξ) of a document through a sigmoid function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our unsupervised bilingual embedding learning method vs. Parallel corpora based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison of the bilingual embeddings and BSWE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of bilingual document representation method (BDR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 shows</head><label>5</label><figDesc>Figure 5 shows the average accuracy curves of different document representation methods with different destruction fraction ν. We vary ν from 0 to 0.9 with an interval of 0.1. From Figure 5 we can see that En-En, Cn-Cn, and En-Cn get similar results. BDR performs constantly better than the other representation methods throughout the interval [0, 0.9]. The absolute superiority of BDR benefits from the enhanced ability of sentiment expression. Meanwhile, when the input x is partially de</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The relationship between accuracies and dimension d as well as that between accuracies and destruction fraction ν.</figDesc><graphic url="image-1859.png" coords="9,118.78,62.80,359.99,273.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>et al. (2014) translated Chinese test da- ta into English and then gave different weight- s to sentiment words according to the subject- predicate component of sentiment words. They got 77.09% accuracy and took the 2nd place in NLP&amp;CC 2013 CLSC share task. The machine translation based approach was limited by the translation errors.</figDesc><table>System 
book 
DVD 
music 
Average 
Chen et al. 
(2014) 
77.00% 78.33% 
75.95% 
77.09% 

Gui et al. 
(2013) 
78.70% 79.65% 
78.30% 
78.89% 

Gui et al. 
(2014) 
80.10% 81.60% 
78.60% 
80.10% 

Zhou et al. 
(2014a) 
80.63% 80.95% 
78.48% 
80.02% 

Our approach 81.05% 81.60% 79.40% 80.68% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance comparisons on the 
NLP&amp;CC 2013 CLSC dataset. 

</table></figure>

			<note place="foot" n="1"> http://mpqa.cs.pitt.edu/lexicons/subj lexicon</note>

			<note place="foot" n="2"> http://tcci.ccf.org.cn/conference/2013/dldoc/evsam03.zip 3 http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip</note>

			<note place="foot" n="4"> http://translate.google.cn/ 5 http://www.datatang.com/data/45485</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We wish to thank the anonymous reviewers for their valuable comments. This research is support-ed by National Natural Science Foundation of Chi-na (Grant No. 61272375).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multilingual Subjectivity Analysis Using Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems 19 (NIPS 06)</title>
		<meeting>Advances in Neural Information Processing Systems 19 (NIPS 06)</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for scientific computing conference</title>
		<meeting>the Python for scientific computing conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>SciPy</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentiment classification based on supervised latent n-gram analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shokoufandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Information and Knowledge Management</title>
		<meeting>the Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Balaraman Ravindran, Vikas Raykar and Amrita Saha. 2013. Multilingual deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop at NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-Language Sentiment Analysis Based on Parser (in Chinese)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xule</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acta Scientiarum Naturalium Universitatis Pekinensis</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks. Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature Selection and Negative Evidence in Automated Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Galavotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECDL-00, 4th European Conference on Research and Advanced Technology for Digital Libraries</title>
		<meeting>ECDL-00, 4th European Conference on Research and Advanced Technology for Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 28th International Conference on Machine Learning</title>
		<meeting>28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A mixed model for cross lingual opinion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamfai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing and Chinese Computing</title>
		<meeting>Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual Opinion Analysis via Negative Transfer Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="860" to="865" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Making large-Scale SVM Learning Practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Universität Dortmund</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment classification of movie reviews using contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active learning for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Churen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing and Chinese Computing</title>
		<meeting>Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Word Vectors for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thumbs up? Sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff Chiung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>Bellevue</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistic</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistic</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Bilingual Embedding Model for Cross-language Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Word Vector Modeling for Sentiment Analysis of Product Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing and Chinese Computing</title>
		<meeting>Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="168" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HHMM-based Chinese Lexical Analyzer ICTCLAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd SIGHAN workshop affiliated with 41th ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="184" to="187" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bridging the Language Gap: Learning Distributed Semantics for Cross-Lingual Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing and Chinese Computing</title>
		<meeting>Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="138" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification based on denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Natural Language Processing and Chinese Computing</title>
		<meeting>Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bilingual Word Embedding for Phrase-Based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
