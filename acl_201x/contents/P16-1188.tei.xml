<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1998">1998-2008. August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
							<email>gdurrett@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division UC Berkeley</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="department" key="dep3">Computer Science Division UC Berkeley</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division UC Berkeley</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="department" key="dep3">Computer Science Division UC Berkeley</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>klein@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division UC Berkeley</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="department" key="dep3">Computer Science Division UC Berkeley</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="1998">1998-2008. August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun&apos;s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system 1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While multi-document summarization is well- studied in the NLP literature <ref type="bibr" target="#b5">(Carbonell and Goldstein, 1998;</ref><ref type="bibr" target="#b15">Gillick and Favre, 2009;</ref><ref type="bibr" target="#b29">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b42">Nenkova and McKeown, 2011</ref>), single-document summarization <ref type="bibr" target="#b40">(McKeown et al., 1995;</ref><ref type="bibr" target="#b36">Marcu, 1998;</ref><ref type="bibr" target="#b34">Mani, 2001;</ref><ref type="bibr" target="#b22">Hirao et al., 2013</ref>) has received less attention in recent years and is generally viewed as more difficult. Con- tent selection is tricky without redundancy across multiple input documents as a guide and sim- ple positional information is often hard to beat <ref type="bibr" target="#b45">(Penn and Zhu, 2008)</ref>. In this work, we tackle the single-document problem by training an ex- pressive summarization model on a large nat- 1 Available at http://nlp.cs.berkeley.edu urally occurring corpus-the New York Times Annotated Corpus <ref type="bibr" target="#b49">(Sandhaus, 2008)</ref> which con- tains around 100,000 news articles with abstrac- tive summaries-learning to select important con- tent with lexical features. This corpus has been explored in related contexts ( <ref type="bibr" target="#b12">Dunietz and Gillick, 2014;</ref><ref type="bibr" target="#b24">Hong and Nenkova, 2014</ref>), but to our knowledge it has not been directly used for single- document summarization.</p><p>To increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms-one syntactic and the other discur- sive. Additionally, we incorporate a model of anaphora resolution and give our system the abil- ity rewrite pronominal mentions, further increas- ing expressivity. In order to guide the model, we incorporate (1) constraints from coreference en- suring that critical pronoun references are clear in the final summary and (2) constraints from syntac- tic and discourse parsers ensuring that sentence re- alizations are well-formed. Despite the complex- ity of these additional constraints, we demonstrate an efficient inference procedure using an ILP- based approach. By training our full system end- to-end on a large-scale dataset, we are able to learn a high-capacity structured model of the summa- rization process, contrasting with past approaches to the single-document task which have typically been heuristic in nature <ref type="bibr" target="#b10">(Daumé and Marcu, 2002;</ref><ref type="bibr" target="#b22">Hirao et al., 2013)</ref>.</p><p>We focus our evaluation on the New York Times Annotated corpus <ref type="bibr" target="#b49">(Sandhaus, 2008)</ref>. According to ROUGE, our system outperforms a document pre- fix baseline, a bigram coverage baseline adapted from a strong multi-document system <ref type="bibr" target="#b15">(Gillick and Favre, 2009)</ref>, and a discourse-informed method from prior work ( <ref type="bibr" target="#b54">Yoshida et al., 2014</ref>). Impos- ing discursive and referential constraints improves human judgments of linguistic clarity and ref- erential structure-outperforming the method of 8i, k x unit i  x unit k if 9j with x ref ij = 0 where the antecedent of r ij is in u k 8j x ref ij = 1 i↵ no prior included textual unit mentions the entity that r ij refers to 8i, k x unit i  x unit k if u i requires u k on the basis of pronoun anaphora Length Constraint max x unit ,x ref <ref type="figure">Figure 1</ref>: ILP formulation of our single-document summarization model. The basic model extracts a set of textual units with binary variables x UNIT subject to a length constraint. These textual units u are scored with weights w and features f . Next, we add constraints derived from both syntactic parses and Rhetorical Structure Theory (RST) to enforce grammaticality. Finally, we add anaphora constraints derived from coreference in order to improve summary coherence. We introduce additional binary variables x REF that control whether each pronoun is replaced with its antecedent using a candidate replacement rij. These are also scored in the objective and are incorporated into the length constraint. <ref type="bibr" target="#b54">Yoshida et al. (2014)</ref> and approaching the clar- ity of a sentence-extractive baseline-and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract im- portant content, but is sufficiently constrained to ensure fluency is not sacrificed as a result. Past work has explored various kinds of struc- ture for summarization. Some work has focused on improving content selection using discourse structure ( <ref type="bibr" target="#b33">Louis et al., 2010;</ref><ref type="bibr" target="#b22">Hirao et al., 2013</ref>), topical structure ( <ref type="bibr" target="#b2">Barzilay and Lee, 2004</ref>), or re- lated techniques ( <ref type="bibr" target="#b41">Mithun and Kosseim, 2011</ref>). Other work has used structure primarily to re- order summaries and ensure coherence ( <ref type="bibr" target="#b3">Barzilay et al., 2001;</ref><ref type="bibr" target="#b1">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b32">Louis and Nenkova, 2012;</ref><ref type="bibr" target="#b7">Christensen et al., 2013</ref>) or to represent content for sentence fusion or abstrac- tion ( <ref type="bibr" target="#b52">Thadani and McKeown, 2013;</ref><ref type="bibr" target="#b47">Pighin et al., 2014</ref>). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model's capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning.</p><formula xml:id="formula_0">8i, k x unit i  x unit k if u i requires u k { X (i,j) ⇥ x ref ij (w &gt; f (r ij )) ⇤ + + X (i,j) x ref ij (|r ij | 1) X i x unit i |u i | X i ⇥ x unit i (w &gt; f (u i )) ⇤ Extraction</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our model is shown in <ref type="figure">Figure 1</ref>. Broadly, our ILP takes a set of textual units u = (u 1 , . . . , u n ) from a document and finds the highest-scoring extractive summary by optimizing over variables</p><formula xml:id="formula_1">x UNIT = x UNIT 1 , . . . , x UNIT n</formula><p>, which are binary in- dicators of whether each unit is included. Tex- tual units are contiguous parts of sentences that serve as the fundamental units of extraction in our model. For a sentence-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams <ref type="bibr" target="#b15">(Gillick and Favre, 2009;</ref><ref type="bibr" target="#b4">Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b24">Hong and Nenkova, 2014;</ref><ref type="bibr" target="#b28">Li et al., 2015</ref>). For our model, type-level n-gram scoring only arises when we compute our loss function in max- margin training (see Section 3).</p><p>In Section 2.1, we discuss grammaticality con- straints, which take the form of introducing de- pendencies between textual units, as shown in <ref type="figure" target="#fig_0">Fig- ure 2</ref>. If one textual unit requires another, it can- not be included unless its prerequisite is. We will show that different sets of requirements can cap- ture both syntactic and discourse-based compres- sion schemes.</p><p>Furthermore, we introduce anaphora constraints (Section 2.2) via a new set of variables that capture the process of rewriting pronouns to make them  explicit mentions. That is, x REF ij = 1 if we should rewrite the jth pronoun in the ith unit with its an- tecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Grammaticality Constraints</head><p>Following work on isolated sentence compression <ref type="bibr" target="#b39">(McDonald, 2006;</ref><ref type="bibr" target="#b8">Clarke and Lapata, 2008)</ref> and compressive summarization <ref type="bibr" target="#b31">(Lin, 2003;</ref><ref type="bibr" target="#b37">Martins and Smith, 2009;</ref><ref type="bibr" target="#b4">Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b53">Woodsend and Lapata, 2012;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013</ref>), we wish to be able to compress sen- tences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated sum- maries as closely possible. <ref type="bibr">2</ref> We explore two ways of deriving units for compression: the RST-based compressions of <ref type="bibr" target="#b22">Hirao et al. (2013)</ref> and the syntac- tic compressions of <ref type="bibr" target="#b4">Berg-Kirkpatrick et al. (2011)</ref>. <ref type="figure" target="#fig_0">Figure 2a</ref> shows how to de- rive compressions from Rhetorical Structure The- ory ( <ref type="bibr" target="#b35">Mann and Thompson, 1988;</ref><ref type="bibr" target="#b6">Carlson et al., 2001</ref>). We show a sentence broken into elemen- <ref type="bibr">2</ref> The features in our model are actually rich enough to learn a sophisticated compression model, but the data we have (abstractive summaries) does not directly provide ex- amples of correct compressions; past work has gotten around this with multi-task learning <ref type="bibr" target="#b0">(Almeida and Martins, 2013</ref>), but we simply treat grammaticality as a constraint from up- stream models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RST compressions</head><p>tary discourse units (EDUs) with RST relations between them. Units marked as SAME-UNIT must both be kept or both be deleted, but other nodes in the tree structure can be deleted as long as we do not delete the parent of an included node. For ex- ample, we can delete the ELABORATION clause, but we can delete neither the first nor last EDU. Arrows depict the constraints this gives rise to in the ILP (see <ref type="figure">Figure 1</ref>): u 2 requires u 1 , and u 1 and u 3 mutually require each other. This is a more con- strained form of compression than was used in past work ( <ref type="bibr" target="#b22">Hirao et al., 2013</ref>), but we find that it im- proves human judgments of fluency (Section 4.3). <ref type="figure" target="#fig_0">Figure 2b</ref> shows two examples of compressions arising from syntactic patterns <ref type="bibr" target="#b4">(Berg-Kirkpatrick et al., 2011</ref>): deletion of the second part of a coordinated NP and dele- tion of a PP modifier to an NP. These patterns were curated to leave sentences as grammatical after be- ing compressed, though perhaps with damaged se- mantic content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic compressions</head><p>Combined compressions <ref type="figure" target="#fig_0">Figure 2c</ref> shows the textual units and requirement relations yielded by combining these two types of compression. On this example, the two schemes capture orthogo- nal compressions, and more generally we find that they stack to give better results for our final sys- tem (see Section 4.3). To actually synthesize tex- tual units and the constraints between them, we start from the set of RST textual units and intro- duce syntactic compressions as new children when they don't cross existing brackets; because syntac- tic compressions are typically narrower in scope, they are usually completely contained in EDUs. <ref type="figure" target="#fig_0">Figure 2d</ref> shows an example of this process: the possible deletion of with Aetna is grafted onto the textual unit and appropriate requirement relations are introduced. The net effect is that the textual unit is wholly included, partially included (with Aetna removed), or not at all.</p><p>Formally, we define an RST tree as T rst = (S rst , π rst ) where S rst is a set of EDU spans (i, j) and π : S → 2 S is a mapping from each EDU span to EDU spans it depends on. Syntactic compres- sions can be expressed in a similar way with trees T syn . These compressions are typically smaller- scale than EDU-based compressions, so we use the following modification scheme. Denote by T syn(kl) a nontrivial (supports some compression) subtree of T syn that is completely contained in an EDU (i, j). We build the following combined compression tree, which we refer to as the aug- mentation of T rst with T syn(kl) :</p><formula xml:id="formula_2">Tcomb = (S ∪ Ssyn(kl) ∪ {(i, k), (l, j)}, πrst ∪ π syn(kl) ∪ {(i, k) → (l, j), (l, j) → (i, k), (k, l) → (i, k)})</formula><p>That is, we maintain the existing tree structure ex- cept for the EDU (i, j), which is broken into three parts: the outer two depend on each other (is a claims adjuster and . from <ref type="figure" target="#fig_0">Figure 2d</ref>) and the in- ner one depends on the others and preserves the tree structure from T syn . We augment T rst with all maximal subtrees of T syn , i.e. all trees that are not contained in other trees that are used in the aug- mentation process. This is broadly similar to the combined com- pression scheme in <ref type="bibr" target="#b26">Kikuchi et al. (2014)</ref> but we use a different set of constraints that more strictly enforce grammaticality. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Anaphora Constraints</head><p>What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory ( <ref type="bibr" target="#b18">Grosz et al., 1995)</ref> and lexical cohesion ( <ref type="bibr" target="#b44">Nishikawa et al., 2014</ref>), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being "orphaned" dur- ing extraction (their antecedents are deleted) are This hasn't been Kellogg's year .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replacement (2.2.1): If :</head><p>The oat-bran craze has cost it market share.</p><p>Otherwise (i.e. if no replacement is possible):</p><formula xml:id="formula_3">x unit 2  x unit 1 u 1 u 2 p 1 p 2 p 3</formula><p>Allow pronoun replacement with the predicted antecedent and add the following constraint:</p><p>Add the following constraint: It, which refers to Kellogg, has several possible an- tecedents from the standpoint of an automatic coreference system <ref type="bibr" target="#b14">(Durrett and Klein, 2014)</ref>. If the coreference sys- tem is confident about its selection (above a threshold α on the posterior probability), we allow for the model to explic- itly replace the pronoun if its antecedent would be deleted (Section 2.2.1). Otherwise, we merely constrain one or more probable antecedents to be included (Section 2.2.2); even if the coreference system is incorrect, a human can often cor- rectly interpret the pronoun with this additional context.</p><p>relatively common: they occur in roughly 60% of examples produced by our summarizer when no anaphora constraints are enforced. This kind of error is particularly concerning for summary inter- pretation and impedes the ability of summaries to convey information effectively <ref type="bibr" target="#b17">(Grice, 1975)</ref>. Our solution is to explicitly impose constraints on the model based on pronoun anaphora resolution. 4 <ref type="figure" target="#fig_1">Figure 3</ref> shows an example of a problem case. If we extract only the second textual unit shown, the pronoun it will lose its antecedent, which in this case is Kellogg. We explore two types of con- straints for dealing with this: rewriting the pro- noun explicitly, or constraining the summary to in- clude the pronoun's antecedent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Pronoun Replacement</head><p>One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we can make an isolated textual unit meaningful even if it contains a pronoun. <ref type="figure" target="#fig_1">Figure 3</ref> shows how this process works. We run the Berkeley Entity Reso- lution System <ref type="bibr" target="#b14">(Durrett and Klein, 2014</ref>) and com- pute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. max i p i &gt; α for a speci- fied threshold α &gt; 1 2 ), we allow ourselves to re- place the pronoun with the first mention of the en- tity corresponding to the pronoun's most likely an- tecedent. In <ref type="figure" target="#fig_1">Figure 3</ref>, if the system correctly deter- mines that Kellogg is the correct antecedent with high probability, we enable the first replacement shown there, which is used if u 2 is included the summary without u 1 . <ref type="bibr">5</ref> As shown in the ILP in <ref type="figure">Figure 1</ref>, we instanti- ate corresponding pronoun replacement variables x REF where x REF ij = 1 implies that the jth pronoun in the ith sentence should be replaced in the sum- mary. We use a candidate pronoun replacement if and only if the pronoun's corresponding (pre- dicted) entity hasn't been mentioned previously in the summary. <ref type="bibr">6</ref> Because we are generally replac- ing pronouns with longer mentions, we also need to modify the length constraint to take this into account. Finally, we incorporate features on pro- noun replacements in the objective, which helps the model learn to prefer pronoun replacements that help it to more closely match the human sum- maries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Pronoun Antecedent Constraints</head><p>Explicitly replacing pronouns is risky: if the coref- erence system makes an incorrect prediction, the intended meaning of the summary may be dam- aged. Fortunately, the coreference model's pos- terior probabilities have been shown to be well- calibrated <ref type="bibr" target="#b43">(Nguyen and O'Connor, 2015)</ref>, mean- ing that cases where it is likely to make errors are signaled by flatter posterior distributions. In this case, we enable a more conservative set of con- straints that include additional content in the sum- mary to make the pronoun reference clear without explicitly replacing it. This is done by requiring the inclusion of any textual unit which contains possible pronoun references whose posteriors sum to at least a threshold parameter β. <ref type="figure" target="#fig_1">Figure 3</ref> shows that this constraint can force the inclusion of u 1 to provide additional context. Although this could still lead to unclear pronouns if text is stitched to- gether in an ambiguous or even misleading way, in practice we observe that the textual units we force to be added almost always occur very recently be- fore the pronoun, giving enough additional context for a human reader to figure out the pronoun's an- tecedent unambiguously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Features</head><p>The features in our model (see <ref type="figure">Figure 1</ref>) consist of a set of surface indicators capturing mostly lex- ical and configurational information. Their pri- mary role is to identify important document con- tent. The first three types of features fire over tex- tual units, the last over pronoun replacements.</p><p>Lexical These include indicator features on non- stopwords in the textual unit that appear at least five times in the training set and analogous POS features. We also use lexical features on the first, last, preceding, and following words for each tex- tual unit. Finally, we conjoin each of these fea- tures with an indicator of bucketed position in the document (the index of the sentence containing the textual unit).</p><p>Structural These features include various con- junctions of the position of the textual unit in the document, its length, the length of its correspond- ing sentence, the index of the paragraph it occurs in, and whether it starts a new paragraph (all val- ues are bucketed).</p><p>Centrality These features capture rough infor- mation about the centrality of content: they consist of bucketed word counts conjoined with bucketed sentence index in the document. We also fire fea- tures on the number of times of each entity men- tioned in the sentence is mentioned in the rest of the document (according to a coreference system), the number of entities mentioned in the sentence, and surface properties of mentions including type and length</p><p>Pronoun replacement These target properties of the pronoun replacement such as its length, its sentence distance from the current mention, its type (nominal or proper), and the identity of the pronoun being replaced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2002</head><p>We learn weights w for our model by training on a large corpus of documents u paired with ref- erence summaries y. We formulate our learning problem as a standard instance of structured SVM (see <ref type="bibr" target="#b50">Smith (2011)</ref> for an introduction). Because we want to optimize explicitly for ROUGE-1, <ref type="bibr">7</ref> we define a ROUGE-based loss function that accom- modates the nature of our supervision, which is in terms of abstractive summaries y that in general cannot be produced by our model. Specifically, we take:</p><formula xml:id="formula_4">(x NGRAM , y) = maxx * ROUGE-1(x * , y) − ROUGE-1(x NGRAM , y)</formula><p>i.e. the gap between the hypothesis's ROUGE score and the oracle ROUGE score achievable under the model (including constraints). Here x NGRAM are indicator variables that track, for each n-gram type in the reference summary, whether that n-gram is present in the system summary. These are the sufficient statistics for computing ROUGE.</p><p>We train the model via stochastic subgradient descent on the primal form of the structured SVM objective ( <ref type="bibr" target="#b48">Ratliff et al., 2007;</ref><ref type="bibr" target="#b27">Kummerfeld et al., 2015)</ref>. In order to compute the subgradient for a given training example, we need to find the most violated constraint on the given instance through a loss-augmented decode, which for a linear model takes the form arg max x w f (x) + (x, y). To do this decode at training time in the context of our model, we use an extended version of our ILP in <ref type="figure">Figure 1</ref> that is augmented to explicitly track type- level n-grams:</p><formula xml:id="formula_5">max x UNIT ,x REF ,x NGRAM i x UNIT i (w f (ui)) + (i,j) x REF ij (w f (rij)) − (x NGRAM , y)</formula><p>  subject to all constraints from <ref type="figure">Figure 1</ref>, and</p><formula xml:id="formula_6">x NGRAM i</formula><p>= 1 iff an included textual unit or replacement contains the ith reference n-gram</p><p>These kinds of variables and constraints are com- mon in multi-document summarization systems that score bigrams <ref type="bibr">(Gillick and Favre, 2009 inter alia)</ref>. Note that since ROUGE is only com- puted over non-stopword n-grams and pronoun replacements only replace pronouns, pronoun re- placement can never remove an n-gram that would otherwise be included.</p><p>For all experiments, we optimize our objective using AdaGrad <ref type="bibr" target="#b11">(Duchi et al., 2011</ref>) with 1 regu- larization (λ = 10 −8 , chosen by grid search), with a step size of 0.1 and a minibatch size of 1. We train for 10 iterations on the training data, at which point held-out model performance no longer im- proves. Finally, we set the anaphora thresholds α = 0.8 and β = 0.6 (see Section 2.2). The val- ues of these and other hyperparameters were de- termined on a held-out development set from our New York Times training data. All ILPs are solved using GLPK version 4.55.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We primarily evaluate our model on a roughly 3000-document evaluation set from the New York Times Annotated Corpus <ref type="bibr" target="#b49">(Sandhaus, 2008)</ref>. We also investigate its performance on the RST Dis- course <ref type="bibr">Treebank (Carlson et al., 2001</ref>), but be- cause this dataset is only 30 documents it pro- vides much less robust estimates of performance. 8 Throughout this section, when we decode a docu- ment, we set the word budget for our summarizer to be the same as the number of words in the corre- sponding reference summary, following previous work ( <ref type="bibr" target="#b22">Hirao et al., 2013;</ref><ref type="bibr" target="#b54">Yoshida et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing</head><p>We preprocess all data using the Berkeley Parser ( <ref type="bibr" target="#b46">Petrov et al., 2006</ref>), specifically the GPU- accelerated version of the parser from <ref type="bibr" target="#b19">Hall et al. (2014)</ref>, and the Berkeley Entity Resolution Sys- tem <ref type="bibr" target="#b14">(Durrett and Klein, 2014)</ref>. For RST discourse analysis, we segment text into EDUs using a semi- Markov CRF trained on the RST treebank with features on boundaries similar to those of <ref type="bibr" target="#b21">Hernault et al. (2010)</ref>, plus novel features on spans includ- ing span length and span identity for short spans.</p><p>To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style of <ref type="bibr" target="#b22">Hirao et al. (2013)</ref>, since their parser is not publicly available. Specifically, Article on Speak-Up, program begun by Westchester County Office for the Aging to bring together elderly and college students.</p><p>National Center for Education Statistics reports students in 4th, 8th and 12th grades scored modestly higher on American history test than five years earlier. Says more than half of high school seniors still show poor command of basic facts. Only 4th graders made any progress in civics test. New exam results are another ingredient in debate over renewing Pres Bush's signature No Child Left Behind Act. Summary:</p><p>Federal officials reported yesterday that students in 4th, 8th and 12th grades had scored modestly higher on an American history test than five years earlier, although more than half of high school seniors still showed poor command of basic facts like the effect of the cotton gin on the slave economy or the causes of the Korean War. Federal officials said they considered the results encouraging because at each level tested, student performance had improved since the last time the exam was administered, in 2001. "In U.S. history there were higher scores in 2006 for all three grades," said Mark Schneider, commissioner of the National Center for Education Statistics, which administers the test, at a Boston news conference that the Education Department carried by Webcast. The results were less encouraging on a national civics test, on which only fourth graders made any progress. The best results in the history test were also in fourth grade, where 70 percent of students attained the basic level of achievement or better. The test results in the two subjects are likely to be closely studied, because Congress is considering the renewal of President Bush's signature education law, the No Child Left Behind Act. A number of studies have shown that because No Child Left Behind requires states… Long before President Bush's proposal to rethink Social Security became part of the national conversation, Westchester County came up with its own dialogue to bring issues of aging to the forefront. Before the White House Conference on Aging scheduled in October, the county's Office for the Aging a year ago started Speak-Up, which stands for Student Participants Embrace Aging Issues of Key Concern, to reach students in the county's 13 colleges and universities. Through a variety of events to bring together the elderly and college students, organizers said they hoped to have by this spring a series of recommendations that could be given to Washington… <ref type="figure">Figure 4</ref>: Examples of an article kept in the NYT50 dataset (top) and an article removed because the summary is too short. The top summary has a rich structure to it, corresponding to various parts of the document (bolded) and including some text that is essentially a direct extraction. we use the first-order projective parsing model of <ref type="bibr" target="#b38">McDonald et al. (2005)</ref> and features from Soricut and Marcu (2003), <ref type="bibr" target="#b21">Hernault et al. (2010)</ref>, and <ref type="bibr" target="#b25">Joty et al. (2013)</ref>. When using the same head anno- tation scheme as <ref type="bibr" target="#b54">Yoshida et al. (2014)</ref>, we out- perform their discourse dependency parser on un- labeled dependency accuracy, getting 56% as op- posed to 53%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">New York Times Corpus</head><p>We now provide some details about the New York Times Annotated corpus. This dataset contains 110,540 articles with abstractive summaries; we split these into 100,834 training and 9706 test ex- amples, based on date of publication (test is all articles published on January 1, 2007 or later). Examples of two documents from this dataset are shown in <ref type="figure">Figure 4</ref>. The bottom example demonstrates that some summaries are extremely short and formulaic (especially those for obituar- ies and editorials). To counter this, we filter the raw dataset by removing all documents with sum- maries that are shorter than 50 words. One benefit of filtering is that the length distribution of our re- sulting dataset is more in line with standard sum- marization evaluations like DUC; it also ensures a sufficient number of tokens in the budget to pro- duce nontrivial summaries. The filtered test set, which we call NYT50, includes 3,452 test exam- ples out of the original 9,706. Interestingly, this dataset is one where the clas- sic document prefix baseline can be substantially outperformed, unlike in some other summariza- tion settings <ref type="bibr" target="#b45">(Penn and Zhu, 2008)</ref>. We show this fact explicitly in Section 4.3, but <ref type="figure" target="#fig_3">Figure 5</ref> provides additional analysis in this regard. We compute or- acle ROUGE-1 sentence-extractive summaries on a 1000-document subset of the training set and look at where the extracted sentences lie in the document. While they certainly skew earlier in the document, they do not all fall within the doc-  <ref type="table">Table 1</ref>: Results on the NYT50 test set (documents with sum- maries of at least 50 tokens) from the New York Times Anno- tated Corpus <ref type="bibr" target="#b49">(Sandhaus, 2008)</ref>. We report ROUGE-1 (R-1), ROUGE-2 (R-2), clarity/grammaticality (CG), and number of unclear pronouns (UP) (lower is better). On content selection, our system substantially outperforms all baselines, our imple- mentation of the tree knapsack system ( <ref type="bibr" target="#b54">Yoshida et al., 2014)</ref>, and learned extractive systems with less compression, even an EDU-extractive system that sacrifices grammaticality. On clarity metrics, our final system performs nearly as well as sentence-extractive systems. The symbols * and † indicate statistically significant gains compared to No Anaphoricity and Tree Knapsack (respectively) with p &lt; 0.05 according to a bootstrap resampling test. We also see that removing either syntactic or EDU-based compressions decreases ROUGE.</p><formula xml:id="formula_7">R-1 ↑ R-2 ↑ CG ↑ UP ↓</formula><p>ument prefix summary. One reason for this is that many of the articles are longer-form pieces that be- gin with a relatively content-free lede of several sentences, which should be identifiable with lexi- cosyntactic indicators as are used in our discrimi- native model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">New York Times Results</head><p>We evaluate our system along two axes: first, on content selection, using ROUGE 9 ( <ref type="bibr" target="#b30">Lin and Hovy, 2003)</ref>, and second, on clarity of language and ref- erential structure, using annotators from Amazon Mechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original docu- ment or the reference summary, and rate each sum- mary in isolation. <ref type="bibr" target="#b16">Gillick and Liu (2010)</ref> showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the rank- ing of systems according to expert judgments. To speed up preprocessing and training time on this corpus, we further restrict our training set to only contain documents with fewer than 100 EDUs. All told, the final system takes roughly 20 hours to make 10 passes through the subsampled training data (22,000 documents) on a single core of an Amazon EC2 r3.4xlarge instance. <ref type="table">Table 1</ref> shows the results on the NYT50 cor- pus. We compare several variants of our sys- tem and baselines. For baselines, we use two variants of first k: one which must stop on a sentence boundary (which gives better linguistic quality) and one which always consumes k to- kens (which gives better ROUGE). We also use a heuristic sentence-extractive baseline that maxi- mizes the document counts (term frequency) of bi- grams covered by the summary, similar in spirit to the multi-document method of <ref type="bibr" target="#b15">Gillick and Favre (2009)</ref>. <ref type="bibr">10</ref> We also compare to our implementa- tion of the Tree Knapsack method of <ref type="bibr" target="#b54">Yoshida et al. (2014)</ref>, which matches their results very closely on the RST Discourse Treebank when discourse trees are controlled for. Finally, we compare sev- eral variants of our system: purely extractive sys- tems operating over sentences and EDUs respec- tively, our full system, and ablations removing ei- ther the anaphoricity component or parts of the compression module.</p><p>In terms of content selection, we see that all of the systems that incorporate end-to-end learning (under "This work") substantially outperform our various heuristic baselines. Our full system using the full compression scheme is substantially better on ROUGE than ablations where the syntactic or discourse compressions are removed. These im- provements reflect the fact that more compression options give the system more flexibility to include key content words. Removing the anaphora res- olution constraints actually causes ROUGE to in- crease slightly (as a result of granting the model flexibility), but has a negative impact on the lin- guistic quality metrics.</p><p>On our linguistic quality metrics, it is no sur- prise that the sentence prefix baseline performs the best. Our sentence-extractive system also does well on these metrics. Compared to the EDU- extractive system with no constraints, our con- strained compression method improves substan- tially on both linguistic quality and reduces the ROUGE-1 ROUGE-2 First k words 23.5 8.3 Tree Knapsack 25.1 8.7 Full 26.3 8.0 <ref type="table">Table 2</ref>: Results for RST Discourse Treebank ( <ref type="bibr" target="#b6">Carlson et al., 2001)</ref>. Differences between our system and the Tree Knap- sack system of <ref type="bibr" target="#b54">Yoshida et al. (2014)</ref> are not statistically sig- nificant, reflecting the high variance in this small (20 docu- ment) test set.</p><p>number of unclear pronouns, and adding the pro- noun anaphora constraints gives further improve- ment. Our final system is approaches the sentence- extractive baseline, particularly on unclear pro- nouns, and achieves substantially higher ROUGE score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RST Treebank</head><p>We also evaluate on the RST Discourse Tree- bank, of which 30 documents have abstractive summaries. Following <ref type="bibr" target="#b22">Hirao et al. (2013)</ref>, we use the gold EDU segmentation from the RST corpus but automatic RST trees. We break this into a 10- document development set and a 20-document test set. <ref type="table">Table 2</ref> shows the results on the RST cor- pus. Our system is roughly comparable to Tree Knapsack here, and we note that none of the differ- ences in the table are statistically significant. We also observed significant variation between multi- ple runs on this corpus, with scores changing by 1-2 ROUGE points for slightly different system variants. <ref type="bibr">11</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a single-document summarization system trained end-to-end on a large corpus. We integrate a compression model that enforces gram- maticality as well as pronoun anaphoricity con- straints that enforce coherence. Our system im- proves substantially over baseline systems on ROUGE while still maintaining good linguistic quality.</p><p>Our system and models are publicly available at http://nlp.cs.berkeley.edu</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Compression constraints on an example sentence. (a) RST-based compression structure like that in Hirao et al. (2013), where we can delete the ELABORATION clause. (b) Two syntactic compression options from Berg-Kirkpatrick et al. (2011), namely deletion of a coordinate and deletion of a PP modifier. (c) Textual units and requirement relations (arrows) after merging all of the available compressions. (d) Process of augmenting a textual unit with syntactic compressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Modifications to the ILP to capture pronoun coherence. It, which refers to Kellogg, has several possible antecedents from the standpoint of an automatic coreference system (Durrett and Klein, 2014). If the coreference system is confident about its selection (above a threshold α on the posterior probability), we allow for the model to explicitly replace the pronoun if its antecedent would be deleted (Section 2.2.1). Otherwise, we merely constrain one or more probable antecedents to be included (Section 2.2.2); even if the coreference system is incorrect, a human can often correctly interpret the pronoun with this additional context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Counts on a 1000-document sample of how frequently both a document prefix baseline and a ROUGE oracle summary contain sentences at various indices in the document. There is a long tail of useful sentences later in the document, as seen by the fact that the oracle sentence counts drop off relatively slowly. Smart selection of content therefore has room to improve over taking a prefix of the document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Ms . Johnson, dressed in jeans and a sweatshirt , is a claims adjuster with Aetna .Johnson , dressed in jeans and a sweatshirt , is a claims adjuster with Aetna .</head><label>Ms</label><figDesc></figDesc><table>NP CC 
NP 

NP 
NP 

NP 
PP 

SAME-UNIT 

(b) Syntactic compressions 
(a) Discourse compressions 

Ms. ELABORATION 

(c) Combined compressions 

Ms. Johnson , dressed in jeans and a sweatshirt , is a claims adjuster with Aetna . 

u 1 
u 2 

u 3 

u 4 u 5 
u 1 
u 2 

u 3 

u 1 
u 2 

u 3 

u 4 
u 5 
u 6 u 7 

(d) Augmentation Process 

is a claims adjuster with Aetna . 
is a claims adjuster with Aetna . 

IN 

</table></figure>

			<note place="foot" n="3"> We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations.</note>

			<note place="foot" n="4"> We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013).</note>

			<note place="foot" n="5"> If the proposed replacement is a proper mention, we replace the pronoun just with the subset of the mention that constitutes a named entity (rather than the whole noun phrase). We control for possessive pronouns by deleting or adding &apos;s as appropriate. 6 Such a previous mention may be a pronoun; however, note that that pronoun would then be targeted for replacement unless its antecedent were included somehow.</note>

			<note place="foot" n="7"> We found that optimizing for ROUGE-1 actually resulted in a model with better performance on both ROUGE-1 and ROUGE-2. We hypothesize that this is because framing our optimization in terms of ROUGE-2 would lead to a less nuanced set of constraints: bigram matches are relatively rare when the reference is a short, abstractive summary, so a loss function based on ROUGE-2 will express a flatter preference structure among possible outputs.</note>

			<note place="foot" n="8"> Tasks like DUC and TAC have focused on multidocument summarization since around 2003, hence the lack of more standard datasets for single-document summarization.</note>

			<note place="foot" n="9"> We use the ROUGE 1.5.5 script with the following command line arguments:-n 2-x-m-s. All given results are macro-averaged recall values over the test set.</note>

			<note place="foot" n="10"> Other heuristic multi-document approaches could be compared to, e.g. He et al. (2012), but a simple term frequency method suffices to illustrate how these approaches can underperform in the single-document setting.</note>

			<note place="foot" n="11"> The system of Yoshida et al. (2014) is unavailable, so we use a reimplementation. Our results differ from theirs due to having slightly different discourse trees, which cause large changes in metrics due to high variance on the test set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF Grant CNS-1237265 and a Google Faculty Research Award. Thanks to Tsutomu Hirao for providing assistance with our reimplementation of the Tree Knapsack model, and thanks the anonymous re-viewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling Local Coherence: An Entity-based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentence Ordering in Multidocument Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Human Language Technology Research</title>
		<meeting>the International Conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jointly Learning to Extract and Compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building a Discourse-tagged Corpus in the Framework of Rhetorical Structure Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGDIAL Workshop on Discourse and Dialogue</title>
		<meeting>the Second SIGDIAL Workshop on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards Coherent MultiDocument Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global Inference for Sentence Compression an Integer Linear Programming Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discourse Constraints for Document Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="441" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A NoisyChannel Model for Document Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A New Entity Salience Task with Millions of Training Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dunietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Easy Victories and Uphill Battles in Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Joint Model for Entity Analysis: Coreference, Typing, and Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Scalable Global Model for Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing</title>
		<meeting>the Workshop on Integer Linear Programming for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-Expert Evaluation of Summarization Systems is Risky</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Logic and Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Acts</title>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Centering: A Framework for Modeling the Local Coherence of Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparser, Better, Faster GPU Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document Summarization Based on Data Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanying</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<meeting>the Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HILDA: A discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norihito</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single-Document Summarization as a Tree Knapsack Problem</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the Estimation of Word Importance for News MultiDocument Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining Intra-and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single Document Summarization based on Nested Tree Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An Empirical Analysis of Optimization for Max-Margin NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using External Resources and Joint Learning for Bigram Weighting in ILP-Based Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Class of Submodular Functions for Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic Evaluation of Summaries Using N-gram CoOccurrence Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving Summarization Performance by Sentence Compression: A Pilot Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Information Retrieval with Asian Languages</title>
		<meeting>the International Workshop on Information Retrieval with Asian Languages</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Coherence Model Based on Syntactic Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discourse Indicators for Content Selection in Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2010 Conference</title>
		<meeting>the SIGDIAL 2010 Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">AutomaticSummarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Benjamins Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rhetorical Structure Theory: Toward a Functional Theory of Text Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving summarization through rhetorical parsing tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Very Large Corpora</title>
		<meeting>the Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Summarization with a Joint Model for Sentence Extraction and Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing</title>
		<meeting>the Workshop on Integer Linear Programming for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online Large-margin Training of Dependency Parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discriminative Sentence Compression With Soft Syntactic Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generating Concise Natural Language Summaries. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-09" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="703" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discourse Structures to Reduce Discourse Incoherence in Blog Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamima</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Kosseim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Recent Advances in Natural Language Processing</title>
		<meeting>Recent Advances in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2?3</biblScope>
			<biblScope unit="page" from="103" to="233" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Posterior Calibration and Exploratory Analysis for Natural Language Processing Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to Generate Coherent Summary with Discriminative Hidden Semi-Markov Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuho</forename><surname>Arita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumi</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiro</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Critical Reassessment of Evaluation Baselines for Speech Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning Accurate, Compact, and Interpretable Tree Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Linguistics and the Association for Computational Linguistics (ACLCOLING)</title>
		<meeting>the Conference on Computational Linguistics and the Association for Computational Linguistics (ACLCOLING)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cornolti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Online) Subgradient Methods for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">J</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The New York Times Annotated Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Linguistic Structure Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sentence Level Discourse Parsing Using Syntactic and Lexical Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Supervised Sentence Fusion with Single-Stage Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing</title>
		<meeting>the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multiple Aspect Summarization Using Integer Linear Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dependency-based Discourse Parser for Single-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
