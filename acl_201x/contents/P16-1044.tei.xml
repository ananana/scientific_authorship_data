<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Representation Learning for Question Answer Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson Core Technologies Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Cicero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson Core Technologies Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson Core Technologies Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson Core Technologies Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson Core Technologies Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Representation Learning for Question Answer Matching</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="464" to="473"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers. In this work, we propose a series of deep learning models to address passage answer selection. To match passage answers to questions accommodating their complex semantic relations, unlike most previous work that utilizes a single deep learning structure, we develop hybrid models that process the text using both convolutional and recurrent neu-ral networks, combining the merits on extracting linguistic information from both structures. Additionally, we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question, which is imperative for better modeling long answer sequences. The results on two public benchmark datasets, InsuranceQA and TREC-QA, show that our proposed models outperform a variety of strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Passage-level answer selection is one of the es- sential components in typical question answering (QA) systems. It can be defined as follows: Given a question and a pool of candidate passages, se- lect the passages that contain the correct answer. The performance of the passage selection task is not only crucial to non-factoid QA systems, where a question is expected to be answered with a se- quence of descriptive text (e.g. the question in Ta- ble 1), but also very important to factoid QA sys- tems, where the answer passage selection step is Question: Does Medicare cover my spouse?</p><p>Ground-truth answer: If your spouse has worked and paid Medicare taxes for the entire required 40 quarters, or is eligible for Medicare by virtue of being disabled or some other reason, your spouse can receive his/her own medicare benefits. If your spouse has not met those qualifications, if you have met them, and if your spouse is age 65, he/she can receive Medicare based on your eligibility. Another candidate answer: If you were married to a Medicare eligible spouse for at least 10 years, you may qualify for Medicare. If you are widowed, and have not remarried, and you were married to your spouse at least 9 months before your spouse's death, you may be eligible for Medicare benefits under a spouse provision. <ref type="table">Table 1</ref>: An example of a question with the ground-truth answer and a negative answer ex- tracted from the InsuranceQA dataset. also known as passage scoring. In factoid QA, if the sentences selected by the passage scorer mod- ule do not contain the answer, it will definitely lead to an incorrect response from the QA system.</p><p>One central challenge of this task lies in the complex and versatile semantic relations observed between questions and passage answers. For ex- ample, while the task of supporting passage selec- tion for factoid QA may be largely cast as a textual entailment problem, what makes an answer better than another in the real world for non-factoid QA often depends on many factors.</p><p>Specifically, different from many other pair- matching NLP tasks, the linguistic similarities be- tween questions and answers may or may not be indicative for our task. This is because, depending on what the question is looking for, a good answer may come in different forms: sometimes a correct answer completes the question precisely with the missing information, and in other scenarios, good answers need to elaborate part of the question to rationalize it, and so on. For instance, the ques- tion in <ref type="table">Table 1</ref> only contains five words, while the best answer uses 60 words for elaboration. On the other hand, the best answers from a pool can also be noisy and include extraneous information irrel- evant to the question. Additionally, while a good answer must relate to the question, they often do not share common lexical units. For instance, in the example question, "cover" is not directly men- tioned in the answer. This issue may confuse sim- ple word-matching systems.</p><p>These challenges consequently make hand- crafting features much less desirable compared to deep learning based methods. Furthermore, they also require our systems to learn how to distin- guish useful pieces from irrelevant ones, and fur- ther, to focus more on the former. Finally, the system should be capable of cap- turing the nuances between the best answer and an acceptable one. For example, the second an- swer in <ref type="table">Table 1</ref> is suitable for a questioner, whose spouse is Medicare eligible, asking about his/her own coverage, while the example question is more likely asked by a person, who is Medicare eligible, asking about his/her spouse' coverage. Clearly, the first answer is more appropriate for the ques- tion, although the second one implicitly answers it. A good system should reflect this preference.</p><p>While this task is usually approached as a pairwise-ranking problem, the best strategy to cap- ture the association between the questions and an- swers is still an open problem. Established ap- proaches normally suffer from two weaknesses at this point. First, prior work, such as <ref type="bibr" target="#b8">(Feng et al., 2015;</ref><ref type="bibr" target="#b25">Wang and Nyberg, 2015)</ref>, resort to ei- ther convolutional neural network (CNN) or re- current neural network (RNN) respectively. How- ever, each structure describes only one semantic perspective of the text. CNN emphasizes the lo- cal interaction within n-gram, while RNN is de- signed to capture long range information and for- get unimportant local information. How to com- bine the merits from both has not been sufficiently explored. Secondly, previous approaches are usu- ally based on independently generated question and answer embeddings; the quality of such rep- resentations, however, usually degrades as the an- swer sequences grow longer.</p><p>In this work, we propose a series of deep learning models in order to address such weak- nesses. We start with the basic discriminative framework for answer selection. We first propose two independent models, Convolutional-pooling LSTM and Convolution-based LSTM, which are designed to benefit from both of the two popu- lar deep learning structures to distinguish better between useful and irrelevant pieces presented in questions and answers. Next, by breaking the in- dependence assumption of the question and an- swer embedding, we introduce an effective atten- tion mechanism to generate answer representa- tions according to the question, such that the em- beddings do not overlook informative parts of the answers. We report experimental results for two answer selection datasets: (1) InsuranceQA <ref type="bibr" target="#b8">(Feng et al., 2015)</ref> 1 , a recently released large-scale non- factoid QA dataset from the insurance domain, and (2) TREC-QA 2 , which was created by <ref type="bibr" target="#b26">Wang et al. (2007)</ref> based on Text REtrieval Conference (TREC) QA track data.</p><p>The contribution of this paper is hence three- fold: 1) We propose hybrid neural networks, which learn better representations for both ques- tions and answers by combining merits of both RNN and CNN. 2) We prove the effectiveness of attention on the answer selection task, which has not been sufficiently explored in prior work. 3) We achieve the state-of-the-art results on both TREC- QA and InsuranceQA datasets.</p><p>The rest of the paper is organized as follows: Section 2 describes the related work for answer se- lection; Section 3 provides the details of the pro- posed models; Experimental settings and results are discussed in Section 4 and 5; Finally, we draw conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Previous work on answer selection normally used feature engineering, linguistic tools, or external re- sources. For example, semantic features were con- structed based on WordNet in ( <ref type="bibr" target="#b29">Yih et al., 2013)</ref>. This model pairs semantically related words based on word semantic relations. In ( <ref type="bibr" target="#b24">Wang and Manning, 2010;</ref><ref type="bibr" target="#b26">Wang et al., 2007</ref>), the answer se- lection problem was transformed to a syntacti-cal matching between the question/answer parse trees. Some work tried to fulfill the matching us- ing minimal edit sequences between dependency parse trees <ref type="bibr" target="#b10">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b28">Yao et al., 2013)</ref>. Discriminative tree-edit feature extraction and engineering over parsing trees were automated in <ref type="bibr" target="#b19">(Severyn and Moschitti, 2013)</ref>. Such methods might suffer from the availability of additional re- sources, the effort of feature engineering and the systematic complexity introduced by the linguis- tic tools, such as parse trees and dependency trees.</p><p>Some recent work has used deep learning meth- ods for the passage-level answer selection task. The approaches normally pursue the solution on the following directions. First, a joint feature vec- tor is constructed based on both the question and the answer, and then the task can be converted into a classification or ranking problem ( <ref type="bibr" target="#b25">Wang and Nyberg, 2015;</ref><ref type="bibr" target="#b12">Hu et al., 2014</ref>). Second, recently proposed models for text generation can intrinsi- cally be used for answer selection and generation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b23">Vinyals and Le, 2015)</ref>. Fi- nally, the question and answer representations can be learned and then matched by certain similarity metrics <ref type="bibr" target="#b8">(Feng et al., 2015;</ref><ref type="bibr" target="#b31">Yu et al., 2014;</ref><ref type="bibr" target="#b6">dos Santos et al., 2015;</ref><ref type="bibr" target="#b15">Qiu and Huang, 2015)</ref>. Fun- damentally, our proposed models belong to the last category.</p><p>Meanwhile, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b22">Sutskever et al., 2014</ref>), machine reading comprehension ( <ref type="bibr" target="#b11">Hermann et al., 2015</ref>), text sum- marization ( <ref type="bibr" target="#b17">Rush et al., 2015</ref>) and text entailment <ref type="bibr" target="#b16">(Rocktäschel et al., 2016)</ref>. Such models learn to focus their attention to specific parts of their input and most of them are based on a one-way attention, in which the attention is basically performed merely over one type of input based on another (e.g. over target languages based on the source languages for machine translation, or over documents according to queries for reading comprehension). Most recently, several two-way attention mechanisms are proposed, where the in- formation from the two input items can influence the computation of each others representations. <ref type="bibr" target="#b16">Rocktäschel et al. (2016)</ref> develop a two-way attention mechanism including another one-way attention over the premise conditioned on the hypothesis, in addition to the one over hypothesis conditioned on premise. dos <ref type="bibr" target="#b7">Santos et al. (2016)</ref> and <ref type="bibr" target="#b30">Yin et al. (2015)</ref> generate interactive attention weights on both inputs by assignment matrices. <ref type="bibr" target="#b30">Yin et al. (2015)</ref> use a simple Euclidean distance to compute the interdependence between the two input texts, while dos <ref type="bibr" target="#b7">Santos et al. (2016)</ref> resort to attentive parameter matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>In this section, we first present our basic discrim- inative framework for answer selection based on long short-term memory (LSTM), which we call QA-LSTM. Next, we detail the proposed hybrid and attentive neural networks that are built on top of the QA-LSTM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LSTM for Answer Selection</head><p>Our LSTM implementation is similar to the one in ( <ref type="bibr" target="#b9">Graves et al., 2013</ref>) with minor mod- ifications.</p><p>Given an input sequence X = {x(1), x(2), · · · , x(n)}, where x(t) is an E- dimension word vector in this paper, the hidden vector h(t) (with size H) at the time step t is up- dated as follows.</p><formula xml:id="formula_0">i t = σ(W i x(t) + U i h(t − 1) + b i ) (1) f t = σ(W f x(t) + U f h(t − 1) + b f ) (2) o t = σ(W o x(t) + U o h(t − 1) + b o ) (3) ˜ C t = tanh(W c x(t) + U c h(t − 1) + b c )(4) C t = i t * ˜ C t + f t * C t−1 (5) h t = o t * tanh(C t )<label>(6)</label></formula><p>There are three gates (input i, forget f and out- put o), and a cell memory vector C t . σ is the sigmoid function. W ∈ R H×E , U ∈ R H×H and b ∈ R H×1 are the network parameters.</p><p>Single-direction LSTMs suffer from the weak- ness of not making use of the contextual informa- tion from the future tokens. Bidirectional LSTMs (biLSTMs) use both the previous and future con- text by processing the sequence in two directions, and generate two sequences of output vectors. The output for each token is the concatenation of the two vectors from both directions, i.e.</p><formula xml:id="formula_1">h t = − → h t ← − h t . QA-LSTM:</formula><p>Our basic answer selection frame- work is shown in <ref type="figure">Figure 1</ref>. Given an input pair (q,a), where q is a question and a is a candidate an- swer, first we retrieve the word embeddings (WEs) of both q and a. Then, we separately apply a biLSTM over the two sequences of WEs. Next, we generate a fixed-sized distributed vector rep- resentations using one of the following three ap- proaches: (1) the concatenation of the last vec- tors on both directions of the biLSTM; (2) average pooling over all the output vectors of the biLSTM; (3) max pooling over all the output vectors. Fi- nally, we use cosine similarity sim(q, a) to score the input (q, a) pair. It is important to note that the same biLSTM is applied to both q and a.</p><p>Similar to <ref type="bibr" target="#b8">(Feng et al., 2015;</ref><ref type="bibr" target="#b27">Weston et al., 2014;</ref><ref type="bibr" target="#b12">Hu et al., 2014</ref>), we define the training ob- jective as a hinge loss.</p><formula xml:id="formula_2">L = max{0, M −sim(q, a + )+sim(q, a − )} (7)</formula><p>where a + is a ground truth answer, a − is an incor- rect answer randomly chosen from the entire an- swer space, and M is a margin. We treat any ques- tion with more than one ground truth as multiple training examples. During training, for each ques- tion we randomly sample K negative answers, but only use the one with the highest L to update the model. Finally, dropout operation is performed on the representations before cosine similarity match- ing.</p><p>The same scoring function, loss function and negative sampling procedure is also used in the NN architectures presented in what follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional LSTMs</head><p>The pooling strategies used in QA-LSTM suffer from the incapability of filtering important local information, especially when dealing with long answer sequences.</p><p>Also, it is well known that LSTM models suc- cessfully keep the useful information from long- range dependency. But the strength has a trade- off effect of ignoring the local n-gram coherence. This can be partially alleviated with bidirectional architectures.</p><p>Meanwhile, the convolutional structures have been widely used in the question answering tasks, <ref type="figure">Figure 1</ref>: Basic Model: QA-LSTM such as ( <ref type="bibr" target="#b31">Yu et al., 2014;</ref><ref type="bibr" target="#b8">Feng et al., 2015;</ref><ref type="bibr" target="#b12">Hu et al., 2014</ref>). Classical convolutional layers usu- ally emphasize the local lexical connections of the n-gram. However, the local pieces are associated with each other only at the pooling step. No long- range dependencies are taken into account during the formulation of convolution vectors.</p><p>Fundamentally, recurrent and convolutional neural networks have their own pros and cons, due to their different topologies. How to keep both merits motivates our studies of the following two hybrid models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Convolutional-pooling LSTMs</head><p>In <ref type="figure">Figure 2</ref> we detail the convolutional-pooling LSTM architecture. In this NN architecture, we replace the simple pooling layers (average/max- pooling) by a convolutional layer, which allows to capture richer local information by applying a convolution over sequences of LSTM output vec- tors. The number of output vectors k (context window size) considered by the convolution is a hyper-parameter of the model. The convolution structure adopted in this work is as follows: Z ∈ R k|h|×L is a matrix where the m-th column is the concatenation of k hidden vectors generated from biLSTM centralized in the m-th word of the sequence, L is the length of the sequence after wide convolution ( <ref type="bibr" target="#b13">Kalchbrenner et al., 2014</ref>). The output of the convolution with c filters is,</p><formula xml:id="formula_3">C = tanh(W cp Z)<label>(8)</label></formula><p>where W cp are network parameters, and C ∈ R c×L . The j-th element of the representation vec- tors (o q and o a ) is computed as follows,</p><formula xml:id="formula_4">[o j ] = max 1&lt;l&lt;L [C j,l ]<label>(9)</label></formula><p>Figure 2: Convolutional-pooling LSTM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Convolution-based LSTMs</head><p>In <ref type="figure" target="#fig_0">Figure 3</ref>, we detail our second hybrid NN ar- chitecture. The aim of this approach is to capture the local n-gram interaction at the lower level us- ing a convolution. At the higher level, we build bidirectional LSTMs, which extract the long range dependency based on convoluted n-gram. Com- bining convolutional and recurrent structures have been investigated in prior work other than question answering ( <ref type="bibr" target="#b5">Donahue et al., 2015;</ref><ref type="bibr" target="#b32">Zuo et al., 2015;</ref><ref type="bibr" target="#b18">Sainath et al., 2015)</ref>. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>,</p><note type="other">the model first retrieves word vectors for each token in the sequence. Next, we compose the matrix D ∈ R kE×L , where each column l in D consists of the concatenation of k word vectors of size E centered at the l-th word.</note><p>The matrix X ∈ R c×L , which is the output of the convolution with c filters is computed as follows:</p><formula xml:id="formula_5">X = tanh(W cb D)<label>(10)</label></formula><p>The matrix X is the input to the biLSTM structure in Eqs. 1-6. After the biLSTM step, we use max- pooling over the biLSTM output vectors to obtain the representations of both q and a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attentive LSTMs</head><p>In the previous subsections, the two most popular deep learning architectures are integrated to gen- erate semantic representations for questions and answers from both the long-range sequential and local n-gram perspectives. QA-LSTM and the two proposed hybrid mod- els are basically siamese networks ( <ref type="bibr" target="#b4">Chopra et al., 2005</ref>). These structures overlook another poten- tial issue. The answers might be extremely long and contain lots of words that are not related to the question at hand. No matter what advanced neural networks are exploited at the answer side, the re- sulting representation might still be distracted by non-useful information. A typical example is the  <ref type="table">Table 1</ref>. If the con- struction of the answer representation is not aware of the input question, the representation might be strongly influenced by n-grams such as "are wid- owed" and "your spouse's death", which are in- formative if we only look at the candidate answer, but are not so important for the input question. We address this problem by developing a simple attention model for the answer vector generation, in order to alleviate this weakness by dynamically aligning the more informative parts of answers to the questions.</p><p>Inspired by the work in ( <ref type="bibr" target="#b11">Hermann et al., 2015</ref>), we develop a very simple but efficient word-level attention on the basic model. In <ref type="figure" target="#fig_1">Figure 4</ref>, we detail our Attentive LSTM architecture. Prior to the av- erage or mean pooling, each biLSTM output vec- tor is multiplied by a softmax weight, which is de- termined by the question representation from biL- STM. Specifically, given the output vector of biL- STM on the answer side at time step t, h a (t), and the question representation, o q , the updated vector h a (t) for each answer token are formulated below.</p><formula xml:id="formula_6">m a,q (t) = W am h a (t) + W qm o q (11) s a,q (t) ∝ exp(w T ms tanh(m a,q (t))) (12) h a (t) = h a (t)s a,q (t)<label>(13)</label></formula><p>where W am , W qm and w ms are attention pa- rameters. Conceptually, the attention mechanism gives more weight to certain words of the can- didate answer, where the weights are computed by taking into consideration information from the question. The expectation is that words in the can- didate answer that are more important with regard to the input question should receive larger weights. The attention mechanism in this paper is con- ceptually analogous to the one used in one-layer  <ref type="table" target="#tab_1">Train Validation Test1 Test2  # of Qs 12887 1000  1800 1800  # of As 18540 1454  2616 2593   Table 2</ref>: Numbers of Qs and As in InsuranceQA. memory network ( <ref type="bibr" target="#b21">Sukhbaatar et al., 2015</ref>). The fundamental difference is that the transformed question vector and answer unit vectors are com- bined in an inner-product pattern in order to gener- ate attentive weights in memory network, whereas this work adopts a summation operation (Eq. 11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">InsuranceQA Experiments</head><p>The first dataset we use to evaluate the proposed approaches is the InsuranceQA, which has been recently proposed by <ref type="bibr" target="#b8">Feng et al. (2015)</ref>. We use the first version of this dataset. This dataset con- tains question and answer pairs from the insurance domain and is already divided into a training set, a validation set, and two test sets. We do not see any obvious categorical differentiation between two tests' questions. We list the numbers of questions and answers of the dataset in <ref type="table">Table 2</ref>. We refer the reader to <ref type="bibr" target="#b8">(Feng et al., 2015)</ref>, for more details regarding the InsuranceQA data. In this dataset, a question may have multiple correct answers, and normally the questions are much shorter than an- swers. The average length of questions in tokens is 7, while the average length of answers is 94. Such difference posts additional challenges for the an- swer selection task. This corpus contains 24981 unique answers in total. For the development and test sets, the InsuranceQA also includes an answer pool of 500 candidate answers for each question. These answer pools were constructed by including the correct answer(s) and randomly selected can- didates from the complete set of unique answers. The top-1 accuracy of the answer selection is re- ported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The proposed models are implemented with Theano ( <ref type="bibr" target="#b1">Bastien et al., 2012</ref>) and all experiments are conducted in a GPU cluster. We use the accu- racy on validation set to select the best epoch and best hyper-parameter settings for testing.</p><p>The word embeddings are pre-trained, using word2vec ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>  <ref type="bibr">3</ref> . The training data for the word embeddings is a Wikipedia cor-pus of 164 million tokens combined with the ques- tions and answers in the InsuranceQA training set. The word vector size is set to 100. Word embed- dings are also part of the parameters and are op- timized during the training. Stochastic Gradient Descent (SGD) is the optimization strategy. The learning rate λ is 1.1. We get the best perfor- mances when the negative answer count K = 50. We also tried different margins in the hing loss function, and finally fixed the margin as M =0.2. We train our models in mini-batches (with batch size as 20), and the maximum length L of ques- tions and answers is 200. Any tokens out of this range are discarded. In order to get more obvious comparison between the proposed models and the basic framework, with respect to the ground-truth answer length in <ref type="figure" target="#fig_2">Fig. 5</ref>, we also provide the results of K = 1. In this case, we set M = 0.1, λ = 0.1 and mini-batches as 100 to get the best perfor- mance on the validation set. Also, the dimen- sion of LSTM output vectors is 141x2 for bidirec- tional LSTM in QA-LSTM, Attentive LSTM and Convolutional-pooling LSTM, such that biLSTM has a comparable number of parameters with a single-direction LSTM with 200 dimensions. For Convolution-based LSTM, since LSTM structure is built on the top of CNN, we fixed the CNN out- put as 282 dimensions and tune the biLSTM hid- den vector size in the experiments.</p><p>Because the sequences within a mini-batch have different lengths, we use a mask matrix to indicate the real length of each sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>For comparison, we report the performances of four baselines in the top group in <ref type="table">Table 3</ref>: two state-of-the-art non-DL approaches and two varia- tions of a strong DL approach based on CNN.</p><p>Bag-of-word: The idf-weighted sum of word vectors is used as a feature vector. The candidates are ranked by the cosine similarity to the question.</p><p>Metzler-Bendersky IR model: A state-of-the- art weighted dependency model ( <ref type="bibr" target="#b2">Bendersky et al., 2010;</ref><ref type="bibr" target="#b3">Bendersky et al., 2011</ref>), which employs a weighted combination of term-based and term proximity-based features to score each candidate.</p><p>Architecture-II in <ref type="bibr" target="#b8">(Feng et al., 2015</ref>): A CNN model is employed to learn distributed representa- tions of questions and answers. Cosine similarity is used to rank answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation Test1 Test2</head><p>Bag-of-word 31.9 32.1 32.2 Metzler-Bendersky IR model 52.7 55.1 50.8 CNN <ref type="bibr" target="#b8">(Feng et al., 2015)</ref> 61.8 62.8 59.2 CNN with GESD <ref type="bibr" target="#b8">(Feng et al., 2015)</ref> 65.4 65.3 61.0 A QA-LSTM (head/tail) 54.8 53.6 51.0 B QA- <ref type="figure" target="#fig_2">LSTM (avg pooling,K=50)</ref> 55</p><note type="other">.0 55.7 52.4 C QA-LSTM (max pooling,K=1) 64.3 63.1 58.0 D QA-LSTM (max pooling,K=50) 66.6 66.6 63.7 E Conv-pooling LSTM (c=4000,K=1) 66.2 64.6 62.2 F Conv-pooling LSTM (c=200,K=50) 66.4 67.4 63.5 G Conv-pooling LSTM (c=400,K=50) 67.8 67.5 64.4 H Conv-based LSTM (|h|=200,K=50) 66.0 66.1 63.0 I Conv-based LSTM (|h|=400,K=50) 67.1 67.6 64.4 J QA-CNN (max-pooling, k = 3) 61.6 62.2 57.9 K Attentive CNN (max-pooling, k = 3) 62.3 63.3 60.2 L Attentive LSTM (avg-pooling K=1) 68.4 68.1 62.2 M Attentive LSTM (avg-pooling K=50) 68.4 67.8 63.2 N Attentive LSTM (max-pooling K=50) 68.9</note><p>69.0 64.8 <ref type="table">Table 3</ref>: The experimental results of InsuranceQA.</p><p>Architecture-II with Geometricmean of Eu- clidean and Sigmoid Dot product (GESD): Co- sine similarity is replaced by GESD, which got the best performance in <ref type="figure" target="#fig_2">(Feng et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and discussions</head><p>In this section, we provide detailed analysis on the experimental results. <ref type="table">Table 3</ref> summarizes the re- sults of our models on InsuranceQA. From Row (A) to (D), we list QA-LSTM without either CNN structure or attention. They vary on the pooling method used. We can see that by concatenating the last vectors from both directions, (A) performs the worst. We see that using max-pooling (C) is much better than average pooling (B). The poten- tial reason may be that the max-pooling extracts more local values for each dimension. Compared to (C), (D) is better, showing the need of multiple negative answers in training.</p><p>Row (E) to (I) show the results of Convolutional-pooling LSTMs and Convolution- based LSTMs with different filter sizes c, biLSTM hidden sizes |h| and negative answer pool size K. Increasing the negative answer pool size, we are allowed to use less filter counts (F vs E). Larger filter counts help on the test accuracies (G vs F) for Convolutional-pooling LSTMs. We have the same observation with larger biLSTM hidden vector size for Convolution-based LSTMs.</p><p>Both convolutional models outperform the plain QA-LSTM (D) by about 1.0% on test1, and 0.7% on test2.</p><p>Rows (L-N) correspond to QA-LSTM with the attention model, with either max-pooling or aver- age pooling. We observe that max-pooling is bet- ter than avg-pooling, which is consistent with QA- LSTMs. In comparison to Model (D), Model (N) shows over 2% improvement on both validation and Test1 sets. And (N) gets improvements over the best baseline in <ref type="table">Table 3</ref> by 3.5%, 3.7% and 3.8% on the validation, Test1 and Test2 sets, re- spectively. Compared to Architecture II in <ref type="bibr" target="#b8">(Feng et al., 2015)</ref>, which involved a large number of CNN filters, (N) model also has fewer parameters.</p><p>We also test the proposed attention mechanism on convolutional networks. (J) replaces the LSTM in QA-LSTM with a convolutional layer. We set the filter size c = 400 and window size k = 3 ac- cording to the validation accuracy. (K) performs the similar attention on the convolutional output of the answers. Similar to biLSTM, the attention on the convolutional layer gives over 2% accu- racy improvement on both test sets, which proves the attention's efficiency on both CNN and RNN structures.</p><p>Finally, we investigate the proposed models on how they perform with respect to long answers. To better illustrate the performance difference, we Models MAP MRR ( <ref type="bibr" target="#b28">Yao et al., 2013)</ref> 0.631 0.748 <ref type="bibr" target="#b19">(Severyn and Moschitti, 2013)</ref> 0.678 0.736 ( <ref type="bibr" target="#b29">Yih et al., 2013</ref>)-BDT 0.694 0.789 ( <ref type="bibr" target="#b29">Yih et al., 2013</ref>)-LCLR 0.709 0.770 ( <ref type="bibr" target="#b25">Wang and Nyberg, 2015)</ref> 0.713 0.791 Architecture-II <ref type="bibr" target="#b8">(Feng et al., 2015)</ref> 0.711 0.800 <ref type="bibr" target="#b20">(Severyn and Moschitti, 2015)</ref> 0.671 0.728 w/o additional features <ref type="bibr" target="#b20">(Severyn and Moschitti, 2015)</ref> 0  The test set results on TREC-QA compare the models with K = 1 (i.e. the mod- els C, E, L). We divide the questions of Test1 and Test2 sets into eleven buckets, according to the average length of their ground truth answers. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, QA-LSTM gets better or simi- lar performance compared to the proposed mod- els on buckets with shorter answers (L ≤ 50, 50 &lt; L ≤55, 55 &lt; L ≤60). As the answer lengths increase, the gap between QA-LSTM and other models becomes more obvious. It suggests the effectiveness of Convolutional-pooling LSTM and Attentive LSTM for long-answer questions.</p><p>In <ref type="bibr" target="#b8">(Feng et al., 2015)</ref>, GESD outperforms co- sine similarity in their models. However, the pro- posed models with GESD as similarity scores do not provide any improvement on the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TREC-QA Experiments</head><p>In this section we detail our experimental setup and results using the TREC-QA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data, metrics and baselines</head><p>We test the models on TREC-QA dataset, cre- ated based on Text REtrieval Conference (TREC) QA track (8-13) data. More detail of the gener- ation steps for this data can be found in ( <ref type="bibr" target="#b26">Wang et al., 2007)</ref>. We follow the exact approach of train/dev/test questions selection in ( <ref type="bibr" target="#b25">Wang and Nyberg, 2015)</ref>, in which all questions with only positive or negative answers are removed. Finally, we have 1162 training, 65 development and 68 test questions. Similar to previous work, we use Mean Average Precision (MAP) and Mean Recip- rocal Rank (MRR) as evaluation metrics, which are evaluated using the official scripts.</p><p>In the top part of <ref type="table" target="#tab_1">Table 4</ref>, we list the perfor- mance of recent prior work on this dataset. We implemented the Architecture II in <ref type="bibr" target="#b8">(Feng et al., 2015</ref>) from scratch. The CNN structure in <ref type="bibr" target="#b20">(Severyn and Moschitti, 2015</ref>) combined with addi- tional human-designed features achieved the best MAP and MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Setup</head><p>We keep the configurations same as those in Insur- anceQA in section 4.1, except the following differ- ences: 1) Following <ref type="bibr" target="#b25">Wang and Nyberg (2015)</ref>, we use 300-dimensional vectors that were trained and provided by <ref type="bibr">word2vec (Mikolov et al., 2013</ref>) us- ing a part of the Google News dataset 4 . 2) Since the word vectors of TREC-QA have a greater di- mension than InsuranceQA, we accordingly have larger biLSTM hidden vectors and CNN filters, in order not to lose information from word vectors. Here we set both of them as 600. 3) We use the models from the epoch with the best MAP on the validation set. 4) We also observe that because of the smaller data size, we need a decayed learn- ing rate λ in order to stablize the models' training. Specifically, we set the initial λ 0 = 1.1, and de- crease it for each epoch T &gt; 1 as λ T = λ 0 /T . 5) We fix the negative answer size K = 50 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The bottom part of <ref type="table" target="#tab_1">Table 4</ref> shows the perfor- mance of the proposed models. For the compar- ison purpose, we replace biLSTM with a convo- lution in Model (A), and also use max-pooling to get question and answer embeddings, and call this model QA-CNN. QA-LSTM (B) improves MAP and MRR in more than 1% when compared to QA-CNN (A). Compared to (B), convolutional- pooling (C) performs better on MAP by 0.9%, and convolution-based models on MAP by 0.4% and MRR by 0.8%. Attentive LSTM is the best proposed model, and outperforms the best base- line ( <ref type="bibr" target="#b20">Severyn and Moschitti, 2015</ref>) by 0.7% on MAP and 2.2% on MRR. Note that the best re- sult in <ref type="bibr" target="#b20">(Severyn and Moschitti, 2015</ref>) was obtained by combining CNN-based features with additional human-defined features. In contrast, our attentive LSTM model achieves higher performance with- out using any human-defined features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we address the following problem for the answer passage selection: how can we con- struct the embeddings for questions and candidate  <ref type="table">Table 3</ref>, on different levels of ground truth answer lengths on each test set. The figures show the accuracy of each bucket.</p><p>answers, in order to better distinguish the correct answers from other candidates? We propose three independent models in two directions. First, we develop two hybrid models which combine the strength of both recurrent and convolutional neu- ral networks. Second, we introduce a simple one- way attention mechanism, in order to generate an- swer embeddings influenced by the question con- text. Such attention fixes the issue of independent generation of the question and answer embeddings in previous work. All proposed models are de- parted from a basic architecture, built on bidirec- tional LSTMs. We conduct experiments on Insur- anceQA and TREC-QA datasets, and the experi- mental results demonstrate that the proposed mod- els outperform a variety of strong baselines. Po- tential future work include: 1) Evaluating the pro- posed approaches for different tasks, such as com- munity QA and textual entailment; 2) Including the sentential attention mechanism; 3) Integrating the hybrid and the attentive mechanisms into a sin- gle framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convolution-based LSTM</figDesc><graphic url="image-3.png" coords="5,90.43,636.87,181.41,100.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attentive LSTM</figDesc><graphic url="image-4.png" coords="5,325.70,613.44,181.42,123.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The accuracy of Test1 and Test2 of InsuranceQA sets for three models, i.e. maxpooling QALSTM (C), Convolutional-pooling LSTM (E) and Attentive LSTM (L) in Table 3, on different levels of ground truth answer lengths on each test set. The figures show the accuracy of each bucket.</figDesc><graphic url="image-5.png" coords="9,72.00,63.45,233.92,142.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> git clone https://github.com/shuzi/insuranceQA.git (We use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2</note>

			<note place="foot" n="3"> https://code.google.com/p/word2vec/</note>

			<note place="foot" n="4"> https://code.google.com/archive/p/word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International conference of learning representations</title>
		<meeting>International conference of learning representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning concept importance using a weighted dependence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the Third ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parameterized concept weighting in verbose queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hybrid representations to retrieve semantically equivalent questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cícero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
	<note>Dasha Bogdanova, and Bianca Zadrozny</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cícero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.03609</idno>
		<title level="m">Attentive pooling networks. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Applying deep learning to answer selection: A study and an open task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for community-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A neural attention model for sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Andrew Senior Oriol Vinyals, and Hasim Sak</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In SIGIR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic tree-edit models with structured latent variables for textual entailment and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the 23rd International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A long shortterm memory model for answer sentence selection in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What is the jeopardy model? a quasisynchronous grammar for qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitamura</forename><surname>Teruko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">#tagspace: Semantic embeddings from hashtags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Answer extraction as sequence tagging with tree edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguist (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguist (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Abcnn: attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schutze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks: Learning spatial dependencies for image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
