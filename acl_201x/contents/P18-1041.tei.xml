<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<email>dinghan.shen@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="440" to="450"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>440</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many deep learning architectures have been proposed to model the composition-ality in text sequences, requiring a substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification ; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings, learned from massive unstruc- tured text data, are widely-adopted building blocks for Natural Language Processing (NLP). By rep- resenting each word as a fixed-length vector, these embeddings can group semantically simi- lar words, while implicitly encoding rich linguis- tic regularities and patterns ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013;</ref><ref type="bibr" target="#b21">Pennington et al., 2014</ref>).</p><p>Leveraging the word-embedding construct, many deep architectures have been proposed to model the compositionality in variable-length text se- quences. These methods range from simple op- erations like addition ( <ref type="bibr" target="#b17">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b10">Iyyer et al., 2015)</ref>, to more sophisticated compo- sitional functions such as Recurrent Neural Net- works (RNNs) <ref type="bibr" target="#b29">(Tai et al., 2015;</ref>), Convolutional Neural Networks (CNNs) ( <ref type="bibr" target="#b12">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b13">Kim, 2014;</ref><ref type="bibr" target="#b35">Zhang et al., 2017a</ref>) and Recursive Neural Networks ( <ref type="bibr" target="#b25">Socher et al., 2011a)</ref>.</p><p>Models with more expressive compositional functions, e.g., RNNs or CNNs, have demon- strated impressive results; however, they are typ- ically computationally expensive, due to the need to estimate hundreds of thousands, if not millions, of parameters ( <ref type="bibr" target="#b20">Parikh et al., 2016)</ref>. In contrast, models with simple compositional functions often compute a sentence or document embedding by simply adding, or averaging, over the word em- bedding of each sequence element obtained via, e.g., <ref type="bibr">word2vec (Mikolov et al., 2013)</ref>, or GloVe ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>). Generally, such a Sim- ple Word-Embedding-based Model (SWEM) does not explicitly account for spatial, word-order in- formation within a text sequence. However, they possess the desirable property of having signif- icantly fewer parameters, enjoying much faster training, relative to RNN-or CNN-based models. Hence, there is a computation-vs.-expressiveness tradeoff regarding how to model the composition- ality of a text sequence.</p><p>In this paper, we conduct an extensive experi- mental investigation to understand when, and why, simple pooling strategies, operated over word em- beddings alone, already carry sufficient informa- tion for natural language understanding. To ac- count for the distinct nature of various NLP tasks that may require different semantic features, we compare SWEM-based models with existing re- current and convolutional networks in a point- by-point manner. Specifically, we consider 17 datasets, including three distinct NLP tasks: doc- ument classification (Yahoo news, Yelp reviews, etc.), natural language sequence matching (SNLI, WikiQA, etc.) and (short) sentence classifica- tion/tagging (Stanford sentiment treebank, TREC, etc.). Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered.</p><p>In order to validate our experimental findings, we conduct additional investigations to understand to what extent the word-order information is uti- lized/required to make predictions on different tasks. We observe that in text representation tasks, many words (e.g., stop words, or words that are not related to sentiment or topic) do not meaning- fully contribute to the final predictions (e.g., sen- timent label). Based upon this understanding, we propose to leverage a max-pooling operation di- rectly over the word embedding matrix of a given sequence, to select its most salient features. This strategy is demonstrated to extract complementary features relative to the standard averaging opera- tion, while resulting in a more interpretable model. Inspired by a case study on sentiment analysis tasks, we further propose a hierarchical pooling strategy to abstract and preserve the spatial infor- mation in the final representations. This strategy is demonstrated to exhibit comparable empirical results to LSTM and CNN on tasks that are sensi- tive to word-order features, while maintaining the favorable properties of not having compositional parameters, thus fast training.</p><p>Our work presents a simple yet strong base- line for text representation learning that is widely ignored in benchmarks, and highlights the gen- eral computation-vs.-expressiveness tradeoff asso- ciated with appropriately selecting compositional functions for distinct NLP problems. Furthermore, we quantitatively show that the word-embedding- based text classification tasks can have the similar level of difficulty regardless of the employed mod- els, using the subspace training ( <ref type="bibr" target="#b15">Li et al., 2018)</ref> to constrain the trainable parameters. Thus, accord- ing to Occam's razor, simple models are preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A fundamental goal in NLP is to develop expres- sive, yet computationally efficient compositional functions that can capture the linguistic structure of natural language sequences. Recently, several studies have suggested that on certain NLP ap- plications, much simpler word-embedding-based architectures exhibit comparable or even superior performance, compared with more-sophisticated models using recurrence or convolutions <ref type="bibr" target="#b20">(Parikh et al., 2016;</ref><ref type="bibr">Vaswani et al., 2017)</ref>. Although complex compositional functions are avoided in these models, additional modules, such as atten- tion layers, are employed on top of the word em- bedding layer. As a result, the specific role that the word embedding plays in these models is not emphasized (or explicit), which distracts from un- derstanding how important the word embeddings alone are to the observed superior performance. Moreover, several recent studies have shown em- pirically that the advantages of distinct composi- tional functions are highly dependent on the spe- cific task <ref type="bibr" target="#b17">(Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b10">Iyyer et al., 2015;</ref><ref type="bibr" target="#b33">Zhang et al., 2015a;</ref><ref type="bibr" target="#b31">Wieting et al., 2015;</ref><ref type="bibr" target="#b1">Arora et al., 2016)</ref>. Therefore, it is of interest to study the practical value of the additional expres- siveness, on a wide variety of NLP problems.</p><p>SWEMs bear close resemblance to Deep Aver- aging Network (DAN) <ref type="bibr" target="#b10">(Iyyer et al., 2015)</ref> or fast- Text ( <ref type="bibr" target="#b11">Joulin et al., 2016</ref>), where they show that average pooling achieves promising results on cer- tain NLP tasks. However, there exist several key differences that make our work unique. First, we explore a series of pooling operations, rather than only average-pooling. Specifically, a hierarchi- cal pooling operation is introduced to incorporate spatial information, which demonstrates superior results on sentiment analysis, relative to average pooling. Second, our work not only explores when simple pooling operations are enough, but also in- vestigates the underlying reasons, i.e., what se- mantic features are required for distinct NLP prob- lems. Third, DAN and fastText only focused on one or two problems at a time, thus a compre- hensive study regarding the effectiveness of vari- ous compositional functions on distinct NLP tasks, e.g., categorizing short sentence/long documents, matching natural language sentences, has hereto- fore been absent. In response, our work seeks to perform a comprehensive comparison with re- spect to simple-vs.-complex compositional func- tions, across a wide range of NLP problems, and reveals some general rules for rationally selecting models to tackle different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models &amp; training</head><p>Consider a text sequence represented as X (ei- ther a sentence or a document), composed of a se- quence of words: {w 1 , w 2 , ...., w L }, where L is the number of tokens, i.e., the sentence/document length. Let {v 1 , v 2 , ...., v L } denote the respective word embeddings for each token, where v l 2 R K . The compositional function, X ! z, aims to combine word embeddings into a fixed-length sen- tence/document representation z. These represen- tations are then used to make predictions about se- quence X. Below, we describe different types of functions considered in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Sequence Encoder</head><p>A widely adopted compositional function is de- fined in a recurrent manner: the model succes- sively takes word vector v t at position t, along with the hidden unit h t1 from the last position t 1, to update the current hidden unit via h t = f (v t , h t1 ), where f (·) is the transition function.</p><p>To address the issue of learning long-term de- pendencies, f (·) is often defined as Long Short- Term Memory (LSTM) (Hochreiter and Schmid- huber, 1997), which employs gates to control the flow of information abstracted from a sequence. We omit the details of the LSTM and refer the in- terested readers to the work by <ref type="bibr" target="#b7">Graves et al. (2013)</ref> for further explanation. Intuitively, the LSTM en- codes a text sequence considering its word-order information, but yields additional compositional parameters that must be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Sequence Encoder</head><p>The Convolutional Neural Network (CNN) archi- tecture <ref type="bibr" target="#b13">(Kim, 2014;</ref><ref type="bibr" target="#b3">Collobert et al., 2011;</ref><ref type="bibr" target="#b6">Gan et al., 2017;</ref><ref type="bibr" target="#b36">Zhang et al., 2017b;</ref><ref type="bibr" target="#b23">Shen et al., 2018</ref>) is another strategy extensively employed as the compositional function to encode text se- quences. The convolution operation considers windows of n consecutive words within the se- quence, where a set of filters (to be learned) are applied to these word windows to generate corre- sponding feature maps. Subsequently, an aggre- gation operation (such as max-pooling) is used on top of the feature maps to abstract the most salient semantic features, resulting in the final representa- tion. For most experiments, we consider a single- layer CNN text model. However, Deep CNN text models have also been developed ( <ref type="bibr" target="#b5">Conneau et al., 2016)</ref>, and are considered in a few of our experi- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Simple Word-Embedding Model</head><p>(SWEM) To investigate the raw modeling capacity of word embeddings, we consider a class of models with no additional compositional parameters to en- code natural language sequences, termed SWEMs. Among them, the simplest strategy is to compute the element-wise average over word vectors for a given sequence ( <ref type="bibr" target="#b31">Wieting et al., 2015;</ref><ref type="bibr" target="#b0">Adi et al., 2016)</ref>:</p><formula xml:id="formula_0">z = 1 L L X i=1 v i .<label>(1)</label></formula><p>The model in <ref type="formula" target="#formula_0">(1)</ref> can be seen as an average pool- ing operation, which takes the mean over each of the K dimensions for all word embeddings, result- ing in a representation z with the same dimension as the embedding itself, termed here SWEM-aver.</p><p>Intuitively, z takes the information of every se- quence element into account via the addition op- eration.</p><p>Max Pooling Motivated by the observation that, in general, only a small number of key words con- tribute to final predictions, we propose another SWEM variant, that extracts the most salient fea- tures from every word-embedding dimension, by taking the maximum value along each dimension of the word vectors. This strategy is similar to the max-over-time pooling operation in convolutional neural networks (Collobert et al., 2011):</p><formula xml:id="formula_1">z = Max-pooling(v 1 , v 2 , ..., v L ) .<label>(2)</label></formula><p>We denote this model variant as SWEM-max.</p><p>Here the j-th component of z is the maximum element in the set {v 1j , . . . , v Lj }, where v 1j is, for example, the j-th component of v 1 . With this pooling operation, those words that are unimpor- tant or unrelated to the corresponding tasks will be ignored in the encoding process (as the com- ponents of the embedding vectors will have small amplitude), unlike SWEM-aver where every word contributes equally to the representation. Considering that SWEM-aver and SWEM-max are complementary, in the sense of accounting for different types of information from text sequences, we also propose a third SWEM variant, where the two abstracted features are concatenated together to form the sentence embeddings, denoted here as SWEM-concat. For all SWEM variants, there are no additional compositional parameters to be learned. As a result, the models only exploit intrin- sic word embedding information for predictions.</p><formula xml:id="formula_2">Model Parameters Complexity Sequential Ops CNN n · K · d O(n · L · K · d) O(1) LSTM 4 · d · (K + d) O(L · d 2 + L · K · d) O(L) SWEM 0 O(L · K) O(1)</formula><p>Hierarchical Pooling Both SWEM-aver and SWEM-max do not take word-order or spatial in- formation into consideration, which could be use- ful for certain NLP applications. So motivated, we further propose a hierarchical pooling layer. Let v i:i+n1 refer to the local window consisting of n consecutive words words,</p><formula xml:id="formula_3">v i , v i+1 , ..., v i+n1 .</formula><p>First, an average-pooling is performed on each local window, v i:i+n1 . The extracted features from all windows are further down-sampled with a global max-pooling operation on top of the rep- resentations for every window. We call this ap- proach SWEM-hier due to its layered pooling. This strategy preserves the local spatial infor- mation of a text sequence in the sense that it keeps track of how the sentence/document is constructed from individual word windows, i.e., n-grams. This formulation is related to bag-of-n-grams method ( <ref type="bibr" target="#b34">Zhang et al., 2015b</ref>). However, SWEM-hier learns fixed-length representations for the n-grams that appear in the corpus, rather than just capturing their occurrences via count features, which may potentially advantageous for prediction purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameters &amp; Computation Comparison</head><p>We compare CNN, LSTM and SWEM wrt their parameters and computational speed. K denotes the dimension of word embeddings, as above. For the CNN, we use n to denote the filter width (as- sumed constant for all filters, for simplicity of analysis, but in practice variable n is commonly used). We define d as the dimension of the final sequence representation. Specifically, d represents the dimension of hidden units or the number of fil- ters in LSTM or CNN, respectively.</p><p>We first examine the number of compositional parameters for each model. As shown in <ref type="table" target="#tab_0">Table 1</ref>, both the CNN and LSTM have a large number of parameters, to model the semantic compositional- ity of text sequences, whereas SWEM has no such parameters. Similar to <ref type="bibr">Vaswani et al. (2017)</ref>, we then consider the computational complexity and the minimum number of sequential operations re- quired for each model. SWEM tends to be more efficient than CNN and LSTM in terms of compu- tation complexity. For example, considering the case where K = d, SWEM is faster than CNN or LSTM by a factor of nd or d, respectively. Further, the computations in SWEM are highly paralleliz- able, unlike LSTM that requires O(L) sequential steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate different compositional functions on a wide variety of supervised tasks, including document categorization, text sequence matching (given a sentence pair, X 1 , X 2 , predict their re- lationship, y) as well as (short) sentence classifi- cation. We experiment on 17 datasets concerning natural language understanding, with correspond- ing data statistics summarized in the Supplemen- tary Material. Our code will be released to encour- age future research.</p><p>We use GloVe word embeddings with K = 300 ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>) as initialization for all our models. Out-Of-Vocabulary (OOV) words are initialized from a uniform distribution with range [0.01, 0.01]. The GloVe embeddings are em- ployed in two ways to learn refined word em- beddings: (i) directly updating each word em- bedding during training; and (ii) training a 300- dimensional Multilayer Perceptron (MLP) layer with ReLU activation, with GloVe embeddings as input to the MLP and with output defining the re- fined word embeddings. The latter approach cor- responds to learning an MLP model that adapts GloVe embeddings to the dataset and task of in- terest. The advantages of these two methods dif- fer from dataset to dataset. We choose the bet- ter strategy based on their corresponding perfor- mances on the validation set. The final classifier is implemented as an MLP layer with dimension se- lected from the set [100, 300, 500, 1000], followed by a sigmoid or softmax function, depending on the specific task.</p><p>Adam <ref type="bibr" target="#b14">(Kingma and Ba, 2014</ref>) is used to opti- mize all models, with learning rate selected from the set [1 ⇥ 10 3 , 3 ⇥ 10 4 , 2 ⇥ 10 4 , 1 ⇥ 10 5 ] (with cross-validation used to select the appro- priate parameter for a given dataset and task). Dropout regularization ( <ref type="bibr" target="#b27">Srivastava et al., 2014</ref>    Interestingly, for the sentiment analysis tasks, both CNN and LSTM compositional functions perform better than SWEM, suggesting that word- order information may be required for analyzing sentiment orientations. This finding is consis- tent with <ref type="bibr" target="#b19">Pang et al. (2002)</ref>, where they hypoth- esize that the positional information of a word in text sequences may be beneficial to predict sen- timent. This is intuitively reasonable since, for instance, the phrase "not really good" and "re- ally not good" convey different levels of nega- tive sentiment, while being different only by their word orderings. Contrary to SWEM, CNN and LSTM models can both capture this type of infor- mation via convolutional filters or recurrent transi- tion functions. However, as suggested above, such word-order patterns may be much less useful for predicting the topic of a document. This may be attributed to the fact that word embeddings alone already provide sufficient topic information of a document, at least when the text sequences con- sidered are relatively long.</p><note type="other">) is Model Yahoo! Ans. AG News Yelp P. Yelp F. DBpedia Bag-of-means</note><note type="other">SWEM-aver 73.14 91.71 93.59 60.66 98.42 SWEM-max 72.66 91.79 93.25 59.63 98.24 SWEM-concat 73.53 92.66 93.76 61.11 98.57 SWEM-hier 73.48 92.48 95.81 63.79 98.54</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Interpreting model predictions</head><p>Although the proposed SWEM-max variant gener- ally performs a slightly worse than SWEM-aver, it extracts complementary features from SWEM- aver, and hence in most cases SWEM-concat ex- hibits the best performance among all SWEM variants. More importantly, we found that the word embeddings learned from SWEM-max tend to be sparse. We trained our SWEM-max model on the Yahoo datasets (randomly initialized). With the learned embeddings, we plot the values for each of the word embedding dimensions, for the entire vocabulary. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, most of the values are highly concentrated around zero, indicating that the word embeddings learned are very sparse. On the contrary, the GloVe word embeddings, for the same vocabulary, are consid- erably denser than the embeddings learned from SWEM-max. This suggests that the model may only depend on a few key words, among the en- tire vocabulary, for predictions (since most words do not contribute to the max-pooling operation in SWEM-max). Through the embedding, the model learns the important words for a given task (those words with non-zero embedding components). In this regard, the nature of max-pooling pro- cess gives rise to a more interpretable model. For a document, only the word with largest value in each embedding dimension is employed for the fi- nal representation. Thus, we suspect that semanti- cally similar words may have large values in some shared dimensions. So motivated, after training the SWEM-max model on the Yahoo dataset, we selected five words with the largest values, among the entire vocabulary, for each word embedding dimension (these words are selected preferentially in the corresponding dimension, by the max op- eration). As shown in <ref type="table" target="#tab_3">Table 3</ref>, the words chosen wrt each embedding dimension are indeed highly relevant and correspond to a common topic (the topics are inferred from words). For example, the words in the first column of <ref type="table" target="#tab_3">Table 3</ref> are all po- litical terms, which could be assigned to the Pol- itics &amp; Government topic. Note that our model can even learn locally interpretable structure that is not explicitly indicated by the label informa- tion. For instance, all words in the fifth column are Chemistry-related. However, we do not have a chemistry label in the dataset, and regardless they should belong to the Science topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text Sequence Matching</head><p>To gain a deeper understanding regarding the mod- eling capacity of word embeddings, we further in- vestigate the problem of sentence matching, in- cluding natural language inference, answer sen- tence selection and paraphrase identification. The corresponding performance metrics are shown in <ref type="table" target="#tab_6">Table 5</ref>. Surprisingly, on most of the datasets con- sidered (except WikiQA), SWEM demonstrates the best results compared with those with CNN or the LSTM encoder. Notably, on SNLI dataset, we observe that SWEM-max performs the best among all SWEM variants, consistent with the findings in Nie and Bansal (2017); <ref type="bibr" target="#b4">Conneau et al. (2017)</ref>, that max-pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset. As a result, with only 120K param- eters, our SWEM-max achieves a test accuracy of 83.8%, which is very competitive among state-of- the-art sentence encoding-based models (in terms of both performance and number of parameters) <ref type="bibr">1</ref> .</p><p>The strong results of the SWEM approach on these tasks may stem from the fact that when matching natural language sentences, it is suffi- cient in most cases to simply model the word-level  alignments between two sequences ( <ref type="bibr" target="#b20">Parikh et al., 2016)</ref>. From this perspective, word-order informa- tion becomes much less useful for predicting rela- tionship between sentences. Moreover, consider- ing the simpler model architecture of SWEM, they could be much easier to be optimized than LSTM or CNN-based models, and thus give rise to better empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Importance of word-order information</head><p>One possible disadvantage of SWEM is that it ig- nores the word-order information within a text se- quence, which could be potentially captured by CNN-or LSTM-based models. However, we em- pirically found that except for sentiment analysis, SWEM exhibits similar or even superior perfor- mance as the CNN or LSTM on a variety of tasks.</p><p>In this regard, one natural question would be: how important are word-order features for these tasks? To this end, we randomly shuffle the words for every sentence in the training set, while keeping the original word order for samples in the test set. The motivation here is to remove the word-order features from the training set and examine how sensitive the performance on different tasks are to word-order information. We use LSTM as the model for this purpose since it can captures word- order information from the original training set.  The results on three distinct tasks are shown in <ref type="table" target="#tab_8">Table 6</ref>. Somewhat surprisingly, for Yahoo and SNLI datasets, the LSTM model trained on shuf- fled training set shows comparable accuracies to those trained on the original dataset, indicating Negative: Friendly staff and nice selection of vegetar- ian options. Food is just okay, not great. Makes me wonder why everyone likes food fight so much. Positive:</p><p>The store is small, but it carries specialties that are difficult to find in Pittsburgh. I was particularly excited to find middle eastern chili sauce and chocolate covered turkish delights. that word-order information does not contribute significantly on these two problems, i.e., topic cat- egorization and textual entailment. However, on the Yelp polarity dataset, the results drop notice- ably, further suggesting that word-order does mat- ter for sentiment analysis (as indicated above from a different perspective). Notably, the performance of LSTM on the Yelp dataset with a shuffled training set is very close to our results with SWEM, indicating that the main difference between LSTM and SWEM may be due to the ability of the former to capture word-order features. Both observations are in consistent with our experimental results in the previous section.</p><p>Case Study To understand what type of sen- tences are sensitive to word-order information, we further show those samples that are wrongly pre- dicted because of the shuffling of training data in <ref type="table" target="#tab_9">Table 7</ref>. Taking the first sentence as an example, several words in the review are generally positive, i.e. friendly, nice, okay, great and likes. However, the most vital features for predicting the sentiment of this sentence could be the phrase/sentence 'is just okay', 'not great' or 'makes me wonder why everyone likes', which cannot be captured without</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MR SST-1 SST-2 Subj TREC RAE ( <ref type="bibr" target="#b26">Socher et al., 2011b)</ref> 77.7 43.2 82.4 - - MV-RNN ( <ref type="bibr" target="#b24">Socher et al., 2012)</ref> 79.0 44.4 82.9 - - LSTM ( <ref type="bibr" target="#b29">Tai et al., 2015)</ref> - 46.4 84.9 - - RNN ( <ref type="bibr" target="#b37">Zhao et al., 2015)</ref> 77.2 - - 93.7 90.2 Constituency Tree-LSTM ( <ref type="bibr" target="#b29">Tai et al., 2015)</ref> - 51.0 88.0 - - Dynamic CNN ( <ref type="bibr" target="#b12">Kalchbrenner et al., 2014</ref>) - 48.5 86.8 - 93.0 CNN <ref type="bibr" target="#b13">(Kim, 2014)</ref> 81.5 48.0 88.1 93.4 93.6 DAN-ROOT ( <ref type="bibr" target="#b10">Iyyer et al., 2015</ref>  considering word-order features. It is worth noting the hints for predictions in this case are actually n- gram phrases from the input document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SWEM-hier for sentiment analysis</head><p>As demonstrated in Section 4.2.1, word-order in- formation plays a vital role for sentiment analysis tasks. However, according to the case study above, the most important features for sentiment predic- tion may be some key n-gram phrase/words from the input document. We hypothesize that incor- porating information about the local word-order, i.e., n-gram features, is likely to largely mitigate the limitations of the above three SWEM variants.</p><p>Inspired by this observation, we propose using an- other simple pooling operation termed as hierar- chical (SWEM-hier), as detailed in Section 3.3.</p><p>We evaluate this method on the two document- level sentiment analysis tasks and the results are shown in the last row of <ref type="table" target="#tab_2">Table 2</ref>. SWEM-hier greatly outperforms the other three SWEM variants, and the corresponding accuracies are comparable to the results of CNN or LSTM <ref type="table" target="#tab_2">(Table 2</ref>). This indicates that the proposed hi- erarchical pooling operation manages to abstract spatial (word-order) information from the input sequence, which is beneficial for performance in sentiment analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Short Sentence Processing</head><p>We now consider sentence-classification tasks (with approximately 20 words on average). We experiment on three sentiment classification datasets, i.e., MR, SST-1, SST-2, as well as subjec- tivity classification (Subj) and question classifica- tion (TREC). The corresponding results are shown in <ref type="table" target="#tab_11">Table 8</ref>. Compared with CNN/LSTM com- positional functions, SWEM yields inferior accu- racies on sentiment analysis datasets, consistent with our observation in the case of document cat- egorization. However, SWEM exhibits compara- ble performance on the other two tasks, again with much less parameters and faster training. Further, we investigate two sequence tagging tasks: the standard CoNLL2000 chunking and CoNLL2003 NER datasets. Results are shown in the Supple- mentary Material, where LSTM and CNN again perform better than SWEMs. Generally, SWEM is less effective at extracting representations from short sentences than from long documents. This may be due to the fact that for a shorter text se- quence, word-order features tend to be more im- portant since the semantic information provided by word embeddings alone is relatively limited.</p><p>Moreover, we note that the results on these rela- tively small datasets are highly sensitive to model regularization techniques due to the overfitting is- sues. In this regard, one interesting future di- rection may be to develop specific regularization strategies for the SWEM framework, and thus make them work better on small sentence classi- fication datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison via subspace training</head><p>We use subspace training ( <ref type="bibr" target="#b15">Li et al., 2018</ref>) to mea- sure the model complexity in text classification problems. It constrains the optimization of the trainable parameters in a subspace of low dimen- sion d, the intrinsic dimension d int defines the minimum d that yield a good solution. Two mod- els are studied: the SWEM-max variant, and the CNN model including a convolutional layer fol- lowed by a FC layer. We consider two settings:</p><p>(1) The word embeddings are randomly intial- ized, and optimized jointly with the model param- eters. We show the performance of direct and sub- space training on AG News dataset in <ref type="figure" target="#fig_2">Figure 2</ref> (a)(b). The two models trained via direct method share almost identical perfomrnace on training and   testing. The subspace training yields similar ac- curacy with direct training for very small d, even when model parameters are not trained at all (d = 0). This is because the word embeddings have the full degrees of freedom to adjust to achieve good solutions, regardless of the employed mod- els. SWEM seems to have an easier loss landspace than CNN for word embeddings to find the best so- lutions. According to Occam's razor, simple mod- els are preferred, if all else are the same.</p><p>(2) The pre-trained GloVe are frozen for the word embeddings, and only the model parameters are optimized. The results on testing datasets of AG News and Yelp P. are shown in <ref type="figure" target="#fig_2">Figure 2 (c)(d)</ref>, respectively. SWEM shows significantly higher accuracy than CNN for a large range of low sub- space dimension, indicating that SWEM is more parameter-efficient to get a decent solution. In <ref type="figure" target="#fig_2">Figure 2</ref>(c), if we set the performance threshold as 80% testing accuracy, SWEM exhibits a lower d int than CNN on AG News dataset. However, in <ref type="figure" target="#fig_2">Figure 2</ref>(d), CNN can leverage more trainable parameters to achieve higher accuracy when d is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Linear classifiers</head><p>To further investigate the quality of representa- tions learned from SWEMs, we employ a linear classifier on top of the representations for pre- diction, instead of a non-linear MLP layer as in the previous section. It turned out that utiliz- ing a linear classifier only leads to a very small performance drop for both Yahoo! Ans. (from 73.53% to 73.18%) and Yelp P. datasets (from 93.76% to 93.66%) . This observation highlights that SWEMs are able to extract robust and infor- mative sentence representations despite their sim- plicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Extension to other languages</head><p>We have also tried our SWEM-concat and SWEM- hier models on Sogou news corpus (with the same experimental setup as <ref type="figure" target="#fig_0">(Zhang et al., 2015b)</ref>), which is a Chinese dataset represented by Pinyin (a phonetic romanization of Chinese). SWEM- concat yields an accuracy of 91.3%, while SWEM-hier (with a local window size of 5) ob- tains an accuracy of 96.2% on the test set. Notably, the performance of SWEM-hier is comparable to the best accuracies of CNN (95.6%) and LSTM (95.2%), as reported in ( <ref type="bibr" target="#b34">Zhang et al., 2015b</ref>). This indicates that hierarchical pooling is more suitable than average/max pooling for Chinese text classifi- cation, by taking spatial information into account. It also implies that Chinese is more sensitive to lo- cal word-order features than English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have performed a comparative study between SWEM (with parameter-free pooling operations) and CNN or LSTM-based models, to represent text sequences on 17 NLP datasets. We further validated our experimental findings through ad- ditional exploration, and revealed some general rules for rationally selecting compositional func- tions for distinct problems. Our findings regard- ing when (and why) simple pooling operations are enough for text sequence representations are sum- marized as follows:</p><p>• Simple pooling operations are surprisingly ef- fective at representing longer documents (with hundreds of words), while recurrent/convolutional compositional functions are most effective when constructing representations for short sentences.</p><p>• Sentiment analysis tasks are more sensitive to word-order features than topic categorization tasks. However, a simple hierarchical pooling layer proposed here achieves comparable results to LSTM/CNN on sentiment analysis tasks.</p><p>• To match natural language sentences, e.g., tex- tual entailment, answer sentence selection, etc., simple pooling operations already exhibit similar or even superior results, compared to CNN and LSTM.</p><p>• In SWEM with max-pooling operation, each in- dividual dimension of the word embeddings con- tains interpretable semantic patterns, and groups together words with a common theme or topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histograms for learned word embeddings (randomly initialized) of SWEM-max and GloVe embeddings for the same vocabulary, trained on the Yahoo! Answer dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Testing on AG News (d)Testing on Yelp P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Performance of subspace training. Word embeddings are optimized in (a)(b), and frozen in (c)(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparisons of CNN, LSTM and SWEM 
architectures. Columns correspond to the number 
of compositional parameters, computational com-
plexity and sequential operations, respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test accuracy on (long) document classification tasks, in percentage. Results marked with ⇤ are 
reported in Zhang et al. (2015b), with  † are reported in Conneau et al. (2016), and with  ‡ are reported in 
Joulin et al. (2016). 

Politics 
Science 
Computer 
Sports 
Chemistry 
Finance 
Geoscience 
philipdru 
coulomb 
system32 
billups 
sio2 (SiO2) proprietorship 
fossil 
justices 
differentiable 
cobol 
midfield 
nonmetal 
ameritrade 
zoos 
impeached 
paranormal 
agp 
sportblogs 
pka 
retailing 
farming 
impeachment 
converge 
dhcp 
mickelson 
chemistry 
mlm 
volcanic 
neocons 
antimatter 
win98 
juventus 
quarks 
budgeting 
ecosystem 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>are no compositional parameters in SWEM, our models have an order of mag- nitude fewer parameters (excluding embeddings) than LSTM or CNN, and are considerably more computationally efficient. As illustrated in Ta- ble 4, SWEM-concat achieves better results on Yahoo! Answer than CNN/LSTM, with only 61K parameters (one-tenth the number of LSTM pa- rameters, or one-third the number of CNN param- eters), while taking a fraction of the training time relative to the CNN or LSTM.</figDesc><table>Top five words with the largest values in a given word-embedding dimension (each column 
corresponds to a dimension). The first row shows the (manually assigned) topic for words in each column. 

employed on the word embedding layer and final 
MLP layer, with dropout rate selected from the 
set [0.2, 0.5, 0.7]. The batch size is selected from 
[2, 8, 32, 128, 512]. 

4.1 Document Categorization 

We begin with the task of categorizing documents 
(with approximately 100 words in average per 
document). We follow the data split in Zhang et al. 
(2015b) for comparability. These datasets can 
be generally categorized into three types: topic 
categorization (represented by Yahoo! Answer 
and AG news), sentiment analysis (represented by 
Yelp Polarity and Yelp Full) and ontology clas-
sification (represented by DBpedia). Results are 
shown in Table 2. Surprisingly, on topic prediction 
tasks, our SWEM model exhibits stronger perfor-
mances, relative to both LSTM and CNN compo-
sitional architectures, this by leveraging both the 
average and max-pooling features from word em-
beddings. Specifically, our SWEM-concat model 
even outperforms a 29-layer deep CNN model 
(Conneau et al., 2016), when predicting topics. 
On the ontology classification problem (DBpedia 
dataset), we observe the same trend, that SWEM 
exhibits comparable or even superior results, rela-
tive to CNN or LSTM models. 
Since there Model 
Parameters Speed 
CNN 
541K 
171s 
LSTM 
1.8M 
598s 
SWEM 
61K 
63s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Speed &amp; Parameters on Yahoo! Answer 
dataset. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance of different models on matching natural language sentences. Results with ⇤ are 
for Bidirectional LSTM, reported in Williams et al. (2017). Our reported results on MultiNLI are only 
trained MultiNLI training set (without training data from SNLI). For MSRP dataset, we follow the setup 
in Hu et al. (2014) and do not use any additional features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Test accuracy for LSTM model trained on 
original/shuffled training set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Test samples from Yelp Polarity dataset 
for which LSTM gives wrong predictions with 
shuffled training data, but predicts correctly with 
the original training set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Test accuracies with different compositional functions on (short) sentence classifications. 

</table></figure>

			<note place="foot" n="1"> See leaderboard at https://nlp.stanford.edu/ projects/snli/ for details.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781</idno>
		<title level="m">Very deep convolutional networks for natural language processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning generic sentence representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="page" from="2380" to="2390" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Shortcutstacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02312</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adaptive convolutional filter generation for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08294</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deconvolutional latent-variable model for text sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. NIPS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The fixed-size ordinallyforgetting encoding method for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="495" to="500" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarial feature matching for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Deconvolutional paragraph representation learning. NIPS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4069" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
