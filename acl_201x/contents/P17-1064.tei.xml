<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Source Syntax for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Modeling Source Syntax for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="688" to="697"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1064</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize parse trees of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly ; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-of- the-art of statistical machine translation (SMT) * Work done at Huawei Noah's Ark Lab, HongKong. on various language pairs ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b9">Jean et al., 2015;</ref>). However, <ref type="bibr" target="#b24">Shi et al. (2016)</ref> show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntax- based SMT that substantially improves translation by integrating various kinds of syntactic knowl- edge ( <ref type="bibr" target="#b12">Liu et al., 2006;</ref><ref type="bibr" target="#b16">Marton and Resnik, 2008;</ref><ref type="bibr" target="#b23">Shen et al., 2008;</ref><ref type="bibr" target="#b11">Li et al., 2013)</ref>. While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax. <ref type="figure" target="#fig_0">Figure 1 (a)</ref> shows a Chinese-to-English translation example of NMT. In this example, the NMT seq2seq model incor- rectly translates the Chinese noun phrase (i.e., /xinsheng /yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence. Statistics on our de- velopment set show that one forth of Chinese noun phrases are translated into discontinuous phrases in English, indicating the substantial disrespect of syntax in NMT translation. 1 <ref type="figure" target="#fig_0">Figure 1</ref> (b) shows another example with over translation, where the noun phrase /liang /ge /nvhai is trans- lated twice in English. Similar to discontinuous translation, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence.</p><p>In this paper we are not aiming at solving any particular issue, either the discontinuous transla- tion or the over translation. Alternatively, we ad- dress how to incorporate explicitly the source syn- tax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general. Specifically, rather than directly assign- ing each source word with manually designed syn- tactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information. On the basis, we systematically propose and compare several different approaches to incorporating the label se- quence into the seq2seq NMT model. Experimen- tation on Chinese-to-English translation demon- strates that all proposed approaches are able to im- prove the translation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attention-based NMT</head><p>As a background and a baseline, in this section, we briefly describe the NMT model with an atten- tion mechanism by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, which mainly consists of an encoder and a decoder, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Encoder The encoding of a source sentence is for- 1 Manually examining 200 random such discontinuously translated noun phrases, we find that 90% of them should be continuously translated according to the reference translation. Decoder The decoder is also an RNN that pre- dicts a target sequence y = (y 1 , ..., y n ). Each tar- get word y i is predicted via a multi-layer percep- tron (MLP) component which is based on a recur- rent hidden state s i , the previous predicted word y i−1 , and a source-side context vector c i . Here, c i is calculated as a weighted sum over source an- notation vectors (h 1 , ..., h m ). The weight vector α i ∈ R m over source annotation vectors is ob- tained by an attention model, which captures the correspondences between the source and the target languages. The attention weight α ij is computed based on the previous recurrent hidden state s i−1 and source annotation vector h j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NMT with Source Syntax</head><p>The conventional NMT models treat a sentence as a sequence of words and ignore external knowl- edge, failing to effectively capture various kinds of inherent structure of the sentence. To lever- age external knowledge, specifically the syntax in the source side, we focus on the parse tree of a sentence and propose three different NMT mod- els that explicitly consider the syntactic structure into encoding. Our purpose is to inform the NMT model the structural context of each word in its corresponding parse tree with the goal that the learned annotation vectors (h 1 , ..., h m ) encode not  only the information of words and their surround- ings, but also structural context in the parse tree. In the rest of this section, we use English sentences as examples to explain our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Syntax Representation</head><p>To obtain the structural context of a word in its parse tree, ideally the model should not only cap- ture and remember the whole parse tree structure, but also discriminate the contexts of any two dif- ferent words. However, considering the lack of efficient way to directly model structural informa- tion, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence. For example, <ref type="figure" target="#fig_3">Figure 3</ref>(c) shows the structural la- bel sequence of <ref type="figure" target="#fig_3">Figure 3</ref>(b) in a simple way fol- lowing a depth-first traversal order. Note that lin- earizing a parse tree in a depth-first traversal or- der into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing ( <ref type="bibr" target="#b28">Vinyals et al., 2015;</ref><ref type="bibr" target="#b4">Choe and Charniak, 2016)</ref>, suggesting that the linearized se- quence can be viewed as an alternative to its tree structure.  There is no doubt that the structural label se- quence is much longer than its word sequence. In order to obtain the structural label annotation vector for w i in word sequence, we simply look for w i 's part-of-speech (POS) tag in the label se- quence and view the tag's annotation vector as w i 's label annotation vector. This is because w i 's POS tag location can also represent w i 's location in the parse tree. For example, in <ref type="figure" target="#fig_3">Figure 3</ref>, word w 1 in (a) maps to l 3 in (c) since l 3 is the POS tag of w 1 . Likewise, w 2 maps to l 5 and w 3 to l 7 . That is to say, we use l 3 's learned annotation vector as w 1 's label annotation vector. and Charniak, 2016) do. However, the performance gap is very small by adding the ending brackets or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RNN Encoders with Source Syntax</head><p>In the next, we first propose two different encoders to augment word annotation vector with its corre- sponding label annotation vector, each of which consists of two RNNs 3 : in one encoder, the two RNNs work independently (i.e., Parallel RNN En- coder) while in another encoder the two RNNs work in a hierarchical way (i.e., Hierarchical RNN Encoder). The difference between the two en- coders lies in how the two RNNs interact. Then, we propose the third encoder with a single RNN, which learns word and label annotation vectors stitchingly (i.e., Mixed RNN Encoder). Since any of the above three approaches focuses only on the encoder as to generate source annotation vectors along with structural information, we keep the rest part of the NMT models unchanged. <ref type="figure" target="#fig_5">Figure 4</ref> (a) illustrates our Parallel RNN encoder, which includes two parallel RNNs: i.e., a word RNN and a structural label RNN. On the one hand, the word RNN, as in conventional NMT models, takes a word sequence as input and output a word annotation vector for each word. On the other hand, the structural label RNN takes the structural label sequence of the word sequence as input and obtains a label annotation vector for each label. Besides, we concatenate each word's word annotation vector and its POS tag's label annotation vector as the final annotation vector for the word. For example, the final annotation vector for word love in <ref type="figure" target="#fig_5">Figure 4</ref>  love's word embedding ew 2 to feed as the input to the word RNN. <ref type="figure" target="#fig_6">Figure 5</ref> presents our Mixed RNN encoder. Similarly, the sequence of input is the linearization of its parse tree (as in <ref type="figure" target="#fig_3">Figure 3 (b)</ref>) following a depth-first traversal or- der, but being mixed with both words and struc- tural labels in a stitching way. It shows that the RNN learns annotation vectors for both the words and the structural labels, though only the annota- tion vectors of words are further fed to decoding (e.g.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel RNN Encoder</head><formula xml:id="formula_0">(a) is [ − − → hw 2 ; ← − − hw 2 ; − → hl 5 ; ← − hl 5 ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed RNN Encoder</head><formula xml:id="formula_1">([ − → h 4 , ← − h 4 ], [ − → h 7 , ← − h 7 ], [ − → h 10 , ← − h 10 ]))</formula><p>. Even though the annotation vectors of structural labels are not directly fed forward for decoding, the error signal is back propagated along the word sequence and allows the annotation vectors of structural labels being updated accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison of RNN Encoders with Source Syntax</head><p>Though all the three encoders model both word sequence and structural label sequence, the dif- ferences lie in their respective model architecture with respect to the degree of coupling the two se- quences:</p><p>• In the Parallel RNN encoder, the word RNN and structural label RNN work in a parallel way. That is to say, the error signal back propagated from the word sequence would not affect the structural label RNN, and vice versa. In contrast, in the Hierarchical RNN encoder, the error signal back propagated from the word sequence has a direct impact on the structural label annotation vectors, and thus on the structural label embeddings. Fi- nally, the Mixed RNN encoder ties the struc- tural label sequence and word sequence to- gether in the closest way. Therefore, the degrees of coupling the word and structural label sequences in these three encoders are like this: Mixed RNN encoder &gt; Hierarchi- cal RNN encoder &gt; Parallel RNN encoder.</p><p>• <ref type="figure" target="#fig_5">Figure 4</ref> and <ref type="figure" target="#fig_6">Figure 5</ref> suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the in- put sequence. Statistics on our training data reveal that the Mixed RNN encoder approxi- mately triples the input sequence length com- pared to conventional NMT encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimentation</head><p>We have presented our approaches to incorporat- ing the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Our training data for the translation task consists of for the translation task. For efficient training of neural networks, we limit the maximum sentence length on both source and target sides to 50. We also limit both the source and target vocabularies to the most frequent 16K words in Chinese and English, covering ap- proximately 95.8% and 98.2% of the two corpora respectively. All the out-of-vocabulary words are mapped to a special token UNK. Besides, the word embedding dimension is 620 and the size of a hid- den layer is 1000. All the other settings are the same as in <ref type="bibr" target="#b0">Bahdanau et al.(2015</ref> The inventory of structural labels includes 16 phrase labels and 32 POS tags. In both our Paral- lel RNN encoder and Hierarchical RNN encoder, we set the embedding dimension of these labels as 100 and the size of a hidden layer as 100. Besides, the maximum structural label sequence length is set to 100. In our Mixed RNN encoder, since we only have one input sequence, we equally treat the structural labels and words (i.e., a structural label is also initialized with 620 dimension embedding). Compared to the baseline NMT model, the only different setting is that we increase the maximum sentence length on source-side from 50 to 150.</p><p>We compare our method with two state-of-the- art models of SMT and NMT:</p><p>• cdec ( <ref type="bibr" target="#b5">Dyer et al., 2010)</ref>: an open source hi- erarchical phrase-based SMT system (Chi- ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. 7</p><p>• RNNSearch: a re-implementation of the at- tentional NMT system (Bahdanau et al., 2015) with slight changes taken from dl4mt tutorial. 8 For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by <ref type="bibr" target="#b3">(Cho et al., 2014b</ref>). It incorporates dropout ( <ref type="bibr" target="#b8">Hinton et al., 2012</ref>) on the output layer and improves the attention model by feeding the lastly gen- erated word. We use AdaDelta <ref type="bibr" target="#b32">(Zeiler, 2012)</ref> to optimize model parameters in training with the mini-batch size of 80. For translation, a beam search with size 10 is employed.     Comparison with the SMT model (cdec) Ta- ble 1 also shows that all NMT systems outper- form the SMT system. This is very consistent with other studies on Chinese-to-English transla- tion ( <ref type="bibr" target="#b17">Mi et al., 2016;</ref><ref type="bibr" target="#b26">Tu et al., 2017b;</ref><ref type="bibr" target="#b29">Wang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>As the proposed Mixed RNN system achieves the best performance, we further look at the RNNSearch system and the Mixed RNN system to explore more on how syntactic information helps in translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects on Long Sentences</head><p>Following <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, we group sen- tences of similar lengths together and compute BLEU scores. <ref type="figure" target="#fig_8">Figure 6</ref> presents the BLEU scores over different lengths of input sentences. It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths. It also shows that the performance drops substantially System AER RNNSearch 50.1 Mixed RNN 47.9 <ref type="table">Table 2</ref>: Evaluation of alignment quality. The lower the score, the better the alignment quality.</p><p>when the length of input sentences increases. This performance trend over the length is consistent with the findings in ( <ref type="bibr" target="#b2">Cho et al., 2014a;</ref><ref type="bibr" target="#b27">Tu et al., 2016</ref><ref type="bibr" target="#b25">Tu et al., , 2017a</ref>). We also observe that the NMT sys- tems perform surprisingly bad on sentences over 50 in length, especially compared to the perfor- mance of SMT system (i.e., cdec). We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the fol- lowing two reasons: (1) the maximum source sen- tence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis on Word Alignment</head><p>Due to the capability of carrying syntactic infor- mation in source annotation vectors, we conjec- ture that our model with source syntax is also beneficial for alignment. To test this hypothe- sis, we carry out experiments of the word align- ment task on the evaluation dataset from <ref type="bibr" target="#b13">Liu and Sun (2015)</ref>, which contains 900 manually aligned Chinese-English sentence pairs. We force the de- coder to output reference translations, as to get au- tomatic alignments between input sentences and their reference translations. To evaluate alignment performance, we report the alignment error rate (AER) <ref type="bibr" target="#b18">(Och and Ney, 2003</ref>) in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref> shows that source syntax information improves the attention model as expected by main- taining an annotation vector summarizing struc- tural information on each source word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis on Phrase Alignment</head><p>The above subsection examines the alignment per- formance at the word level. In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit. Given a source phrase XP, we use word alignments to examine if the phrase is translated continuously (Cont.), or dis-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>XP Cont. Dis. Un.  <ref type="table">Table 3</ref>: Percentages (%) of syntactic phrases in our test sets being translated continuously, discon- tinuously, or not being translated. Here PP is for prepositional phrase, NP for noun phrase, CP for clause headed by a complementizer, QP for quain- ter phrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNNSearch</head><p>continuously (Dis.), or if it is not translated at all (Un.). There are some phrases, such as noun phrases (NPs), prepositional phrases (PPs) that we usu- ally expect to have a continuous translation. With respect to several such types of phrases, <ref type="table">Table 3</ref> shows how these phrases are translated. From the table, we see that translations of RNNSearch system do not respect source syntax very well. For example, in RNNSearch translations, 57.3%, 33.6%, and 9.1% of PPs are translated continu- ously, discontinuously, and untranslated, respec- tively. Fortunately, our Mixed RNN system is able to have more continuous translation for those phrases. <ref type="table">Table 3</ref> also suggests that there is still much room for NMT to show more respect to syn- tax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis on Over Translation</head><p>To estimate the over translation generated by NMT, we propose ratio of over translation (ROT):</p><formula xml:id="formula_2">ROT = w i t(wi) |w| (1)</formula><p>where |w| is the number of words in consider- ation, t(w i ) is the times of over translation for word w i . Given a word w and its translation e = e 1 e 2 . . . e n , we have:</p><formula xml:id="formula_3">t(w) = |e| − |uniq(e)| (2)</formula><p>where |e| is the number of words in w's transla- tion e, while |uniq(e)| is the number of unique words in e. For example, if a source word  /xiangkang is translated as hong kong hong kong, we say it being over translated 2 times. <ref type="table" target="#tab_7">Table 4</ref> presents ROT grouped by some typical POS tags. It is not surprising that RNNSearch sys- tem has high ROT with respect to POS tags of NR (proper noun) and CD (cardinal number): this is due to the fact that the two POS tags include high percentage of unknown words which tend to be translated multiple times in translation. Words of DT (determiner) are another source of over trans- lation since they are usually translated to multiple the in English. It also shows that by introducing source syntax, Mixed RNN system alleviates the over translation issue by 18%: ROT drops from 5.5% to 4.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis on Rare Word Translation</head><p>We analyze the translation of source-side rare words that are mapped to a special token UNK. Given a rare word w, we examine if it is translated into a non-UNK word (non-UNK), UNK (UNK), or if it is not translated at all (Un.). <ref type="table">Table 5</ref> shows how source-side rare words are translated. The four POS tags listed in the table account for about 90% of all rare words in the test sets. It shows that in Mixed RNN system is more likely to translate source-side rare words into UNK on the target side. This is reasonable since the source side rare words tends to be translated into rare words in the target side. Moreover, it is hard to obtain its correct non-UNK translation when a source-side rare word is replaced as UNK.</p><p>Note that our approach is compatible with with approaches of open vocabulary. Taking the sub- System POS non-UNK UNK Un.  <ref type="table">Table 5</ref>: Percentages (%) of rare words in our test sets being translated into a non-UNK word (non- UNK), UNK (UNK), or if it is not translated at all (Un.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNNSearch</head><p>word approach (  as an exam- ple, for a word on the source side which is divided into several subword units, we can synthesize sub- POS nodes that cover these units. For example, if misunderstand/VB is divided into units of mis and understand, we construct substructure (VB (VB-F mis) (VB-I understand)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>While there has been substantial work on lin- guistically motivated SMT, approaches that lever- age syntax for NMT start to shed light very re- cently. Generally speaking, NMT can provide a flexible mechanism for adding linguistic knowl- edge, thanks to its strong capability of automati- cally learning feature representations. <ref type="bibr" target="#b6">Eriguchi et al. (2016)</ref> propose a tree-to- sequence model that learns annotation vectors not only for terminal words, but also for non-terminal nodes. They also allow the attention model to align target words to non-terminal nodes. Our ap- proach is similar to theirs by using source-side phrase parse tree. However, our Mixed RNN sys- tem, for example, incorporates syntax informa- tion by learning annotation vectors of syntactic la- bels and words stitchingly, but is still a sequence- to-sequence model, with no extra parameters and with less increased training time.</p><p>Sennrich and Haddow (2016) define a few lin- guistically motivated features that are attached to each individual words. Their features include lem- mas, subword tags, POS tags, dependency labels, etc. They concatenate feature embeddings with word embeddings and feed the concatenated em-beddings into the NMT encoder. On the contrast, we do not specify any feature, but let the model implicitly learn useful information from the struc- tural label sequence. <ref type="bibr" target="#b24">Shi et al. (2016)</ref> design a few experiments to in- vestigate if the NMT system without external lin- guistic input is capable of learning syntactic infor- mation on the source-side as a by-product of train- ing. However, their work is not focusing on im- proving NMT with linguistic input. Moreover, we analyze what syntax is disrespected in translation from several new perspectives. <ref type="bibr" target="#b7">García-Martínez et al. (2016)</ref> generalize NMT outputs as lemmas and morphological factors in order to alleviate the issues of large vocabulary and out-of-vocabulary word translation. The lem- mas and corresponding factors are then used to generate final words in target language. Though they use linguistic input on the target side, they are limited to the word level features. Phrase level, or even sentence level linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have investigated whether and how source syntax can explicitly help NMT to im- prove its translation accuracy.</p><p>To obtain syntactic knowledge, we linearize a parse tree into a structural label sequence and let the model automatically learn useful infor- mation through it. Specifically, we have de- scribed three different models to capture the syn- tax knowledge, i.e., Parallel RNN, Hierarchi- cal RNN, and Mixed RNN. Experimentation on Chinese-to-English translation shows that all pro- posed models yield improvements over a state-of- the-art baseline NMT system. It is also interesting to note that the simplest model (i.e., Mixed RNN) achieves the best performance, resulting in obtain- ing significant improvements of 1.4 BLEU points on NIST MT 02 to 05.</p><p>In this paper, we have also analyzed the transla- tion behavior of our improved system against the state-of-the-art NMT baseline system from several perspectives. Our analysis shows that there is still much room for NMT translation to be consistent with source syntax. In our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syn- tactic features (e.g., features showing the syntactic role that a word is playing) for NMT, and employ- ing the source syntax to constrain and guild the attention models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of NMT translation that fail to respect source syntax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention-based NMT model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of an input sentence (a), its parse tree (b), and the parse tree's sequential form (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 2</head><label>2</label><figDesc>We have also tried to include the ending brackets in the structural label sequence, as what (Vinyals et al., 2015; Choe</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The graphical illustration of the Parallel RNN encoder (a) and the Hierarchical RNN encoder (b). Here, − − → hw j and ← − − hw j are the forward and backward hidden states for word w j , − → hl i and ← − hl i are for structural label l i , ew j is the word embedding for word w j , and is for concatenation operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The graphical illustration of the Mixed RNN encoder. Here, − → h j and ← − h j are the forward and backward hidden annotation vectors for j-th item, which can be either a word or a structural label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 :</head><label>1</label><figDesc>Evaluation of the translation performance. † and ‡: significant over RNNSearch at 0.05/0.01, tested by bootstrap resampling (Koehn, 2004). "+" is the additional number of parameters or training time on the top of the baseline system RNNSearch. Column Time indicates the training time in minutes per epoch for different NMT models the similar size of additional parameters, result- ing from the RNN model for structural label se- quences (about 0.1M parameters) and catering ei- ther the augmented annotation vectors (as shown in Figure 4 (a)) or the augmented word embed- dings (as shown in Figure 4 (b)) (the remain pa- rameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of the generated translations with respect to the lengths of the input sentences.</figDesc><graphic url="image-1.png" coords="6,308.41,232.54,216.00,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table>the translation performances mea-
sured in BLEU score. Clearly, all the proposed 
NMT models with source syntax improve the 
translation accuracy over all test sets, although 
there exist considerable differences among differ-
ent variants. 

Parameters The three proposed models introduce 
new parameters in different ways. As a baseline 
model, RNNSearch has 60.6M parameters. Due to 
the infrastructure similarity, the Parallel RNN sys-
tem and the Hierarchical RNN system introduce </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). It is not surprising that the Mixed RNN system does not require any additional parameters since though the input sequence becomes longer, we keep the vocabulary size unchanged, resulting in no additional parameters.</figDesc><table>Speed Introducing the source syntax slightly 
slows down the training speed. When running on 
a single GPU GeForce GTX 1080, the baseline 
model speeds 153 minutes per epoch with 14K 
updates while the proposed structural label RNNs 
in both Parallel RNN and Hierarchical RNN sys-
tems only increases the training time by about 6% 
(thanks to the small size of structural label embed-
dings and annotation vectors), and the Mixed RNN 
system spends 26% more training time to cater the 
triple sized input sequence. 

Comparison with the baseline NMT model 
(RNNSearch) While all the three proposed NMT 
models outperform RNNSearch, the Parallel RNN 
system and the Hierarchical RNN system achieve 
similar accuracy (e.g., 36.6 v.s. 36.7). Besides, 
the Mixed RNN system achieves the best accu-
racy overall test sets with the only exception of 
NIST MT 02. Over all test sets, it outperforms 
RNNSearch by 1.4 BLEU points and outperforms 
the other two improved NMT models by 0.3∼0.4 
BLEU points, suggesting the benefits of high de-
gree of coupling the word sequence and the struc-
tural label sequence. This is very encouraging 
since the Mixed RNN encoder is the simplest, 
without introducing new parameters and with only 
slight additional training time. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ratio of over translation (ROT) on test 
sets. Here NR is for proper noun, CD for cardi-
nal number, DT for determiner, and NN for nouns 
except proper nouns and temporal nouns. 

</table></figure>

			<note place="foot" n="7"> https://github.com/redpony/cdec 8 https://github.com/nyu-dl/ dl4mt-tutorial</note>

			<note place="foot" n="9"> Though the maximum source length limit in Mixed RNN system is set to 150, it approximately contains 50 words in maximum.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank three anony-mous reviewers for providing helpful comments, and also acknowledge Xing Wang, Xiangyu Duan, Zhengxian Gong for useful discussions. This work was supported by National Natural Science Foun-dation of <ref type="bibr">China (Grant No. 61525205, 61331011, 61401295</ref>).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2015</title>
		<meeting>ICLR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machinetranslation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST 2014</title>
		<meeting>SSST 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010 System Demonstrations</title>
		<meeting>ACL 2010 System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Factored neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04621</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for wmt&apos;15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT 2015</title>
		<meeting>WMT 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<meeting>EMNLP 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling syntactic and semantic structures in hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2013</title>
		<meeting>HLT-NAACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-COLING 2006</title>
		<meeting>ACL-COLING 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive unsupervised word alignment with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2015</title>
		<meeting>AAAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT 2015</title>
		<meeting>IWSLT 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Soft syntactic constraints for hierarchical phrased-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT 2008</title>
		<meeting>ACL-HLT 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2283" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL 2007</title>
		<meeting>HLTNAACL 2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new string-to-dependency machine translation algorithm with a target dependency language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT 2008</title>
		<meeting>ACL-HLT 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Does string-based neural MT learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="87" to="99" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2017</title>
		<meeting>AAAI 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural machine translation advised by statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2017</title>
		<meeting>AAAI 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3330" to="3336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Penn Chinese Treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
