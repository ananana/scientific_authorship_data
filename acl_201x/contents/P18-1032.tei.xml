<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Poerner</surname></persName>
							<email>poerner@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="340" to="350"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>340</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>DNNs are complex models that combine linear transformations with different types of nonlinear- ities. If the model is deep, i.e., has many layers, then its behavior during training and inference is notoriously hard to understand. This is a problem for both scientific method- ology and real-world deployment. Scientific methodology demands that we understand our models. In the real world, a decision (e.g., "your blog post is offensive and has been removed") by itself is often insufficient; in addition, an expla- nation of the decision may be required (e.g., "our system flagged the following words as offensive"). The European Union plans to mandate that intelli- gent systems used for sensitive applications pro- vide such explanations (European General Data Protection Regulation, expected 2018, cf. Good- man and Flaxman <ref type="bibr">(2016)</ref>).</p><p>A number of post hoc explanation methods for DNNs have been proposed. Due to the complexity of the DNNs they explain, these methods are nec- essarily approximations and come with their own sources of error. At this point, it is not clear which of these methods to use when reliable explanations for a specific DNN architecture are needed.</p><p>Definitions. (i) A task method solves an NLP problem, e.g., a GRU that predicts sentiment.</p><p>(ii) An explanation method explains the behav- ior of a task method on a specific input. For our purpose, it is a function φ(t, k, X) that assigns real-valued relevance scores for a target class k (e.g., positive) to positions t in an input text X (e.g., "great food"). For this example, an ex- planation method might assign: φ(1, k, X) &gt; φ(2, k, X).</p><p>(iii) An (explanation) evaluation paradigm quantitatively evaluates explanation methods for a task method, e.g., by assigning them accuracies.</p><p>Contributions. (i) We present novel evaluation paradigms for explanation methods for two classes of common NLP tasks (see §2). Crucially, nei- ther paradigm requires manual annotations and our methodology is therefore broadly applicable.</p><p>(ii) Using these paradigms, we perform a com- prehensive evaluation of explanation methods for NLP ( §3). We cover the most important classes of task methods, RNNs and CNNs, as well as the recently proposed Quasi-RNNs.</p><p>(iii) We introduce LIMSSE ( §3.6), an expla- nation method inspired by LIME ( <ref type="bibr">Ribeiro</ref>   From : kolstad @ cae.wisc.edu ( Joel Kolstad ) Subject : Re : Can Radio Freq . Be Used To Measure Distance ? <ref type="bibr">[...]</ref> What is the difference between vertical and horizontal ? Gravity ? Does n't gravity pull down the photons and cause a doppler shift or something ? ( Just kidding ! )</p><formula xml:id="formula_0">grad L2 1p</formula><p>If you find faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for details . ) Thank you . 'The Armenians just shot and shot . Maybe coz they 're 'quality' cars ; -) 200 posts/day . <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation paradigms</head><p>In this section, we introduce two novel evalua- tion paradigms for explanation methods on two types of common NLP tasks, small context tasks and large context tasks. Small context tasks are defined as those that can be solved by finding short, self-contained indicators, such as words and phrases, and weighing them up (i.e., tasks where CNNs with pooling can be expected to perform well). We design the hybrid document paradigm for evaluating explanation methods on small con- text tasks. Large context tasks require the cor- rect handling of long-distance dependencies, such as subject-verb agreement. <ref type="bibr">1</ref> We design the mor- phosyntactic agreement paradigm for evaluating explanation methods on large context tasks. We could also use human judgments for evaluation. While we use <ref type="bibr" target="#b25">Mohseni and Ragan (2018)</ref>'s manual relevance benchmark for com- parison, there are two issues with it: (i) Due to the cost of human labor, it is limited in size and domain. (ii) More importantly, a good explana- tion method should not reflect what humans at- tend to, but what task methods attend to. For in- stance, the family name "Kolstad" has 11 out of its 13 appearances in the 20 newsgroups corpus in sci.electronics posts. Thus, task methods probably learn it as a sci.electronics indicator. Indeed, the <ref type="bibr">1</ref> Consider deciding the number of <ref type="bibr">[verb]</ref> in "the children in the green house said that the big telescope <ref type="bibr">[verb]</ref>" vs. "the children in the green house who broke the big telescope <ref type="bibr">[verb]</ref>". The local contexts of "children" or " <ref type="bibr">[verb]</ref>" do not suffice to solve this problem, instead, the large context of the entire sentence has to be considered. explanation method in <ref type="figure">Fig 1 (top)</ref> marks "Kolstad" as relevant, but the human annotator does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Small context: Hybrid document paradigm</head><p>Given a collection of documents, hybrid docu- ments are created by randomly concatenating doc- ument fragments. We assume that, on average, the most relevant input for a class k in a hybrid doc- ument is located in a fragment that stems from a document with gold label k. Hence, an explana- tion method succeeds if it places maximal rele- vance for k inside the correct fragment. Formally, let x t be a word inside hybrid docu- ment X that originates from a document X with gold label y(X ). x t 's gold label y(X, t) is set to y(X ). Let f (X) be the class assigned to the hybrid document by a task method, and let φ be an explanation method as defined above. Let rmax(X, φ) denote the position of the maximally relevant word in X for the predicted class f (X). If this maximally relevant word comes from a doc- ument with the correct gold label, the explanation method is awarded a hit:</p><formula xml:id="formula_1">hit(φ, X) = I[y X, rmax(X, φ) = f (X)] (1)</formula><p>where I[P ] is 1 if P is true and 0 otherwise. <ref type="figure">In  Fig 1 (bottom)</ref>, the explanation method grad L2 1p places rmax outside the correct (underlined) frag- ment. Therefore, it does not get a hit point, while limsse ms s does. The pointing game accuracy of an explana- tion method is calculated as its total number of hit points divided by the number of possible hit points. This is a form of the pointing game paradigm from computer vision (   predicts the agreeing feature in w should pay at- tention to v. For example, in the sentence "the children with the telescope are home", the num- ber of the verb (plural for "are") can be predicted from the subject ("children") without looking at the verb. If the language allows for v and w to be far apart <ref type="figure" target="#fig_4">(Fig 3, top)</ref>, successful task methods have to be able to handle large contexts. <ref type="bibr" target="#b23">Linzen et al. (2016)</ref> show that English verb number can be predicted by a unidirectional LSTM with accuracy &gt; 99%, based on left context alone. When a task method predicts the correct number, we expect successful explanation meth- ods to place maximal relevance on the subject:</p><formula xml:id="formula_2">hit target (φ, X) = I[rmax(X, φ) = target(X)]</formula><p>where target(X) is the location of the subject, and rmax is calculated as above. Regardless of whether the prediction is correct, we expect rmax to fall onto a noun that has the predicted number:</p><formula xml:id="formula_3">hit feat (φ, X) = I[feat X, rmax(X, φ) = f (X)]</formula><p>where feat(X, t) is the morphological feature (here: number) of x t . <ref type="figure" target="#fig_1">In Fig 2,</ref> rmax on "link" gives a hit target point (and a hit feat point), rmax on "editor" gives a hit feat point. grad L2 s does not get any points as "history" is not a plural noun.</p><p>Labels for this task can be automatically gen- erated using part-of-speech taggers and parsers, which are available for many languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Explanation methods</head><p>In this section, we define the explanation meth- ods that will be evaluated. For our purpose, ex- planation methods produce word relevance scores φ(t, k, X), which are specific to a given class k and a given input X. φ(t, k, X) &gt; φ(t , k, X) means that x t contributed more than x t to the task method's (potential) decision to classify X as k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gradient-based explanation methods</head><p>Gradient-based explanation methods approximate the contribution of some DNN input i to some out- put o with o's gradient with respect to i ( <ref type="bibr" target="#b32">Simonyan et al., 2014</ref>). In the following, we consider two output functions o(k, X), the unnormalized class score s(k, X) and the class probability p(k|X):</p><formula xml:id="formula_4">s(k, X) = w k · h(X) + b k (2) p(k|X) = exp s(k, X) K k =1 exp s(k , X) (3)</formula><p>where k is the target class, h(X) the document representation (e.g., an RNN's final hidden layer),</p><formula xml:id="formula_5">w k (resp. b k ) k's weight vector (resp. bias).</formula><p>The simple gradient of o(k, X) w.r.t. i is:</p><formula xml:id="formula_6">grad 1 (i, k, X) = ∂o(k, X) ∂i (4)</formula><p>grad 1 underestimates the importance of inputs that saturate a nonlinearity ( <ref type="bibr" target="#b31">Shrikumar et al., 2017)</ref>. To address this, Sundararajan et al. <ref type="formula">(2017)</ref> integrate over all gradients on a linear interpola- tion α ∈ [0, 1] between a baseline input ¯ X (here: all-zero embeddings) and X:</p><formula xml:id="formula_7">grad (i, k, X) = 1 α=0 ∂o(k, ¯ X+α(X− ¯ X)) ∂i ∂α ≈ 1 M M m=1 ∂o(k, ¯ X+ m M (X− ¯ X)) ∂i (5)</formula><p>where M is a big enough constant (here: 50).</p><p>In NLP, symbolic inputs (e.g., words) are often represented as one-hot vectors x t ∈ {1, 0} |V | and embedded via a real-valued matrix: e t = M x t . Gradients are computed with respect to individual entries of E = [ e 1 . . . e |X| ].  and Hechtlinger (2016) use the L2 norm to reduce vectors of gradients to single values:</p><formula xml:id="formula_8">φ grad L2 (t, k, X) = ||grad( e t , k, E)|| (6)</formula><p>where grad( e t , k, E) is a vector of elementwise gradients w.r.t. e t . Denil et al. <ref type="formula">(2015)</ref> use the dot product of the gradient vector and the embedding 2 , i.e., the gradient of the "hot" entry in x t :</p><formula xml:id="formula_9">φ grad dot (t, k, X) = e t · grad( e t , k, E) (7)</formula><p>We use "grad 1 " for Eq 4, "grad " for Eq 5, " p " for Eq 3, " s " for Eq 2, "L2" for Eq 6 and "dot" for Eq 7. This gives us eight explanation meth- ods:</p><formula xml:id="formula_10">grad L2 1s , grad L2 1p , grad dot 1s , grad dot 1p , grad L2 s , grad L2 p , grad dot s , grad dot p . 2 For grad dot , replace et with et − ¯ et.</formula><p>Since our baseline embeddings are all-zeros, this is equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer-wise relevance propagation</head><p>Layer-wise relevance propagation (LRP) is a backpropagation-based explanation method devel- oped for fully connected neural networks and CNNs ( <ref type="bibr" target="#b6">Bach et al., 2015)</ref> and later extended to <ref type="bibr">LSTMs (Arras et al., 2017b)</ref>. In this paper, we use Epsilon LRP (Eq 58, <ref type="bibr" target="#b6">Bach et al. (2015)</ref>). Re- member that the activation of neuron j, a j , is the sum of weighted upstream activations, i a i w i,j , plus bias b j , squeezed through some nonlinearity. We denote the pre-nonlinearity activation of j as a j . The relevance of j, R(j), is distributed to up- stream neurons i proportionally to the contribution that i makes to a j in the forward pass:</p><formula xml:id="formula_11">R(i) = j R(j) a i w i,j a j + esign(a j )<label>(8)</label></formula><p>This ensures that relevance is conserved between layers, with the exception of relevance attributed to b j . To prevent numerical instabilities, esign(a ) returns − if a &lt; 0 and otherwise. We set = .001. The full algorithm is:</p><formula xml:id="formula_12">R(L k ) = s(k, X)I[k = k]</formula><p>... recursive application of Eq 8 ...</p><formula xml:id="formula_13">φ lrp (t, k, X) = dim( et) j=1 R(e t,j )</formula><p>where L is the final layer, k the target class and R(e t,j ) the relevance of dimension j in the t'th embedding vector. For → 0 and provided that all nonlinearities up to the unnormalized class score are relu, Epsilon LRP is equivalent to the prod- uct of input and raw score gradient (here: grad dot 1s ) ( <ref type="bibr" target="#b18">Kindermans et al., 2016</ref>). In our experiments, the second requirement holds only for CNNs.</p><p>Experiments by <ref type="bibr" target="#b0">Ancona et al. (2017)</ref> (see §6) suggest that LRP does not work well for LSTMs if all neurons -including gates -participate in backpropagation. We therefore use <ref type="bibr" target="#b4">Arras et al. (2017b)</ref>'s modification and treat sigmoid-activated gates as time step-specific weights rather than neu- rons. For instance, the relevance of LSTM candi- date vector g t is calculated from memory vector c t and input gate vector i t as</p><formula xml:id="formula_14">R(g t,d ) = R(c t,d ) g t,d · i t,d c t,d + esign(c t,d )</formula><p>This is equivalent to applying Eq 8 while treating i t as a diagonal weight matrix. The gate neurons in i t do not receive any relevance themselves. See supplementary material for formal definitions of Epsilon LRP for different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DeepLIFT</head><p>DeepLIFT ( <ref type="bibr" target="#b31">Shrikumar et al., 2017</ref>) is another backpropagation-based explanation method. Un- like LRP, it does not explain s(k, X), but s(k, X)−s(k, ¯ X), where ¯ X is some baseline input (here: all-zero embeddings). Following Ancona et al. (2018) (Eq 4), we use this backpropagation rule:</p><formula xml:id="formula_15">R(i) = j R(j) a i w i,j − ¯ a i w i,j a j − ¯ a j + esign(a j − ¯ a j )</formula><p>where ¯ a refers to the forward pass of the base- line. Note that the original method has a dif- ferent mechanism for avoiding small denomina- tors; we use esign for compatibility with LRP. The DeepLIFT algorithm is started with</p><formula xml:id="formula_16">R(L k ) = s(k, X)−s(k, ¯ X) I[k = k].</formula><p>On gated (Q)RNNs, we proceed analogous to LRP and treat gates as weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cell decomposition for gated RNNs</head><p>The cell decomposition explanation method for LSTMs <ref type="bibr" target="#b26">(Murdoch and Szlam, 2017</ref>) decomposes the unnormalized class score s(k, X) (Eq 2) into additive contributions. For every time step t, we compute how much of c t "survives" until the final step T and contributes to s(k, X). This is achieved by applying all future forget gates f , the final tanh nonlinearity, the final output gate o T , as well as the class weights of k to c t . We call this quantity "net load of t for class k":</p><formula xml:id="formula_17">nl(t, k, X) = w k · o T tanh ( T j=t+1 f j ) c t</formula><p>where and are applied elementwise. The rel- evance of t is its gain in net load relative to t − 1:</p><formula xml:id="formula_18">φ decomp (t, k, X) = nl(t, k, X) − nl(t − 1, k, X).</formula><p>For GRU, we change the definition of net load:</p><formula xml:id="formula_19">nl(t, k, X) = w k · ( T j=t+1 z j ) h t</formula><p>where z are GRU update gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Input perturbation methods</head><p>Input perturbation methods assume that the re- moval or masking of relevant inputs changes the output ( <ref type="bibr" target="#b34">Zeiler and Fergus, 2014</ref>). Omission- based methods remove inputs completely <ref type="bibr">(Kádár et al., 2017)</ref>, while occlusion-based methods re- place them with a baseline ( <ref type="bibr" target="#b22">Li et al., 2016b</ref>). In computer vision, perturbations are usually applied to patches, as neighboring pixels tend to correlate <ref type="bibr" target="#b36">(Zintgraf et al., 2017)</ref>. To calculate the omit N (resp. occ N ) relevance of word x t , we delete (resp. occlude), one at a time, all N -grams that contain x t , and average the change in the unnormalized class score from Eq 2:</p><formula xml:id="formula_20">φ [omit|occ] N (t, k, X) = N j=1 s(k, [ e 1 . . . e |X| ]) −s(k, [ e 1 . . . e t−N −1+j ] ¯ E[ e t+j . . . e |X| ]) 1</formula><p>N where e t are embedding vectors, denotes con- catenation and ¯ E is either a sequence of length zero (φ omit ) or a sequence of N baseline (here: all-zero) embedding vectors (φ occ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">LIMSSE: LIME for NLP</head><p>Local Interpretable Model-agnostic Explanations (LIME) ( <ref type="bibr" target="#b28">Ribeiro et al., 2016</ref>) is a framework for explaining predictions of complex classifiers. LIME approximates the behavior of classifier f in the neighborhood of input X with an interpretable (here: linear) model. The interpretable model is trained on samples Z 1 . . . Z N (here: N = 3000), which are randomly drawn from X, with "gold la- bels" f (Z 1 ) . . . f (Z N ).</p><p>Since RNNs and CNNs respect word or- der, we cannot use the bag of words sam- pling method from the original description of LIME. Instead, we introduce Local Inter- pretable Model-agnostic Substring-based Expla- nations (LIMSSE). LIMSSE uniformly samples a length l n (here: 1 ≤ l n ≤ 6) and a start- ing point s n , which define the substring Z n = [ x sn . . . x sn+ln−1 ]. To the linear model, Z n is rep- resented by a binary vector z n ∈ {0, 1} |X| , where</p><formula xml:id="formula_21">z n,t = I[s n ≤ t &lt; s n + l n ].</formula><p>We learn a linear weight vectorˆvvectorˆ vectorˆv k ∈ R |X| , whose entries are word relevances for k, i.e., φ limsse (t, k, X) = ˆ v k,t . To optimize it, we experi- ment with three loss functions. The first, which we will refer to as limsse bb , assumes that our DNN is a total black box that delivers only a classification:</p><formula xml:id="formula_22">ˆ v k = argmin v k n − log σ( z n · v k ) I[f (Z n ) = k] + log 1 − σ( z n · v k ) I[f (Z n ) = k]</formula><p>where f (Z n ) = argmax k p(k |Z n ) . The black box approach is maximally general, but insensitive to the magnitude of evidence found in Z n . Hence, we also test magnitude-sensitive loss functions:</p><formula xml:id="formula_23">ˆ v k = argmin v k n z n · v k − o(k, Z n ) 2</formula><p>where o(k, Z n ) is one of s(k, Z n ) or p(k|Z n ). We refer to these as limsse ms s and limsse ms p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hybrid document experiment</head><p>For the hybrid document experiment, we use the 20 newsgroups corpus (topic classification) <ref type="bibr" target="#b19">(Lang, 1995)</ref> and reviews from the 10th yelp dataset challenge (binary sentiment analysis) <ref type="bibr">3</ref>  In all five architectures, the resulting document rep- resentation is projected to 20 (resp. two) dimen- sions using a fully connected layer, followed by a softmax. See supplementary material for details on training and regularization. After training, we sentence-tokenize the test sets, shuffle the sentences, concatenate ten sen- tences at a time and classify the resulting hybrid documents. Documents that are assigned a class that is not the gold label of at least one con- stituent word are discarded (yelp: &lt; 0.1%; 20 newsgroups: 14% -20%). On the remaining docu- ments, we use the explanation methods from §3 to find the maximally relevant word for each predic- tion. The random baseline samples the maximally relevant word from a uniform distribution.</p><p>For reference, we also evaluate on a hu- man judgment benchmark (Mohseni and Ra- gan (2018), <ref type="table">Table 2</ref>, C11-C15). It contains column C01 C02 C03 C04 C05 C06 C07 C08 C09 C10 C11 C12 C13 C14 C15 C16 C17 C18 C19 C20 C21 C22 C23 C24 C25 C26 C27 hybrid document experiment man. groundtruth morphosyntactic agreement experiment hittarget hit feat yelp 20 newsgroups 20 newsgroups f (X) = y(X) f (X) = y <ref type="table">(X)   φ   GRU  QGRU  LSTM  QLSTM  CNN  GRU  QGRU  LSTM  QLSTM  CNN  GRU  QGRU  LSTM  QLSTM  CNN  GRU  QGRU  LSTM  QLSTM  GRU  QGRU  LSTM  QLSTM  GRU  QGRU  LSTM</ref>   <ref type="table">Table 2</ref>: Pointing game accuracies in hybrid document experiment (left), on manually annotated bench- mark (middle) and in morphosyntactic agreement experiment (right). hit target (resp. hit feat ): maximal relevance on subject (resp. on noun with the predicted number feature). Bold: top explanation method. Underlined: within 5 points of top explanation method.</p><p>188 documents from the 20 newsgroups test set (classes sci.med and sci.electronics), with one manually created list of relevant words per doc- ument. We discard documents that are incorrectly classified (20% -27%) and define: hit(φ, X) = I[rmax(X, φ) ∈ gt(X)], where gt(X) is the man- ual ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Morphosyntactic agreement experiment</head><p>For the morphosyntactic agreement experiment, we use automatically annotated English Wikipedia sentences by <ref type="bibr" target="#b23">Linzen et al. (2016)</ref>  The gold label of a sentence is the number of its verb, i.e., y(X) = feat(X, T + 1). <ref type="bibr">5</ref> www.tallinzen.net/media/rnn_ agreement/agr_50_mostcommon_10K.tsv.gz</p><p>As task methods, we replicate <ref type="bibr" target="#b23">Linzen et al. (2016)</ref>'s unidirectional LSTM (R 50 randomly initialized word embeddings, hidden size 50). We also train unidirectional GRU, QGRU and QLSTM architectures with the same dimension- ality. We use the explanation methods from §3 to find the most relevant word for predictions on the test set. As described in §2.2, explanation methods are awarded a hit target (resp. hit feat ) point if this word is the subject (resp. a noun with the predicted number feature). For reference, we use a random baseline as well as a baseline that assumes that the most relevant word directly precedes the verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Explanation methods</head><p>Our experiments suggest that explanation methods for neural NLP differ in quality.</p><p>As in previous work (see §6), gradient L2 norm (grad L2 ) performs poorly, especially on RNNs. We assume that this is due to its inability to distinguish relevances for and against k.</p><p>Gradient embedding dot product (grad dot ) is competitive on CNN <ref type="table">(Table 2</ref>, grad dot 1p C05, grad dot 1s C10, C15), presumably because relu is linear on positive inputs, so gradients are exact in-decomp initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...] deeplift initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...] limsse ms p initially a pagan culture , detailed information about the return of the christian religion to the islands during the norse-era [is ...]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lrp</head><p>Your day is done . Definitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone . We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n't fit you in . " It 's good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce .</p><formula xml:id="formula_24">limsse ms p</formula><p>Your day is done . Definitely looking forward to going back . All three were outstanding ! I would highly recommend going here to anyone . We will see if anyone returns the message my boyfriend left . The price is unbelievable ! And our guys are on lunch so we ca n't fit you in . " It 's good , standard froyo . The pork shoulder was THAT tender . Try it with the Tomato Basil cram sauce . Integrated gradient (grad ) mostly outper- forms simple gradient (grad 1 ), though not consis- tently (C01, C07). Contrary to expectation, in- tegration did not help much with the failure of the gradient method on LSTM on 20 newsgroups (grad dot 1 vs. grad dot in C08, C13), which we had assumed to be due to saturation of tanh on large absolute activations in c. Smaller intervals may be needed to approximate the integration, however, this means additional computational cost.</p><p>The gradient of s(k, X) performs better or sim- ilar to the gradient of p(k|X). The main exception is yelp (grad dot 1s vs. grad dot 1p , C01-C05). This is probably due to conflation by p(k|X) of evidence for k (numerator in Eq 3) and against competi- tor classes (denominator). In a two-class scenario, there is little incentive to keep classes separate, leading to information flow through the denomi- nator. In future work, we will replace the two- way softmax with a one-way sigmoid such that φ(t, 0, X) := −φ(t, 1, X).</p><p>LRP and DeepLIFT are the most consistent explanation methods across evaluation paradigms and task methods. (The comparatively low point- ing game accuracies on the yelp QRNNs and CNN (C02, C04, C05) are probably due to the fact that they explain s(k, .) in a two-way softmax, see above.) On CNN (C05, C10, C15), LRP and grad dot 1s perform almost identically, suggest- ing that they are indeed quasi-equivalent on this ar- chitecture (see §3.2). On (Q)RNNs, modified LRP and DeepLIFT appear to be superior to the gradi- ent method (lrp vs. grad dot 1s , deeplift vs. grad dot s , C01-C04, C06-C09, C11-C14, C16-C27).</p><p>Decomposition performs well on LSTM, es- pecially in the morphosyntactic agreement exper- iment, but it is inconsistent on other architec- tures. Gated RNNs have a long-term additive and a multiplicative pathway, and the decomposition method only detects information traveling via the additive one. <ref type="bibr" target="#b24">Miao et al. (2016)</ref> show qualita- tively that GRUs often reorganize long-term mem- ory abruptly, which might explain the difference between LSTM and GRU. QRNNs only have ad- ditive recurrent connections; however, given that c t (resp. h t ) is calculated by convolution over sev- eral time steps, decomposition relevance can be in- correctly attributed inside that window. This likely is the reason for the stark difference between the performance of decomposition on QRNNs in the hybrid document experiment and on the manually labeled data (C07, C09 vs. C12, C14). Overall, we do not recommend the decomposition method, because it fails to take into account all routes by which information can be propagated.</p><p>Omission and occlusion produce inconsis- tent results in the hybrid document experiment. <ref type="bibr" target="#b31">Shrikumar et al. (2017)</ref> show that perturbation methods can lack sensitivity when there are more relevant inputs than the "perturbation window" covers. In the morphosyntactic agreement experi- ment, omission is not competitive; we assume that this is because it interferes too much with syntactic structure. occ 1 does better (esp. C16-C19), possi- bly because an all-zero "placeholder" is less dis- ruptive than word removal. But despite some high scores, it is less consistent than other explanation methods.</p><p>Magnitude-sensitive LIMSSE (limsse ms ) consistently outperforms black-box LIMSSE (limsse bb ), which suggests that numerical out- puts should be used for approximation where possible. In the hybrid document experiment, magnitude-sensitive LIMSSE outperforms the other explanation methods (exceptions: C03, C05). However, it fails in the morphosyntactic agreement experiment (C16-C27). In fact, we expect LIMSSE to be unsuited for large context problems, as it cannot discover dependencies whose range is bigger than a given text sample. <ref type="figure" target="#fig_4">In Fig 3 (top)</ref>, limsse ms p highlights any singular noun without taking into account how that noun fits into the overall syntactic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation paradigms</head><p>The assumptions made by our automatic evalua- tion paradigms have exceptions: (i) the correlation between fragment of origin and relevance does not always hold (e.g., a positive review may contain negative fragments, and will almost certainly con- tain neutral fragments); (ii) in morphological pre- diction, we cannot always expect the subject to be the only predictor for number. <ref type="figure" target="#fig_1">In Fig 2 (bottom)</ref> for example, "few" is a reasonable clue for plural despite not being a noun. This imperfect ground truth means that absolute pointing game accura- cies should be taken with a grain of salt; but we argue that this does not invalidate them for com- parisons.</p><p>We also point out that there are characteristics of explanations that may be desirable but are not reflected by the pointing game. <ref type="figure" target="#fig_4">Consider Fig 3  (bottom)</ref>. Both explanations get hit points, but the lrp explanation appears "cleaner" than limsse ms p , with relevance concentrated on fewer tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work 6.1 Explanation methods</head><p>Explanation methods can be divided into local and global methods <ref type="bibr" target="#b13">(Doshi-Velez and Kim, 2017)</ref>. Global methods infer general statements about what a DNN has learned, e.g., by clustering docu- ments (Aubakirova and Bansal, 2016) or n-grams ( <ref type="bibr">Kádár et al., 2017</ref>) according to the neurons that they activate. <ref type="bibr" target="#b21">Li et al. (2016a)</ref> compare embed- dings of specific words with reference points to measure how drastically they were changed dur- ing training. In computer vision, <ref type="bibr" target="#b32">Simonyan et al. (2014)</ref> optimize the input space to maximize the activation of a specific neuron. Global explanation methods are of limited value for explaining a spe- cific prediction as they represent average behavior. Therefore, we focus on local methods.</p><p>Local explanation methods explain a decision taken for one specific input at a time. We have attempted to include all important local methods for NLP in our experiments (see §3). We do not address self-explanatory models (e.g., atten- tion ( <ref type="bibr" target="#b7">Bahdanau et al., 2015</ref>) or rationale models ( <ref type="bibr" target="#b20">Lei et al., 2016)</ref>), as these are very specific archi- tectures that may not be not applicable to all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Explanation evaluation</head><p>According to <ref type="bibr" target="#b13">Doshi-Velez and Kim (2017)</ref>'s taxonomy of explanation evaluation paradigms, application-grounded paradigms test how well an explanation method helps real users solve real tasks (e.g., doctors judge automatic diagnoses); human-grounded paradigms rely on proxy tasks (e.g., humans rank task methods based on expla- nations); functionally-grounded paradigms work without human input, like our approach. <ref type="bibr" target="#b2">Arras et al. (2016</ref><ref type="bibr">) (cf. Samek et al. (2016</ref>) propose a functionally-grounded explanation eval- uation paradigm for NLP where words in a cor- rectly (resp. incorrectly) classified document are deleted in descending (resp. ascending) order of relevance. They assume that the fewer words must be deleted to reduce (resp. increase) accuracy, the better the explanations. According to this metric, LRP ( §3. An issue with the word deletion paradigm is that it uses syntactically broken inputs, which may in- troduce artefacts <ref type="bibr" target="#b33">(Sundararajan et al., 2017</ref>). In our hybrid document paradigm, inputs are syntac- tically intact (though semantically incoherent at the document level); the morphosyntactic agree- ment paradigm uses unmodified inputs.</p><p>Another class of functionally-grounded evalu- ation paradigms interprets the performance of a secondary task method, on inputs that are derived from (or altered by) an explanation method, as a proxy for the quality of that explanation method. <ref type="bibr" target="#b26">Murdoch and Szlam (2017)</ref> build a rule-based classifier from the most relevant phrases in a cor- pus (task method: LSTM). The classifier based on decomp ( §3.4) outperforms the gradient-based classifier, which is in line with our results. Ar- ras et al. (2017a) build document representations by summing over word embeddings weighted by relevance scores (task method: CNN). They show that K-nearest neighbor performs better on doc-ument representations derived with LRP than on those derived with grad L2 , which also matches our results. <ref type="bibr" target="#b12">Denil et al. (2015)</ref> condense documents by extracting top-K relevant sentences, and let the original task method (CNN) classify them. The accuracy loss, relative to uncondensed documents, is smaller for grad dot than for heuristic baselines.</p><p>In the domain of human-based evaluation paradigms, <ref type="bibr" target="#b28">Ribeiro et al. (2016)</ref> compare differ- ent variants of LIME ( §3.6) by how well they help non-experts clean a corpus from words that lead to overfitting. <ref type="bibr" target="#b30">Selvaraju et al. (2017)</ref> assess how well explanation methods help non-experts iden- tify the more accurate out of two object recogni- tion CNNs. These experiments come closer to real use cases than functionally-grounded paradigms; however, they are less scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>We conducted the first comprehensive evaluation of explanation methods for NLP, an important un- dertaking because there is a need for understand- ing the behavior of DNNs.</p><p>To conduct this study, we introduced evalua- tion paradigms for explanation methods for two classes of NLP tasks, small context tasks (e.g., topic classification) and large context tasks (e.g., morphological prediction). Neither paradigm re- quires manual annotations. We also introduced LIMSSE, a substring-based explanation method inspired by LIME and designed for NLP.</p><p>Based on our experimental results, we recom- mend LRP, DeepLIFT and LIMSSE for small con- text tasks and LRP and DeepLIFT for large con- text tasks, on all five DNN architectures that we tested. On CNNs and possibly GRUs, the (inte- grated) gradient embedding dot product is a good alternative to DeepLIFT and LRP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Code</head><p>Our implementation of LIMSSE, the gradi- ent, perturbation and decomposition meth- ods can be found in our branch of the keras package: www.github.com/ NPoe/keras.</p><p>To re-run our experiments, see scripts in www.github.com/NPoe/ neural-nlp-explanation-experiment. Our LRP implementation (same repository) is adapted from <ref type="bibr" target="#b4">Arras et al. (2017b)</ref>  <ref type="bibr">6</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: verb context classified singular. Green: evidence for singular. Task method: GRU. Bottom: verb context classified plural. Green: evidence for plural. Task method: LSTM. Underlined: subject. Bold: rmax position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>We train five DNNs per corpus: a bidirectional GRU (Cho et al., 2014), a bidirectional LSTM (Hochreiter and Schmidhuber, 1997), a 1D CNN with global max pooling (Collobert et al., 2011), a bidirec- tional Quasi-GRU (QGRU), and a bidirectional Quasi-LSTM (QLSTM). The Quasi-RNNs are 1D CNNs with a feature-wise gated recursive pooling layer (Bradbury et al., 2017). Word embeddings are R 300 and initialized with pre-trained GloVe embeddings (Pennington et al., 2014) 4 . The main layer has a hidden size of 150 (bidirectional ar- chitectures: 75 dimensions per direction). For the QRNNs and CNN, we use a kernel width of 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 .</head><label>5</label><figDesc>For our pur- pose, a sample consists of: all words preceding the verb: X = [x 1 · · · x T ]; part-of-speech (POS) tags: pos(X, t) ∈ {VBZ, VBP, NN, NNS, . . .}; and the position of the subject: target(X) ∈ [1, T ]. The number feature is derived from the POS: feat(X, t) =      Sg if pos(X, t) ∈ {VBZ, NN} Pl if pos(X, t) ∈ {VBP, NNS} n/a otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top: verb context classified singular. Task method: LSTM. Bottom: hybrid yelp review, classified positive. Task method: QLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2) outperforms grad L2 on CNNs (Arras et al., 2016) and LSTMs (Arras et al., 2017b) on 20 newsgroups. Ancona et al. (2017) perform the same experiment with a binary sentiment analy- sis LSTM. Their graph shows occ 1 , grad dot 1 and grad dot tied in first place, while LRP, DeepLIFT and the gradient L1 norm lag behind. Note that their treatment of LSTM gates in LRP / DeepLIFT differs from our implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>...] limsse ms s If you find faith to be honest , show me how . David The whole denominational mindset only causes more problems , sadly . ( See section 7 for details . ) Thank you . 'The Armenians just shot and shot . Maybe coz they 're 'quality' cars ; -) 200 posts/day . [...]</head><label></label><figDesc></figDesc><table>Figure 1: Top: sci.electronics post (not hybrid). Underlined: Manual relevance ground truth. 
Green: evidence for sci.electronics. Task method: CNN. Bottom: hybrid newsgroup post, classified 
talk.politics.mideast. Green: evidence for talk.politics.mideast. Underlined: talk.politics.mideast frag-
ment. Task method: QGRU. Italics: OOV. Bold: rmax position. See supplementary for full texts. 

2016) that is designed for word-order sensitive 
task methods (e.g., RNNs, CNNs). We show em-
pirically that LIMSSE, LRP (Bach et al., 2015) 
and DeepLIFT (Shrikumar et al., 2017) are the 
most effective explanation methods ( §4): LRP and 
DeepLIFT are the most consistent methods, while 
LIMSSE wins the hybrid document experiment. 

</table></figure>

			<note place="foot" n="3"> www.yelp.com/dataset_challenge 4 http://nlp.stanford.edu/data/glove. 840B.300d.zip</note>

			<note place="foot" n="6"> https://github.com/ArrasL/LRP_for_ LSTM</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified view of gradientbased attribution methods for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz¨oztirelicengiz¨</forename><surname>Cengiz¨oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing System</title>
		<meeting><address><addrLine>Long Beach, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards better understanding of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz¨oztirelicengiz¨</forename><surname>Cengiz¨oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explaining predictions of non-linear classifiers in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Representation Learning for NLP</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What is relevant in a text document?: An interpretable machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">181142</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining recurrent neural network predictions in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpreting neural networks to improve politeness comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malika</forename><surname>Aubakirova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2035" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ask the GRU: Multi-task learning for deep text recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Recommender Systems</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extraction of salient sentences from labelled documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Demiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A roadmap for a rigorous science of interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1702.08608</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">European union regulations on algorithmic decision-making and a &quot;right to explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Human Interpretability in Machine Learning</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interpretation of prediction models using the input gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Hechtlinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akos</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="780" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigating the influence of noise and distractors on the interpretation of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Dähne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Tahoe City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>abs/1612.08220</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simplifying long short-term memory acoustic models for fast training and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2284" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A humangrounded evaluation benchmark for local explanations of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric D Ragan</surname></persName>
		</author>
		<idno>abs/1801.05075</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic rule extraction from long short term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Why should I trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2660" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Amsterdam</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing deep neural network decisions: Prediction difference analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luisa M Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tameem</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
