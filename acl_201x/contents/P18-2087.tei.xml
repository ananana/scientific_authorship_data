<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sense-Aware Neural Models for Pun Location in Texts</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitao</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sense-Aware Neural Models for Pun Location in Texts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="volume">546</biblScope>
							<biblScope unit="page" from="546" to="551"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A homographic pun is a form of wordplay in which one signifier (usually a word) suggests two or more meanings by exploiting polysemy for an intended humorous or rhetorical effect. In this paper, we focus on the task of pun location, which aims to identify the pun word in a given short text. We propose a sense-aware neu-ral model to address this challenging task. Our model first obtains several WSD results for the text, and then leverages a bidi-rectional LSTM network to model each sequence of word senses. The outputs at each time step for different LSTM networks are then concatenated for prediction. Evaluation results on the benchmark SemEval 2017 dataset demonstrate the efficacy of our proposed model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There exists a class of language constructs known as puns in natural language utterances and texts, and the speaker or writer intends for a certain word or other lexical item to be interpreted as simulta- neously carrying two or more separate meanings. Though puns are an important feature in many dis- course types, they have attracted relatively little at- tention in the area of natural language processing.</p><p>A pun is a form of wordplay in which a word suggests two or more meanings by exploiting pol- ysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetor- ical effect <ref type="bibr" target="#b7">(Miller et al., 2017)</ref>. Puns where the two meanings share the same pronunciation are known as homographic puns, which are the focus of this study. For example, the following punning joke exploits contrasting meanings of the word "inter- est" and it is a homographic pun.</p><p>I used to be a banker but I lost interest. Since the pun word plays the key role in form- ing a pun, it is very important and meaningful to identify the pun word in a given text. The task of identifying the pun word is known as pun lo- cation, which is defined in SemEval 2017 Task 7 <ref type="bibr">1</ref> . In order to address this special task, various approaches have been attempted, including rule based approach <ref type="bibr" target="#b17">(Vechtomova, 2017)</ref>, knowledge- based approach <ref type="bibr" target="#b2">(Indurthi and Oota, 2017;</ref><ref type="bibr" target="#b18">Xiu et al., 2017)</ref> and supervised approach <ref type="bibr" target="#b13">(Pramanick and Das, 2017;</ref><ref type="bibr" target="#b4">Mikhalkova and Karyakin, 2017)</ref>. However, these approaches do not achieve good results, and the best F1 score for homographic pun location is just 0.6631, which is achieved by the Idiom Savant system with a knowledge based ap- proach ( <ref type="bibr" target="#b0">Doogan et al., 2017)</ref>. The results demon- strate that pun location is a very challenging task.</p><p>In order to address this challenging task and improve the state-of-the-art results, we propose a sense-aware neural model in this study. Our model first obtains several WSD (Word Sense Disam- biguation) results for the text, and leverages a bidi- rectional LSTM network to model each sequence of word senses. The outputs at each time step for different LSTM networks are then concatenated for pun word prediction. Evaluation results of cross-validation on the benchmark SemEval 2017 dataset demonstrate the efficacy of our proposed model. The contributions of this paper are summarized as follows:</p><p>• We propose a novel sense-aware neural model to address the pun location task.</p><p>• Our proposed model outperforms several base- line neural models and achieves the state-of-the- art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pun detection aims to determine whether a given short text contains a pun ( <ref type="bibr" target="#b7">Miller et al., 2017)</ref>. Various methods have been proposed to address this task, including WSD based methods <ref type="bibr" target="#b11">(Pedersen, 2017</ref>), PMI-based methods ( <ref type="bibr" target="#b15">Sevgili et al., 2017</ref>) supervised methods ( <ref type="bibr" target="#b18">Xiu et al., 2017;</ref><ref type="bibr" target="#b2">Indurthi and Oota, 2017;</ref><ref type="bibr" target="#b13">Pramanick and Das, 2017;</ref><ref type="bibr" target="#b4">Mikhalkova and Karyakin, 2017;</ref><ref type="bibr" target="#b16">Vadehra, 2017)</ref>. More specifically, the bi-directional RNN has been used in <ref type="bibr" target="#b2">(Indurthi and Oota, 2017)</ref>, and vote-based ensemble classifier is used by <ref type="bibr" target="#b16">(Vadehra, 2017)</ref>. Pun location is a more challenging task than pun detection, because it aims to find the actual pun word in the given text. Previous works find some clues about puns in the texts. For example, pun is more likely appeared towards the end of sentences <ref type="bibr" target="#b11">(Pedersen, 2017;</ref><ref type="bibr" target="#b8">Miller and Turkovi´cTurkovi´c, 2016)</ref>. Many puns have a particularly strong asso- ciations with other words in the contexts ( <ref type="bibr" target="#b15">Sevgili et al., 2017)</ref>. A variety of methods have been pro- posed to locate the pun words. For example, UWa- terloo system constructs a rule-based pun locator that scores candidate words according to eleven simple heuristics <ref type="bibr" target="#b17">(Vechtomova, 2017)</ref>. BuzzSaw system attempts to locate the pun in a sentence by selecting the polysemous word with the two most dissimilar senses ( <ref type="bibr" target="#b10">Oele and Evang, 2017)</ref>. Du- luth system identifies the last word which changed senses between different word sense disambigua- tion results <ref type="bibr" target="#b11">(Pedersen, 2017)</ref>. Fermi system uses Bi-directional RNN to learn a classification model <ref type="bibr" target="#b2">(Indurthi and Oota, 2017</ref>). Idiom Savant system uses n-grams features, and only content words in- cluding nouns, verbs, adverbs and adjectives are considered as candidate words ( <ref type="bibr" target="#b0">Doogan et al., 2017)</ref>. Pun interpretation is considered a subse- quent step for pun location, and it aims to annotate the two meanings of the given pun by reference to WordNet sense keys. In the work of <ref type="bibr" target="#b6">(Miller and Gurevych, 2015)</ref>, traditional language-agnostic WSD approaches are adapted to "disambiguate" puns, and rather to identify their double meanings.</p><p>Word Sense Disambiguation (WSD) is also re- lated to our work. Some prior works compute overlaps of glosses between the target word and its context <ref type="bibr" target="#b3">(Lesk, 1986)</ref>. These approaches de- rive information from some lexicon thesauruses for WSD, including WordNet <ref type="bibr" target="#b1">(Fellbaum, 1998)</ref> and <ref type="bibr">BabelNet (Navigli and Ponzetto, 2012)</ref>. Su- pervised models, including neural models, have been successfully applied to WSD <ref type="bibr" target="#b19">(Yuan et al., 2016;</ref><ref type="bibr" target="#b14">Raganato et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Neural Model (BM)</head><p>The task of pun location needs to locate the ex- act pun word in each short text or sentence. We regard pun location as a word-level classification task, and attempt to train a model that can predict whether a word in a sentence is a pun or not. A word will be regarded as a pun word with high probability when it is a noun, verb, adjective or adverb, therefore, we only try to make prediction of one word when it has one of the four kinds of parts of speech tags.</p><p>Our baseline model for pun word prediction is similar to <ref type="bibr" target="#b2">(Indurthi and Oota, 2017)</ref> and it adopts a bi-directional LSTM network to accomplish this task. The neural network architecture of the model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The input to the net- work is the embeddings of words, and we use the pre-trained word embeddings by using word2vec <ref type="bibr" target="#b5">(Mikolov et al., 2013</ref>) on the Wikipedia corpus whose size is over 11G. The hidden state of the forward LSTM and the hidden state of the back- ward LSTM are concatenated at each time step (word), and we get the concatenated hidden vec- tors for all words: h 1 , ..., h n . The vector h i for each word having one of the four kinds of POS tags is then sent to a two-layer feed-forward neu- ral network with tanh as activation function, and the output is a real number o i . We then use the sig- moid function σ on o i to make prediction. Since in the experimental data there is only one pun word in each sentence, we will take the k-th word as pun word if and only if o k is the largest number out of all o i , i = 1, ..., n, and σ(o k ) &gt; 0.5. We use the cross-entropy loss in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sense-Aware Neural Model (SAM)</head><p>The baseline neural model is built on the word level and the word senses can only be implicitly captured by the model. Moreover, a pun word usually has two senses in the sentence, while the baseline neural model cannot disambiguate them. In order to improve the prediction performance, we propose a sense-award neural model which is built on WSD results. Two or more WSD results are obtained by using different WSD algorithms or different configurations, and the WSD results may be different. The sequence of word senses cor- responding to each WSD result is modeled by a Different from the Duluth system (Peder- sen, 2017) which identifies the last word which changed senses between different runs of the WordNet::SenseRelate::AllWords disambiguation algorithm, we do not use the WordNet-based WSD results. Furthermore, we do not heuristically iden- tify the pun word but propose a neural model to achieve this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Sense Disambiguation and Sense Embedding</head><p>In order to obtain sense inventory and the sense embeddings for each word, we choose SenseGram ( <ref type="bibr" target="#b12">Pelevina et al., 2016)</ref>. The SenseGram toolkit is available online 2 , and it can take as an input the word embeddings and split different senses of the input words. For instance, the vector for the word "table" will be split into " that we do not use WordNet as sense inventory be- cause the sense inventory is too fine-grained and many words are not included in WordNet. Given each target word ω and its context words C = {c 1 , ..., c k } in the sentence, we want to as- sign a sense vector to ω from the set of its sense vectors S = {s 1 , ..., s m }. We use two simple WSD methods for achieving this. The first WSD strategy is based on the sigmoid function. c c is the mean of the contextual embeddings of words in C and the sense embedding of ω is chosen as s * = arg max</p><formula xml:id="formula_0">s i ∈S 1 1 + e −cc·s i<label>(1)</label></formula><p>Let c w be the mean of the word embeddings of words in C, which is different from c c . The sec- ond disambiguation strategy is based on the cosine similarity function.</p><formula xml:id="formula_1">s * = arg max s i ∈S c w · s i c w · s i<label>(2)</label></formula><p>For each WSD strategy, we can set different window sizes of 3 and 50 (the maximum sentence length in the corpus) as different configurations. By obtaining different WSD results, we expect to well capture the characteristics of homographic puns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Model Details</head><p>The proposed sense-aware neural model differs from the baseline neural model in that it mod- els multiple sequences of word senses correspond- ing to different WSD results. In other words, the sense-aware model works on the sense level, but the baseline model works on the word level.</p><p>The architecture of the sense-aware model is il- lustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, which contains several bi- directional LSTM networks. For each WSD result, the sequence of sense embeddings is taken as input for a bi-directional LSTM network. Assuming we have K WSD results, the outputs h j i (j = 1, ..., K) by K different bi-directional LSTM networks for the same i-th word (i.e., the i-th time step) are then concatenated into one vector, and the vector is sent to a two-layer feed-forward neural network and a sigmoid function for prediction. The loss func- tion is the same as that of the baseline model. Our sense-aware model can be considered as applying the baseline model on different WSD results and then combining the outputs for prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>We use stochastic gradient descent to train the neu- ral models with a batch size of 225 and 512 hidden units, and the learning rate is 0.0001 and the size of embeddings and hidden vectors is 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We use the benchmark dataset from SemEval 2017 Task 7. There are a total of 2250 sentences in the dataset, 1607 of which contain a pun. For Pun Location, we only use sentences with pun words for evaluation, the same as the task setting. Since no training data is provided, so we test our mod- els with 10-fold cross validation. We combine the output results on each test set of all 10 folds and then calculate precision, recall and F-score on the combined set. Thus the scores are comparable to the official results based on the whole test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word Sense Disambiguation</head><p>As is mentioned in section 4.1, we can obtain four WSD results with two different strategies and two different window sizes. We make three groups based on four WSD results: Group 1 (G1) contains two WSD results with the first WSD strategy with two window sizes of 3 and 50; Group 2 (G2) con- tains two WSD results with the second WSD strat- egy with two window sizes; Group 3 (G3) contains all results in Group 1 and Group 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Results</head><p>We compared our proposed SAM model (w/ three groups of WSD results) with the baseline model BM. We also apply the bi-directional LSTM model on the sequence of senses for each single WSD result and thus get BiLSTM-WSD1 through BiLSTM-WSD4. Moreover, we apply SVM and CRF models with various features (e.g., ngram, POS tagging, word location, word similarity, etc.) on this task. <ref type="table">Table 1</ref> shows the results. In the table, we also present the results of the best participating system Idiom Savant, and two official baselines (last word and max. polysemy). We can see that the baseline BM model does not perform well, while the CRF model performs very well. The re- sults of BiLSTM-WSD1 through BiLSTM-WSD4 are much better than the BM model, which indi- cates that the sense-level prediction is more suit- able than the word-level prediction. Our proposed SAM model with different groups of WSD results can further improve the performance, because dif- ferent WSD results may provide complementary information for pun location. The SAM model with G1 performs the best, even outperforming the SAM model with more WSD results (G3), which indicates the necessity for choosing proper WSD results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we apply the neural network mod- els to the pun location task. We proposed a novel sense-aware neural model to leveraging multiple WSD results. Evaluation results on the benchmark SemEval 2017 dataset demonstrate the efficacy of our proposed model. In future work, we will test with more advanced WSD algorithms and try to address the pun interpretation task. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Baseline neural model with bidirectional LSTMs</figDesc><graphic url="image-1.png" coords="3,87.25,62.81,187.77,190.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sense-aware neural model with bidirectional LSTMs</figDesc><graphic url="image-2.png" coords="4,84.44,62.81,193.38,329.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table ( data</head><label>(</label><figDesc></figDesc><table>)" and 
"table (furniture)". SenseGram induces sense in-
ventory from existing word embeddings via clus-
tering of ego-networks of related words. In our 
work, the Wikipedia corpus is used to train word 
embeddings (together with contextual embeddings 
of words) by using word2vec and then the word 
embeddings are used by SenseGram for inducing 
sense inventory and sense embeddings. The word 
similarity graph used by SenseGram is built based 
on the similarity between word embeddings. Note 

2 https://github.com/tudarmstadt-lt/sensegram 

</table></figure>

			<note place="foot" n="1"> http://alt.qcri.org/semeval2017/task7/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by National Natural Sci-ence Foundation of China <ref type="formula">(61772036, 61331011)</ref> and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of In-telligent Press Media Technology). We thank the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Idiom savant at semeval2017 task 7: Detection and interpretation of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Samuel Doogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA; Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fermi at semeval-2017 task 7: Detection and interpretation of homographic puns in english language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayasaradhi</forename><surname>Indurthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subba Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="457" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries:how to tell a pine cone from an ice cream cone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Acm Special Interest Group for Design of Communication</title>
		<imprint>
			<biblScope unit="page" from="24" to="26" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Mikhalkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Karyakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05479</idno>
		<title level="m">Punfields at semeval-2017 task 7: Employing roget&apos;s thesaurus in automatic pun recognition and interpretation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic disambiguation of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 7: Detection and interpretation of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hempelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards the automatic detection and identification of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mladen</forename><surname>Turkovi´cturkovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Journal of Humour Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Buzzsaw at semeval-2017 task 7: Global vs. local context for interpreting and locating homographic english puns with sense embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieke</forename><surname>Oele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="444" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Duluth at semeval-2017 task 7 : Puns upon a midnight dreary, lexical semantics for the weak and weary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<idno>abs/1704.08388</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making sense of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pelevina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Arefiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Representation Learning for Nlp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="174" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ju-cse-nlp at semeval 2017 task 7: Employing rules to detect and interpret english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Pramanick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="432" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1156" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nhance at semeval-2017 task 7: A computational approach using word association for puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>¨ Ozge Sevgili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selma</forename><surname>Ghotbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tekir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="436" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uwav at semeval-2017 task 7: Automated feature-based system for locating puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Vadehra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uwaterloo at semeval-2017 task 7: Locating the pun using syntactic characteristics and corpus-based metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ecnu at semeval-2017 task 7: Using supervised and unsupervised methods to detect and locate english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuan</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised word sense disambiguation with neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Altendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07012</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
