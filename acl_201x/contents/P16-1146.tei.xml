<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing Spectral Learning for Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9LE</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9LE</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing Spectral Learning for Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1546" to="1556"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe a search algorithm for optimizing the number of latent states when estimating latent-variable PCFGs with spectral methods. Our results show that contrary to the common belief that the number of latent states for each nontermi-nal in an L-PCFG can be decided in isolation with spectral methods, parsing results significantly improve if the number of latent states for each nonterminal is globally optimized, while taking into account interactions between the different nontermi-nals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean , Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Latent-variable probabilistic context-free gram- mars (L-PCFGs) have been used in the natural lan- guage processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by <ref type="bibr" target="#b16">Matsuzaki et al. (2005)</ref> and <ref type="bibr" target="#b22">Prescher (2005)</ref>, with Matsuzaki et al. us- ing the expectation-maximization (EM) algorithm to estimate them. Their performance on syntac- tic parsing of English at that stage lagged behind state-of-the-art parsers. <ref type="bibr" target="#b20">Petrov et al. (2006)</ref> showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser -the number of la- tent states associated with the various nontermi- nals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merges nonterminals (with latent state information) with the aim to optimize the likeli- hood of the training data. Together with other types of fine tuning of the parsing model, this led to state-of-the-art results for English parsing.</p><p>In more recent work, <ref type="bibr" target="#b8">Cohen et al. (2012)</ref> de- scribed a different family of estimation algorithms for L-PCFGs. This so-called "spectral" family of learning algorithms is compelling because it offers a rigorous theoretical analysis of statistical conver- gence, and sidesteps local maxima issues that arise with the EM algorithm.</p><p>While spectral algorithms for L-PCFGs are compelling from a theoretical perspective, they have been lagging behind in their empirical results on the problem of parsing. In this paper we show that one of the main reasons for that is that spectral algorithms require a more careful tuning proce- dure for the number of latent states than that which has been advocated for until now. In a sense, the relationship between our work and the work of <ref type="bibr" target="#b9">Cohen et al. (2013)</ref> is analogous to the relation- ship between the work by <ref type="bibr" target="#b20">Petrov et al. (2006)</ref> and the work by <ref type="bibr" target="#b16">Matsuzaki et al. (2005)</ref>: we suggest a technique for optimizing the number of latent states for spectral algorithms, and test it on eight languages.</p><p>Our results show that when the number of la- tent states is optimized using our technique, the parsing models the spectral algorithms yield per- form significantly better than the vanilla-estimated models, and for most of the languages -better than the Berkeley parser of <ref type="bibr" target="#b20">Petrov et al. (2006)</ref>.</p><p>As such, the contributions of this parser are two- fold:</p><p>• We describe a search algorithm for optimiz-ing the number of latent states for spectral learning.</p><p>• We describe an analysis of spectral algo- rithms on eight languages (until now the re- sults of L-PCFG estimation with spectral al- gorithms for parsing were known only for English). Our parsing algorithm is rather language-generic, and does not require sig- nificant linguistically-oriented adjustments.</p><p>In addition, we dispel the common wisdom that more data is needed with spectral algorithms. Our models yield high performance on treebanks of varying sizes from 5,000 sentences (Hebrew and Swedish) to 40,472 sentences (German).</p><p>The rest of the paper is organized as follows. In §2 we describe notation and background. §3 further investigates the need for an optimization of the number of latent states in spectral learn- ing and describes our optimization algorithm, a search algorithm akin to beam search. In §4 we de- scribe our experiments with natural language pars- ing for Basque, French, German, Hebrew, Hungar- ian, Korean, Polish and Swedish. We conclude in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Notation</head><p>We denote by [n] the set of integers {1, . . . , n}. An L-PCFG is a 5-tuple (N , I, P, f, n) where:</p><p>• N is the set of nonterminal symbols in the grammar. I ⊂ N is a finite set of intermi- nals. P ⊂ N is a finite set of preterminals. We assume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of nonter- minals into two subsets.</p><p>• f : N → N is a function that maps each non- terminal a to the number of latent states it uses. The set [m a ] includes the possible hid- den states for nonterminal a.</p><p>• [n] is the set of possible words.</p><p>• For all a ∈ I, b ∈ N , c ∈ N ,</p><formula xml:id="formula_0">h 1 ∈ [m a ], h 2 ∈ [m b ], h 3 ∈ [m c ], we have a binary context-free rule a(h 1 ) → b(h 2 ) c(h 3 ).</formula><p>•</p><formula xml:id="formula_1">For all a ∈ P, h ∈ [m a ], x ∈ [n], we have a lexical context-free rule a(h) → x.</formula><p>The estimation of an L-PCFG requires an as- signment of probabilities (or weights) to each of the rules a(h 1 ) → b(h 2 ) c(h 3 ) and a(h) → x, and also an assignment of starting probabilities for each a(h), where a ∈ I and h ∈ [m a ]. Estima- tion is usually assumed to be done from a set of parse trees (a treebank), where the latent states are not included in the data -only the "skeletal" trees which consist of nonterminals in N .</p><p>L-PCFGs, in their symbolic form, are related to regular tree grammars, an old grammar formal- ism, but they were introduced as statistical mod- els for parsing with latent heads more recently by <ref type="bibr" target="#b16">Matsuzaki et al. (2005)</ref> and <ref type="bibr" target="#b22">Prescher (2005)</ref>. Earlier work about L-PCFGs by <ref type="bibr" target="#b16">Matsuzaki et al. (2005)</ref> used the expectation-maximization (EM) algorithm to estimate the grammar probabilities. Indeed, given that the latent states are not ob- served, EM is a good fit for L-PCFG estimation, since it aims to do learning from incomplete data. This work has been further extended by <ref type="bibr" target="#b20">Petrov et al. (2006)</ref> to use EM in a coarse-to-fine fashion: merging and splitting nonterminals using the la- tent states to optimize the number of latent states for each nonterminal. <ref type="bibr" target="#b8">Cohen et al. (2012)</ref> presented a so-called spec- tral algorithm to estimate L-PCFGs. This algo- rithm uses linear-algebraic procedures such as sin- gular value decomposition (SVD) during learning. The spectral algorithm of Cohen et al. builds on an estimation algorithm for HMMs by <ref type="bibr" target="#b14">Hsu et al. (2009)</ref>. 1 <ref type="bibr" target="#b9">Cohen et al. (2013)</ref> experimented with this spectral algorithm for parsing English. A dif- ferent variant of a spectral learning algorithm for L-PCFGs was developed by <ref type="bibr" target="#b7">Cohen and Collins (2014)</ref>. It breaks the problem of L-PCFG estima- tion into multiple convex optimization problems which are solved using EM.</p><p>The family of L-PCFG spectral learning algo- rithms was further extended by <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref>. They presented a simplified version of the algorithm of <ref type="bibr" target="#b8">Cohen et al. (2012)</ref> that es- timates sparse grammars and assigns probabili- ties (instead of weights) to the rules in the gram- mar, and as such does not suffer from the prob- lem of negative probabilities that arise with the original spectral algorithm (see discussion in <ref type="bibr" target="#b9">Cohen et al., 2013)</ref>. In this paper, we use the algo- rithms by <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref> and Cohen 1 A related algorithm for weighted tree automata (WTA) was developed by <ref type="bibr" target="#b0">Bailly et al. (2010)</ref>. However, the con- version from L-PCFGs to WTA is not straightforward, and information is lost in this conversion. See also ( <ref type="bibr" target="#b23">Rabusseau et al., 2016)</ref>.  <ref type="formula">2012)</ref>, and we compare them against state- of-the-art L-PCFG parsers such as the Berkeley parser ( <ref type="bibr" target="#b20">Petrov et al., 2006</ref>). We also compare our algorithms to other state-of-the-art parsers where elaborate linguistically-motivated feature specifi- cations ( <ref type="bibr" target="#b13">Hall et al., 2014</ref>), annotations <ref type="bibr" target="#b11">(Crabbé, 2015)</ref> and formalism conversions <ref type="bibr">(FernándezGonzález and Martins, 2015)</ref> are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimizing Spectral Estimation</head><p>In this section, we describe our optimization algo- rithm and its motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spectral Learning of L-PCFGs and Model Size</head><p>The family of spectral algorithms for latent- variable PCFGs rely on feature functions that are defined for inside and outside trees. Given a tree, the inside tree for a node contains the entire sub- tree below that node; the outside tree contains ev- erything in the tree excluding the inside tree. <ref type="figure" target="#fig_0">Fig- ure 1</ref> shows an example of inside and outside trees for the nonterminal VP in the parse tree of the sen- tence "the mouse chased the cat". With L-PCFGs, the model dictates that an in- side tree and an outside tree that are connected at a node are statistically conditionally independent of each other given the node label and the latent state that is associated with it. As such, one can identify the distribution over the latent states for a given nonterminal a by using the cross-covariance matrix of the inside and the outside trees, Ω a . For more information on the definition of this cross- covariance matrix, see <ref type="bibr" target="#b8">Cohen et al. (2012)</ref> and <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref>.</p><p>The L-PCFG spectral algorithms use singular value decomposition (SVD) on Ω a to reduce the dimensionality of the feature functions. If Ω a is computed from the true L-PCFG distribution then the rank of Ω a (the number of non-zero singular values) gives the number of latent states according to the model.</p><p>In the case of estimating Ω a from data gener- ated from an L-PCFG, the number of latent states for each nonterminal can be exposed by capping it when the singular values of Ω a are smaller than some threshold value. This means that spectral al- gorithms give a natural way for the selection of the number of latent states for each nonterminal a in the grammar.</p><p>However, when the data from which we esti- mate an L-PCFG model are not drawn from an L- PCFG (the model is "incorrect"), the number of non-zero singular values (or the number of singu- lar values which are large) is no longer sufficient to determine the number of latent states for each nonterminal. This is where our algorithm comes into play: it optimizes the number of latent search for each nonterminal by applying a search algo- rithm akin to beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimizing the Number of Latent States</head><p>As mentioned in the previous section, the number of non-zero singular values of Ω a gives a criterion to determine the number of latent states m a for a given nonterminal a. In practice, we cap m a not to include small singular values which are close to 0, because of estimation errors of Ω a .</p><p>This procedure does not take into account the interactions that exist between choices of latent state numbers for the various nonterminals. In principle, given the independence assumptions that L-PCFGs make, choosing the nonterminals based only on the singular values is "statistically correct." However, because in practice the mod- eling assumptions that we make (that natural lan- guage parse trees are drawn from an L-PCFG) do not hold, we can improve further the accuracy of the model by taking into account the nonterminal interaction. Another source of difficulty in choos- ing the number of latent states based the singu- lar values of Ω a is sampling error: in practice, we are using data to estimate Ω a , and as such, even if the model is correct, the rank of the estimated matrix does not have to correspond to the rank of Ω a according to the true distribution. As a mat- ter of fact, in addition to neglecting small singular values, the spectral methods of <ref type="bibr" target="#b9">Cohen et al. (2013)</ref> and <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref> also cap the num- ber of latent states for each nonterminal to an up-Inputs: An input treebank divided into training and devel- opment set. A basic spectral estimation algorithm S with its default setting. An integer k denoting the size of the beam. An integer m denoting the upper bound on the number of latent states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm: (Step 0: Initialization)</head><p>• Set Q, a queue of size k, to be empty.</p><p>• Estimate an L-PCFG GS : (N , I, P, fS , n) using S.</p><p>• Initialize f = fS , a function that maps each nontermi- nal a ∈ N to the number of latent states.</p><p>• Let L be a list of nonterminals (a1, . . . , aM ) such that ai ∈ N for which to optimize the number of latent states.</p><p>• Let s be the F1 score for the above L-PCFG GS on the development set.</p><p>• Put in Q the element (s, 1, f, coarse).</p><p>• The queue is ordered by s, the first element of tuples, in the queue.</p><p>(</p><p>Step 1: Search, repeat until termination happens)</p><p>• Dequeue the queue into (s, j, f, t) where j is the index in the input nonterminal list L.</p><p>• If j = (M + 1), return f .</p><p>• If t is coarse then for each m0 ∈ {1, 5, 10, . . . , m}:</p><p>• Let f0 be such that ∀a = aj f0(a) = f (a) and f0(aj) = m0.</p><p>• Train an L-PCFG G0 using S but with f0.</p><p>• Let s0 be the F1 score for G0 on the development set.</p><p>• Enqueue into Q: (s0, j, f0, refine).</p><p>• If t is refine then for each m0 ∈ {f (a) + | ∈ {−4, −3, −2, −1, 0, 1, 2, 3, 4}}:</p><p>• Let f0 be such that ∀a = aj f0(a) = f (a) and f0(aj) = m0.</p><p>• Train an L-PCFG G0 using S but with f0.</p><p>• Let s0 be the F1 score for G0 on the development set.</p><p>• Enqueue into Q: (s0, j + 1, f0, coarse).  <ref type="formula">(2005)</ref> by taking into account the interactions between the nonter- minals and their latent state numbers in the train- ing data. They use the EM algorithm to split and merge nonterminals using the latent states, and op- timize the number of latent states for each nonter- minal such that it maximizes the likelihood of a training treebank. Their refined grammar success- fully splits nonterminals to various degrees to cap- ture their complexity. We take the analogous step with spectral methods. We propose an algorithm where we first compute Ω a on the training data and then we optimize the number of latent states for each nonterminal by optimizing the PARSE- VAL metric <ref type="bibr">(Black et al., 1991</ref>) on a development set.</p><p>Our optimization algorithm appears in <ref type="figure" target="#fig_1">Figure 2</ref>. The input to the algorithm is training and develop- ment data in the form of parse trees, a basic spec- tral estimation algorithm S in its default setting, an upper bound m on the number of latent states that can be used for the different nonterminals and a beam size k which gives a maximal queue size for the beam. The algorithm aims to learn a func- tion f that maps each nonterminal a to the number of latent states. It initializes f by estimating a de- fault grammar G S : (N , I, P, f S , n) using S and setting f = f S . It then iterates over a ∈ N , im- proving f such that it optimizes the PARSEVAL metric on the development set.</p><p>The state of the algorithm includes a queue that consists of tuples of the form (s, j, f, t) where f is an assignment of latent state numbers to each nonterminal in the grammar, j is the index of a nonterminal to be explored in the input nontermi- nal list L, s is the F 1 score on the development set for a grammar that is estimated with f and t is a tag that can either be coarse or refine.</p><p>The algorithm orders these tuples by s in the queue, and iteratively dequeues elements from the queue. Then, depending on the label t, it either makes a refined search for the number of latent states for a j , or a more coarse search. As such, the algorithm can be seen as a variant of a beam search algorithm.</p><p>The search algorithm can be used with any training algorithm for L-PCFGs, including the al- gorithms of <ref type="bibr" target="#b9">Cohen et al. (2013)</ref> and <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref>. These methods, in their default set- ting, use a function f S which maps each nonter- minal a to a fixed number of latent states m a it uses. In this case, S takes as input training data, in the form of a treebank, decomposes into in- side and outside trees at each node in each tree in the training set; and reduces the dimensionality of the inside and outside feature functions by running lang.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basque</head><p>French German-N German-T <ref type="table">Hebrew Hungarian  Korean  Polish  Swedish   train   sent.  7,577  14,759  18,602  40,472  5,000  8,146  23,010  6,578  5,000  tokens  96,565 443,113  328,531  719,532  128,065  170,221  301,800 66,814  76,332  lex. size  25,136  27,470  48,509  77,219  15,971  40,775  85,671  21,793  14,097  #nts  112  222  208  762  375  112  352  198</ref>   <ref type="table">Table 1</ref>: Statistics about the different datasets used in our experiments for the training ("train"), development ("dev") and test ("test") sets. "sent." denotes the number of sentences in the dataset, "tokens" denotes the total number of words in the dataset, "lex. size" denotes the vocabulary size in the training set and "#nts" denotes the number of nonterminals in the training set after binarization.</p><p>SVD on the cross-covariance matrix Ω a of the in- side and the outside trees, for each nonterminal a. <ref type="bibr" target="#b9">Cohen et al. (2013)</ref> estimate the parameters of the L-PCFG up to a linear transformation using f (a) non-zero singular values of Ω a , whereas Narayan and Cohen (2015) use the feature representations induced from the SVD step to cluster instances of nonterminal a in the training data into f (a) clus- ters; these clusters are then treated as latent states that are "observed." Finally, Narayan and Cohen follow up with a simple frequency count maxi- mum likelihood estimate to estimate the parame- ters in the L-PCFG with these latent states. An important point to make is that the learning algorithms of <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref> and <ref type="bibr" target="#b9">Cohen et al. (2013)</ref> are relatively fast, 2 in comparison to the EM algorithm. They require only one iter- ation over the data. In addition, the SVD step of S for these learning algorithms is computed just once for a large m. The SVD of a lower rank can then be easily computed from that SVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe our setup for parsing experiments on a range of languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets We experiment with nine treebanks consisting of eight different morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. <ref type="table">Table 1</ref> shows the statistics of 9 different treebanks with their splits into training, development and test sets. and Swedish) are taken from the workshop on Statistical Parsing of Morphologically Rich Lan- guages (SPMRL; <ref type="bibr" target="#b24">Seddah et al., 2013</ref>). The Ger- man corpus in the SPMRL workshop is taken from the TiGer corpus (German-T, <ref type="bibr" target="#b5">Brants et al., 2004</ref>). We also experiment with another German cor- pus, the NEGRA corpus (German-N, <ref type="bibr" target="#b25">Skut et al., 1997)</ref>, in a standard evaluation split. <ref type="bibr">3</ref> Words in the SPMRL datasets are annotated with their mor- phological signatures, whereas the NEGRA cor- pus does not contain any morphological informa- tion.</p><p>Data preprocessing and treatment of rare words We convert all trees in the treebanks to a binary form, train and run the parser in that form, and then transform back the trees when doing eval- uation using the PARSEVAL metric. In addition, we collapse unary rules into unary chains, so that our trees are fully binarized. The column "#nts" in <ref type="table">Table 1</ref> shows the number of nonterminals af- ter binarization in the various treebanks. Before binarization, we also drop all functional informa- tion from the nonterminals. We use fine tags for all languages except Korean. This is in line with <ref type="bibr" target="#b1">Björkelund et al. (2013)</ref>. <ref type="bibr">4</ref> For Korean, there are 2,825 binarized nonterminals making it impracti- cal to use our optimization algorithm, so we use the coarse tags. <ref type="bibr" target="#b1">Björkelund et al. (2013)</ref> have shown that the morphological signatures for rare words are useful to improve the performance of the Berkeley parser.</p><p>In our preliminary experiments with na¨ıvena¨ıve spectral estimation, we preprocess rare words in the train- ing set in two ways: (i) we replace them with their corresponding POS tags, and (ii) we replace them with their corresponding POS+morphological sig- natures. We follow <ref type="bibr" target="#b1">Björkelund et al. (2013)</ref> and consider a word to be rare if it occurs less than 20 times in the training data. We experimented both with a version of the parser that does not ignore and does ignore letter cases, and discovered that the parser behaves better when case is not ignored.</p><p>Spectral algorithms: subroutine choices The latent state optimization algorithm will work with either the clustering estimation algorithm of <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref> or the spectral algo- rithm of <ref type="bibr" target="#b9">Cohen et al. (2013)</ref>. In our setup, we first run the latent state optimization algorithm with the clustering algorithm. We then run the spectral algorithm once with the optimized f from the clustering algorithm. We do that because the clustering algorithm is significantly faster to itera- tively parse the development set, because it leads to sparse estimates.</p><p>Our optimization algorithm is sensitive to the initialization of the number of latent states as- signed to each nonterminals as it sequentially goes through the list of nonterminals and chooses latent state numbers for each nonterminal, keeping latent state numbers for other nonterminals fixed. In our setup, we start our search algorithm with the best model from the clustering algorithm, controlling for all hyperparameters; we tune f , the function which maps each nonterminal to a fixed number of latent states m, by running the vanilla version with different values of m for different languages. Based on our preliminary experiments, we set m to 4 for Basque, Hebrew, Polish and Swedish; 8 for German-N; 16 for German-T, Hungarian and Korean; and 24 for French.</p><p>We use the same features for the spectral meth- ods as in <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref> for German- N. For the SPMRL datasets we do not use the head features. These require linguistic understanding of the datasets (because they require head rules for propagating leaf nodes in the tree), and we discov- ered that simple heuristics for constructing these rules did not yield an increase in performance.</p><p>We use the kmeans function in Matlab to do the clustering for the spectral algorithm of <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref>. We experimented with several versions of k-means, and discovered that the version that works best in a set of prelimi- nary experiments is hard k-means. <ref type="bibr">5</ref> Decoding and evaluation For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. This is just a bare-bones PCFG that is estimated using maximum likelihood estimation (with frequency count). The parser takes part-of-speech tagged sentences as input. We tag the German-N data us- ing the Turbo Tagger <ref type="figure" target="#fig_0">(Martins et al., 2010)</ref>. For the languages in the SPMRL data we use the Mar- Mot tagger of <ref type="bibr">Müeller et al. (2013)</ref> to jointly pre- dict the POS and morphological tags. <ref type="bibr">6</ref> The parser itself can assign different part-of-speech tags to words to avoid parse failure. This is also particu- larly important for constituency parsing with mor- phologically rich languages. It helps mitigate the problem of the taggers to assign correct tags when long-distance dependencies are present.</p><p>For all results, we report the F 1 measure of the PARSEVAL metric ( <ref type="bibr">Black et al., 1991)</ref>. We use the EVALB program 7 with the parame- ter file COLLINS.prm <ref type="bibr" target="#b10">(Collins, 1999</ref>) for the German-N data and the SPMRL parameter file, spmrl.prm, for the SPMRL data ( <ref type="bibr" target="#b24">Seddah et al., 2013)</ref>.</p><p>In this setup, the latent state optimization algo- rithm terminates in few hours for all datasets ex- cept French and German-T. The German-T data has 762 nonterminals to tune over a large develop- ment set consisting of 5,000 sentences, whereas, the French data has a high average sentence length of 31.43 in the development set. <ref type="bibr">8</ref> Following <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref>, we fur- ther improve our results by using multiple spec- tral models where noise is added to the underlying features in the training set before the estimation of each model. <ref type="bibr">9</ref> Using the optimized f , we estimate lang.  <ref type="table">Table 2</ref>: Results on the development datasets. "Bk" makes use of the Berkeley parser with its coarse-to-fine mechanism to optimize the number of latent states ( <ref type="bibr" target="#b20">Petrov et al., 2006</ref>). For Bk, "van" uses the vanilla treatment of rare words using signatures defined by <ref type="bibr" target="#b20">Petrov et al. (2006)</ref>, whereas "rep." uses the morphological signatures instead. "Cl" uses the algorithm of Narayan and Cohen (2015) and "Sp" uses the algorithm of <ref type="bibr" target="#b9">Cohen et al. (2013)</ref>. In Cl, "van (pos)" and "van (rep)" are vanilla estima- tions (i.e., each nonterminal is mapped to fixed number of latent states) replacing rare words by POS or POS+morphological signatures, respectively. The best of these two models is used with our optimization algorithm in "opt". For Sp, "van" uses the best setting for unknown words as Cl. Best result in each column from the first seven rows is in bold. In addition, our best performing models from rows 3-7 are marked with * . "Bk multiple" shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking <ref type="bibr" target="#b6">(Charniak and Johnson, 2005</ref>). "Cl multiple" gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding <ref type="bibr" target="#b18">(Narayan and Cohen, 2015</ref>  <ref type="table">Table 3</ref>: Results on the test datasets. "Bk" denotes the best Berkeley parser result reported by the shared task organizers <ref type="bibr" target="#b24">(Seddah et al., 2013</ref>). For the German-N data, Bk results are taken from Petrov (2010). "Cl van" shows the performance of the best vanilla models from <ref type="table">Table 2</ref> on the test set. "Cl opt" and "Sp opt" give the result of our algorithm on the test set. We also include results from <ref type="bibr" target="#b13">Hall et al. (2014)</ref>, <ref type="bibr" target="#b11">Crabbé (2015)</ref> and <ref type="bibr" target="#b12">Fernández-González and Martins (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basque French German-N German-T Hebrew Hungarian Korean Polish Swedish</head><p>80 models for each of noise induction mechanisms in Narayan and Cohen: Dropout, Gaussian (ad- ditive) and Gaussian (multiplicative). To decode with multiple noisy models, we train the MaxEnt reranker of <ref type="bibr" target="#b6">Charniak and Johnson (2005)</ref>. <ref type="bibr">10</ref> Hi- erarchical decoding with "maximal tree coverage" over MaxEnt models, further improves our accu- racy. See Narayan and Cohen (2015) for more de- tails on the estimation of a diverse set of models, and on decoding with them.</p><p>estimates than the dense estimates of <ref type="bibr" target="#b9">Cohen et al. (2013)</ref>. 10 Implementation: https://github.com/BLLIP/ bllip-parser.</p><p>More specifically, we used the programs extract-spfeatures, cvlm-lbfgs and best-indices. extract-spfeatures uses head fea- tures, we bypass this for the SPMRL datasets by creating a dummy heads.cc file. cvlm-lbfgs was used with the default hyperparameters from the Makefile. <ref type="table">Table 2 and Table 3</ref> give the results for the various languages. <ref type="bibr">11</ref> Our main focus is on comparing the coarse-to-fine Berkeley parser ( <ref type="bibr" target="#b20">Petrov et al., 2006</ref>) to our method. However, for the sake of com- pleteness, we also present results for other parsers, such as parsers of <ref type="bibr" target="#b13">Hall et al. (2014)</ref>, <ref type="bibr">FernándezGonzález and</ref><ref type="bibr" target="#b12">Martins (2015) and</ref><ref type="bibr" target="#b11">Crabbé (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In line with <ref type="bibr" target="#b1">Björkelund et al. (2013)</ref>, our pre- liminary experiments with the treatment of rare words suggest that morphological features are useful for all SPMRL languages except French. Specifically, for Basque, Hungarian and Korean, improvements are significantly large.</p><p>Our results show that the optimization of the  number of latent states with the clustering and spectral algorithms indeed improves these algo- rithms performance, and these increases general- ize to the test sets as well. This was a point of concern, since the optimization algorithm goes through many points in the hypothesis space of parsing models, and identifies one that behaves op- timally on the development set -and as such it could overfit to the development set. However, this did not happen, and in some cases, the increase in accuracy of the test set after running our optimiza- tion algorithm is actually larger than the one for the development set. While the vanilla estimation algorithms (with- out latent state optimization) lag behind the Berke- ley parser for many of the languages, once the number of latent states is optimized, our parsing models do better for Basque, Hebrew, Hungar- ian, Korean, Polish and Swedish. For German- T we perform close to the Berkeley parser (78.2 vs. 78.3). It is also interesting to compare the clustering algorithm of <ref type="bibr" target="#b18">Narayan and Cohen (2015)</ref> to the spectral algorithm of <ref type="bibr" target="#b9">Cohen et al. (2013)</ref>. In the vanilla version, the spectral algorithm does better in most cases. However, these differences are narrowed, and in some cases, overcome, when the number of latent states is optimized. Decod- ing with multiple models further improves our ac- curacy. Our "Cl multiple" results lag behind "Bk multiple." We believe this is the result of the need of head features for the MaxEnt models. <ref type="bibr">12</ref> Our results show that spectral learning is a viable alternative to the use of expectation- 12 <ref type="bibr" target="#b1">Björkelund et al. (2013)</ref> also use the MaxEnt raranker with multiple models of the Berkeley parser, and in their case also the performance after the raranking step is not always significantly better. See footnote 10 on how we create dummy head-features for our MaxEnt models. maximization coarse-to-fine techniques. As we discuss later, further improvements have been in- troduced to state-of-the-art parsers that are orthog- onal to the use of a specific estimation algorithm. Some of them can be applied to our setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Analysis</head><p>In addition to the basic set of parsing results, we also wanted to inspect the size of the parsing mod- els when using the optimization algorithm in com- parison to the vanilla models. <ref type="table">Table 4</ref> gives this analysis. In this table, we see that in most cases, on average, the optimization algorithm chooses to enlarge the number of latent states. However, for German-T and Korean, for example, the optimiza- tion algorithm actually chooses a smaller model than the original vanilla model.</p><p>We further inspected the behavior of the optimization algorithm for the preterminals in German-N, for which the optimal model chose (on average) a larger number of latent states. <ref type="table">Table 5</ref> describes this analysis. We see that in most cases, the optimization algorithm chose to decrease the number of latent states for the various pretermi- nals, but in some cases significantly increases the number of latent states. <ref type="bibr">13</ref> Our experiments dispel another "common wis- dom" about spectral learning and training data size. It has been believed that spectral learning do not behave very well when small amounts of data are available (when compared to maximum likelihood estimation algorithms such as EM) - however we see that our results do better than the Berkeley parser for several languages with small <ref type="table">PWAT  64  2  2  TRUNC  614  8  1  PIS  1,628  8  8  KON  8,633  8  30  XY  135  3  1  VAPP  363  6  4  $*LRB*  13,681  8  6  PPER  4,979  8  100  NP|NN  88  2  1  PDS  988  8  8  ADJD  6,419  8  60  $.  17,699  8  3  VMINF  177  3  5  AVP|ADV  211  4  11  KOUS  2,456  8  1  APPRART  6,217  8  15  PTKA  162  3  1  FM  578  8  3  PIAT  1,061  8  8  ADJA  18,993  8  10  VP|VVINF  409  6  2  VVIMP  76  2  1  NP|PPER  382  6  1  APPR  26,717  8  7  PRELAT  94  2  1  KOUI  339  5  2  VVPP  5,005  8  20  VVFIN  13,444  8  3  AP|ADJD  178  3  1  VAINF  1,024  8  1  PP|PROAV  174  3  1  $,  16,631  8  1  APPO  89  2  2  PRELS  2,120  8  40  VAFIN  8,814  8  1  VVINF  4,382  8  10  PWS  361  6  1  CARD  6,826  8  8  PTKNEG  1,884  8  8  ART  35,003  8  10  KOKOM  800  8  37  NE  17,489  8  6  PTKZU  1,586  8  1  ADV  15,566  8  8  VP|VVPP  844  8  5  PRF  2,158  8  1  VVIZU  479  7  1  PIDAT  1,254  8  20  PWAV  689  8  1  PDAT  1,129  8  1  PPOSAT  2,295  8  6  NN  68,056  8  12  APZR  134  3  2  PROAV  1,479  8  10  PTKVZ  1,864  8  3  VMFIN  3,177  8  1   Table 5</ref>: A comparison of the number of latent states for each preterminal for the German-N model, before ("b.") running the latent state number optimization algorithm and after running it ("a."). Note that some of the preterminals denote unary rules that were collapsed (the nonterminals in the chain are separated by |). We do not show rare preterminals with b. and a. both being 1.</p><note type="other">preterminal freq. b. a. preterminal freq. b. a. preterminal freq. b. a. preterminal freq. b. a.</note><p>training datasets, such as Basque, Hebrew, Pol- ish and Hungarian. The source of this common wisdom is that ML estimators tend to be statis- tically "efficient:" they extract more information from the data than spectral learning algorithms do. Indeed, there is no reason to believe that spectral algorithms are statistically efficient. However, it is not clear that indeed for L-PCFGs with the EM algorithm, the ML estimator is statistically effi- cient either. MLE is statistically efficient under specific assumptions which are not clearly satis- fied with L-PCFG estimation. In addition, when the model is "incorrect," (i.e. when the data is not sampled from L-PCFG, as we would expect from natural language treebank data), spectral al- gorithms could yield better results because they can mimic a higher order model. This can be understood through HMMs. When estimating an HMM of a low order with data which was gener- ated from a higher order model, EM does quite poorly. However, if the number of latent states (and feature functions) is properly controlled with spectral algorithms, a spectral algorithm would learn a "product" HMM, where the states in the lower order model are the product of states of a higher order. <ref type="bibr">14</ref> State-of-the-art parsers for the SPMRL datasets improve the Berkeley parser in ways which are or- thogonal to the use of the basic estimation algo- rithm and the method for optimizing the number of latent states. They include transformations of the treebanks such as with unary rules <ref type="bibr" target="#b1">(Björkelund et al., 2013</ref>), a more careful handling of unknown words and better use of morphological informa- <ref type="bibr">14</ref> For example, a trigram HMM can be reduced to a bigram HMM where the states are products of the original trigram HMM. tion such as decorating preterminals with such in- formation <ref type="bibr">(Björkelund et al., 2014;</ref>, with careful feature specifications ( <ref type="bibr" target="#b13">Hall et al., 2014</ref>) and head-annotations <ref type="bibr" target="#b11">(Crabbé, 2015)</ref>, and other techniques. Some of these tech- niques can be applied to our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We demonstrated that a careful selection of the number of latent states in a latent-variable PCFG with spectral estimation has a significant effect on the parsing accuracy of the L-PCFG. We de- scribed a search procedure to do this kind of optimization, and described parsing results for eight languages (with nine datasets). Our results demonstrate that when comparing the expectation- maximization with coarse-to-fine techniques to our spectral algorithm with latent state optimiza- tion, spectral learning performs better on six of the datasets. Our results are comparable to other state- of-the-art results for these languages. Using a di- verse set of models to parse these datasets further improves the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The inside tree (left) and outside tree (right) for the nonterminal VP in the parse tree (S (NP (D the) (N mouse)) (VP (V chased) (NP (D the) (N cat)))) for the sentence "the mouse chased the cat."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A search algorithm for finding the optimal number of latent states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Eight out of the nine datasets (Basque, French, German-T, Hebrew, Hungarian, Korean, Polish 2 It has been documented in several papers that the fam- ily of spectral estimation algorithms is faster than algorithms such as EM, not just for L-PCFGs. See, for example, Parikh et al. (2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>4: A comparison of the number of latent states for the different nonterminals before and after running our latent state number optimization algorithm. The index i ranges over preterminals and interminals, with xi denoting the number of latent states for nonterminal i with the vanilla version of the estimation algorithm and yi denoting the number of latent states for nonterminal i after running the optimization algorithm. The divergence figure ("div.") is a calculation of i |xi − yi|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> We use the first 18,602 sentences as a training set, the next 1,000 sentences as a development set and the last 1,000 sentences as a test set. This corresponds to an 80%-10%-10% split of the treebank. 4 In their experiments Björkelund et al. (2013) found that fine tags were not useful for Basque also; they did not find a proper explanation for that. In our experiments, however, we found that fine tags were useful for Basque. To retrieve the fine tags, we concatenate coarse tags with their refinement feature (&quot;AZP&quot;) values.</note>

			<note place="foot" n="5"> To be more precise, we use the Matlab function kmeans while passing it the parameter &apos;start&apos;=&apos;sample&apos; to randomly sample the initial centroid positions. In our experiments, we found that default initialization of centroids differs in Matlab14 (random) and in Matlab15 (kmeans++). Our estimation performs better with random initialization. 6 See Björkelund et al. (2013) for the performance of the MarMot tagger on the SPMRL datasets. 7 http://nlp.cs.nyu.edu/evalb/ 8 To speed up tuning on the French data, we drop sentences with length &gt;46 from the development set, dropping its size from 12,35 to 1,006. 9 We only use the algorithm of Narayan and Cohen (2015) for the noisy model estimation. They have shown that decoding with noisy models performs better with their sparse</note>

			<note place="foot" n="11"> See more in http://cohort.inf.ed.ac.uk/ lpcfg/.</note>

			<note place="foot" n="13"> Interestingly, most of the punctuation symbols, such as $ * LRB * , $. and $,, drop their latent state number to a significantly lower value indicating that their interactions with other nonterminals in the tree are minimal.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank David McClosky for his help with running the BLLIP parser and his comments on the paper and also the three anonymous reviewers for their helpful comments. We also thank Eugene Charniak, DK Choe and Geoff Gordon for useful discussions. Finally, thanks to Djamé Seddah for providing us with the SPMRL datasets and to Thomas Müller and Anders Björkelund for providing us the MarMot models. This research was supported by an EP-SRC grant (EP/L02411X/1) and an EU H2020 grant (688139/H2020-ICT-2015; SUMMA).</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A spectral approach for probabilistic grammatical inference on trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Algorithmic Learning Theory</title>
		<meeting>International Conference on Algorithmic Learning Theory</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Ozlem C ¸ Etino˘ Glu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Müeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seeker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C ¸ Etino˘</forename><surname>Ozlem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introducing the IMS-Wrocław-Szeged-CIS entry at the SPMRL 2014 shared task: Reranking and morphosyntax meet unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Fale´nskafale´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szántó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezra</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Gdaniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Ingria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">L</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">Y</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DARPA Workshop on Speech and Natural Language</title>
		<meeting>DARPA Workshop on Speech and Natural Language</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TIGER: Linguistic interpretation of a German corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Hansen-Schirra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="597" to="620" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A provably correct learning algorithm for latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">F</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual discriminative lexicalized phrase structure parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benoit Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parsing as reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-González</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Less grammar, more features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT</title>
		<meeting>COLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TurboParsers: Dependency parsing by approximate variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diversity in spectral learning for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A spectral algorithm for latent junction trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariya</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabi</forename><surname>Ishteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Teodoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Head-driven PCFGs with latent-head statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detlef</forename><surname>Prescher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low-rank approximation of weighted tree automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>The 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nskiWoli´nski, Alina Wróblewska, and Eric Villemonte de la Clérgerie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koldo</forename><forename type="middle">Gojenola</forename><surname>Galletebeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Przepiórkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An annotation scheme for free word order languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigitte</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ANLP</title>
		<meeting>ANLP</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Special techniques for constituent parsing of morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Szántó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
