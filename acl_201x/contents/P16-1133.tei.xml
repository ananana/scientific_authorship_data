<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Lingual Sentiment Classification with Bilingual Document Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Lingual Sentiment Classification with Bilingual Document Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1403" to="1412"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual sentiment classification aims to adapt the sentiment resource in a resource-rich language to a resource-poor language. In this study, we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages. Different from previous research which only gets bilingual word embedding, our Bilingual Document Representation Learning model BiDRL directly learns document representations. Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space. The experiments are based on the multilingual multi-domain Amazon review dataset. We use English as the source language and use Japanese, German and French as the target languages. The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis for online user-generated con- tents has become a hot research topic during the last decades. Among all the sentiment analysis tasks, polarity classification is the most widely s- tudied topic. It has been proved to be invaluable in many applications, such as opinion polling <ref type="bibr" target="#b19">(Tang et al., 2012</ref>), customer feedback tracking <ref type="bibr" target="#b5">(Gamon, 2004</ref>), election prediction ( <ref type="bibr" target="#b20">Tumasjan et al., 2010)</ref>, stock market prediction <ref type="bibr" target="#b3">(Bollen et al., 2011</ref>) and so on.</p><p>Most of the current sentiment classification sys- tems are built on supervised machine learning algorithms which require manually labelled da- ta. However, sentiment resources are usually un- balanced in different languages. Cross-lingual sentiment classification aims to leverage the re- sources in a resource-rich language (such as En- glish) to classify the sentiment polarity of texts in a resource-poor language (such as Japanese). The biggest challenge for cross-lingual sentimen- t classification is the vocabulary gap between the source language and the target language. This problem is addressed with different strategies in different approaches. <ref type="bibr" target="#b21">Wan (2009)</ref> use machine translation tools to translate the training data di- rectly into the target language. <ref type="bibr" target="#b13">Meng et al. (2012)</ref> and <ref type="bibr" target="#b11">Lu et al. (2011)</ref> exploit parallel unlabeled da- ta to bridge the language barrier. <ref type="bibr" target="#b18">Prettenhofer and Stein (2010)</ref> use correspondence learning al- gorithm to learn a map between the source lan- guage and the target language. Recently, repre- sentation learning methods has been proposed to solve the cross-lingual classification problem <ref type="bibr" target="#b22">(Xiao and Guo, 2013;</ref><ref type="bibr" target="#b24">Zhou et al., 2015</ref>). These meth- ods aim to learn common feature representations for different languages. However, most of the cur- rent researches only focus on bilingual word em- bedding. In addition, these models only use the se- mantic correlations between aligned words or sen- tences in different languages while the sentiment correlations are ignored.</p><p>In this study, we propose a cross-lingual repre- sentation learning model BiDRL which simulta- neously learns both the word and document repre- sentations in both languages. We propose a joint learning algorithm which exploits both monolin- gual and bilingual constraints. The monolingual constraints help to model words and documents in each individual language while the bilingual con- straints help to build a consistent embedding space across languages.</p><p>For each individual language, we extend the paragraph vector model ( <ref type="bibr" target="#b9">Le and Mikolov, 2014)</ref> to obtain word and document embeddings. The traditional paragraph vector model is fully unsu- pervised without using the valuable sentiment la- bels. We extend it into a semi-supervised manner by forcing the positive and negative documents to fall into different sides of a classification hyper- plane. Learning task-specific embedding has been proved to be effective in previous research. To ad- dress the cross-language problem, different strate- gies are proposed to obtain a consistent embedding space across different languages. Both sentiment and semantic relatedness are exploited while pre- vious studies only use the semantic connection be- tween parallel sentences or documents.</p><p>The performance of BiDRL is evaluated on a multilingual multi-domain Amazon review dataset <ref type="bibr" target="#b18">(Prettenhofer and Stein, 2010)</ref>. By selecting En- glish as the source language, a total of nine tasks are evaluated with different combinations of three different target languages and three different do- mains. The proposed method achieves the state- of-the-art performance on all the tasks.</p><p>The main contributions of this study are sum- marized as follows:</p><p>1) We propose a novel representation learn- ing method BiDRL which directly learns bilingual document representations for cross-lingual senti- ment classification. Different from previous stud- ies which only obtain word embeddings, our mod- el can learn vector representations for both words and documents in bilingual texts.</p><p>2) Our model leverages both the semantic and sentiment correlations between bilingual docu- ments. Not only the parallel documents but al- so the documents with the same sentiment are re- quired to get similar representations.</p><p>3) Our model achieves the state-of-the-art per- formances on nine benchmark cross-lingual sen- timent classification tasks and it consistently out- performs the existing methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentiment analysis is the field of studying and an- alyzing people's opinions, sentiments, evaluation- s, appraisals, attitudes, and emotions (Liu, 2012). Most of the previous sentiment analysis research- es focus on customer reviews and classifying the sentiment polarity is the most widely studied task <ref type="bibr" target="#b16">(Pang et al., 2002</ref>).</p><p>Cross-lingual sentiment classification is a pop- ular topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great im- portance for the area since it can exploit the ex- isting labeled information in a source language to build a sentiment classification system in any other target language. It saves us from manually label- ing data for all the languages in the world which is expensive and time-consuming. Cross-lingual sentiment classification has been extensively stud- ied in the very recent years. <ref type="bibr" target="#b14">Mihalcea et al. (2007)</ref> translate English subjectivity words and phrases into the target language to build a lexicon-based classifier. <ref type="bibr" target="#b1">Banea et al. (2010)</ref> also use the machine translation service to obtain parallel corpus. It in- vestigates several questions based on the parallel corpus including both the monolingual sentiment classification and cross-lingual sentiment classifi- cation. <ref type="bibr" target="#b21">Wan (2009)</ref> translates both the training da- ta (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. The co-training algo- rithm <ref type="bibr" target="#b2">(Blum and Mitchell, 1998</ref>) is used to com- bine the bilingual models together and improve the performance. In addition to the translation-based methods, several studies utilize parallel corpus or existing resources to bridge the language barrier. Balamurali (2012) use WordNet senses as features for supervised sentiment classification. They use the linked WordNets of two languages to bridge the language gap. <ref type="bibr" target="#b11">Lu et al. (2011)</ref> consider the multilingual scenario where small amount of la- beled data is available in the target language. They attempted to jointly classify the sentiment for both source language and target language. <ref type="bibr" target="#b13">Meng et al. (2012)</ref> propose a generative cross-lingual mixture model to leverage unlabeled bilingual parallel da- ta. <ref type="bibr" target="#b18">Prettenhofer and Stein (2010)</ref> use the structural correspondence learning algorithm to learn a map between the source language and the target lan- guage. <ref type="bibr" target="#b23">Xiao and Guo (2014)</ref> treat the bilingual feature learning problem as a matrix completion task.</p><p>This work is also related to bilingual repre- sentation learning. <ref type="bibr" target="#b25">Zou et al. (2013)</ref> propose to use word alignment as the constraints in bilin- gual word embedding. Each word in one language should be similar to the aligned words in another language. <ref type="bibr" target="#b6">Gouws et al. (2015)</ref> propose a similar algorithm but only use sentence-level alignment. It tries to minimize a sampled L2-loss between the bag-of-words sentence vectors of the parallel cor-pus. <ref type="bibr" target="#b22">Xiao and Guo (2013)</ref> learn different repre- sentations for words in different languages. Part of the word vector is shared among different lan- guages and the rest is language-dependent. <ref type="bibr" target="#b8">Klementiev et al. (2012)</ref> treat the task as a multi-task learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. <ref type="bibr">Hermann and Blunsom (2015)</ref> propose the bilin- gual CVM model which directly minimizes the representation of a pair of parallel documents. The document representation is calculated with a com- position function based on words. Chandar A P et al. <ref type="formula" target="#formula_1">(2014)</ref> and <ref type="bibr" target="#b24">Zhou et al. (2015)</ref> use the autoen- coder to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences.  pro- pose the bilingual skip-gram model which lever- ages the word alignment between parallel sen- tences.  extend the paragraph vector model to force bilingual sentences to share the same sentence vector.</p><p>This study differs with the existing works in the following three aspects, 1) we exploit both the se- mantic and sentiment correlations of the bilingual texts. Existing bilingual embedding algorithm- s only use the semantic connection between par- allel sentences or documents. 2) Our algorithm learns both the word and document representation- s. Most of the previous studies simply compute the average of the word vectors in a document. 3) Sentiment labels are used in our embedding algo- rithm by introducing a classification hyperplane. It not only helps to achieve better embedding perfor- mance in each individual language but also helps to bridge the language barrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework</head><p>Firstly we introduce several notations used in BiDRL. Let S and S u denote the documents from the training dataset and the documents from the unlabeled dataset in the source language respec- tively. For each document d ∈ S, it has a senti- ment label y ∈ {1, −1}. We denote the sentiment label set of all the documents in S as Y . Let T and T u denote the documents from the test dataset and the documents from the unlabeled dataset in the target language. The documents in the training and test datasets in the source and target languages are translated into the other language using the on- We denote them as T s (the translation of S) and S t (the translation of T ). We wish to learn a D-dimensional vector representation for all the documents in the dataset.</p><p>The general framework of BiDRL is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. After we obtain the data in the source and target languages, we propose both the mono- lingual and bilingual constraints to learn the mod- el. The monolingual constraints help to model words and documents in each individual language. The bilingual constraints help to build a consis- tent embedding space across different languages. The joint learning framework is semi-supervised which uses the sentiment labels Y of the training documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Constraints</head><p>In this subsection, we describe the representation learning algorithm for the source and target lan- guages. We start from the paragraph vector model ( <ref type="bibr" target="#b9">Le and Mikolov, 2014</ref>) which has been proved to be one of the state-of-the-art methods for doc- ument modeling. In the paragraph vector frame- work, both documents and words are mapped to unique vectors. Each document is treated as a u- nique token which is the context of all the words in the document. Therefore, each word in the doc- </p><formula xml:id="formula_0">log p(w | C, d) (1)</formula><p>where D is the document set, w and C are the word and its context in a document d. The only d- ifference between paragraph vector algorithm and the well-known word2vec algorithm ( <ref type="bibr" target="#b15">Mikolov et al., 2013</ref>) is the additional document vector. The conditional probability of predicting a word from its context is modeled via softmax which is very expensive to compute. It is usually ap- proximately solved via negative sampling. A log- bilinear model is used instead to predict whether two words are in the same context. For a word and context pair (w, C) in a document d, the objective function becomes,</p><formula xml:id="formula_1">L1 = − log σ(v T w · 1 k + 1 · (v d + c∈C vc))− n i=1 E w ∈pn(w) (log σ(−v T w · 1 k + 1 · (v d + c∈C vc)))<label>(2)</label></formula><p>where σ(·) is the sigmoid function, c is a contex- t word in C, k is the window size, v w and v c are the vectors for words and context words, v d is the vector for the document d, w is the negative sam- ple from the noise distribution P n (w). The paragraph vector model captures the se- mantic relations between words and documents. In addition, we hope that the vector representation of a document can directly reflect its sentimen- t. The document vectors associated with different sentiments should fall into different positions in the embedding space. We introduce a logistic re- gression classifier into the embedding algorithm. For each labeled document d with the label y, we use it to classify the sentiment based on the cross entropy loss function,</p><formula xml:id="formula_2">L 2 = −y log σ((x T v d + b) − (1 − y) log σ(−x T v d − b))<label>(3)</label></formula><p>where x is the weight vector for the features and b is the bias, v d is the document vector. Combining the above two loss functions, we ob- tain the monolingual embedding algorithm. For the source language, we get the word and docu- ment representations via</p><formula xml:id="formula_3">arg min L s = d∈S∪Su (w,C)∈d L 1 + d∈S L 2 (4)</formula><p>The embedding for the target language is ob- tained similarly,</p><formula xml:id="formula_4">arg min L t = d∈T ∪Tu (w,C)∈d L 1 + d∈Ts L 2 (5)</formula><p>where the labeled dataset T s is translated from the source language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilingual Constraints</head><p>The key problem of bilingual representation learn- ing is to obtain a consistent embedding space across the source and the target languages. We propose three strategies for BiDRL to bridge the language gap.</p><p>The first strategy is to share the classification hyperplane in Equation 3. The logistics regression parameter x and b are the same in the source and the target languages. By sharing the same classi- fication parameter, bilingual documents with the same sentiment will fall into similar areas in the embedding space. Therefore, introducing the lo- gistic regression classifier in our embedding algo- rithm not only helps to obtain task-specific embed- ding but also helps to narrow the language barrier.</p><p>The second strategy is to minimize the differ- ence between a pair of parallel documents, i.e., the original documents and the translated documents. Such word or document based regularizer is wide- ly used in previous works ( <ref type="bibr" target="#b6">Gouws et al., 2015;</ref><ref type="bibr" target="#b25">Zou et al., 2013</ref>). We simply measure the differences of two documents via the Euclidean Distance. The parallel documents refer to the documents in S and T and their corresponding translations (S t and T s ). It leads to the following loss function,</p><formula xml:id="formula_5">L 3 = (ds,dt)∈(S,Ts) v ds − v dt 2 + (ds,dt)∈(St,T ) v ds − v dt 2<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">(d s , d t )</formula><p>is a pair of parallel documents and v * is their vector representations. Our third strategy aims to generate similar rep- resentations for texts with the same sentiment. The traditional paragraph vector model focuses on modeling the semantic relationship between words and documents while our method aims to preserve the sentiment relationship as well. For each doc- ument d ∈ S in the source language, we find K least similar documents with the same sentimen- t. The document similarity is measured by cosine similarity using TF-IDF features. We hope that these K documents should have similar represen- tation with document d despite of their textual d- ifference. We denote the K documents as Q s and their parallel documents in the target language as Q t . It leads to the following loss function,</p><formula xml:id="formula_7">L 4 = d∈S ( ds∈Qs v ds − v d 2 + dt∈Qt v dt − v d 2 )<label>(7)</label></formula><p>where v * denotes the vector representation of a document.</p><p>Combining the monolingual embedding algo- rithm and the cross-lingual correlations, we have the overall objective function as follows,</p><formula xml:id="formula_8">arg min L = L s + L t + L 3 + L 4<label>(8)</label></formula><p>which can be solved using stochastic gradient de- scent (SGD).</p><p>After learning BiDRL, we represent each doc- ument in the training and test dataset by the con- catenation of its vector representation in both the source and the target languages. In particular, for each training document d ∈ S, we represent it as</p><formula xml:id="formula_9">[v d v d ]</formula><p>where d is its corresponding translation and v * is the learned document representation in BiDRL. Similarly, for each test document d ∈ T , we represent it as <ref type="bibr">[v d v d ]</ref>. Afterwards, a logistic regression classifier is trained using the concate- nated feature vectors of S. The polarity of the re- views in T can be predicted by applying the clas- sifier on the concatenated feature vectors of T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use the multilingual multi-domain Amazon re- view dataset 2 created by <ref type="bibr" target="#b18">(Prettenhofer and Stein, 2010)</ref>. It contains three different domains book, DVD and music. Each domain has reviews in four different languages English, German, French and Japanese. In our experiments, we use English as the source language and the rest three as target lan- guages. Therefore, we have a total of nine tasks with different combinations of three domains and three target languages. For each task, the training and test datasets have 1000 positive reviews and 1000 negative reviews. There are also several t- housand of unlabeled reviews but the quantity of them varies significantly for different tasks. Fol- lowing <ref type="bibr" target="#b18">(Prettenhofer and Stein, 2010)</ref>, when there are more than 50000 unlabeled reviews we ran- domly selected 50000 of them, otherwise we use all the unlabeled reviews. The detailed statistics of the dataset are shown in <ref type="table">Table 1</ref>.</p><p>We translated the 2000 training reviews and 2000 test reviews into the other languages using Google Translate. <ref type="bibr" target="#b18">Prettenhofer and Stein (2010)</ref> has already provided the translation of the test da- ta. We only need to translate the English training data into the three target languages. All the review texts are tokenized and converted into lowercase. We use Mecab 3 to segment the Japanese reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>In the bilingual representation learning algorithm, we set the vector size as 200 and the context win- dows as 10. The learning rate is set to 0.025 fol-   Japanese book 50000 50000 DVD 30000 50000 music 25000 50000 <ref type="table">Table 1</ref>: The amount of unlabeled reviews used in the experiments. There are also 1000 positive and 1000 negative reviews both for training and test in each task, i.e. |S| = |T | = 2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Language Domain MT-BOW MT-PV CL-SCL BSE CR-RL Bi-PV BiDRL</head><p>lowing word2vec and it declines with the train- ing procedure. K is empirically chosen as 10. The algorithm runs 10 iterations on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline</head><p>We introduce several state-of-the-art methods used for comparison in our experiment as follows.</p><p>MT-BOW: It learns a classifier in the source language using bag-of-words features and the test data is translated into the source language vi- a Google Translate. We directly use the experi- mental results reported in <ref type="bibr" target="#b18">(Prettenhofer and Stein, 2010)</ref>.</p><p>MT-PV: We translate the training data into the target language and also translate the test data into the source language. In both the source and target languages, we use the paragraph vector model to learn the vector representation of the documents. Therefore, each document can be represented by the concatenation of the vector in two languages. A logistic regression classifier is trained using the concatenated feature vectors similarly to BiDRL. MT-PV can be regarded as a simplified version of BiDRL without the L 2 , L 3 and L 4 regularizers.</p><p>CL-SCL: It is the cross-lingual structural corre- spondence learning algorithm proposed by <ref type="bibr" target="#b18">(Prettenhofer and Stein, 2010)</ref>. It learns a map between the bag-of-words representations in the source and the target languages. It also leverages Google Translate to obtain the word translation oracle.</p><p>BSE: It is the bilingual embedding method of ( <ref type="bibr" target="#b19">Tang et al., 2012)</ref>. It aims to learn two differen- t mapping matrices for the source and target lan- guages. The two matrices map the bag-of-words representations in the source and the target lan- guages into the same feature space. <ref type="bibr" target="#b19">Tang et al. (2012)</ref> only report their results on 6 of the 9 tasks.</p><p>CR-RL: It is the bilingual word representation learning method of <ref type="bibr" target="#b22">(Xiao and Guo, 2013</ref>). It learn- s different representations for words in different languages. Part of the word vector is shared a- mong different languages and the rest is language- dependent. The document representation is calcu- lated by taking average over all words in the doc- ument.</p><p>Bi-PV:  extended the para- graph vector model into bilingual setting by shar- ing the document representation of a pair of par- allel documents. Their method requires large amounts of parallel data and does not need the ma- chine translation service during test phase. In our setting, there are not enough parallel data to train the model and it will lead to an unfair comparison without using the machine translated text. We im- plement a variant of their method which learns the vector representation for the training and test da- ta using both the original and the translated texts. Each pair of parallel documents shares the same document representation.</p><p>We also implement the method of ( <ref type="bibr" target="#b24">Zhou et al., 2015</ref>) which is originally designed for the English-Chinese cross-lingual sentiment classifi- cation task. We find that it is not very adapt- able in our case because the negation pattern and sentiment words are hard to choose for our target languages. The results of our replication do not achieve comparable results with the rest method- s and are not listed here to avoid misleading the readers. <ref type="table" target="#tab_1">Table 2</ref> shows the experimental results for all the baselines and our method. For all the nine tasks, our bilingual document representation learning method achieves the state-of-the-art performance. The two most simple approaches MT-BOW imple- mented by <ref type="bibr" target="#b18">(Prettenhofer and Stein, 2010)</ref> and MT- PV implemented by us are both strong baselines. They achieve comparable results with the more complex baselines on many tasks. MT-PV per- forms better than MT-BOW on most tasks which proves that the representation learning method is more useful than the traditional bag-of-words fea- tures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head><p>The three word-based representation learning methods CL-SCL, BSE and CR-RL achieve sim- ilar results with the simple model MT-BOW and only outperform it on some tasks. However, the document representation learning methods MT- PV, Bi-PV and BiDRL performs much better. It shows that capturing the compositionality of words is important for sentiment classification. The isolated word representations are not enough to model the whole document. The Bi-PV model outperforms MT-PV on most tasks and shows that the authors idea of learning a single representation for a pair of parallel documents is more useful than learning them separately.</p><p>For all the baselines and our method, the perfor- mance of the English-Japanese tasks is lower than that of the English-German and English-French tasks. It is reasonable because the English lan- guage is much closer to German and French than Japanese. The machine translation tool also per- forms better when translating between the Western languages.</p><p>Our BiDRL model outperforms all the existing methods on all the tasks. The accuracy is over 80% on all the six tasks for the two European tar- get languages. The mean accuracy of the nine tasks shows a significant gap between BiRDL and the existing models. It achieves an improvement about 3% compared to the previous state-of-the- art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Sensitivity Study</head><p>In this subsection, we investigate the influence of the vector size of our representation learning algo- rithm. We conduct the experiments by changing the vector size from 50 to 400. For each parameter setting, we run the algorithm for ten times and get the mean accuracy.</p><p>The results of MT-PV and BiDRL on all the nine tasks are shown in <ref type="figure" target="#fig_4">Figure 3</ref>. For almost all the tasks, we can observe that our model BiDRL steadily outperforms the strong baseline MT-PV. It proves the efficacy of our bilingual embedding constraints.</p><p>For most of the nine tasks including DE- DVD, DE-MUSIC, FR-DVD, FR-MUSIC and JP- MUSIC, the performance of BiDRL increases with the growth of the vector size at the begin- ning and remains stable afterwards. For the rest tasks, our model responds less sensitively to the change of the vector size and the prediction accu- racy keeps steady. However, the results of MT-PV show no regular patterns with the change of the vector size which makes it hard to choose a satis- fying parameter value.</p><p>The parameter K is empirically chosen as 10 because we find that its value has little influence to our model when it is chosen between 10 and 50. Selecting a small K will help to accelerate the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of the Sentiment Information</head><p>The traditional paragraph vector model only mod- els the semantic relatedness between texts via the word co-occurrence statistics. In this study, we propose to learn the bilingual representation uti- lizing the sentiment information. Firstly, we in- troduce a classification hyperplane to separate the embedding of texts with different polarities, i.e the loss function L 2 . Secondly, we consider the texts with the same sentiment but has largely different textual expressions. They are forced the have sim- ilar representations, i.e. L 4 . <ref type="table">Table 3</ref> shows the  <ref type="table">Table 3</ref>: Influence of the sentiment information. We only show the mean accuracy of the nine tasks due to space limit.</p><p>MT-PV can be regarded as BiDRL without all the sentiment information. It achieves lower re- sults than the other three methods. We can also observe that removing L 2 or L 4 both decreases the accuracy. It proves that the sentiment information helps BiDRL to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this study, we propose a bilingual documen- t representation learning method for cross-lingual sentiment classification. Different from previ- ous studies which only get bilingual word embed- dings, we directly learn the vector representation for documents in different languages. We propose three strategies to achieve a consistent embedding space for the source and target languages. Both sentiment and semantic correlations are exploited in our algorithm while previous works only use the semantic relatedness between parallel docu- ments. Our model is evaluated on a benchmarking dataset which contains three different target lan- guages and three different domains. Several state- of-the-art methods including several bilingual rep- resentation learning models are used for compar- ison. Our algorithm outperforms all the baseline methods on all the nine tasks in the experiment.</p><p>Our future work will focus on extending the bilingual document representation model into the multilingual scenario. We will try to learn a single embedding space for a source language and mul- tiple target languages simultaneously. In addition, we will also explore the possibility of using more complex neural network models such as convolu- tional neural network and recurrent neural network to build bilingual document representation system. gram <ref type="bibr">(863 Program) of China (2015AA015403, 2014AA015102)</ref> and IBM Global Faculty Award Program. We thank the anonymous reviewers for their helpful comments. Xiaojun Wan is the corre- sponding author.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Framework of BiDRL</figDesc><graphic url="image-1.png" coords="3,316.08,62.80,203.40,242.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Paragraph vector</figDesc><graphic url="image-2.png" coords="4,74.03,62.81,214.20,126.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Mikolov et al. (2013) set P n (w) as the 3 4 power of the uni- gram distribution which outperforms the unigram and the uniform distribution significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>German</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Influence of vector size for the nine cross-lingual sentiment classification tasks</figDesc><graphic url="image-3.png" coords="8,58.07,62.81,484.13,307.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Cross-lingual sentiment classification accuracy for the nine tasks. For all the methods, we get ten different runs of the algorithm and calculate the mean accuracy.</figDesc><table>Target 
Domain 
|S U | 
|T U | 
Language 

German 
book 
50000 50000 
DVD 
30000 50000 
music 
25000 50000 

French 
book 
50000 32000 
DVD 
30000 9000 
music 
25000 16000 

</table></figure>

			<note place="foot" n="1"> http://translate.google.com/</note>

			<note place="foot" n="2"> https://www.uni-weimar.de/medien/ webis/corpora/corpus-webis-cls-10/ 3 http://taku910.github.io/mecab/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by National Natural Science Foundation of China (61331011), Na-tional Hi-Tech Research and Development Pro-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment analysis for indian languages using linked wordnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Balamurali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2012 International Conference on Computational Linguistics (COLING)</title>
		<meeting>2012 International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity: Are more languages better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on computational linguistics</title>
		<meeting>the 23rd international conference on computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Twitter mood predicts the stock market</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">841</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52rd Annual Meeting of the Association for Computational Linguistic</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>52rd Annual Meeting of the Association for Computational Linguistic</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING2012</title>
		<meeting>COLING2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint bilingual sentiment classification with unlabeled parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual mixture model for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning multilingual subjective language via cross-lingual projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2007</title>
		<meeting>ACL2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning distributed representations for multilingual text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Crosslanguage text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantitative study of individual emotional states in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghai</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvis</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Affective Computing</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting elections with twitter: What 140 characters reveal about political sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andranik</forename><surname>Tumasjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><forename type="middle">G</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabell</forename><forename type="middle">M</forename><surname>Sandner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICWSM</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="178" to="185" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised representation learning for cross-lingual text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2013</title>
		<meeting>EMNLP-2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1465" to="1475" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised matrix completion for cross-lingual text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
