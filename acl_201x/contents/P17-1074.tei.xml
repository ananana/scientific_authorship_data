<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
						</author>
						<title level="a" type="main">Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="793" to="805"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1074</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as &quot;Good&quot; or &quot;Accept-able&quot; in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical Error Correction (GEC) systems are often only evaluated in terms of overall perfor- mance because system hypotheses are not anno- tated. This can be misleading however, and a sys- tem that performs poorly overall may in fact out- perform others at specific error types. This is sig- nificant because a robust specialised system is ac- tually more desirable than a mediocre general sys- tem. Without an error type analysis however, this information is completely unknown.</p><p>The main aim of this paper is hence to rec- tify this situation and provide a method by which parallel error correction data can be automatically annotated with error type information. This not only facilitates error type evaluation, but can also be used to provide detailed error type feedback to non-native learners. Given that different cor- pora are also annotated according to different stan- dards, we also attempted to standardise existing datasets under a common error type framework.</p><p>Our approach consists of two main steps. First, we automatically extract the edits between paral- lel original and corrected sentences by means of a linguistically-enhanced alignment algorithm <ref type="bibr" target="#b5">(Felice et al., 2016</ref>) and second, we classify them ac- cording to a new, rule-based framework that re- lies solely on dataset-agnostic information such as lemma and part-of-speech. We demonstrate the value of our approach, which we call the ERRor ANnotation Toolkit (ERRANT) <ref type="bibr">1</ref> , by carrying out a detailed error type analysis of each system in the CoNLL-2014 shared task on grammatical er- ror correction ( <ref type="bibr">Ng et al., 2014)</ref>.</p><p>It is worth mentioning that despite an in- creased interest in GEC evaluation in recent years <ref type="bibr" target="#b1">(Dahlmeier and Ng, 2012;</ref><ref type="bibr" target="#b4">Felice and Briscoe, 2015;</ref><ref type="bibr" target="#b0">Bryant and Ng, 2015;</ref><ref type="bibr" target="#b10">Napoles et al., 2015;</ref><ref type="bibr" target="#b7">Grundkiewicz et al., 2015;</ref><ref type="bibr" target="#b17">Sakaguchi et al., 2016)</ref>, ERRANT is the only toolkit currently capable of producing error types scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Edit Extraction</head><p>The first stage of automatic annotation is edit ex- traction. Specifically, given an original and cor- rected sentence pair, we need to determine the start and end boundaries of any edits. This is funda- mentally an alignment problem:</p><p>We took a guide tour on center city . We took a guided tour of the city center . <ref type="table">Table 1</ref>: A sample alignment between an original and corrected sentence <ref type="bibr" target="#b5">(Felice et al., 2016</ref>).</p><p>The first attempt at automatic edit extraction was made by <ref type="bibr" target="#b18">Swanson and Yamangil (2012)</ref>, who simply used the Levenshtein distance to align par- allel original and corrected sentences. As the Levenshtein distance only aligns individual to- kens however, they also merged all adjacent non- matches in an effort to capture multi-token edits. <ref type="bibr" target="#b19">Xue and Hwa (2014)</ref> subsequently improved on Swanson and Yamangil's work by training a max- imum entropy classifier to predict whether edits should be merged or not.</p><p>Most recently, <ref type="bibr" target="#b5">Felice et al. (2016)</ref> pro- posed a new method of edit extraction using a linguistically-enhanced alignment algorithm sup- ported by a set of merging rules. More specifi- cally, they incorporated various linguistic informa- tion, such as part-of-speech and lemma, into the cost function of the Damerau-Levenshtein 2 algo- rithm to make it more likely that tokens with sim- ilar linguistic properties aligned. This approach ultimately proved most effective at approximating human edits in several datasets (80-85% F 1 ), and so we use it in the present study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Automatic Error Typing</head><p>Having extracted the edits, the next step is to as- sign them error types. While Swanson and Ya- mangil (2012) did this by means of maximum entropy classifiers, one disadvantage of this ap- proach is that such classifiers are biased towards their particular training corpora. For example, a classifier trained on the First Certificate in English (FCE) corpus <ref type="bibr">(Yannakoudakis et al., 2011</ref>) is un- likely to perform as well on the National Univer- sity of Singapore Corpus of Learner English (NU- CLE) <ref type="bibr" target="#b1">(Dahlmeier and Ng, 2012)</ref> or vice versa, be- cause both corpora have been annotated according to different standards (cf. <ref type="bibr" target="#b19">Xue and Hwa (2014)</ref>). Instead, a dataset-agnostic error type classifier is much more desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Rule-Based Error Type Framework</head><p>To solve this problem, we took inspiration from <ref type="bibr" target="#b18">Swanson and Yamangil's (2012)</ref> observation that most error types are based on part-of-speech (POS) categories, and wrote a rule to classify an edit based only on its automatic POS tags. We then added another rule to similarly differenti- ate between Missing, Unnecessary and Replace-ment errors depending on whether tokens were inserted, deleted or substituted. Finally, we ex- tended our approach to classify errors that are not well-characterised by POS, such as Spelling or Word Order, and ultimately assigned all error types based solely on automatically-obtained, ob- jective properties of the data.</p><p>In total, we wrote roughly 50 rules. While many of them are very straightforward, significant atten- tion was paid to discriminating between different kinds of verb errors. For example, despite all hav- ing the same correction, the following sentences contain different types of common learner errors:</p><p>(a) He IS asleep now.</p><p>[ While the final three rules could certainly be re- ordered, we informally found the above sequence performed best during development. It is also worth mentioning that this is a somewhat simpli- fied example and that there are additional rules to discriminate between auxiliary verbs, main verbs and multi verb expressions. Nevertheless, the above case exemplifies our approach, and a more complete description of all rules is provided with the software.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Dataset-Agnostic Classifier</head><p>One of the key strengths of a rule-based ap- proach is that by being dependent only on auto- matic mark-up information, our classifier is en- tirely dataset independent and does not require la- belled training data. This is in contrast with ma- chine learning approaches which not only learn dataset specific biases, but also presuppose the ex- istence of sufficient quantities of training data. A second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned a particular error category. In contrast, human and machine learning classification decisions are often much less transparent.</p><p>Finally, by being fully deterministic, our ap- proach bypasses bias effects altogether and should hence be more consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Automatic Markup</head><p>The prerequisites for our rule-based classifier are that each token in both the original and corrected sentence is POS tagged, lemmatized, stemmed and dependency parsed. We use spaCy 3 v1.7.3 for all but the stemming, which is performed by the Lan- caster Stemmer in NLTK. <ref type="bibr">4</ref> Since fine-grained POS tags are often too detailed for the purposes of error evaluation, we also map spaCy's Penn Treebank style tags to the coarser set of Universal Depen- dency tags. <ref type="bibr">5</ref> We use the latest Hunspell GB-large word list 6 to help classify non-word errors. The marked-up tokens in an edit span are then input to the classifier and an error type is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Error Categories</head><p>The complete list of 25 error types in our new framework is shown in <ref type="table" target="#tab_2">Table 2</ref>. Note that most of them can be prefixed with 'M:', 'R:' or 'U:', depending on whether they describe a Missing, Replacement, or Unnecessary edit, to enable evaluation at different levels of granularity (see Appendix A for all valid combinations). This means we can choose to evaluate, for exam- ple, only replacement errors (anything prefixed by 'R:'), only noun errors (anything suffixed with 'NOUN') or only replacement noun errors ('R:NOUN'). This flexibility allows us to make more detailed observations about different aspects of system performance.</p><p>One caveat concerning error scheme design is that it is always possible to add new categories for increasingly detailed error types; for instance, we currently label [could → should] a tense error, when it might otherwise be considered a modal error. The reason we do not call it a modal er- ror, however, is because it would then become less clear how to handle other cases such as <ref type="bibr">[can → should]</ref> and [has eaten → should eat], which might be considered a more complex combination of modal and tense error. As it is impractical to create new categories and rules to differentiate be- tween such narrow distinctions however, our final framework aims to be a compromise between in- formativeness and practicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Classifier Evaluation</head><p>As our new error scheme is based solely on au- tomatically obtained properties of the data, there are no gold standard labels against which to evalu- ate classifier performance. For this reason, we in- stead carried out a small-scale manual evaluation, where we simply asked 5 GEC researchers to rate the appropriateness of the predicted error types for 200 randomly chosen edits in context (100 from FCE-test and 100 from CoNLL-2014) as "Good", "Acceptable" or "Bad". "Good' meant the chosen type was the most appropriate for the given edit, "Acceptable" meant the chosen type was appropri- ate, but probably not optimum, while "Bad" meant the chosen type was not appropriate for the edit. Raters were warned that the edit boundaries had been determined automatically and hence might be unusual, but that they should focus on the appropriateness of the error type regardless of whether they agreed with the boundary or not.</p><p>It is worth stating that the main purpose of this evaluation was not to evaluate the specific strengths and weaknesses of the classifier, but rather ascertain how well humans believed the pre- dicted error types characterised each edit. GEC is known to be a highly subjective task (Bryant and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rater</head><p>Good Acceptable Bad 1 92.0% 4.0% 4.0% 2 89.5% 6.5% 4.0% 3 83.0% 13.0% 4.0% 4</p><p>84.5% 11.0% 4.5% 5 82.5% 15.5% 2.0% OVERALL 86.3%</p><p>10.0% 3.7% <ref type="table">Table 3</ref>: The percent distribution for how each ex- pert rated the appropriateness of the predicted er- ror types. E.g. Rater 3 considered 83% of all pre- dicted types to be "Good".</p><p>Ng, 2015) and so we were more interested in over- all judgements than specific disagreements. The results from this evaluation are shown in <ref type="table">Table 3</ref>. Significantly, all 5 raters considered at least 95% of the predicted error types to be ei- ther "Good" or "Acceptable", despite the degree of noise introduced by automatic edit extraction. Furthermore, whenever raters judged an edit as "Bad", this could usually be traced back to a POS or parse error; e.g. [ring → rings] might be considered a NOUN:NUM or VERB:SVA er- ror depending on whether the POS tagger consid- ered both sides of the edit nouns or verbs. Inter- annotator agreement was also good at 0.724 κ f ree <ref type="bibr" target="#b13">(Randolph, 2005)</ref>.</p><p>In contrast, although incomparable on account of the different metric and error scheme, the best results using machine learning were between 50- 70% F <ref type="bibr">1 (Felice et al., 2016)</ref>. Ultimately however, we believe the high scores awarded by the raters validates the efficacy of our rule-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Error Type Scoring</head><p>Having described how to automatically annotate parallel sentences with ERRANT, we now also have a method to annotate system hypotheses; this is the first step towards an error type evaluation. Since no scorer is currently capable of calculating error type performance however <ref type="bibr" target="#b1">(Dahlmeier and Ng, 2012;</ref><ref type="bibr" target="#b4">Felice and Briscoe, 2015;</ref><ref type="bibr" target="#b10">Napoles et al., 2015)</ref>, we instead built our own.</p><p>Fortunately, one benefit of explicitly annotat- ing system hypotheses is that it makes evaluation much more straightforward. In particular, for each sentence, we only need to compare the edits in the hypothesis against the edits in each respective ref- erence and measure the overlap. Any edit with the same span and correction in both files is hence a true positive (TP), while unmatched edits in the hypothesis and references are false positives (FP) and false negatives (FN) respectively. These re- sults can then be grouped by error type for the pur- poses of error type evaluation.</p><p>Finally, it is worth noting that this scorer is much simpler than other scorers in GEC which typically incorporate edit extraction or alignment directly into their algorithms. Our approach, on the other hand, treats edit extraction and evalua- tion as separate tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gold Reference vs. Auto Reference</head><p>Before evaluating an automatically annotated hy- pothesis against its reference, we must also ad- dress another mismatch: namely that hypothe- sis edits must be extracted and classified auto- matically, while reference edits are typically ex- tracted and classified manually using a different framework. Since evaluation is now reduced to a straightforward comparison between two files however, it is especially important that the hypoth- esis and references are both processed in the same way. For instance, a hypothesis edit [have eating → has eaten] will not match the reference edits <ref type="bibr">[have → has]</ref> and <ref type="bibr">[eating → eaten]</ref> because the former is one edit while the latter is two edits, even though they equate to the same thing.</p><p>To solve this problem, we can reprocess the ref- erences in the same way as the hypotheses. In other words, we can apply ERRANT to the refer- ences such that each reference edit is subject to the same automatic extraction and classification crite- ria as each hypothesis edit. While it may seem un- orthodox to discard gold reference information in favour of automatic reference information, this is necessary to minimise the difference between hy- pothesis and reference edits and also standardise error type annotations.</p><p>To show that automatic references are feasible alternatives to gold references, we evaluated each team in the CoNLL-2014 shared task using both types of reference with the M 2 scorer (Dahlmeier and Ng, 2012), the de facto standard of GEC evaluation, and our own scorer. <ref type="table" target="#tab_4">Table 4</ref> hence shows that there is little difference between the overall scores for each team, and we formally validated this hypothesis for precision, recall and F 0.5 by means of bootstrap significance testing ( <ref type="bibr" target="#b3">Efron and Tibshirani, 1993)</ref>. Ultimately, we found no statistically significant difference  between automatic and gold references (1,000 iterations, p &gt; .05) which leads us to conclude that our automatic references are qualitatively as good as human references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with the M 2 Scorer</head><p>Despite using the same metric, <ref type="table" target="#tab_4">Table 4</ref> also shows that the M 2 scorer tends to produce slightly higher F 0.5 scores than our own. This initially led us to believe that our scorer was underestimating per- formance, but we subsequently found that instead the M 2 scorer tends to overestimate performance (cf. <ref type="bibr" target="#b4">Felice and Briscoe (2015)</ref> and <ref type="bibr" target="#b10">Napoles et al. (2015)</ref>).</p><p>In particular, given a choice between matching [have eating → has eaten] from Annotator 1 or <ref type="bibr">[have → has]</ref> and <ref type="bibr">[eating → eaten]</ref> from Anno- tator 2, the M 2 scorer will always choose Anno- tator 2 because two true positives (TP) are worth more than one. Similarly, whenever the scorer encounters two false positives (FP) within a cer- tain distance of each other, <ref type="bibr">7</ref>   <ref type="table">Table 5</ref>: Precision, recall and F 0.5 for Missing, Unnecessary, and Replacement errors for each team. A dash indicates the team's system did not attempt to correct the given error type (TP+FP = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CoNLL-2014 Shared Task Analysis</head><p>To demonstrate the value of ERRANT, we applied it to the data produced in the CoNLL-2014 shared task ( <ref type="bibr">Ng et al., 2014</ref>). Specifically, we automati- cally annotated all the system hypotheses and offi- cial reference files. 8 Although ERRANT can be applied to any dataset of parallel sentences, we chose to evaluate on CoNLL-2014 because it rep- resents the largest collection of publicly available GEC system output. For more information about the systems in CoNLL-2014, we refer the reader to the shared task paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Edit Operation</head><p>In our first category experiment, we simply inves- tigated the performance of each system in terms of Missing, Replacement and Unnecessary edits.</p><p>The results are shown in <ref type="table">Table 5</ref> with additional information in Appendix B, <ref type="table" target="#tab_12">Table 10</ref>. The most surprising result is that five teams (AMU, IPN, PKU, RAC, UFC) failed to correct any unnecessary token errors at all. This is note- worthy because unnecessary token errors account for roughly 25% of all errors in the CoNLL-2014 test data and so failing to address them signifi- cantly limits a system's maximum performance. While the reason for this is clear in some cases, e.g. UFC's rule-based system was never designed to tackle unnecessary tokens <ref type="bibr" target="#b8">(Gupta, 2014)</ref>, it is less clear in others, e.g. there is no obvious rea- son why AMU's SMT system failed to learn when 8 http://www.comp.nus.edu.sg/ ∼ nlp/conll14st.html to delete tokens <ref type="bibr" target="#b9">(Junczys-Dowmunt and Grundkiewicz, 2014</ref>). AMU's result is especially re- markable given that their system still came 3rd overall despite this limitation.</p><p>In contrast, CUUI's classifier approach (Ro- zovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB's hybrid MT approach <ref type="bibr" target="#b6">(Felice et al., 2014</ref>) signif- icantly outperformed all others in terms of miss- ing token errors. It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall perfor- mance (Rozovskaya and Roth, 2016). <ref type="table" target="#tab_7">Table 6</ref> shows precision, recall and F 0.5 for each of the error types in our proposed framework for each team in CoNLL-2014. As some error types are more common than others, we also provide the TP, FP and FN counts used to make this table in Appendix B, <ref type="table" target="#tab_13">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">General Error Types</head><p>Overall, CAMB was the most successful team in terms of error types, achieving the highest F- score in 10 (out of 24) error categories, followed by AMU, who scored highest in 6 categories. All but 3 teams (IITB, IPN and POST) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement dif- ferent error types. Only CAMB attempted to cor- rect at least 1 error from every category.</p><p>Other interesting observations we can make from this   <ref type="table">Table 7</ref>: Detailed breakdown of Determiner errors for two teams.</p><p>• Despite the prevalence of spell checkers nowadays, many teams did not seem to em- ploy them; this would have been an easy way to boost overall performance.</p><p>• Although several teams built specialised clas- sifiers for DET and PREP errors, CAMB's hybrid MT approach still outperformed them. This might be because the classifiers were trained using a different error type framework however.</p><p>• CUUI's classifiers significantly outper- formed all other approaches at ORTH and VERB:FORM errors. This suggests classi- fiers are well-suited to these error types.</p><p>• Although UFC's rule-based approach was the best at VERB:SVA errors, CUUI's classifier was not very far behind.</p><p>• Only AMU managed to correct any CONJ errors.</p><p>• Content word errors (i.e. ADJ, ADV, NOUN and VERB) were unsurprisingly very difficult for all teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detailed Error Types</head><p>In addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail. For example, <ref type="table">Table 7</ref> shows the breakdown of Determiner errors for two teams us- ing different approaches in terms of edit operation.</p><p>Note that this is a representative example of de- tailed error type performance, as an analysis of all error type combinations for all teams would take up too much space.  <ref type="table">Table 8</ref>: Each team's performance at correcting multi-token edits; i.e. there are at least two tokens on one side of the edit.</p><p>While CAMB's hybrid MT approach achieved a higher score than CUUI's classifier overall, our more detailed evaluation reveals that CUUI actu- ally outperformed CAMB at Replacement Deter- miner errors. We also learn that CAMB scored twice as highly on M:DET and U:DET than it did on R:DET and that CUUI's significantly higher U:DET recall was offset by a lower precision. Ul- timately, this shows that even though one approach might be better than another overall, different ap- proaches may still have complementary strengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multi Token Errors</head><p>Another benefit of explicitly annotating all hy- pothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size. <ref type="table">Table 8</ref> hence shows the over- all performance for each team at correcting multi- token edits, where a multi-token edit is an edit that has at least two tokens on either side. In the CoNLL-2014 test set, there are roughly 220 such edits (about 10% of all edits).</p><p>In general, teams did not do well at multi-token edits. In fact only three teams achieved scores greater than 10% F 0.5 and all of them used MT (AMU, CAMB, UMC). This is significant because recent work has suggested that the main goal of GEC should be to produce fluent-sounding, rather than just grammatical sentences, even though this often requires complex multi-token edits <ref type="bibr" target="#b17">(Sakaguchi et al., 2016</ref>). If no system is particularly adept at correcting multi-token errors however, ro- bust fluency correction will likely require more so- phisticated methods than are currently available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>800</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Correction</head><p>Figure 1: The difference between detection and correction scores for each team overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Detection vs. Correction</head><p>Another important aspect of GEC that is seldom reported in the literature is that of error detection; i.e. the extent to which a system can identify er- roneous tokens in text. This can be calculated by comparing the edit overlap between the hypothesis and reference files regardless of the proposed cor- rection in a manner similar to Recognition evalu- ation in the HOO shared tasks for GEC ( <ref type="bibr" target="#b2">Dale and Kilgarriff, 2011)</ref>. <ref type="figure">Figure 1</ref> hence shows how each team's score for detection differed in relation to their score for correction. While CAMB scored highest for de- tection overall, it is interesting to note that CUUI ultimately performed slightly better than CAMB at correction. This suggests CUUI was more suc- cessful at correcting the errors they detected than CAMB. In contrast, IPN and PKU are notable for detecting significantly more errors than they were able to correct. Nevertheless, a system's ability to detect errors, even if it is unable to correct them, is still likely to be valuable information to a learner ( <ref type="bibr" target="#b14">Rei and Yannakoudakis, 2016)</ref>.</p><p>Finally, although we do not do so here, our scorer is also capable of providing a detailed er- ror type breakdown for detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we described ERRANT, a grammat- ical ERRor ANnotation Toolkit designed to au- tomatically annotate parallel error correction data with explicit edit spans and error type information. ERRANT can be used to not only facilitate a de- tailed error type evaluation in GEC, but also to standardise existing error correction corpora and reduce annotator workload. We release ERRANT with this paper.</p><p>Our approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits. This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas. A small-scale evaluation of our classifier found that each rater considered &gt;95% of the predicted error types as either "Good" (85%) or "Acceptable" (10%).</p><p>We demonstrated the value of ERRANT by car- rying out a detailed evaluation of system error type performance for all teams in the CoNLL- 2014 shared task on Grammatical Error Correc- tion. We found that different systems had differ- ent strengths and weaknesses which we hope re- searchers can exploit to further improve general performance. <ref type="bibr">Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011</ref>. A new dataset and method for au- tomatically grading ESOL texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Lin- guistics, Portland, Oregon, USA, pages 180-189. http://www.aclweb.org/anthology/P11-1019.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Complete list of valid error code combinations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Code</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The list of 25 main error categories in our new framework with examples and explanations.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Overall scores for each team in CoNLL-
2014 using gold and auto references with both the 
M 2 scorer and our simpler edit comparison ap-
proach. All scores are in terms of F 0.5 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>table include :</head><label>include</label><figDesc></figDesc><table>AMU CAMB CUUI 

IITB 
IPN NTHU 
PKU POST 
RAC SJTU 
UFC UMC 

ADJ 

P 
4.88 
9.09 
-
0.00 
0.00 
0.00 
66.67 
0.00 
12.50 
0.00 
-
0.00 
R 
6.67 
13.89 
-
0.00 
0.00 
0.00 
7.14 
0.00 
3.57 
0.00 
-
0.00 
F 0.5 
5.15 
9.77 
-
0.00 
0.00 
0.00 
25.00 
0.00 
8.33 
0.00 
-
0.00 

ADJ:FORM 

P 
55.56 
75.00 100.00 100.00 
0.00 
33.33 100.00 
50.00 
8.00 
-
-100.00 
R 
62.50 
60.00 
33.33 
40.00 
0.00 
37.50 
28.57 
14.29 
40.00 
-
-
60.00 
F 0.5 
56.82 
71.43 
71.43 
76.92 
0.00 
34.09 
66.67 
33.33 
9.52 
-
-
88.24 

ADV 

P 
6.67 
11.54 
0.00 
0.00 
0.00 
0.00 
0.00 
-
0.00 
4.76 
-
8.77 
R 
2.94 
20.45 
0.00 
0.00 
0.00 
0.00 
0.00 
-
0.00 
3.03 
-
12.50 
F 0.5 
5.32 
12.64 
0.00 
0.00 
0.00 
0.00 
0.00 
-
0.00 
4.27 
-
9.33 

CONJ 

P 
6.25 
0.00 
-
-
0.00 
0.00 
-
-
-
0.00 
-
0.00 
R 
7.69 
0.00 
-
-
0.00 
0.00 
-
-
-
0.00 
-
0.00 
F 0.5 
6.49 
0.00 
-
-
0.00 
0.00 
-
-
-
0.00 
-
0.00 

CONTR 

P 
29.17 
40.00 
46.15 
-
0.00 
-
-
33.33 
0.00 
66.67 
-
28.57 
R 
100.00 
33.33 
85.71 
-
0.00 
-
-
57.14 
0.00 
40.00 
-
33.33 
F 0.5 
33.98 
38.46 
50.85 
-
0.00 
-
-
36.36 
0.00 
58.82 
-
29.41 

DET 

P 
33.33 
36.16 
30.92 
21.43 
0.00 
36.03 
29.35 
26.09 
0.00 
43.88 
-
36.21 
R 
14.09 
43.03 
51.91 
0.92 
0.00 
28.46 
7.85 
49.41 
0.00 
12.54 
-
23.66 
F 0.5 
26.18 
37.35 
33.64 
3.92 
0.00 
34.21 
18.96 
28.81 
0.00 
29.25 
-
32.74 

MORPH 

P 
55.56 
59.15 
55.88 
28.57 
1.16 
27.87 
20.80 
27.78 
32.69 100.00 
40.00 
43.75 
R 
48.91 
47.73 
20.88 
5.41 
1.39 
21.52 
30.59 
12.50 
21.25 
2.74 
5.00 
15.91 
F 0.5 
54.09 
56.45 
41.85 
15.38 
1.20 
26.32 
22.22 
22.32 
29.51 
12.35 
16.67 
32.41 

NOUN 

P 
20.90 
25.27 
0.00 
28.57 
4.35 
0.00 
0.00 
10.00 
10.53 
0.00 
-
27.78 
R 
12.39 
19.49 
0.00 
2.20 
2.17 
0.00 
0.00 
1.92 
1.92 
0.00 
-
9.90 
F 0.5 
18.37 
23.86 
0.00 
8.40 
3.62 
0.00 
0.00 
5.43 
5.56 
0.00 
-
20.41 

NOUN:INFL 

P 
60.00 
60.00 
50.00 
-
25.00 100.00 
62.50 
66.67 
66.67 
0.00 
-
-
R 
85.71 
66.67 
71.43 
-
16.67 
33.33 
62.50 
57.14 
66.67 
0.00 
-
-
F 0.5 
63.83 
61.22 
53.19 
-
22.73 
71.43 
62.50 
64.52 
66.67 
0.00 
-
-

NOUN:NUM 

P 
49.42 
44.20 
44.06 
41.18 
14.38 
44.05 
29.39 
31.05 
29.00 
54.29 
-
44.29 
R 
56.14 
53.74 
59.49 
3.87 
11.28 
47.62 
42.54 
56.20 
36.45 
10.27 
-
16.94 
F 0.5 
50.63 
45.83 
46.47 
14.06 
13.63 
44.72 
31.33 
34.10 
30.23 
29.23 
-
33.48 

NOUN:POSS 

P 
20.00 
66.67 
-
-
-
-
14.29 
0.00 
0.00 
25.00 
-
50.00 
R 
14.29 
10.53 
-
-
-
-
5.26 
0.00 
0.00 
4.55 
-
5.00 
F 0.5 
18.52 
32.26 
-
-
-
-
10.64 
0.00 
0.00 
13.16 
-
17.86 

ORTH 

P 
60.00 
66.67 
73.81 
-
3.45 
0.00 
28.57 
49.32 
16.57 
-
-
50.00 
R 
11.11 
40.00 
59.62 
-
4.55 
0.00 
6.90 
64.29 
49.12 
-
-
17.24 
F 0.5 
31.91 
58.82 
70.45 
-
3.62 
0.00 
17.54 
51.72 
19.10 
-
-
36.23 

OTHER 

P 
20.34 
23.60 
10.34 
0.00 
2.33 
1.37 
14.29 
10.00 
0.00 
0.00 
-
11.58 
R 
6.92 
10.03 
0.83 
0.00 
0.31 
0.58 
0.58 
1.13 
0.00 
0.00 
-
3.15 
F 0.5 
14.65 
18.57 
3.14 
0.00 
1.01 
1.07 
2.49 
3.90 
0.00 
0.00 
-
7.54 

PART 

P 
71.43 
33.33 
25.00 
-
-
16.67 
-
-
-
50.00 
-
20.00 
R 
20.83 
15.38 
4.76 
-
-
21.74 
-
-
-
9.52 
-
11.11 
F 0.5 
48.08 
27.03 
13.51 
-
-
17.48 
-
-
-
27.03 
-
17.24 

PREP 

P 
47.56 
41.44 
33.33 
75.00 
0.00 
10.71 
-
21.74 
0.00 
36.59 
-
20.53 
R 
16.05 
35.66 
13.49 
1.44 
0.00 
12.35 
-
2.17 
0.00 
7.18 
-
13.36 
F 0.5 
34.15 
40.14 
25.76 
6.70 
0.00 
11.01 
-
7.76 
0.00 
20.11 
-
18.54 

PRON 

P 
41.18 
20.37 
0.00 
0.00 
11.11 
50.00 100.00 
27.27 
5.00 
0.00 
-
22.92 
R 
9.72 
13.41 
0.00 
0.00 
1.69 
2.82 
1.54 
4.62 
1.52 
0.00 
-
13.92 
F 0.5 
25.00 
18.46 
0.00 
0.00 
5.26 
11.49 
7.25 
13.76 
3.42 
0.00 
-
20.30 

PUNCT 

P 
25.00 
60.47 
37.21 100.00 
0.00 
44.83 
-
27.27 
0.00 
5.00 
-
43.02 
R 
3.52 
15.48 
10.60 
1.85 
0.00 
8.97 
-
6.34 
0.00 
0.96 
-
23.13 
F 0.5 
11.26 
38.24 
24.77 
8.62 
0.00 
24.90 
-
16.42 
0.00 
2.72 
-
36.71 

SPELL 

P 
76.92 
77.55 
0.00 
0.00 
25.00 
0.00 
44.17 
68.63 
73.98 
-
-100.00 
R 
63.83 
41.76 
0.00 
0.00 
4.23 
0.00 
71.29 
71.43 
85.85 
-
-
1.37 
F 0.5 
73.89 
66.20 
0.00 
0.00 
12.61 
0.00 
47.81 
69.17 
76.09 
-
-
6.49 

VERB 

P 
18.84 
15.12 
-
0.00 
7.69 
0.00 
14.29 
0.00 
0.00 
0.00 
-
16.33 
R 
8.23 
8.33 
-
0.00 
0.74 
0.00 
0.70 
0.00 
0.00 
0.00 
-
5.37 
F 0.5 
14.98 
13.00 
-
0.00 
2.66 
0.00 
2.94 
0.00 
0.00 
0.00 
-
11.59 

VERB:FORM 

P 
34.92 
36.36 
68.75 
0.00 
8.77 
35.11 
30.77 
25.00 
34.41 
28.57 
-
31.11 
R 
23.40 
25.00 
24.18 
0.00 
5.75 
35.11 
35.56 
3.45 
32.65 
4.65 
-
16.09 
F 0.5 
31.79 
33.33 
50.23 
0.00 
7.94 
35.11 
31.62 
11.11 
34.04 
14.08 
-
26.22 

VERB:INFL 

P 
100.00 100.00 
-
-100.00 100.00 
50.00 100.00 100.00 
-
0.00 
-
R 
100.00 100.00 
-
-
50.00 
50.00 
50.00 
50.00 100.00 
-
0.00 
-
F 0.5 100.00 100.00 
-
-
83.33 
83.33 
50.00 
83.33 100.00 
-
0.00 
-

VERB:SVA 

P 
49.09 
44.05 
54.80 
50.00 
24.56 
50.56 
56.25 
32.69 
35.56 
59.09 
81.58 
60.00 
R 
27.55 
32.74 
71.85 
1.12 
14.58 
67.16 
18.75 
17.35 
31.07 
13.83 
29.25 
15.00 
F 0.5 
42.45 
41.20 
57.53 
5.15 
21.60 
53.19 
40.18 
27.78 
34.56 
35.71 
60.08 
37.50 

VERB:TENSE 

P 
20.55 
26.27 
70.00 
66.67 
3.70 
31.25 
9.38 
20.00 
22.78 
14.81 100.00 
31.25 
R 
8.72 
17.51 
4.12 
1.25 
0.61 
2.98 
3.66 
2.31 
20.57 
2.45 
0.63 
12.05 
F 0.5 
16.16 
23.88 
16.67 
5.81 
1.84 
10.78 
7.14 
7.91 
22.30 
7.38 
3.05 
23.70 

WO 

P 
-
38.89 
0.00 
66.67 
-
-
-
0.00 
0.00 
-
-
41.18 
R 
-
33.33 
0.00 
14.29 
-
-
-
0.00 
0.00 
-
-
35.00 
F 0.5 
-
37.63 
0.00 
38.46 
-
-
-
0.00 
0.00 
-
-
39.77 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Precision, recall and F 0.5 for each team and error type. A dash indicates the team's system did 
not attempt to correct the given error type (TP+FP = 0). The highest F-score for each type is highlighted. 

799 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>There are 55 total possible error types. This table shows all of them except UNK, which 
indicates an uncorrected error. A dash indicates an impossible combination. 

B TP, FP and FN counts for various CoNLL-2014 results 

AMU 
CAMB 
CUUI 
IITB 
Type TP FP 
FN 
TP FP 
FN 
TP FP 
FN 
TP FP 
FN 
Missing 
58 
74 
347 131 154 
310 
77 215 
347 
2 
11 
336 
Replacement 428 722 1162 477 794 1219 381 449 1277 
20 
47 1320 
Unnecessary 
0 
0 
412 125 365 
330 158 304 
316 
6 
7 
385 

IPN 
NTHU 
PKU 
POST 
Type TP FP 
FN 
TP FP 
FN 
TP FP 
FN 
TP FP 
FN 
Missing 
1 
34 
339 
46 
88 
358 
16 
32 
350 
52 115 
344 
Replacement 
53 484 1319 299 784 1262 279 663 1243 312 629 1302 
Unnecessary 
0 
2 
389 
65 122 
342 
0 
1 
397 155 434 
317 

RAC 
SJTU 
UFC 
UMC 
Type TP FP 
FN 
TP FP 
FN 
TP FP 
FN 
TP FP 
FN 
Missing 
1 
65 
368 
15 
9 
323 
0 
0 
339 
99 148 
321 
Replacement 325 780 1236 
47 
46 1325 
36 
14 1326 143 269 1331 
Unnecessary 
0 
5 
407 
45 210 
351 
0 
0 
381 
74 365 
357 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>True Positive, False Positive and False Negative counts for each team in terms of Missing, 
Replacement and Unnecessary edits. The total number of edits may vary for each system, as this depends 
on the individual references that are chosen during evaluation. These results were used to make Table 5. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>True Positive, False Positive and False Negative counts for each error type for each team. 
These results were used to make Table 6. </table></figure>

			<note place="foot" n="1"> https://github.com/chrisjbryant/errant</note>

			<note place="foot" n="2"> Damerau-Levenshtein is an extension of Levenshtein that also handles transpositions; e.g. AB→BA</note>

			<note place="foot" n="3"> https://spacy.io/ 4 http://www.nltk.org/ 5 http://universaldependencies.org/tagset-conversion/ en-penn-uposf.html 6 https://sourceforge.net/projects/wordlist/files/speller/ 2017.01.22/</note>

			<note place="foot" n="7"> The distance is controlled by the max unchanged words parameter which is set to 2 by default.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How far are we from fully automatic high quality grammatical error correction?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1068" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="697" to="707" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N12-1067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Helping Our Own: The HOO 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards a standard evaluation method for grammatical error detection and correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic extraction of learner errors in ESL sentences using linguistically enhanced alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1079" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Kochmar</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-1702" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human evaluation of grammatical error correction systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gillian</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1052" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grammatical error detection using tagger disagreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anubhav</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-1706" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The AMU system in the CoNLL-2014 shared task: Grammatical error correction by dataintensive and feature-rich statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-1703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-2097" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="588" to="593" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadiwinoto</surname></persName>
		</author>
		<imprint>
			<pubPlace>Raymond Hendy Susanto, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W/W14/W14-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. ACL</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task. ACL<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Free-marginal multirater kappa: An alternative to Fleiss&apos; fixedmarginal multirater kappa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Justus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randolph</surname></persName>
		</author>
		<ptr target="http://files.eric.ed.gov/fulltext/ED490661.pdf" />
	</analytic>
	<monogr>
		<title level="m">Joensuu University Learning and Instruction Symposium</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compositional sequence labeling models for error detection in learner writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1112" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The IllinoisColumbia system in the CoNLL-2014 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-1704" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P16-1208" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reassessing the goals of grammatical error correction: Fluency instead of grammaticality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<ptr target="https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/800" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Correction detection and error type selection as an ESL educational aid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elif</forename><surname>Yamangil</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N12-1037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="357" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved correction detection in revised ESL sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huichao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-2098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="599" to="604" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
