<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Word Meta-Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wenpeng@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Word Meta-Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1351" to="1360"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings-distributed representations of words-in deep learning are beneficial for many tasks in NLP. However , different embedding sets vary greatly in quality and characteristics of the captured information. Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning metaembeddings. Experiments on word similarity and analogy tasks and on part-of-speech tagging show better performance of metaembeddings compared to individual embedding sets. One advantage of metaembeddings is the increased vocabulary coverage. We release our metaembeddings publicly at http:// cistern.cis.lmu.de/meta-emb.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, deep neural network (NN) models have achieved remarkable results in NLP <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014;</ref>.</p><p>One reason for these results are word embeddings, compact distributed word representations learned in an unsupervised manner from large corpora ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b19">Mnih and Hinton, 2009;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b20">Pennington et al., 2014</ref>).</p><p>Some prior work has studied differences in per- formance of different embedding sets. For exam- ple, <ref type="bibr" target="#b3">Chen et al. (2013)</ref> show that the embedding sets HLBL <ref type="bibr" target="#b19">(Mnih and Hinton, 2009</ref>), SENNA <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref>, <ref type="bibr" target="#b26">Turian (Turian et al., 2010</ref>) and Huang ( <ref type="bibr" target="#b12">Huang et al., 2012</ref>) have great variance in quality and characteristics of the semantics captured. <ref type="bibr" target="#b9">Hill et al. (2014;</ref><ref type="bibr" target="#b10">2015a)</ref> show that embeddings learned by NN machine transla- tion models can outperform three representative monolingual embedding sets: <ref type="bibr">word2vec (Mikolov et al., 2013b</ref>), GloVe ( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>) and CW <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref>. <ref type="bibr" target="#b0">Bansal et al. (2014)</ref> find that Brown clustering, SENNA, CW, Huang and word2vec yield significant gains for dependency parsing. Moreover, using these repre- sentations together achieved the best results, sug- gesting their complementarity. These prior stud- ies motivate us to explore an ensemble approach. Each embedding set is trained by a different NN on a different corpus, hence can be treated as a distinct description of words. We want to lever- age this diversity to learn better-performing word embeddings. Our expectation is that the ensemble contains more information than each component embedding set.</p><p>The ensemble approach has two benefits. First, enhancement of the representations: metaembed- dings perform better than the individual embed- ding sets. Second, coverage: metaembeddings cover more words than the individual embedding sets. The first three ensemble methods we intro- duce are CONC, SVD and 1TON and they directly only have the benefit of enhancement. They learn metaembeddings on the overlapping vocabulary of the embedding sets. CONC concatenates the vec- tors of a word from the different embedding sets. SVD performs dimension reduction on this con- catenation. 1TON assumes that a metaembedding for the word exists, e.g., it can be a randomly initialized vector in the beginning, and uses this metaembedding to predict representations of the word in the individual embedding sets by projec- tions -the resulting fine-tuned metaembedding is expected to contain knowledge from all individual embedding sets.</p><p>To also address the objective of increased cov- erage of the vocabulary, we introduce 1TON + , a modification of 1TON that learns metaembed- dings for all words in the vocabulary union in one step. Let an out-of-vocabulary (OOV) word w of embedding set ES be a word that is not cov- ered by ES (i.e., ES does not contain an embed- ding for w). 1 1TON + first randomly initializes the embeddings for OOVs and the metaembeddings, then uses a prediction setup similar to 1TON to update metaembeddings as well as OOV embed- dings. Thus, 1TON + simultaneously achieves two goals: learning metaembeddings and extending the vocabulary (for both metaembeddings and in- vidual embedding sets).</p><p>An alternative method that increases cover- age is MUTUALLEARNING. MUTUALLEARNING learns the embedding for a word that is an OOV in embedding set from its embeddings in other em- bedding sets. We will use MUTUALLEARNING to increase coverage for CONC, SVD and 1TON, so that these three methods (when used together with MUTUALLEARNING) have the advantages of both performance enhancement and increased coverage.</p><p>In summary, metaembeddings have two benefits compared to individual embedding sets: enhance- ment of performance and improved coverage of the vocabulary. Below, we demonstrate this ex- perimentally for three tasks: word similarity, word analogy and POS tagging.</p><p>If we simply view metaembeddings as a way of coming up with better embeddings, then the alter- native is to develop a single embedding learning algorithm that produces better embeddings. Some improvements proposed before have the disadvan- tage of increasing the training time of embedding learning substantially; e.g., the NNLM presented in ( <ref type="bibr" target="#b1">Bengio et al., 2003</ref>) is an order of magnitude less efficient than an algorithm like word2vec and, more generally, replacing a linear objective func- tion with a nonlinear objective function increases training time. Similarly, fine-tuning the hyperpa- rameters of the embedding learning algorithm is complex and time consuming. In terms of cover- age, one might argue that we can retrain an ex- isting algorithm like word2vec on a bigger cor- pus. However, that needs much longer training time than our simple ensemble approaches which achieve coverage as well as enhancement with less effort. In many cases, it is not possible to retrain using a different algorithm because the corpus is not publicly available. But even if these obsta- cles could be overcome, it is unlikely that there ever will be a single "best" embedding learn- ing algorithm. So the current situation of multi- ple embedding sets with different properties be- ing available is likely to persist for the forseeable future. Metaembedding learning is a simple and efficient way of taking advantage of this diver- sity. As we will show below they combine several complementary embedding sets and the resulting metaembeddings are stronger than each individual set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Related work has focused on improving perfor- mance on specific tasks by using several embed- ding sets simultaneously. To our knowledge, there is no work that aims to learn generally useful metaembeddings from individual embedding sets.</p><p>Tsuboi (2014) incorporates word2vec and GloVe embeddings into a POS tagging system and finds that using these two embedding sets together is better than using them individually. Similarly, <ref type="bibr" target="#b26">Turian et al. (2010)</ref> find that using Brown clus- ters, CW embeddings and HLBL embeddings for Name Entity Recognition and chunking tasks to- gether gives better performance than using these representations individually. <ref type="bibr" target="#b14">Luo et al. (2014)</ref> adapt CBOW ( <ref type="bibr" target="#b16">Mikolov et al., 2013a</ref>) to train word embeddings on differ- ent datasets -a Wikipedia corpus, search click- through data and user query data -for web search ranking and for word similarity. They show that using these embeddings together gives stronger re- sults than using them individually.</p><p>Both  and ( <ref type="bibr" target="#b32">Zhang et al., 2016</ref>) try to incorporate multiple embedding sets into channels of convolutional neural network system for sentence classification tasks. The bet- ter performance also hints the complementarity of component embedding sets, however, such kind of incorporation brings large numbers of training pa- rameters.</p><p>In sum, these papers show that using multiple embedding sets is beneficial. However, they ei- ther use embedding sets trained on the same cor- pus ( <ref type="bibr" target="#b26">Turian et al., 2010</ref>) or enhance embedding sets by more training data, not by innovative learn- ing algorithms ( <ref type="bibr" target="#b14">Luo et al., 2014</ref>), or make the system architectures more complicated <ref type="bibr">(Yin and Vocab Size Dim Training Data HLBL (Mnih and Hinton, 2009)</ref> 246,122 100 Reuters English newswire <ref type="bibr">August 1996</ref><ref type="bibr">-August 1997</ref><ref type="bibr" target="#b12">Huang (Huang et al., 2012</ref> 100,232 50 April 2010 snapshot of Wikipedia Glove ( <ref type="bibr" target="#b20">Pennington et al., 2014)</ref> 1,193,514 300 42 billion tokens of web data, from Common Crawl CW <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref> 268,810 200 Reuters English newswire August 1996-August 1997 word2vec ( <ref type="bibr" target="#b17">Mikolov et al., 2013b)</ref> 929,022 300 About 100 billion tokens from Google News <ref type="table">Table 1</ref>: Embedding Sets (Dim: dimensionality of word embeddings).</p><p>Schütze, 2015; <ref type="bibr" target="#b32">Zhang et al., 2016)</ref>. In our work, we can leverage any publicly available embed- ding set learned by any learning algorithm. Our metaembeddings (i) do not require access to re- sources such as large computing infrastructures or proprietary corpora; (ii) are derived by fast and simple ensemble learning from existing embed- ding sets; and (iii) have much lower dimensional- ity than a simple concatentation, greatly reducing the number of parameters in any system that uses them. An alternative to learning metaembeddings from embeddings is the MVLSA method that learns powerful embeddings directly from multi- ple data sources <ref type="bibr" target="#b22">(Rastogi et al., 2015)</ref>. <ref type="bibr" target="#b22">Rastogi et al. (2015)</ref> combine a large number of data sources and also run two experiments on the embedding sets Glove and word2vec. In contrast, our fo- cus is on metaembeddings, i.e., embeddings that are exclusively based on embeddings. The ad- vantages of metaembeddings are that they outper- form individual embeddings in our experiments, that few computational resources are needed, that no access to the original data is required and that embeddings learned by new powerful (includ- ing nonlinear) embedding learning algorithms in the future can be immediately taken advantage of without any changes being necessary to our basic framework. In future work, we hope to compare MVLSA and metaembeddings in effectiveness (Is using the original corpus better than using embed- dings in some cases?) and efficiency (Is using SGD or SVD more efficient and in what circum- stances?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Embedding Sets</head><p>In this work, we use five released embedding sets.  <ref type="table">Table 1</ref> gives a summary of the five embedding sets.</p><p>The intersection of the five vocabularies has size 35,965, the union has size 2,788,636.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ensemble Methods</head><p>This section introduces the four ensemble meth- ods: CONC, SVD, 1TON and 1TON + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CONC: Concatenation</head><p>In CONC, the metaembedding of w is the con- catenation of five embeddings, one each from the five embedding sets. For GloVe, we perform L2 normalization for each dimension across the vo- cabulary as recommended by the GloVe authors. Then each embedding of each embedding set is L2-normalized. This ensures that each embedding set contributes equally (a value between -1 and 1) when we compute similarity via dot product.</p><p>We would like to make use of prior knowl- edge and give more weight to well performing em- bedding sets. In this work, we give GloVe and word2vec weight i &gt; 1 and weight 1 to the other three embedding sets. We use MC30 <ref type="bibr" target="#b18">(Miller and Charles, 1991)</ref> as dev set, since all embedding sets fully cover it. We set i = 8, the value in <ref type="figure">Figure 1</ref> where performance reaches a plateau. After L2 normalization, GloVe and word2vec embeddings are multiplied by i and remaining embedding sets are left unchanged.</p><p>The dimensionality of CONC metaembeddings is k = 100+50+300+200+300 = 950. We also tried equal weighting, but the results were much worse, hence we skip reporting it. It nevertheless gives us insight that simple concatenation, without studying the difference among embedding sets, is unlikely to achieve enhancement. The main disad- vantage of simple concatenation is that word em- beddings are commonly used to initialize words in DNN systems; thus, the high-dimensionality of concatenated embeddings causes a great increase in training parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SVD: Singular Value Decomposition</head><p>We do SVD on above weighted concatenation vec- tors of dimension k = 950.</p><p>Given a set of CONC representations for n words, each of dimensionality k, we compute an SVD decomposition C = U SV T of the corre- sponding n × k matrix C. We then use U d , the first d dimensions of U , as the SVD metaembeddings of the n words. We apply L2-normalization to embeddings; similarities of SVD vectors are com- puted as dot products.</p><p>d denotes the dimensionality of metaembed- dings in SVD, 1TON and 1TON + . We use d = 200 throughout and investigate the impact of d be- low. rectangles denote known embeddings. The target to learn is the metaembedding (shown as shaded rectangle). Metaembeddings are initialized ran- domly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">1TON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: 1toN</head><p>Let c be the number of embedding sets under consideration, V 1 , V 2 , . . . , V i , . . . , V c their vocab- ularies and V ∩ = ∩ c i=1 V i the intersection, used as training set. Let V * denote the metaembedding space. We define a projection f * i from space V * to space V i (i = 1, 2, . . . , c) as follows:</p><formula xml:id="formula_0">ˆ w i = M * i w * (1)</formula><p>where</p><formula xml:id="formula_1">M * i ∈ R d i ×d , w * ∈ R d is the metaembed- ding of word w in space V * andˆwandˆ andˆw i ∈ R d i is the projected (or learned) representation of word w in space V i .</formula><p>The training objective is as follows:</p><formula xml:id="formula_2">E = c i=1 k i · ( w∈V ∩ | ˆ w i − w i | 2 + l 2 · |M * i | 2 ) (2)</formula><p>In Equation 2, k i is the weight scalar of the i th em- bedding set, determined in Section 4.1, i.e, k i = 8 for GloVe and word2vec embedding sets, other- wise k i = 1; l 2 is the weight of L2 normalization. The principle of 1TON is that we treat each in- dividual embedding as a projection of the metaem- bedding, similar to principal component analysis. An embedding is a description of the word based on the corpus and the model that were used to cre- ate it. The metaembedding tries to recover a more comprehensive description of the word when it is trained to predict the individual descriptions.</p><p>1TON can also be understood as a sentence modeling process, similar to DBOW ( <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>). The embedding of each word in a sentence s is a partial description of s. DBOW combines all partial descriptions to form a com- prehensive description of s. DBOW initializes the sentence representation randomly, then uses this representation to predict the representations of in- dividual words. The sentence representation of s corresponds to the metaembedding in 1TON; and the representations of the words in s correspond to the five embeddings for a word in 1TON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">1TON +</head><p>Recall that an OOV (with respect to embedding set ES) is defined as a word unknown in ES. 1TON + is an extension of 1TON that learns embeddings for OOVs; thus, it does not have the limitation that it can only be run on overlapping vocabulary.   to select the best ensemble method for a particular application. MUTUALLEARNING is applied to learn OOV embeddings for all c embedding sets; however, for ease of exposition, let us assume we want to compute embeddings for OOVs for embedding set j only, based on known embeddings in the other c − 1 embedding sets, with indexes i ∈ {1 . . . j − 1, j + 1 . . . c}. We do this by learning c − 1 map- pings f ij , each a projection from embedding set E i to embedding set E j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MUTUALLEARNING</head><p>Similar to Section 4.3, we train mapping f ij on the intersection V i ∩ V j of the vocabularies covered by the two embedding sets. Formally,</p><formula xml:id="formula_3">ˆ w j = f ij (w i ) = M ij w i where M ij ∈ R d j ×d i , w i ∈ R d i</formula><p>denotes the representation of word w in space V i andˆwandˆ andˆw j is the projected metaembed- ding of word w in space V j . Training loss has the same form as Equation 2 except that there is no " c i=1 k i " term. A total of c − 1 projections f ij are trained to learn OOV embeddings for embed- ding set j.</p><p>Let w be a word unknown in the vocabulary V j of embedding set j, but known in V 1 , V 2 , . . . , V k . To compute an embedding for w in V j , we first compute the k projections f 1j (w 1 ), f 2j (w 2 ), . . ., f kj (w k ) from the source spaces V 1 , V 2 , . . . , V k to the target space V j . Then, the element-wise aver- age of f 1j (w 1 ), f 2j (w 2 ), . . ., f kj (w k ) is treated as the representation of w in V j . Our motivation is that -assuming there is a true representation of w in V j and assuming the projections were learned well -we would expect all the projected vectors to be close to the true representation. Also, each source space contributes potentially complemen- tary information. Hence averaging them is a bal- ance of knowledge from all source spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We train NNs by back-propagation with AdaGrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>) and mini-batches. <ref type="table" target="#tab_1">Table 2</ref> gives hyperparameters.</p><p>We report results on three tasks: word similar- ity, word analogy and POS tagging.  <ref type="table">WS353 MC30  MEN  RW  sem.</ref> syn. tot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ind-full 1 HLBL 22.1 (1) 35.7 (3) 41.5 (0) 30.7 (128) 19.1 (892) 27.1 (423) 22.8 (198) 24.7 2 Huang 9.7 (3) 61.7 (18) 65.9 (0) 30.1 (0) 6.4 (982) 8.4 (1016) 11.9 (326) 10.4 3 GloVe 45.3 (0) 75.4 (18) 83.6 (0) 81.6 (0) 48.7 (21) 81.4 (0) 70.1 (0) 75.2 4 CW 15.6 (1) 28.4 (3) 21.7 (0) 25.7 (129) 15.3 (896) 17.4 (423) 5.0 (198) 10.5 5 W2V</head><p>44.2 (0) 69.8 (0) 78.9 (0) 78.2 (54) 53.4 <ref type="formula">(209)</ref>   <ref type="table">Table 3</ref>: Results on five word similarity tasks (Spearman correlation metric) and analogical reasoning (accuracy). The number of OOVs is given in parentheses for each result. "ind-full/ind-overlap": indi- vidual embedding sets with respective full/overlapping vocabulary; "ensemble": ensemble results using all five embedding sets; "discard": one of the five embedding sets is removed. If a result is better than all methods in "ind-overlap", then it is bolded. Significant improvement over the best baseline in "ind-overlap" is underlined (online toolkit from http://vassarstats.net/index.html for Spearman correlation metric, test of equal proportions for accuracy, p &lt; .05).</p><p>RW <ref type="formula">(21</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Word Similarity and Analogy Tasks</head><p>We evaluate on SimLex-999 ( <ref type="bibr" target="#b11">Hill et al., 2015b</ref>), WordSim353 ( <ref type="bibr" target="#b7">Finkelstein et al., 2001</ref>), MEN ( <ref type="bibr" target="#b2">Bruni et al., 2014</ref>) and RW ( <ref type="bibr" target="#b15">Luong et al., 2013</ref>). For completeness, we also show results for MC30, the validation set. The word analogy task proposed in ( <ref type="bibr" target="#b17">Mikolov et al., 2013b</ref>) consists of questions like, "a is to b as c is to ?". The dataset contains 19,544 such ques- tions, divided into a semantic subset of size 8869 and a syntactic subset of size 10,675. Accuracy is reported.</p><p>We also collect the state-of-the-art report for each task. SimLex-999: <ref type="figure">(Wieting et al., 2015)</ref>, WS353: <ref type="figure">(Halawi et al., 2012)</ref>. Not all state-of- the-art results are included in <ref type="table">Table 3</ref>. One reason is that a fair comparison is only possible on the shared vocabulary, so methods without released embeddings cannot be included. In addition, some prior systems can possibly generate better per- formance, but those literature reported lower re- sults than ours because different hyperparameter setup, such as smaller dimensionality of word em- beddings or different evaluation metric. In any case, our main contribution is to present ensem- ble frameworks which show that a combination of complementary embedding sets produces better- performing metaembeddings. <ref type="table">Table 3</ref> reports results on similarity and anal- ogy. Numbers in parentheses are the sizes of words in the datasets that are uncovered by inter- section vocabulary. We do not consider them for fair comparison. Block "ind-full" (1-5) lists the performance of individual embedding sets on the full vocabulary. Results on lines 6-34 are for the intersection vocabulary of the five embedding sets: "ind-overlap" contains the performance of individ- ual embedding sets, "ensemble" the performance of our four ensemble methods and "discard" the performance when one component set is removed.</p><p>The four ensemble approaches are very promis- ing <ref type="bibr">(31)</ref><ref type="bibr">(32)</ref><ref type="bibr">(33)</ref><ref type="bibr">(34)</ref>. For CONC, discarding HLBL, Huang or CW does not hurt performance: CONC (31), CONC(-HLBL) (11), CONC(-Huang) (12) and CONC(-CW) (14) beat each individual embedding set (6-10) in all tasks. GloVe contributes most in SimLex-999, WS353, MC30 and MEN; word2vec contributes most in RW and word analogy tasks. SVD (32) reduces the dimensionality of CONC from 950 to 200, but still gains performance in SimLex-999 and MEN. GloVe contributes most in SVD (larger losses on line 18 vs. lines <ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr">[19]</ref><ref type="bibr">[20]</ref>. Other embeddings contribute inconsistently.</p><p>1TON performs well only on word analogy, but it gains great improvement when discarding CW embeddings (24). 1TON + performs better than 1TON: it has stronger results when considering all embedding sets, and can still outperform individ- ual embedding sets while discarding HLBL (26), <ref type="bibr">Huang (27)</ref> or CW <ref type="bibr">(29)</ref>.</p><p>These results demonstrate that ensemble meth- ods using multiple embedding sets produce stronger embeddings. However, it does not mean the more embedding sets the better. Whether an embedding set helps, depends on the complemen- tarity of the sets and on the task. CONC, the simplest ensemble, has robust per- formance. However, size-950 embeddings as input means a lot of parameters to tune for DNNs. The other three methods (SVD, 1TON, 1TON + ) have the advantage of smaller dimensionality. SVD re- duces CONC's dimensionality dramatically and still is competitive, especially on word similar- ity. 1TON is competitive on analogy, but weak on word similarity. 1TON + performs consistently strongly on word similarity and analogy. <ref type="table">Table 3</ref> uses the metaembeddings of intersec- tion vocabulary, hence it shows directly the qual- ity enhancement by our ensemble approaches; this enhancement is not due to bigger coverage.</p><p>System comparison of learning OOV embed- dings. In <ref type="table" target="#tab_4">Table 4</ref>, we extend the vocabularies of each individual embedding set ("ind" block) and our ensemble approaches ("ensemble" block) to the vocabulary union, reporting results on RW and analogy -these tasks contain the most OOVs. As both word2vec and GloVe have full coverage on analogy, we do not rereport them in this table. This subtask is specific to "coverage" property. Appar- ently, our mutual learning and 1TON + can cover the union vocabulary, which is bigger than each in- dividual embedding sets. But the more important issue is that we should keep or even improve the embedding quality, compared with their original embeddings in certain component sets.</p><p>For each embedding set, we can compute the representation of an OOV (i) as a randomly initial- ized vector (RND); (ii) as the average of embed- dings of all known words (AVG); (iii) by MUTU- ALLEARNING (ml) and <ref type="formula">(iv)</ref>   <ref type="table">Table 5</ref>: POS tagging results on six target domains. "baselines" lists representative systems for this task, including FLORS. "+indiv / +meta": FLORS with individual embedding set / metaembeddings. Bold means higher than "baselines" and "+indiv".</p><p>would not make sense to replace these OOV em- beddings computed by 1TON + with embeddings computed by "RND/AVG/ml". Hence, we do not report "RND/AVG/ml" results for 1TON + . <ref type="table" target="#tab_4">Table 4</ref> shows four interesting aspects. (i) MU- TUALLEARNING helps much if an embedding set has lots of OOVs in certain task; e.g., MUTUAL- LEARNING is much better than AVG and RND on RW, and outperforms RND considerably for CONC, SVD and 1TON on analogy. However, it cannot make big difference for HLBL/CW on analogy, probably because these two embedding sets have much fewer OOVs, in which case AVG and RND work well enough. (ii) AVG produces bad results for CONC, SVD and 1TON on anal- ogy, especially in the syntactic subtask. We notice that those systems have large numbers of OOVs in word analogy task. If for analogy "a is to b as c is to d", all four of a, b, c, d are OOVs, then they are represented with the same average vector. Hence, similarity between b − a + c and each OOV is 1.0. In this case, it is almost impossible to predict the correct answer d. Unfortunately, methods CONC, SVD and 1TON have many OOVs, resulting in the low numbers in  <ref type="figure">Comparing 1TON-ml with 1TON + , 1TON</ref> + is better than "ml" on RW and semantic task, while performing worse on syntactic task. <ref type="figure">Figure 4</ref> shows the influence of dimensionality d for SVD, 1TON and 1TON + . Peak performance for different data sets and methods is reached for d ∈ <ref type="bibr">[100,</ref><ref type="bibr">500]</ref>. There are no big differences in the averages across data sets and methods for high enough d, roughly in the interval <ref type="bibr">[150,</ref><ref type="bibr">500]</ref>. In summary, as long as d is chosen to be large enough (e.g., ≥ 150), performance is robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Domain Adaptation for POS Tagging</head><p>In this section, we test the quality of those individ- ual embedding embedding sets and our metaem- beddings in a Part-of-Speech (POS) tagging task. For POS tagging, we add word embeddings into FLORS 7 ( <ref type="bibr" target="#b23">Schnabel and Schütze, 2014</ref>) which is the state-of-the-art POS tagger for unsupervised domain adaptation.</p><p>FLORS tagger. It treats POS tagging as a window-based (as opposed to sequence classifica- tion), multilabel classification problem using LIB- LINEAR, 8 a linear SVM. A word's representation consists of four feature vectors: one each for its suffix, its shape and its left and right distributional neighbors. Suffix and shape features are standard features used in the literature; our use of them in FLORS is exactly as described in ( <ref type="bibr" target="#b23">Schnabel and Schütze, 2014)</ref>.</p><p>Let f (w) be the concatenation of the two distri- butional and suffix and shape vectors of word w. Then FLORS represents token v i as follows:</p><formula xml:id="formula_4">f (v i−2 ) ⊕ f (v i−1 ) ⊕ f (v i ) ⊕ f (v i+1 ) ⊕ f (v i+2 )</formula><p>where ⊕ is vector concatenation. Thus, token v i is tagged based on a 5-word window.</p><p>FLORS is trained on sections 2-21 of Wall Street Journal (WSJ) and evaluate on the devel- opment sets of six different target domains: five SANCL ( <ref type="bibr" target="#b21">Petrov and McDonald, 2012</ref>) domains - newsgroups, weblogs, reviews, answers, emails - and sections 22-23 of WSJ for in-domain testing.</p><p>Original FLORS mainly depends on distribu- tional features. We insert word's embedding as the fifth feature vector. All embedding sets (except for 1TON + ) are extended to the union vocabulary by MUTUALLEARNING. We test if this additional feature can help this task. <ref type="table">Table 5</ref> gives results for some representa- 7 cistern.cis.lmu.de/flors (  8 liblinear.bwaldvogel. <ref type="bibr">de (Fan et al., 2008)</ref> tive systems ("baselines"), FLORS with individ- ual embedding sets ("+indiv") and FLORS with metaembeddings ("+meta"). Following conclu- sions can be drawn. (i) Not all individual embed- ding sets are beneficial in this task; e.g., HLBL embeddings make FLORS perform worse in 11 out of 12 cases. (ii) However, in most cases, embeddings improve system performance, which is consistent with prior work on using embed- dings for this type of task ( <ref type="bibr" target="#b28">Xiao and Guo, 2013;</ref><ref type="bibr" target="#b29">Yang and Eisenstein, 2014;</ref><ref type="bibr" target="#b25">Tsuboi, 2014)</ref>. (iii) Metaembeddings generally help more than the in- dividual embedding sets, except for SVD (which only performs better in 3 out of 12 cases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This work presented four ensemble methods for learning metaembeddings from multiple embed- ding sets: CONC, SVD, 1TON and 1TON + . Experiments on word similarity and analogy and POS tagging show the high quality of the metaembeddings; e.g., they outperform GloVe and word2vec on analogy. The ensemble meth- ods have the added advantage of increasing vo- cabulary coverage. We make our metaem- beddings available at http://cistern.cis. lmu.de/meta-emb.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(i) HLBL. Hierarchical log-bilinear (Mnih and Hinton, 2009) embeddings released by Turian et al. (2010); 2 246,122 word embeddings, 100 di- mensions; training corpus: RCV1 corpus (Reuters English newswire, August 1996 -August 1997). 2 metaoptimize.com/projects/wordreprs (ii) Huang. 3 Huang et al. (2012) incorporate global context to deal with challenges raised by words with multiple meanings; 100,232 word em- beddings, 50 dimensions; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe 4 (Pen- nington et al., 2014). 1,193,514 word embed- dings, 300 dimensions; training corpus: 42 billion tokens of web data, from Common Crawl. (iv) CW (Collobert and Weston, 2008). Released by Turian et al. (2010); 5 268,810 word embeddings, 200 dimensions; training corpus: same as HLBL. (v) word2vec (Mikolov et al., 2013b) CBOW; 6 929,022 word embeddings (we discard phrase embeddings), 300 dimensions; training corpus: Google News (about 100 billion words).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 depictsFigure 1 :</head><label>21</label><figDesc>Figure 2 depicts the simple neural network we employ to learn metaembeddings in 1TON. White</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 1toN + Figure 3 depicts 1TON +. In contrast to Figure 2, we assume that the current word is an OOV in embedding sets 3 and 5. Hence, in the new learning task, embeddings 1, 2, 4 are known, and embeddings 3 and 5 and the metaembedding are targets to learn. We initialize all OOV representations and metaembeddings randomly and use the same mapping formula as for 1TON to connect a metaembedding with the individual embeddings. Both metaembedding and initialized OOV embeddings are updated during training. Each embedding set contains information about only a part of the overall vocabulary. However, it can predict what the remaining part should look like by comparing words it knows with the information other embedding sets provide about these words. Thus, 1TON + learns a model of the dependencies between the individual embedding sets and can use these dependencies to infer what the embedding of an OOV should look like. CONC, SVD and 1TON compute metaembeddings only for the intersection vocabulary. 1TON + computes metaembeddings for the union of all individual vocabularies, thus greatly increasing the coverage of individual embedding sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Hyperparameters. bs: batch size; lr: learning rate; l 2 : L2 weight.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of effectiveness of four methods for learning OOV embeddings. RND: random 
initialization. AVG: average of embeddings of known words. ml: MUTUALLEARNING. RW(21) means 
there are still 21 OOVs for the vocabulary union. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>by 1TON + . 1TON + learns OOV embeddings for individual embedding sets and metaembeddings simultaneously, and it</figDesc><table>50 

100 
150 
200 
250 
300 
350 
400 
450 
500 
55 

60 

65 

70 

75 

80 

85 

90 

Dimension of svd 

Performance (%) 

WC353 

MC 

RG 

SCWS 

RW 

(a) Performance vs. d of SVD 

Dimension of O2M 

50 
100 
150 
200 
250 
300 
350 
400 
450 
500 

Performance (%) 

50 

55 

60 

65 

70 

75 

80 

85 

WC353 
MC 
RG 
SCWS 
RW 

(b) Performance vs. d of 1TON 

Dimension of O2M+ 

50 
100 
150 
200 
250 
300 
350 
400 
450 
500 

Performance (%) 

50 

55 

60 

65 

70 

75 

80 

85 

90 

WC353 
MC 
RG 
SCWS 
RW 

(c) Performance vs. d of 1TON + 

Figure 4: Influence of dimensionality 

newsgroups reviews 
weblogs 
answers 
emails 
wsj 
ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV 

baselines 
TnT 
88.66 54.73 90.40 56.75 93.33 74.17 88.55 48.32 88.14 58.09 95.76 88.30 
Stanford 
89.11 56.02 91.43 58.66 94.15 77.13 88.92 49.30 88.68 58.42 96.83 90.25 
SVMTool 
89.14 53.82 91.30 54.20 94.21 76.44 88.96 47.25 88.64 56.37 96.63 87.96 
C&amp;P 
89.51 57.23 91.58 59.67 94.41 78.46 89.08 48.46 88.74 58.62 96.78 88.65 
FLORS 
90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.15 89.44 62.61 96.59 90.37 

+indiv 

FLORS+HLBL 90.01 62.64 92.54 74.19 94.19 79.55 90.25 62.06 89.33 62.32 96.53 91.03 
FLORS+Huang 90.68 68.53 92.86 77.88 94.71 84.66 90.62 65.04 89.62 64.46 96.65 91.69 
FLORS+GloVe 90.99 70.64 92.84 78.19 94.69 86.16 90.54 65.16 89.75 65.61 96.65 92.03 
FLORS+CW 
90.37 69.31 92.56 77.65 94.62 84.82 90.23 64.97 89.32 65.75 96.58 91.36 
FLORS+W2V 90.72 72.74 92.50 77.65 94.75 86.69 90.26 64.91 89.19 63.75 96.40 91.03 

+meta 
FLORS+CONC 91.87 72.64 92.92 78.34 95.37 86.69 90.69 65.77 89.94 66.90 97.31 92.69 
FLORS+SVD 90.98 70.94 92.47 77.88 94.50 86.49 90.75 64.85 89.88 65.99 96.42 90.36 
FLORS+1TON 91.53 72.84 93.58 78.19 95.65 87.62 91.36 65.36 90.31 66.48 97.66 92.86 
FLORS+1TON + 91.52 72.34 93.14 78.32 95.65 87.29 90.77 65.28 89.93 66.72 97.14 92.55 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>(iii) MUTUALLEARN-
</table></figure>

			<note place="foot" n="1"> We do not consider words in this paper that are not covered by any of the individual embedding sets. OOV refers to a word that is covered by a proper subset of ESs.</note>

			<note place="foot" n="3"> ai.stanford.edu/ ˜ ehhuang 4 nlp.stanford.edu/projects/glove 5 metaoptimize.com/projects/wordreprs 6 code.google.com/p/Word2Vec</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of Deutsche Forschungsgemeinschaft (DFG): grant SCHU 2246/8-2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="809" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The expressive power of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Not all neural embeddings are born equal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Coline Devin, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Embedding word similarity with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Coline Devin, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="665" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-trained multi-view word embedding using two-side neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1982" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overview of the 2012 shared task on parsing the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SANCL</title>
		<meeting>SANCL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiview LSA: Representation learning via generalized CCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="556" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FLORS: Fast and simple domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural networks leverage corpuswide information for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="938" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multichannel variable-size convolution for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online updating of word representations for part-of-speech taggging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1329" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MGNC-CNN: A simple approach to exploiting multiple word embeddings for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
