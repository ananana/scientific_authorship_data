<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Objective Optimization for the Joint Disambiguation of Nouns and Named Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">DFKI Alt-Moabit 91c Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">DFKI Alt-Moabit 91c Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">DFKI Alt-Moabit 91c Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">DFKI Alt-Moabit 91c Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Objective Optimization for the Joint Disambiguation of Nouns and Named Entities</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="596" to="605"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present a novel approach to joint word sense disambiguation (WSD) and entity linking (EL) that combines a set of complementary objectives in an exten-sible multi-objective formalism. During disambiguation the system performs continuous optimization to find optimal probability distributions over candidate senses. The performance of our system on nominal WSD as well as EL improves state-of-the-art results on several corpora. These improvements demonstrate the importance of combining complementary objectives in a joint model for robust disambiguation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of automatically assigning the correct meaning to a given word or entity mention in a document is called word sense disambiguation (WSD) <ref type="bibr" target="#b23">(Navigli, 2009)</ref> or entity linking (EL) ( <ref type="bibr" target="#b1">Bunescu and Pasca, 2006</ref>), respectively. Suc- cessful disambiguation requires not only an un- derstanding of the topic or domain a document is dealing with, but also a deep analysis of how an in- dividual word is used within its local context. For example, the meanings of the word "newspaper", as in the company or the physical product, often cannot be distinguished by the global topic of the document it was mentioned in, but by recogniz- ing which type of meaning fits best into the local context of its mention. On the other hand, for an ambiguous entity mention such as a person name, e.g., "Michael Jordan", it is important to recognize the domain or topic of the wider context to distin- guish, e.g., between the basketball player and the machine learning expert.</p><p>The combination of the two most com- monly employed reference knowledge bases for WSD and EL, WordNet <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref> and <ref type="bibr">Wikipedia, in BabelNet (Navigli and Ponzetto, 2012)</ref>, has enabled a new line of research towards the joint disambiguation of words and named en- tities. <ref type="bibr">Babelfy (Moro et al., 2014</ref>) has shown the potential of combining these two tasks in a purely knowledge-driven approach that jointly finds connections between potential word senses on a global, document level. On the other hand, typical supervised methods ( <ref type="bibr" target="#b33">Zhong and Ng, 2010</ref>) trained on sense-annotated datasets are usually quite successful in dealing with individual words in their local context on a sentence level. <ref type="bibr" target="#b7">Hoffart et al. (2011)</ref> recognize the importance of combin- ing both local and global context for robust dis- ambiguation. However, their approach is limited to EL and optimization is performed in a discrete setting.</p><p>We present a system that combines disambigua- tion objectives for both global and local contexts into a single multi-objective function. The result- ing system is flexible and easily extensible with complementary objectives. In contrast to prior <ref type="bibr">work (Hoffart et al., 2011;</ref><ref type="bibr" target="#b19">Moro et al., 2014</ref>) we model the problem in a continuous setting based on probability distributions over candidate mean- ings instead of a binary treatment of candidate meanings during disambiguation. Our approach combines knowledge from various sources in one robust model. The system uses lexical and ency- clopedic knowledge for the joint disambiguation of words and named entities, and exploits local context information of a mention to infer the type of its meaning. We integrate prior statistics from surface strings to candidate meanings in a "nat- ural" way as starting probability distributions for each mention.</p><p>The contributions of our work are the following:</p><p>• a model for joint nominal WSD and EL that outperforms previous state-of-the-art systems on both tasks • an extensible framework for multi-objective disambiguation • an extensive evaluation of the approach on multiple standard WSD and EL datasets • the first work that employs continuous op- timization techniques for disambiguation (to our knowledge) • publicly available code, resources and models at https://bitbucket.org/ dfki-lt-re-group/mood</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Our system detects mentions in texts and disam- biguates their meaning to one of the candidate senses extracted from a reference knowledge base. The integral parts of the system, namely mention detection, candidate search and disambiguation are described in detail in this section. The model requires a tokenized, lemmatized and POS-tagged document as input; the output are sense-annotated mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Source</head><p>We employ BabelNet 2.5.1 as our reference knowledge base (KB). BabelNet is a multilingual semantic graph of concepts and named entities that are represented by synonym sets, called Ba- bel synsets. It is composed of lexical and encyclo- pedic resources, such as WordNet and Wikipedia. Babel synsets comprise several Babel senses, each of which corresponds to a sense in another knowl- edge base. For example the Babel synset of "Neil Armstrong" contains multiple senses in- cluding for example "armstrong#n#1" (WordNet), "Neil Armstrong" (Wikipedia). All synsets are in- terlinked by conceptual-semantic and lexical re- lations from WordNet and semantic relations ex- tracted from links between Wikipedia pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mention Extraction &amp; Entity Detection</head><p>We define a mention to be a sequence of tokens in a given document. The system extracts mentions for all content words (nouns, verbs, adjectives, ad- verbs) and multi-token units of up to 7 tokens that contain at least one noun. In addition, we apply a NER-tagger to identify named entity (NE) men- tions. Our approach distinguishes NEs from com- mon nouns because there are many common nouns also referring to NEs, making disambiguation un- necessarily complicated. For example, the word "moon" might refer to songs, films, video games, etc., but we should only consider these meanings if the occurrence suggests that it is used as a NE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Candidate Search</head><p>After potential mentions are extracted, the sys- tem tries to identify their candidate meanings, i.e., the appropriate synsets. Mentions without any candidates are discarded. There are various re- sources one can exploit to map surface strings to candidate meanings. However, existing methods or resources especially for NEs are either miss- ing many important mappings 1 or contain many noisy mappings 2 . Therefore, we created a can- didate mapping strategy that tries to avoid noisy mappings while including all potentially correct candidates. Our approach employs several heuris- tics that aim to avoid noise. Their union yields an almost complete mapping that includes the cor- rect candidate meaning for 97-100% of the exam- ples in the test datasets. Candidate mentions are mapped to synsets based on similarity of their sur- face strings or lemmas. If the surface string or lemma of a mention matches the lemma of a syn- onym in a synset that has the same part of speech, the synset will be considered as a candidate mean- ing. We allow partial matches for BabelNet syn- onyms derived from Wikipedia titles or redirec- tions. However, partial matching is restricted to synsets that belong either to the semantic category "Place" or "Agent". We make use of the seman- tic category information provided by the DBpe- dia ontology <ref type="bibr">3</ref> . A partial match allows the sur- face string of a mention to differ by up to 3 to- kens from the Wikipedia title (excluding every- thing in parentheses) if the partial string occurred at least once as an anchor for the corresponding Wikipedia page. E.g., for the Wikipedia title Arm- strong School District (Pennsylvania), the fol- lowing surface strings would be considered matches: "Armstrong School District (Pennsylva- nia)", "Armstrong School District", "Armstrong", but not "School" or "District", since they were never used as an anchor. If there is no match we try the same procedure applied to the lowercase forms of the surface string or the lemma. For persons we allow matches to all partial names, e.g., only first name, first and middle name, last name, etc. In addition to the aforementioned candidate ex- traction we also match surface strings to candidate entities mentioned on their respective disambigua-tion pages in Wikipedia <ref type="bibr">4</ref> . For cases where ad- jectives should be disambiguated as nouns, e.g., "English" as a country to "England", we allow candidate mappings through the pertainment rela- tion from WordNet. Finally, frequently annotated surface strings in Wikipedia are matched to their corresponding entities, where we stipulate "fre- quently" to mean that the surface string occurs at least 100 times as anchor in Wikipedia and the en- tity was either at least 100 times annotated by this surface string or it was annotated above average.</p><p>The distinction between nouns and NEs im- poses certain restrictions on the set of potential candidates. Candidate synsets for nouns are noun synsets considered as "Concepts" in BabelNet (as opposed to "Named Entities") in addition to all synsets of WordNet senses. On the other hand, candidate synsets for NEs comprise all nominal Babel synsets. Thus, the range of candidate sets for NEs properly contains the one for nouns. We include all nominal synsets as potential candidates for NEs because the distinction of NEs and sim- ple concepts is not always clear in BabelNet. For example the synset for "UN" (United Nations) is considered a concept whereas it could also be con- sidered a NE. Finally, if there is no candidate for a potential nominal mention, we try to find NE can- didates for it before discarding it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-Objective Disambiguation</head><p>We formulate the disambiguation as a continuous, multi-objective optimization problem. Individual objectives model different aspects of the disam- biguation problem. Maximizing these objectives means assigning high probabilities to candidate senses that contribute most to the combined ob- jective. After maximization, we select the candi- date meaning with the highest probability as the disambiguated sense. Our model is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Given a set of objectives O the overall objective function O is defined as the sum of all normalized objectives O ∈ O given a set of mentions M :</p><formula xml:id="formula_0">O(M ) = O∈O |M O | |M | · O(M ) O max (M ) − O min (M )</formula><p>.</p><p>(1) The continuous approach has several advan- tages over a discrete setting. First, we can ex-</p><formula xml:id="formula_1">Armstrong -Armstrong_(crater) 0.6 -Neil_Armstrong 0.2 -Louis_Armstrong 0.1 ... jazz -jazz_(music) 0.3 -jazz_(rhetoric) 0.3 -... Mentions M play -play_(game) 0.4 -play_(instrument) 0.2 -... Armstrong -Armstrong_(crater) 0.3 -Neil_Armstrong 0.1 -Louis_Armstrong 0.5 -...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mentions M Objectives</head><p>. . .</p><p>While not_converged or i &lt; max_iterations ploit well established continuous optimization al- gorithms, such as conjugate gradient or LBFGS. Second, by optimizing upon probability distribu- tions we are optimizing the actually desired result, in contrast to densest sub-graph algorithms where normalized confidence scores are calculated after- wards, e.g., <ref type="bibr" target="#b19">Moro et al. (2014)</ref>. Third, discrete optimization usually works on a single candidate per iteration whereas in a continuous setting, prob- abilities are adjusted for each candidate, which is computationally advantageous for highly ambigu- ous documents.</p><formula xml:id="formula_2">play -play_(game) 0.1 -play_(instrument) 0.6 -... jazz -jazz_(music) 0.8 -jazz_(rhetoric) 0.1 -...</formula><p>We normalize each objective using the differ- ence of its maximum and minimum value for a given document, which makes the weighting of the objectives different for each document. The maximum/minimum values can be calculated ana- lytically or, if this is not possible, by running the optimization algorithm with only the given objec- tive for an approximate estimate for the maximum and with its negated form for an approximate min- imum. Normalization is important for optimiza- tion because it ensures that the individual gradi- ents have similar norms on average for each ob- jective. Without normalization, optimization is bi- ased towards objectives with large gradients.</p><p>Given that one of the objectives can be applied to only a fraction of all mentions (e.g., only nomi- nal mentions), we scale each objective by the frac- tion of mentions it is applied to.</p><p>Note that our formulation could easily be ex- tended to using additional coefficients for each ob-jective. However, these hyper-parameters would have to be estimated on development data and therefore, this method could hurt generalization.</p><p>Prior Another advantage of working with prob- ability distributions over candidates is the easy in- tegration of prior information. For example, the word "Paris" without further context has a strong prior on its meaning as a city instead of a per- son. Our approach utilizes prior information in form of frequency statistics over candidate synsets for a mention's surface string. These priors are derived from annotation frequencies provided by WordNet and Wikipedia. We make use of oc- currence frequencies extracted by DBpedia Spot- light ( <ref type="bibr" target="#b3">Daiber et al., 2013</ref>) for synsets containing Wikipedia senses in case of NE disambiguation. For nominal WSD, we employ frequency statis- tics from WordNet for synsets containing Word- Net senses. Laplace-smoothing is applied to all prior frequencies. The priors serve as initializa- tion for the probability distributions over candi- date synsets. Note that we use priors "naturally", i.e., as actual priors for initialization only and not during disambiguation itself. They should not be applied during disambiguation because these pri- ors can be very strong and are not domain inde- pendent. However, they provide a good initializa- tion which is important for successful continuous optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Disambiguation Objectives</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Coherence Objective</head><p>Jointly disambiguating all mentions within a doc- ument has been shown to have a large impact on disambiguation quality, especially for named enti- ties ( <ref type="bibr" target="#b10">Kulkarni et al., 2009)</ref>. It requires a measure- ment of semantic relatedness between concepts that can for example be extracted from a semantic network like BabelNet. However, semantic net- works usually suffer from data sparsity where im- portant links between concepts might be missing. To deal with this issue, we adopt the idea of using semantic signatures from <ref type="bibr" target="#b19">Moro et al. (2014)</ref>. Fol- lowing their approach, we create semantic signa- tures for concepts and named entities by running a random walk with restart (RWR) in the seman- tic network. We count the times a vertex is vis- ited during RWR and define all frequently visited vertices to be the semantic signature (i.e., a set of highly related vertices) of the starting concept or named entity vertex.</p><p>Our coherence objective aims at maximizing the semantic relatedness among selected candidate senses based on their semantic signatures S c . We define the continuous objective using probability distributions p m (c) over the candidate set C m of each mention m ∈ M in a document as follows:</p><formula xml:id="formula_3">O coh (M ) = m∈M c∈Cm m ∈M m =m c ∈C m s(m, c, m , c ) s(m, c, m , c ) = p m (c) · p m (c ) · 1((c, c ) ∈ S) p m (c) = e wm,c c ∈Cm e w m,c ,<label>(2)</label></formula><p>where 1 denotes the indicator function and p m (c)</p><p>is a softmax function. The only free, optimizable parameters are the softmax weights w m . This ob- jective includes all mentions, i.e., M O coh = M . It can be interpreted as finding the densest subgraph where vertices correspond to mention-candidate pairs and edges to semantic signatures between candidate synsets. However, in contrast to a dis- crete setup, each vertex is now weighted by its probability and therefore each edge is weighted by the product of its adjacent vertex probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Type Objective</head><p>One of the biggest problems for supervised ap- proaches to WSD is the limited size and synset coverage of available training datasets such as SemCor ( <ref type="bibr" target="#b17">Miller et al., 1993)</ref>. One way to cir- cumvent this problem is to use a coarser set of se- mantic classes that groups synsets together. Pre- vious studies on using semantic classes for dis- ambiguation showed promising results <ref type="bibr">(IzquierdoBeviá et al., 2006</ref>). For example, WordNet pro- vides a mapping, called lexnames, of synsets into 45 types, which is based on the syntactic cate- gories of synsets and their logical groupings <ref type="bibr">5</ref> . In WordNet 13.5% of all nouns are ambiguous with an average ambiguity of 2.79 synsets per lemma. Given a noun and a type (lexname), the percentage of ambiguous nouns drops to 7.1% for which the average ambiguity drops to 2.33. This indicates that exploiting type classification for disambigua- tion can be very useful.</p><p>Similarly, for EL it is important to recognize the type of an entity mention in a local context. For example, in the phrase "London beats Manch- ester" it is very likely that the two city names refer to sports clubs and not to the cities. We utilize an existing mapping from Wikipedia pages to types from the DBpedia ontology, restricting the set of target types to the following: "Activity", "Organ- isation", "Person", "Event", "Place" and "Misc" for the rest.</p><p>We train a multi-class logistic regression model for each set of types that calculates probability distributions q m (t) over WN-or DBpedia-types t given a noun-or a NE-mention m, respectively. The features used as input to the model are the fol- lowing:</p><p>• word embedding of mention's surface string Type classification is included as an objective in the model as defined in equation 3. It puts type specific weights derived from type classification on candidate synsets, enforcing candidates of fit- ting type to have higher probabilities. The objec- tive is only applied to noun, NE and verb men- tions, i.e.,</p><formula xml:id="formula_4">M Otyp = M n ∪ M N E ∪ M v . O typ (M ) = m∈M O typ c∈Cm q m (t c ) · p m (c) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularization Objective</head><p>Because candidate priors for NE mentions can be very high, we add an additional L2-regularization objective for NE mentions:</p><formula xml:id="formula_5">O L2 (M ) = − λ 2 m∈M N E w m 2 2<label>(4)</label></formula><p>The regularization objective is integrated in the overall objective function as it is, i.e., it is not nor- malized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|D| |M| KB SemEval-2015-13 (Sem15) 4 757 BN (to be published) SemEval-2013-12 (Sem13) 13 1931 BN SemEval-2013-12 (Sem13) 13 1644 WN (Navigli et al., 2013) SemEval-2007-17 (Sem07) 3 159 WN (Pradhan et al., 2007) Senseval 3 (Sen3) 4 886 WN (Snyder and Palmer, 2004) AIDA-CoNLL-testb (AIDA) 216 4530 Wiki</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated our approach on 7 different datasets, comprising 3 WSD datasets annotated with Word- Net senses, 2 datasets annotated with Wikipedia articles for EL and 2 more recent datasets anno- tated with Babel synsets. <ref type="table">Table 1</ref> contains a list of all datasets.</p><p>Besides these test datasets we used SemCor ( <ref type="bibr" target="#b17">Miller et al., 1993)</ref> as training data for WSD and the training part of the AIDA CoNLL dataset for EL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>For the creation of semantic signatures we choose the same parameter set as defined by <ref type="bibr" target="#b19">Moro et al. (2014)</ref>. We run the random walk with a restart probability of 0.85 for a total of 1 million steps for each vertex in the semantic graph and keep ver- tices visited at least 100 times as semantic signa- tures.</p><p>The L2-regularization objective for named enti- ties is employed with λ = 0.001, which we found to perform best on the training part of the AIDA- CoNLL dataset.</p><p>We trained the multi-class logistic regression model for WN-type classification on SemCor and for DBpedia-type classification on the training part of the AIDA-CoNLL dataset using LBFGS and L2-Regularization with λ = 0.01 until con- vergence.</p><p>Our system optimizes the combined multi- objective function using Conjugate Gradient System KB Description IMS ( <ref type="bibr" target="#b33">Zhong and Ng, 2010)</ref> WN supervised, SVM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KPCS (Hoffart et al., 2011)</head><p>Wiki greedy densest-subgraph on combined mention-entity, entity-entity measures <ref type="bibr">KORE (Hoffart et al., 2012)</ref> Wiki extension of KPCS with keyphrase relatedness mea- sure between entities MW <ref type="bibr" target="#b18">(Milne and Witten, 2008)</ref> Wiki Normalized Google Dis- tance Babelfy ( <ref type="bibr" target="#b19">Moro et al., 2014</ref>) BN greedy densest-subgraph on semantic signatures <ref type="table">Table 2</ref>: Systems used for comparison during eval- uation.</p><p>( <ref type="bibr" target="#b6">Hestenes and Stiefel, 1952</ref>) with up to a maxi- mum of 1000 iterations per document. We utilized existing implementations from FACTORIE version 1.1 ( <ref type="bibr" target="#b14">McCallum et al., 2009)</ref> for logistic regression, NER tagging and Conju- gate Gradient optimization. For NER tagging we used a pre-trained stacked linear-chain CRF <ref type="bibr" target="#b11">(Lafferty et al., 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Systems</head><p>We compare our approach to state-of-the-art re- sults on all datasets and a most frequent sense (MFS) baseline. The MFS baseline selects the candidate with the highest prior as described in section 2.4. <ref type="table">Table 2</ref> contains a list of all sys- tems we compared against. We use Babelfy as our main baseline, because of its state-of-the-art per- formance on all datasets and because it also em- ployed BabelNet as its sense inventory. Note that Babelfy achieved its results with different setups for WSD and EL, in contrast to our model, which uses the same setup for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">General Results</head><p>We report the performance of all systems in terms of F1-score. To ensure fairness we restricted the candidate sets of the target mentions in each dataset to candidates of their respective reference KB. Note that our candidate mapping strategy en- sures for all datasets a 97%−100% chance that the target synset is within a mention's candidate set.</p><p>This section presents results on the evaluation datasets divided by their respective target KBs: WordNet, Wikipedia and BabelNet.</p><p>WordNet <ref type="table" target="#tab_2">Table 3</ref> shows the results on three datasets for the disambiguation of nouns to Word-   <ref type="table">Table 4</ref>: Results for NEs on Wikipedia annotated datasets.</p><p>Net. Our approach exhibits state-of-the-art re- sults outperforming all other systems on two of the three datasets. The model performs slightly worse on the Senseval 3 dataset because of one docu- ment in particular where the F1 score is very low compared to the MFS baseline. On the other three documents, however, it performs as good or even better. In general, results from the literature are al- ways worse than the MFS baseline on this dataset. A strong improvement can be seen on the SemEval 2013 Task 12 dataset (Sem13), which is also the largest dataset. Our system achieves an improve- ment of nearly 7% F1 over the best other system, which translates to an error reduction of roughly 20% given that every word mention gets anno- tated. Besides the results presented in <ref type="table" target="#tab_2">Table 3</ref>, we also evaluated the system on the SemEval 2007 Task 7 dataset for coarse grained WSD, where it achieved 85.5% F1 compared to the best previ- ously reported result of 85.5% F1 from <ref type="bibr" target="#b26">Ponzetto et al. (2010)</ref> and Babelfy with 84.6%.</p><p>Wikipedia The performance on entity linking was evaluated against state-of-the-art systems on two different datasets. The results in <ref type="table">Table 4</ref> demonstrate that our model can compete with the best existing models, showing superior results es- pecially on the large AIDA CoNLL 6 test dataset comprising 216 news texts, where we achieve an error reduction of about 16%, resulting in a new state-of-the-art of 85.1% F1. On the other hand, our system is slightly worse on the KORE dataset compared to Babelfy (6 errors more in to- tal), which might be due to the strong priors and   the small context. However, the dataset is rather small, containing only 50 sentences, and has been artificially tailored to the use of highly ambiguous entity mentions. For example, persons are most of the time only mentioned by their first names.</p><p>It is an interesting dataset because it requires the system to employ a lot of background knowledge about mentioned entities.</p><p>BabelNet <ref type="table" target="#tab_4">Table 5</ref> shows the results on the 2 ex- isting BabelNet annotated datasets. To our knowl- edge, our system shows the best performance on both datasets in the literature. An interesting ob- servation is that the F1 score on SemEval 2013 with BabelNet as target KB is lower compared to WordNet as target KB. The reason is that ambigu- ity rises for nominal mentions by including con- cepts from Wikipedia that do not exist in WordNet. For example, the Wikipedia concept "formal lan- guage" becomes a candidate for the surface string "language".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detailed Results</head><p>We also experimented with different objective combinations, namely "type only" (O typ ), "coher- ence only" (O coh + O L2 ) and "all" (O coh + O typ + O L2 ), to evaluate the impact of the different objec- tives. <ref type="table" target="#tab_5">Table 6</ref> shows results of employing individ- ual configurations compared to the MFS baseline. Results for only using coherence or type exhibit varying performance on the datasets, but still con- sistently exceed the strong MFS baseline. Com- bining both objectives always yields better results compared to all other configurations. This find- ing is important because it proves that the objec- tives proposed in this work are indeed comple- mentary, and thus demonstrates the significance of combining complementary approaches in one ro- bust framework such as ours.</p><p>An additional observation was that DBpedia- type classification slightly overfitted on the AIDA CoNLL training part. When removing DBpedia- type classification from the type objective, results increased marginally on some datasets except for the AIDA CoNLL dataset, where results decreased by roughly 3% F1. The improvements of using DBpedia-type classification are mainly due to the fact that the classifier is able to correctly clas- sify names of places in tables consisting of sports scores not to the "Place" type but to the "Organi- zation" type. Note that the AIDA CoNLL dataset (train and test) contains many of those tables. This shows that including supervised objectives into the system helps when data is available for the do- main.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Generalization</head><p>We evaluated the ability of our system to gener- alize to different domains based on the SemEval 2015 Task 13 dataset. It includes documents from the bio-medical, the math&amp;computer and general domains. Our approach performs particularly well on the bio-medical domain with 86.3% F1 (MFS: 77.3%). Results on the math&amp;computer domain (58.8% F1, MFS: 57.0%), however, reveal that performance still strongly depends on the docu- ment topic. This indicates that either the employed resources do not cover this domain as well as oth- ers, or that it is generally more difficult to dis- ambiguate. Another potential explanation is that enforcing only pairwise coherence does not take the hidden concepts computer and maths into ac- count, which connect all concepts, but are never actually mentioned. An interesting point for future research might be the introduction of an additional objective or the extension of the coherence objec- tive to allow indirect connections between candi- date meanings through shared topics or categories.</p><p>Besides these very specific findings, the model's ability to generalize is strongly supported by its good results across all datasets, covering a variety of different topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>WSD Approaches to WSD can be distinguished by the kind of resource exploited. The two main resources for WSD are sense annotated datasets and knowledge bases. Typical supervised ap-proaches like IMS ( <ref type="bibr" target="#b33">Zhong and Ng, 2010)</ref> train classifiers that learn from existing, annotated ex- amples. They suffer from the sparsity of sense annotated datasets that is due to the data acqui- sition bottleneck <ref type="bibr" target="#b25">(Pilehvar and Navigli, 2014)</ref>. There have been approaches to overcome this issue through the automatic generation of such resources based on bootstrapping <ref type="bibr" target="#b24">(Pham et al., 2005</ref>), sentences containing unambiguous rela- tives of senses ( <ref type="bibr" target="#b13">Martinez et al., 2008)</ref> or exploit- ing Wikipedia <ref type="bibr" target="#b29">(Shen et al., 2013</ref>). On the other hand, knowledge-based approaches achieve good performances rivaling state-of-the-art supervised systems ( <ref type="bibr" target="#b26">Ponzetto and Navigli, 2010)</ref> by using ex- isting structured knowledge <ref type="bibr" target="#b12">(Lesk, 1986;</ref><ref type="bibr" target="#b0">Agirre et al., 2014</ref>), or take advantage of the structure of a given semantic network through connectivity or centrality measures ( <ref type="bibr" target="#b31">Tsatsaronis et al., 2007;</ref><ref type="bibr" target="#b20">Navigli and Lapata, 2010)</ref>. Such systems benefit from the availability of numerous KBs for a variety of domains. We believe that both knowledge-based approaches and supervised methods have unique, complementary abilities that need to be combined for sophisticated disambiguation.</p><p>EL Typical EL systems employ supervised ma- chine learning algorithms to classify or rank can- didate entities ( <ref type="bibr" target="#b1">Bunescu and Pasca, 2006;</ref><ref type="bibr" target="#b18">Milne and Witten, 2008;</ref><ref type="bibr" target="#b32">Zhang et al., 2010)</ref>. Com- mon features include popularity metrics based on Wikipedia's graph structure or on name mention frequency ( <ref type="bibr">Dredze et al., 2010;</ref><ref type="bibr" target="#b5">Han and Zhao, 2009)</ref>, similarity metrics exploring Wikipedia's concept relations <ref type="bibr" target="#b5">(Han and Zhao, 2009)</ref>, and string similarity features. <ref type="bibr">Mihalcea and Csomai (2007)</ref> disambiguate each mention independently given its sentence level context only. In contrast, <ref type="bibr" target="#b2">Cucerzan (2007)</ref> and <ref type="bibr" target="#b10">Kulkarni et al. (Kulkarni et al., 2009</ref>) recognize the interdependence be- tween entities in a wider context. The most sim- ilar work to ours is that of <ref type="bibr" target="#b7">Hoffart et al. (2011)</ref> which was the first that combined local and global context measures in one robust model. However, objectives and the disambiguation algorithm differ from our work. They represent the disambigua- tion task as a densest subgraph problem where the least connected entity is eliminated in each itera- tion. The discrete treatment of candidate entities can be problematic especially at the beginning of disambiguation where it is biased towards men- tions with many candidates. <ref type="bibr">Babelfy (Moro et al., 2014</ref>) is a knowledge- based approach for joint WSD and EL that also uses a greedy densest subgraph algorithm for dis- ambiguation. It employs a single coherence model based on semantic signatures similar to our coher- ence objective. The system's very good perfor- mance indicates that the semantic signatures pro- vide a powerful resource for joint disambiguation. However, because we believe it is not sufficient to only enforce semantic agreement among nouns and entities, our approach includes an objective that also focuses on the local context of mentions, making it more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions &amp; Future Work</head><p>We have presented a novel approach for the joint disambiguation of nouns and named enti- ties based on an extensible framework. Our sys- tem employs continuous optimization on a multi- objective function during disambiguation. The integration of complementary objectives into our formalism demonstrates that robust disambigua- tion can be achieved by considering both the local and the global context of a mention. Our model outperforms previous state-of-the-art systems for nominal WSD and for EL. It is the first system that achieves such results on various WSD and EL datasets using a single setup.</p><p>In future work, new objectives should be inte- grated into the framework and existing objectives could be enhanced. For example, it would be in- teresting to express semantic relatedness contin- uously rather than in a binary setting for the co- herence objective. Additionally, using the entire model during training could ensure better com- patibility between the different objectives. At the moment, the model itself is composed of different pre-trained models that are only combined during disambiguation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our multi-objective approach to WSD &amp; EL for the example sentence: Armstrong plays jazz. Mentions are disambiguated by iteratively updating probability distributions over their candidate senses with respect to the given objective gradients O i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>sum of word embeddings of all sentence words excluding stopwords • word embedding of the dependency parse parent • collocations of surrounding words as in Zhong et al. (2010) • POS tags with up to 3 tokens distance to m • possible types of candidate synsets We employed pre-trained word embeddings from Mikolov et al. (2013) instead of the words them- selves to increase generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Results for nouns on WordNet annotated datasets.</head><label>3</label><figDesc></figDesc><table>System 
AIDA KORE 
MFS 
70.1 
35.4 
KPCS 
82.2 
55.6 
KORE-LSH-G 
81.8 
64.6 
MW 
82.3 
57.6 
Babelfy 
82.1 
71.5 
Our 
85.1 
67.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : Results for nouns and NEs on BabelNet annotated datasets.</head><label>5</label><figDesc></figDesc><table>System 
Sem13 Sem15 AIDA 
MFS 
66.7 
71.1 
70.1 
Otyp 
68.1 
73.8 
78.0 
O coh + OL2 
68.1 
69.6 
82.7 
O coh + Otyp + OL2 
71.5 
75.4 
85.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Detailed results for nouns and NEs on 
BabelNet annotated datasets and AIDA CoNLL. 

</table></figure>

			<note place="foot" n="1"> e.g., using only the synonyms of a synset 2 e.g., partial matches for all synonyms of a synset 3 http://wiki.dbpedia.org/Ontology</note>

			<note place="foot" n="4"> provided by DBpedia at http://wiki.dbpedia. org/Downloads2014</note>

			<note place="foot" n="5"> http://wordnet.princeton.edu/man/ lexnames.5WN.html</note>

			<note place="foot" n="6"> the largest, freely available dataset for EL.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research was partially supported by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (01IW14002), BBDC (01IS14013E), and by the German Federal Ministry of Economics and Energy (BMWi) through the project SD4M (01MD15007B), and by Google through a Fo-cused Research Award granted in July 2013.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random walks for knowledge-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Bunescu and Pasca2006</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale named entity disambiguation based on wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving efficiency and accuracy in multilingual entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Daiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd International Conference on Computational Linguistics</title>
		<editor>Dredze et al.2010] Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin</editor>
		<meeting>of the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="277" to="285" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Named entity disambiguation by leveraging wikipedia semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao2009] Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th ACM conference on Information and knowledge management</title>
		<meeting>of the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Methods of conjugate gradients for solving linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stiefel1952] Magnus</forename><surname>Hestenes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Hestenes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Stiefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kore: keyphrase overlap relatedness for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Ba Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>of the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
	<note>Hoffart et al.2012</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spanish all-words semantic class disambiguation using cast3lb corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Izquierdo-Beviá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICAI 2006: Advances in Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collective annotation of wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th annual international conference on Systems documentation</title>
		<meeting>of the 5th annual international conference on Systems documentation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the use of automatically acquired examples for all-nouns word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="79" to="107" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FACTORIE: Probabilistic programming via imperatively defined factor graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wikify!: linking documents to encyclopedic knowledge</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>of the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
	<note>Rada Mihalcea and Andras Csomai</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the workshop on Human Language Technology</title>
		<meeting>of the workshop on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th ACM conference on Information and knowledge management</title>
		<meeting>of the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: A unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Moro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An experimental study of graph connectivity for unsupervised word sense disambiguation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="678" to="692" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 12: Multilingual word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (SEM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word sense disambiguation with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the national conference on artificial intelligence</title>
		<editor>Thanh Phong Pham, Hwee Tou Ng, and Wee Sun Lee</editor>
		<meeting>of the national conference on artificial intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page">1093</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A large-scale pseudoword-based evaluation framework for stateof-the-art word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="837" to="881" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Pilehvar and Navigli2014</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge-rich word sense disambiguation rivaling supervised systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>of the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1522" to="1531" />
		</imprint>
	</monogr>
	<note>Ponzetto and Navigli2010</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">English lexical sample, srl and all words</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>of the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coarse to fine grained sense disambiguation in wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SEM</title>
		<meeting>of SEM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The english all-words task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word sense disambiguation with spreading activation networks generated from thesauri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsatsaronis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1725" to="1730" />
		</imprint>
	</monogr>
	<note>George Tsatsaronis, Michalis Vazirgiannis, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Entity linking leveraging: automatically generated annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd International Conference on Computational Linguistics</title>
		<meeting>of the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1290" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng2010] Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL 2010 System Demonstrations</title>
		<meeting>of the ACL 2010 System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
