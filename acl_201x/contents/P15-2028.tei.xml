<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hassle-Free Unsupervised Domain Adaptation Method Using Instance Similarity Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Systems</orgName>
								<orgName type="department" key="dep2">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<email>jingjiang@smu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Management University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Hassle-Free Unsupervised Domain Adaptation Method Using Instance Similarity Features</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="168" to="173"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a simple yet effective unsu-pervised domain adaptation method that can be generally applied for different NLP tasks. Our method uses unlabeled target domain instances to induce a set of instance similarity features. These features are then combined with the original features to represent labeled source domain instances. Using three NLP tasks, we show that our method consistently out-performs a few baselines, including SCL, an existing general unsupervised domain adaptation method widely used in NLP. More importantly, our method is very easy to implement and incurs much less computational cost than SCL.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distri- bution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has pro- posed both techniques designed for specific NLP tasks <ref type="bibr" target="#b3">(Chan and Ng, 2007;</ref><ref type="bibr" target="#b5">Daume III and Jagarlamudi, 2011;</ref><ref type="bibr" target="#b21">Yang et al., 2012;</ref><ref type="bibr" target="#b16">Plank and Moschitti, 2013;</ref><ref type="bibr" target="#b11">Hu et al., 2014;</ref><ref type="bibr" target="#b14">Nguyen and Grishman, 2014</ref>) and general approaches applicable to different tasks ( <ref type="bibr" target="#b1">Blitzer et al., 2006;</ref><ref type="bibr" target="#b7">Daumé III, 2007;</ref><ref type="bibr" target="#b12">Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b8">Dredze and Crammer, 2008;</ref><ref type="bibr" target="#b18">Titov, 2011)</ref>. With the recent trend of applying deep learn- ing in NLP, deep learning-based domain adap- tation methods <ref type="bibr" target="#b9">(Glorot et al., 2011;</ref><ref type="bibr" target="#b4">Chen et al., 2012;</ref><ref type="bibr" target="#b19">Yang and Eisenstein, 2014</ref>) have also been adopted for NLP tasks <ref type="bibr" target="#b20">(Yang and Eisenstein, 2015)</ref>.</p><p>There are generally two settings of domain adaptation. We use supervised domain adaptation to refer to the setting when a small amount of la- beled target data is available, and when no such data is available during training we call it unsu- pervised domain adaptation.</p><p>Although many domain adaptation methods have been proposed, for practitioners who wish to avoid implementing or tuning sophisticated or computationally expensive methods due to either lack of enough machine learning background or limited resources, simple approaches are often more attractive. A notable example is the frus- tratingly easy domain adaptation method proposed by <ref type="bibr" target="#b7">Daumé III (2007)</ref>, which simply augments the feature space by duplicating features in a clever way. However, this method is only suit- able for supervised domain adaptation. A later semi-supervised version of this easy adaptation method uses unlabeled data from the target do- main <ref type="bibr" target="#b6">(Daumé III et al., 2010)</ref>, but it still requires some labeled data from the target domain. In this paper, we propose a general unsupervised domain adaptation method that is almost equally hassle- free but does not use any labeled target data.</p><p>Our method uses a set of unlabeled target in- stances to induce a new feature space, which is then combined with the original feature space. We explain analytically why the new feature space may help domain adaptation. Using a few dif- ferent NLP tasks, we then empirically show that our method can indeed learn a better classifier for the target domain than a few baselines. In partic- ular, our method performs consistently better than or competitively with Structural Correspondence Learning (SCL) <ref type="bibr" target="#b1">(Blitzer et al., 2006</ref>), a well- known unsupervised domain adaptation method in NLP. Furthermore, compared with SCL and other advanced methods such as the marginalized struc- tured dropout method <ref type="bibr" target="#b19">(Yang and Eisenstein, 2014</ref>) and a recent feature embedding method <ref type="bibr" target="#b20">(Yang and Eisenstein, 2015)</ref>, our method is much easier to implement.</p><p>In summary, our main contribution is a simple, effective and theoretically justifiable unsupervised domain adaptation method for NLP problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adaptation with Similarity Features</head><p>We first introduce the necessary notation needed for presenting our method. Without loss of gen- erality, we assume a binary classification problem where each input is represented as a feature vec- tor x from an input vector space X and the out- put is a label y ∈ {0, 1}. This assumption is general because many NLP tasks such as text cat- egorization, NER and relation extraction can be cast into classification problems and our discus- sion below can be easily extended to multi-class settings. We further assume that we have a set of labeled instances from a source domain, denoted by</p><formula xml:id="formula_0">D s = {(x s i , y s i )} N i=1</formula><p>. We also have a set of un- labeled instances from a target domain, denoted by</p><formula xml:id="formula_1">D t = {x t j } M j=1</formula><p>. We assume a general setting of learning a linear classifier, which is essentially a weight vector w such that x is labeled as 1 if w x ≥ 0. <ref type="bibr">1</ref> A naive method is to simply learn a classifier from D s . The goal of unsupervised domain adap- tation is to make use of both D s and D t to learn a good w for the target domain. It has to be assumed that the source and the target domains are similar enough such that adaptation is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Method</head><p>Our method works as follows. We first randomly select a subset of target instances from D t and normalize them. We refer to the resulting vectors as exemplar vectors, denoted by E = {e (k) } K k=1 . Next, we transform each source instance x into a new feature vector by computing its similarity with each e (k) , as defined below:</p><formula xml:id="formula_2">g(x) = [s(x, e (1) ), . . . , s(x, e (K) )] , (1)</formula><p>where indicates transpose and s(x, x ) is a sim- ilarity function between x and x . In our work we use dot product as s. <ref type="bibr">2</ref> Once each labeled source domain instance is transformed into a K- dimensional vector by Equation 1, we can ap- pend this vector to the original feature vector of the source instance and use the combined feature vectors of all labeled source instances to train a classifier. To apply this classifier to the target do- main, each target instance also needs to add this K-dimensional induced feature vector.</p><p>It is worth noting that the exemplar vectors are randomly chosen from the available target in- stances and no special trick is needed. Overall, the method is fairly easy to implement, and yet as we will see in Section 3, it performs surpris- ingly well. We also want to point out that our in- stance similarity features bear strong similarity to what was proposed by <ref type="bibr" target="#b17">Sun and Lam (2013)</ref>, but their work addresses a completely different prob- lem and we developed our method independently of their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Justification</head><p>In this section, we provide some intuitive justifica- tion for our method without any theoretical proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning in the Target Subspace</head><p>Blitzer et al. (2011) pointed out that the hope of unsupervised domain adaptation is to "couple" the learning of weights for target-specific features with that of common features. We show our in- duced feature representation is exactly doing this.</p><p>First, we review the claim by <ref type="bibr" target="#b2">Blitzer et al. (2011)</ref>. We note that although the input vector space X is typically high-dimensional for NLP tasks, the actual space where input vectors lie can have a lower dimension because of the strong fea- ture dependence we observe with NLP tasks. For example, binary features defined from the same feature template such as the previous word are mutually exclusive. Furthermore, the actual low- dimensional spaces for the source and the target domains are usually different because of domain- specific features and distributional difference be- tween the domains. Borrowing the notation used by <ref type="bibr" target="#b2">Blitzer et al. (2011)</ref>, define subspace X s to be the (lowest dimensional) subspace of X spanned by all source domain input vectors. Similarly, a subspace X t can be defined. Define X s,t = X s X t , the shared subspace between the two do- mains. Define X s,⊥ to be the subspace that is or- thogonal to X s,t but together with X s,t spans X s , that is, X s,⊥ + X s,t = X s . Similarly we can define X ⊥,t . Essentially X s,t , X s,⊥ and X ⊥,t are the shared subspace and the domain-specific subspaces, and they are mutually orthogonal. We can project any input vector x into the three subspaces defined above as follows:</p><formula xml:id="formula_3">x = x s,t + x s,⊥ + x ⊥,t .</formula><p>Similarly, any linear classifier w can be decom- posed into w s,t , w s,⊥ and w ⊥,t , and</p><formula xml:id="formula_4">w x = w s,t x s,t + w s,⊥ x s,⊥ + w ⊥,t x ⊥,t .</formula><p>For a naive method that simply learns w from D s , the learned component w ⊥,t will be 0, because the component x ⊥,t of any source instance is 0, and therefore the training error would not be reduced by any non-zero w ⊥,t . Moreover, any non-zero w s,⊥ learned from D s would not be useful for the target domain because for all target instances we have x s,⊥ = 0. So for a w learned from D s , only its component w s,t is useful for domain transfer. <ref type="bibr" target="#b2">Blitzer et al. (2011)</ref> argues that with unlabeled target instances, we can hope to "couple" the learning of w ⊥,t with that of w s,t . We show that if we use only our induced feature representation without appending it to the original feature vec- tor, we can achieve this. We first define a ma- trix M E whose column vectors are the exemplar vectors from E. Then g(x) can be rewritten as M E x. Let w denote a linear classifier learned from the transformed labeled data. w makes pre- diction based on w M E x, which is the same as (M E w ) x. This shows that the learned classifier w for the induced features is equivalent to a linear classifier w = M E w for the original features.</p><p>It is not hard to see that M E w is essentially k w k e (k) , i.e. a linear combination of vectors in E. Because e (k) comes from X t , we can write</p><formula xml:id="formula_5">e (k) = e (k) s,t + e (k) ⊥,t . Therefore we have w = k w k e (k) s,t ws,t + k w k e (k) ⊥,t w ⊥,t .</formula><p>There are two things to note from the formula above.</p><p>(1) The learned classifier w does not have any component in the subspace X s,⊥ , which is good because such a component would not be use- ful for the target domain. (2) The learned w ⊥,t will unlikely be zero because its learning is "coupled" with the learning of w s,t through w . In effect, we pick up target specific features that correlate with useful common features.</p><p>In practice, however, we need to append the in- duced features to the original features to achieve good adaptation results. One may find this counter-intuitive because this results in an ex- panded instead of restricted hypothesis space. Our explanation is that because of the typical L 2 regu- larizer used during training, there is an incentive to shift the weight mass to the additional induced fea- tures. The need to combine the induced features with original features was also reported in previ- ous domain adaptation work such as SCL ( <ref type="bibr" target="#b1">Blitzer et al., 2006</ref>) and marginalized denoising autoen- coders ( <ref type="bibr" target="#b4">Chen et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reduction of Domain Divergence</head><p>Another theory on domain adaptation developed by <ref type="bibr" target="#b0">Ben-David et al. (2010)</ref> essentially states that we should use a hypothesis space that can achieve low error on the source domain while at the same time making it hard to separate source and tar- get instances. If we use only our induced fea- tures, then X s,⊥ is excluded from the hypothesis space. This is likely to make it harder to distin- guish source and target instances. To verify this, in <ref type="table" target="#tab_0">Table 1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difference from EA++</head><p>The easy domain adaptation method EA proposed by Daumé III (2007) has later been extended to a semi-supervised version EA++ <ref type="bibr" target="#b6">(Daumé III et al., 2010)</ref>, where unlabeled data from the target do- main is also used. Theoretical justifications for both EA and EA++ are given by . Here we briefly discuss how our method is different from EA++ in terms of using unla- beled data. In both EA and EA++, since labeled target data is available, the algorithms still learn two classifiers, one for each domain. In our al- gorithm, we only learn a single classifier using labeled data from the source domain. In EA++, unlabeled target data is used to construct a reg- ularizer that brings the two classifiers of the two domains closer. Specifically, the regularizer de- fines a penalty if the source classifier and the tar- get classifier make different predictions on an un- labeled target instance. However, with this regu- larizer, EA++ does not strictly restrict either the source classifier or the target classifier to lie in the target subspace X t . In contrast, as we have pointed out above, when only the induced features are used, our method leverages the unlabeled tar- get instances to force the learned classifier to lie in X t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks and Data Sets</head><p>We consider the following NLP tasks. Personalized Spam Filtering (Spam): The data set comes from ECML/PKDD 2006 discovery challenge. The goal is to adapt a spam filter trained on a common pool of 4000 labeled emails to three individual users' personal inboxes, each contain- ing 2500 emails. We use bag-of-word features for this task, and we report classification accuracy. Gene Name Recognition (NER): The data set comes from BioCreAtIvE Task 1B <ref type="bibr" target="#b10">(Hirschman et al., 2005</ref>). It contains three sets of Medline ab- stracts with labeled gene names. Each set corre- sponds to a single species (fly, mouse or yeast). We consider domain adaptation from one species to another. We use standard NER features includ- ing words, POS tags, prefixes/suffixes and contex- tual features. We report F1 scores for this task. Relation Extraction (Relation): We use the ACE2005 data where the annotated documents are from several different sources such as broadcast news and conversational telephone speech. We re- port the F1 scores of identifying the 7 major rela- tion types. We use standard features including en- tity types, entity head words, contextual words and other syntactic features derived from parse trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methods for Comparison</head><p>Naive uses the original features.</p><p>Common uses only features commonly seen in both domains.</p><p>SCL is our implementation of Structural Corre- spondence <ref type="bibr">Learning (Blitzer et al., 2006</ref>). We set the number of induced features to 50 based on pre- liminary experiments. For pivot features, we fol- low the setting used by <ref type="bibr" target="#b1">Blitzer et al. (2006)</ref> and se- lect the features with a term frequency more than 50 in both domains.</p><p>PCA uses principal component analysis on D t to obtain K-dimensional induced feature vectors and then appends them to the original feature vectors.</p><p>ISF is our method using instance similarity fea- tures. We first transform each training instance to a K-dimensional vector according to Equation 1 and then append the vector to the original vector.</p><p>For all the three NLP tasks and the methods above that we compare, we employ the logistic re- gression (a.k.a. maximum entropy) classification algorithm with L 2 regularization to train a clas- sifier, which means the loss function is the cross entropy error. We use the L-BFGS optimization algorithm to optimize our objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>In <ref type="table">Table 2</ref>, we show the comparison between our method and Naive, Common and SCL. For ISF, the parameter K is set to 100 for Spam, 50 for NER and 500 for Relation after tuning. As we can see from the table, Common, which removes source domain specific features during training, can sometimes improve the classification perfor- mance, but this is not consistent and the improve- ment is small. SCL can improve the performance in most settings for all three tasks, which confirms the general effectiveness of this method. For our method ISF, we can see that on average it outper- forms both Naive and SCL significantly. When we zoom into the different source-target domain pairs of the three tasks, we can see that ISF out- performs SCL in most of the cases. This shows that our method is competitive despite its simplic- ity. It is also worth pointing out that SCL incurs much more computational cost than ISF.</p><p>We next compare ISF with PCA. Because PCA is also expensive, we only managed to run it on the Spam task. <ref type="table" target="#tab_3">Table 3</ref> shows that ISF also out- performs PCA significantly.  <ref type="table">Table 2</ref>: Comparison of performance on three NLP tasks. For each source-target pair of each task, the performance shown is the average of 5-fold cross validation. We also report the overall average performance for each task. We tested statistical significance only for the overall average performance and found that ISF was significantly better than both Naive and SCL with p &lt; 0.05 (indicated by * * ) based on the Wilcoxon signed-rank test.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We presented a hassle-free unsupervised domain adaptation method. The method is simple to im- plement, fast to run and yet effective for a few NLP tasks, outperforming SCL, a widely-used un- supervised domain adaptation method. We believe the proposed method can benefit a large number of practitioners who prefer simple methods than so- phisticated domain adaptation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>we show the following errors based on three feature representations: ( 1 )</head><label>1</label><figDesc>The training error on the source domain (ˆ ε s ). (2) The classi- fication error when we train a classifier to sepa- rate source and target instances. (3) The error on the target domain using the classifier trained from the source domain (ˆ ε t ). ISF-means only our in- duced instance similarity features are used while ISF uses combined feature vectors. The results show that ISF achieves relatively lowˆεlowˆ lowˆε s and in- creases the domain separation error. These two factors lead to a reduction inˆεinˆ inˆε t . featuresˆεs featuresˆ featuresˆεs domain separation errorˆεt errorˆ errorˆεt Original</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Three errors of different feature representations on 

a spam filtering task. K is 200 for ISF-and ISF. We expect a 
lowˆεtlowˆ lowˆεt whenˆεswhenˆ whenˆεs is low and domain separation error is high. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Comparison between ISF and PCA.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> A bias feature that is always set to be 1 can be added to allow a non-zero threshold. 2 We find that normalizing the exemplar vectors results in better performance empirically. On the other hand, if we normalize both the exemplar vectors and each instance x, i.e. if we use cosine similarity as s, the performance is similar to not normalizing x.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank the reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation with coupled subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation with active learning for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><forename type="middle">Eddie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for machine translation by mining unseen words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Frustratingly easy semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>the 2010 Workshop on Domain Adaptation for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online methods for multi-domain learning and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="689" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-eight International Conference on Machine Learning</title>
		<meeting>the Twenty-eight International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overview of BioCreAtIvE task 1B: normailized gene lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Colosimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Suppl 1</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Polylingual tree-based topic models for translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1166" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-regularization based semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Employing word representations and regularization for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="68" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust domain adaptation for relation extraction via clustering consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Minh Luan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="807" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiplekernel, multiple-instance similarity features for efficient visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chensheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3050" to="3061" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation by constraining inter-domain variability of latent feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast easy unsupervised domain adaptation with marginalized structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="538" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised multi-domain adaptation with feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="672" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information-theoretic multi-view domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
