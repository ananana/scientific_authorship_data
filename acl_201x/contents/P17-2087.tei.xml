<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paria</forename><surname>Jamshid Lou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
						</author>
						<title level="a" type="main">Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="547" to="553"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2087</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a model for disflu-ency detection in spontaneous speech transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel Model (NCM) to generate n-best candidate dis-fluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis. The LSTM language model scores, along with other features, are used in a MaxEnt reranker to identify the most plausible analysis. We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the state-of-the-art in disfluency detection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Disfluency is a characteristic of spontaneous speech which is not present in written text. Disfluencies are informally defined as interrup- tions in the normal flow of speech that occur in different forms, including false starts, correc- tions, repetitions and filled pauses. According to <ref type="bibr" target="#b21">Shriberg's (1994)</ref> definition, the basic pattern of speech disfluencies contains three parts: reparan- dum 1 , interregnum and repair. Example 1 illus- trates a disfluent structure, where the reparandum to Boston is the part of the utterance that is re- placed, the interregnum uh, I mean is an optional part of a disfluent structure that consists of a filled pause uh and a discourse marker I mean and the re- pair to Denver replaces the reparandum. The flu- ent version of Example 1 is obtained by deleting <ref type="bibr">1</ref> Reparandum is sometimes called edit.</p><p>reparandum and interregnum words. </p><p>While disfluency rate varies with the context, age and gender of speaker, <ref type="bibr" target="#b1">Bortfeld et al. (2001)</ref> reported disfluencies once in every 17 words. Such frequency is high enough to reduce the read- ability of speech transcripts. Moreover, disfluen- cies pose a major challenge to natural language processing tasks, such as dialogue systems, that rely on speech transcripts ( <ref type="bibr" target="#b15">Ostendorf et al., 2008)</ref>. Since such systems are usually trained on fluent, clean corpora, it is important to apply a speech disfluency detection system as a pre-processor to find and remove disfluencies from input data. By disfluency detection, we usually mean identifying and deleting reparandum words. Filled pauses and discourse markers belong to a closed set of words, so they are trivial to detect .</p><p>In this paper, we introduce a new model for de- tecting restart and repair disfluencies in sponta- neous speech transcripts called LSTM Noisy Chan- nel Model (LSTM-NCM). The model uses a Noisy Channel Model (NCM) to generate n-best candi- date disfluency analyses, and a Long Short-Term Memory (LSTM) language model to rescore the NCM analyses. The language model scores are used as features in a MaxEnt reranker to select the most plausible analysis. We show that this novel approach improves the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Approaches to disfluency detection task fall into three main categories: sequence tagging, parsing- based and noisy channel model. The sequence tagging models label words as fluent or disfluent using a variety of different techniques, including conditional random fields <ref type="bibr" target="#b16">(Ostendorf and Hahn, 2013;</ref><ref type="bibr" target="#b24">Zayats et al., 2014;</ref><ref type="bibr" target="#b4">Ferguson et al., 2015)</ref>, hidden Markov models ( <ref type="bibr" target="#b12">Liu et al., 2006;</ref><ref type="bibr" target="#b20">Schuler et al., 2010)</ref> or recurrent neural networks <ref type="bibr" target="#b8">(Hough and Schlangen, 2015;</ref><ref type="bibr" target="#b25">Zayats et al., 2016)</ref>. Al- though sequence tagging models can be easily generalized to a wide range of domains, they require a specific state space for disfluency de- tection, such as begin-inside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence. The parsing-based approaches refer to parsers that detect disfluen- cies, as well as identifying the syntactic structure of the sentence <ref type="bibr" target="#b18">(Rasooli and Tetreault, 2013;</ref><ref type="bibr" target="#b7">Honnibal and Johnson, 2014;</ref><ref type="bibr" target="#b23">Yoshikawa et al., 2016)</ref>. Training a parsing-based model requires large an- notated tree-banks that contain both disfluencies and syntactic structures. Noisy channel models (NCMs) use the similarity between reparandum and repair as an indicator of disfluency. However, applying an effective language model (LM) inside an NCM is computationally complex. To allevi- ate this problem, some researchers use more effec- tive LMs to rescore the NCM disfluency analyses.  applied a syntactic parsing-based LM trained on the fluent version of the Switchboard corpus to rescore the disfluency analyses. Zwarts and Johnson (2011) trained ex- ternal n-gram LMs on a variety of large speech and non-speech corpora to rank the analyses. Us- ing the external LM probabilities as features into the reranker improved the baseline NCM ( ). The idea of applying exter- nal language models in the reranking process of the NCM motivates our model in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LSTM Noisy Channel Model</head><p>We follow  in us- ing an NCM to find the n-best candidate disflu- ency analyses for each sentence. The NCM, how- ever, lacks an effective language model to capture more complicated language structures. To over- come this problem, our idea is to use different LSTM language models to score the underlying fluent sentences of the analyses proposed by the NCM and use the language model scores as fea- tures to a MaxEnt reranker to select the best anal- ysis. In the following, we describe our model and its components in detail.</p><p>In the NCM of speech disfluency, we assume that there is a well-formed source utterance X to which some noise is added and generates a disflu- ent utterance Y as follows. X = a flight to Denver Y = a flight to Boston uh I mean to Denver Given Y , the goal of the NCM is to find the most likely source sentencê X such that:</p><formula xml:id="formula_1">ˆ X = arg max X P(Y |X)P(X)<label>(2)</label></formula><p>As shown in Equation 2, the NCM contains two components: the channel model P(Y |X) and the language model P(X). Calculating the channel model and language model probabilities, the NCM generates 25-best candidate disfluency analyses as follows. <ref type="formula">(3)</ref> Example 3 shows sample outputs of the NCM, where potential reparandum words are specified with strikethrough text. The MaxEnt reranker is applied on the candidate analyses of the NCM to select the most plausible one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Channel Model</head><p>We assume that X is a substring of Y , so the source sentence X is obtained by deleting words from Y . For each sentence Y , there are only a finite number of potential source sentences. However, with the increase in the length of Y , the number of possible source sentences X grows exponen- tially, so it is not feasible to do exhaustive search. Moreover, since disfluent utterances may contain an unbounded number of crossed dependencies, a context-free grammar is not suitable for finding the alignments. The crossed dependencies refer to the relation between repair and reparandum words which are usually the same or very similar words in roughly the same order as in Example 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(4)</head><p>We apply a Tree Adjoining Grammar (TAG) based transducer (  which is a more expressive formalism and pro- vides a systematic way of formalising the chan- nel model. The TAG channel model encodes the crossed dependencies of speech disfluency, rather than reflecting the syntactic structure of the sen- tence. The TAG transducer is effectively a simple first-order Markov model which generates each word in the reparandum conditioned on the pre- ceding word in the reparandum and the corre- sponding word in the repair. Further detail about the TAG channel model can be found in ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Model</head><p>The language model of the NCM evaluates the fluency of the sentence with disfluency removed. The language model is expected to assign a very high probability to a fluent sentence X (e.g. a flight to Denver) and a lower probability to a sen- tence Y which still contains disfluency (e.g. a flight to Boston uh I mean to Denver). However, it is computationally complex to use an effective language model within the NCM. The reason is the polynomial-time dynamic programming pars- ing algorithms of TAG can be used to search for likely repairs if they are used with simple language models such as a bigram LM ). The bigram LM within the NCM is too simple to capture more complicated lan- guage structure. In order to alleviate this problem, we follow Zwarts and Johnson (2011) by training LMs on different corpora, but we apply state-of- the-art recurrent neural network (RNN) language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM</head><p>We use a long short-term memory (LSTM) neu- ral network for training language models. LSTM is a particular type of recurrent neural net- works which has achieved state-of-the-art perfor- mance in many tasks including language mod- elling ( <ref type="bibr" target="#b13">Mikolov et al., 2010;</ref><ref type="bibr" target="#b11">Jozefowicz et al., 2016)</ref>. LSTM is able to learn long dependencies between words, which can be highly beneficial for the speech disfluency detection task. Moreover, it allows for adopting a distributed representation of words by constructing word embedding ( <ref type="bibr" target="#b14">Mikolov et al., 2013)</ref>.</p><p>We train forward and backward (i.e. input sen- tences are given in reverse order) LSTM language models using truncated backpropagation through time algorithm <ref type="bibr" target="#b19">(Rumelhart et al., 1986</ref>) with mini- batch size 20 and total number of epochs 13. The LSTM model has two layers and 200 hidden units. The initial learning rate for stochastic gra- dient optimizer is chosen to 1 which is decayed by 0.5 for each epoch after maximum epoch 4. We limit the maximum sentence length for training our model due to the high computational complex- ity of longer histories in the LSTM. In our exper- iments, considering maximum 50 words for each sentence leads to good results. The size of word embedding is 200 and it is randomly initialized for all LSTM LMs 2 .</p><p>Using each forward and backward LSTM lan- guage model, we assign a probability to the under- lying fluent parts of each candidate analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reranker</head><p>In order to rank the the 25-best candidate disflu- ency analyses of the NCM and select the most suitable one, we apply the MaxEnt reranker pro- posed by . We use the fea- ture set introduced by <ref type="bibr" target="#b26">Zwarts and Johnson (2011)</ref>, but instead of n-gram scores, we apply the LSTM language model probabilities. The features are so good that the reranker without any external lan- guage model is already a state-of-the-art system, providing a very strong baseline for our work. The reranker uses both model-based scores (in- cluding NCM scores and LM probabilities) and surface pattern features (which are boolean in- dicators) as described in <ref type="table">Table 1</ref>. Our reranker optimizes the expected f-score approximation de- scribed in Zwarts and Johnson (2011) with L2 reg- ularisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Corpora for Language Modelling</head><p>In this work, we train forward and backward LSTM language models on Switchboard (Godfrey and Holliman, 1993) and Fisher ( <ref type="bibr" target="#b3">Cieri et al., 2004</ref>) corpora. Fisher consists of 2.2 × 10 7 tokens of transcribed text, but disfluencies are not annotated in it. Switchboard is the largest available cor- pus (1.2 × 10 6 tokens) in which disfluencies are annotated according to <ref type="bibr" target="#b21">Shriberg's (1994)</ref> scheme. Since the bigram language model of the NCM is trained on this corpus, we cannot directly use Switchboard to build LSTM LMs. The reason is that if the training data of Switchboard is used both for predicting language fluency and optimizing the loss function, the reranker will overestimate the model-based features 1-2. forward &amp; backward LSTM LM scores 3-7. log probability of the entire NCM 8. sum of the log LM probability &amp; the log channel model probability plus number of ed- its in the sentence 9. channel model probability surface pattern features 10. CopyFlags X Y: if there is an exact copy in the input text of length X (1 ≤ X ≤ 3) and the gap between the copies is Y (0 ≤ Y ≤ 3) 11. WordsFlags L n R: number of flags to the left (L) and to the right (R) of a 3-gram area (0 ≤ L, R ≤ 1) 12. SentenceEdgeFlags B L: it captures the lo- cation and length of disfluency. The Boolean B sentence initial or sentence final disfluency, L (1 ≤ L ≤ 3) records the length of the flags. <ref type="table">Table 1</ref>: The features used in the reranker. They, except for the first and second one, were applied by <ref type="bibr" target="#b26">Zwarts and Johnson (2011)</ref>.</p><p>weight related to the LM features extracted from Switchboard. This is because the fluent sentence itself is part of the language model ( <ref type="bibr" target="#b26">Zwarts and Johnson, 2011)</ref>. As a solution, we apply a k-fold cross-validation (k = 20) to train the LSTM lan- guage models when using Switchboard corpus.</p><p>We follow <ref type="bibr" target="#b2">Charniak and Johnson (2001)</ref> in splitting Switchboard corpus into training, devel- opment and test set. The training data consists of all sw <ref type="bibr">[23]</ref> * .dps files, development training con- sists of all sw4 <ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr">[8]</ref><ref type="bibr">[9]</ref> * .dps files and test data con- sists of all sw4[0-1] * .dps files. Following , we remove all partial words and punctuation from the training data. Although partial words are very strong indicators of disflu- ency, standard speech recognizers never produce them in their outputs, so this makes our evaluation both harder and more realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We assess the proposed model for disfluency de- tection with all MaxEnt features described in Ta- ble 1 against the baseline model. The noisy chan- nel model with exactly the same reranker features except the LSTM LMs forms the baseline model. To evaluate our system, we use two metrics f-score and error rate. <ref type="bibr" target="#b2">Charniak and Johnson (2001)</ref> used the f-score of labelling reparanda or "edited" words, while <ref type="bibr" target="#b5">Fiscus et al (2004)</ref> de- fined an "error rate" measure, which is the number of words falsely labelled divided by the number of reparanda words. Since only 6% of words are disfluent in Switchboard corpus, accuracy is not a good measure of system performance. F-score, on the other hand, focuses more on detecting "edited" words, so it is a decent metric for highly skewed data.</p><p>According to <ref type="table" target="#tab_1">Tables 2 and 3</ref>, the LSTM noisy channel model outperforms the baseline. The experiment on Switchboard and Fisher corpora demonstrates that the LSTM LMs provide infor- mation about the global fluency of an analysis that the local features of the reranker do not capture. The LSTM language model trained on Switch- board corpus results in the greatest improvement. Switchboard is in the same domain as the test data and it is also disfluency annotated. Either or both of these might be the reason why Switch- board seems to be better in comparison with Fisher which is a larger corpus and might be expected to make a better language model. Moreover, the backward LSTMs have better performance in comparison with the forward ones. It seems when sentences are fed in reverse order, the model can more easily detect the unexpected word order as- sociated with the reparandum to detect disfluen- cies. In other words, that the disfluency is ob- served "after" the fluent repair in a backward lan- guage model is helpful for recognizing disfluen- cies.   We compare the performance of Kneser-Ney smoothed 4-gram language models with the LSTM corresponding on the reranking process of the noisy channel model. We estimate the 4- gram models and assign probabilities to the flu- ent parts of disflueny analyses using the SRILM toolkit <ref type="bibr" target="#b22">(Stolcke, 2002)</ref>. <ref type="table" target="#tab_4">As Tables 4 and 5</ref> show including scores from a conventional 4-gram lan- guage model does not improve the model's abil- ity to find disfluencies, suggesting that the LSTM model contains all the useful information that the 4-gram model does. In order to give a more gen- eral idea on the performance of LSTM over stan- dard LM, we evaluate our model when the lan- guage model scores are used as the only features of the reranker. The f-score for the NCM alone without applying the reranker is 78.7, while using 4-gram language model scores in the reranker in- creases the f-score to 81.0. Replacing the 4-gram scores with LSTM language model probabilities leads to further improvement, resulting an f-score 82.   We also compare our best model on the develop- ment set to the state-of-the-art methods in the liter- ature. As shown in <ref type="table">Table 6</ref>, the LSTM noisy chan- nel model outperforms the results of prior work, achieving a state-of-the-art performance of 86.8. It also has better performance in comparison with <ref type="bibr" target="#b4">Ferguson et al. (2015)</ref> and <ref type="bibr">Zayat et al.'s (2016)</ref> models, even though they use richer input that in- cludes prosodic features or partial words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we present a new model for dis- fluency detection from spontaneous speech tran- Model f-score <ref type="bibr" target="#b23">Yoshikawa et al. (2016)</ref> 62.5  79.7  81.0 <ref type="bibr" target="#b18">Rasooli and Tetreault (2013)</ref> 81.4 Qian and <ref type="bibr" target="#b17">Liu (2013)</ref> 82.1 <ref type="bibr" target="#b7">Honnibal and Johnson (2014)</ref> 84.1 <ref type="bibr">Ferguson et al. (2015) * 85.4 Zwarts and</ref><ref type="bibr" target="#b26">Johnson (2011)</ref> 85.7 <ref type="bibr" target="#b25">Zayats et al. (2016)</ref> * 85.9 LSTM-NCM 86.8 <ref type="table">Table 6</ref>: Comparison of the LSTM-NCM to state- of-the-art methods on the dev set. *Models have used richer input.</p><p>scripts. It uses a long short-term memory neural network language model to rescore the candidate disfluency analyses produced by a noisy channel model. The LSTM language model scores as fea- tures in a MaxEnt reranker improves the model's ability to detect restart and repair disfluencies. The model outperforms other models reported in the literature, including models that exploit richer in- formation from the input. As future work, we ap- ply more complex LSTM language models such as sequence-to-sequence on the reranking process of the noisy channel model. We also intend to in- vestigate the effect of integrating LSTM language models into other kinds of disfluency detection models, such as sequence labelling and parsing- based models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F-scores on the dev set for a variety of 
LSTM language models. 

baseline 
27.0 
corpus 
forward backward both 
Switchboard 
25.5 
24.8 
24.3 
Fisher 
25.6 
25.0 
25.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Expected error rates on the dev set for a 
variey of LSTM language models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F-score for 4-gram, LSTM and combina-
tion of both language models. 

baseline 
27.0 
corpus 
4-gram LSTM both 
Switchboard 
27.5 
24.3 
26 
Fisher 
26.6 
25.3 
26 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Expected error rates for 4-gram, LSTM 
and combination of both language models. 

</table></figure>

			<note place="foot" n="2"> All code is written in TensorFlow (Abadi et al., 2015)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous review-ers for their insightful comments and sugges-tions. This research was supported by a Google award through the Natural Language Understand-ing Focused Program, and under the Australian Research Councils Discovery Projects funding scheme (project number DP160102156).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Disfluency rates in conversation: Effects of age, relationship, topic, role, and gender</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heather</forename><surname>Bortfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="147" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Edit detection and parsing for transcribed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N01-1016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies. Stroudsburg, USA, NAACL&apos;01</title>
		<meeting>the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies. Stroudsburg, USA, NAACL&apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fisher English training speech part 1 transcripts LDC2004T19. Published by: Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disfluency detection with a semi-Markov model and prosodic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Denver, USA, NAACL&apos;15</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Denver, USA, NAACL&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Results of the fall 2004 STT and MDE evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Sanders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Rich Transcription Fall Workshop</title>
		<meeting>Rich Transcription Fall Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Switchboard-1 release 2 LDC97S62. Published by: Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Holliman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint incremental disfluency detection and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/Q14-1011" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for incremental disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the 16th Annual Conference of the International Speech Communication Association (INTERSPEECH)<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A TAG-based noisy channel model of speech repairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P04-1005" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics<address><addrLine>Barcelona, Spain, ACL&apos;04</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An improved model for recognizing disfluencies in conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Rich Transcription Workshop</title>
		<meeting>Rich Transcription Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enriching speech recognition with automatic detection of sentence boundaries and disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolckeand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Hillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1526" to="1540" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH). Makuhari</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH). Makuhari<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 27th Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech segmentation and its impact on spoken document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Hillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">G</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Maskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sequential repetition model for improved disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyun</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the 14th Annual Conference of the International Speech Communication Association (INTERSPEECH)<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2624" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disfluency detection using multi-step stacked learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N13-1102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta, USA, NAACL&apos;13</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Atlanta, USA, NAACL&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="820" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint parsing and disfluency detection in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D13-1013" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pdp Research</forename><surname>Group</surname></persName>
		</author>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Broad-coverage parsing using human-like memory constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Preliminaries to a theory of speech disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Berkeley, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SRILM: An extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Spoken Language Processing</title>
		<meeting>International Conference on Spoken Language Processing<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint transition-based dependency parsing and disfluency detection for automatic speech recognition texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D16-1109" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1036" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-domain disfluency and repair detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the 15th Annual Conference of the International Speech Communication Association (INTERSPEECH)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2907" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Disfluency detection using a bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the 16th Annual Conference of the International Speech Communication Association (INTERSPEECH)<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2523" to="2527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The impact of language models and loss functions on repair disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Zwarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="http://aclweb.org/anthology/P11-1071" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Portland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
