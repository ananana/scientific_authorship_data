<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grammatical Error Correction: Machine Translation and Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Virginia Tech Blacksburg</orgName>
								<orgName type="institution" key="instit2">University of Illinois Urbana</orgName>
								<address>
									<postCode>24060, 61820</postCode>
									<region>VA, IL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<email>danr@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Virginia Tech Blacksburg</orgName>
								<orgName type="institution" key="instit2">University of Illinois Urbana</orgName>
								<address>
									<postCode>24060, 61820</postCode>
									<region>VA, IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Grammatical Error Correction: Machine Translation and Classifiers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2205" to="2215"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We focus on two leading state-of-the-art approaches to grammatical error correction machine learning classification and machine translation. Based on the comparative study of the two learning frameworks and through error analysis of the output of the state-of-the-art systems, we identify key strengths and weaknesses of each of these approaches and demonstrate their complementarity. In particular, the machine translation method learns from parallel data without requiring further linguistic input and is better at correcting complex mistakes. The classification approach possesses other desirable characteristics , such as the ability to easily generalize beyond what was seen in training, the ability to train without human-annotated data, and the flexibility to adjust knowledge sources for individual error types. Based on this analysis, we develop an algorithmic approach that combines the strengths of both methods. We present several systems based on resources used in previous work with a relative improvement of over 20% (and 7.4 F score points) over the previous state-of-the-art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the majority of English speakers today, En- glish is not the first language. These writers make a variety of grammar and usage mistakes that are not addressed by standard proofing tools. Re- cently, there has been a spike in research on gram- matical error correction (GEC), correcting writing mistakes made by learners of English as a Sec- ond Language, including four shared tasks: HOO ( <ref type="bibr">Dale and Kilgarriff, 2011;</ref><ref type="bibr" target="#b2">Dale et al., 2012</ref>  <ref type="table">Table 1</ref>: (Lack of) progress in GEC over the last few years.</p><p>CoNLL ( <ref type="bibr" target="#b26">Ng et al., 2013;</ref>). These shared tasks facilitated progress on the problem within the framework of two leading methods - machine learning classification and statistical ma- chine translation (MT). The top CoNLL system combined a rule-based module with MT <ref type="bibr" target="#b6">(Felice et al., 2014</ref>). The second system that scored almost as highly used machine learning classification ( , and the third system used MT <ref type="bibr" target="#b18">(Junczys-Dowmunt and Grundkiewicz, 2014</ref>). Furthermore,  showed that a combination of the two methods is beneficial, but the advantages of each method have not been fully exploited.</p><p>Despite success of various methods and the growing interest in the task, the key differences be- tween the leading approaches have not been iden- tified or made explicit, which could explain the lack of progress on the task. <ref type="table">Table 1</ref> shows ex- isting state-of-the-art since CoNLL-2014. The top results are close, suggesting that several groups have competitive systems. Two improvements (of &lt;3 points) were published since then ( <ref type="bibr" target="#b24">Mizumoto and Matsumoto, 2016)</ref>.</p><p>The purpose of this work is to gain a better un- derstanding of the values offered by each method and to facilitate progress on the task, building on the advantages of each approach. Through bet- ter understanding of the methods, we exploit the strengths of each technique and, building on exist- ing architecture, develop superior systems within each framework. Further combination of these systems yields even more significant improve- ments over existing state-of-the-art. We make the following contributions:</p><p>• We examine two state-of-the-art approaches to GEC and identify strengths and weaknesses of the respective learning frameworks.</p><p>• We perform an error analysis of the output of two state-of-the-art systems, and demonstrate how the methods differ with respect to the types of lan- guage misuse handled by each.</p><p>• We exploit the strengths of each framework: with classifiers, we explore the ability to learn from native data, i.e. without supervision, and the flexibility to adjust knowledge sources to specific error types; with MT, we leverage the ability to learn without further linguistic input and to bet- ter identify complex mistakes that cannot be easily defined in a classifier framework.</p><p>• As a result, we build several systems that com- bine the strengths of both frameworks and demon- strate substantial progress on the task. Specif- ically, the best system outperforms the previous best result by 7.4 F score points.</p><p>Section 2 describes related work. Section 3 presents error analysis. In Section 4, we develop classifier and MT systems that make use of the strengths of each framework. Section 5 shows how to combine the two approaches. Section 6 con- cludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We first introduce the CoNLL-2014 shared task and briefly describe the state-of-the-art GEC sys- tems in the competition and beyond. Next, an overview of the two leading methods is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CoNLL-2014 shared task and approaches</head><p>CoNLL-2014 training data (henceforth CoNLL- train) is a corpus of learner essays (1.2M words) written by students at the National University of Singapore ( <ref type="bibr">Dahlmeier et al., 2013</ref>), corrected and error-tagged. The CoNLL-2013 test set was in- cluded in <ref type="bibr">CoNLL-2014</ref> and is used as develop- ment. Both the development and the test sets are also from the student population studying at the same University but annotated separately. We re- port results on the CoNLL-2014 test.</p><p>The annotation includes specifying the relevant correction as well as the information about each error type. The tagset consists of 28 categories. <ref type="table" target="#tab_2">Table 2</ref> illustrates the 11 most frequent errors in the development data; errors are marked with an asterisk, and ∅ denotes a missing word. The ma- jority of these errors are related to grammar but also include mechanical, collocation, and other er- rors.</p><p>An F-based scorer, named M2, was used to score the systems <ref type="bibr">(Dahlmeier and Ng, 2012</ref>). The metric in CoNLL-2014 was F0.5, i.e. weighing precision twice as much as recall. Two types of annotations were used: original and revised. We follow the recommendations of the organizers and use the original data ( ).</p><p>The approaches varied widely: classifiers, MT, rules, hybrid systems. <ref type="table" target="#tab_3">Table 3</ref> summarizes the top five systems. The top team used a hybrid system that combined rules and MT. The second system developed classifiers for common grammatical er- rors. The third system used MT.</p><p>As for external resources, the top 1 and top 3 teams used additional learner data to train their MT systems, the Cambridge University Press Learners' Corpus and the Lang-8 corpus <ref type="bibr" target="#b25">(Mizumoto et al., 2011</ref>), respectively. Many teams also used native English datasets. The most common ones are the Web1T corpus ( <ref type="bibr">Brants and Franz, 2006</ref>), the CommonCrawl dataset, which is sim- ilar to Web1T, and the English Wikipedia. Several teams used off-the-shelf spellcheckers.</p><p>In addition,  made an at- tempt at combining MT and classifiers. They used CoNLL-train and Lang-8 as non-native data and English Wikipedia as native data. We be- lieve that the reason this study did not yield sig- nificant improvements <ref type="table">(Table 1)</ref> is that individual strengths of each framework have not been fully exploited. Further, each system was applied sepa- rately and decisions were combined using a gen- eral MT combination technique <ref type="bibr" target="#b15">(Heafield et al., 2009</ref>). Finally, <ref type="bibr" target="#b24">Mizumoto and Matsumoto (2016)</ref> attempt to improve an MT system also trained on Lang-8 with discriminative re-ranking using part-of-speech (POS) and dependency features but only obtain a small improvement. These results suggest that standard combination and re-ranking techniques are not sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Overview of the State-of-the-Art</head><p>The statistical machine translation approach is based on the noisy-channel model. The best trans- lation for a foreign sentence f is:</p><formula xml:id="formula_0">e * = arg max e p(e)p(f |e)</formula><p>Error type</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rel. freq. (%) Examples</head><p>Article <ref type="table">(ArtOrDet)</ref> 19.93 *∅/The government should help encourage *the/∅ breakthroughs as well as *a/∅ complete medication system .</p><p>Wrong collocation (Wci) 12.51 Some people started to *think/wonder if electronic products can replace human beings for better performances .</p><p>Noun number <ref type="formula">(</ref>   The model consists of two components: a lan- guage model assigning a probability p(e) for any target sentence e, and a translation model that as- signs a conditional probability p(f |e). The lan- guage model is learned using a monolingual cor- pus in the target language. The parameters of the translation model are estimated from a par- allel corpus, i.e. the set of foreign sentences and their corresponding translations into the tar- get language. In error correction, the task is cast as translating from erroneous learner writing into corrected well-formed English. The MT approach relies on the availability of a parallel corpus for learning the translation model. In case of error correction, a set of learner sentences and their cor- rections functions as a parallel corpus.</p><p>State-of-the-art MT systems are phrase-based, i.e. parallel data is used to derive a phrase-based lexicon ( <ref type="bibr" target="#b20">Koehn et al., 2003</ref>). The resulting lexicon consists of a list of pairs (seq f , seq e ) where seq f is a sequence of one or more foreign words, seq e is a predicted translation. Each pair comes with an associated score. At decoding time, all phrases from sentence f are collected with their corre- sponding translations observed in training. These are scored together with the language modeling scores and may include other features. The phrase- based approach by <ref type="bibr" target="#b20">Koehn et al. (2003)</ref> uses a log- linear model <ref type="bibr" target="#b28">(Och and Ney, 2002)</ref>, and the best correction maximizes the following:</p><formula xml:id="formula_1">e * = arg max e P (e|f ) (1) = arg max e exp( M m=1 λ m h m (e, f ))</formula><p>where h m is a feature function, such as lan- guage model score and translation scores, and λ m corresponds to a feature weight. The classifier approach is based on the context- sensitive spelling correction methodology <ref type="bibr" target="#b11">(Golding and Roth, 1996;</ref><ref type="bibr" target="#b12">Golding and Roth, 1999;</ref><ref type="bibr" target="#b0">Banko and Brill, 2001;</ref><ref type="bibr">Carlson et al., 2001;</ref><ref type="bibr">Carlson and Fette, 2007)</ref> and goes back to earlier ap- proaches to article and preposition error correction ( <ref type="bibr" target="#b17">Izumi et al., 2003;</ref><ref type="bibr" target="#b13">Han et al., 2006;</ref><ref type="bibr" target="#b9">Gamon et al., 2008;</ref><ref type="bibr" target="#b5">Felice and Pulman, 2008;</ref><ref type="bibr" target="#b10">Gamon, 2010;</ref><ref type="bibr">Dahlmeier and Ng, 2011;</ref><ref type="bibr">Dahlmeier and Ng, 2012)</ref>. The classifier approach to error correction has been prominent for a long time before MT, since building a classifier does not require having annotated learner data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT</head><p>Classifier (1a) Error coverage: ability to address a wide variety of error phenomena +All errors occurring in the train- ing data are automatically covered -Only errors covered by the classi- fiers; new errors need to be added explicitly (1b) Error complexity: ability to handle com- plex and interacting mistakes that go beyond word boundaries +Automatically through parallel data, via phrase-based lexicons -Need to develop via specific ap- proaches (2) Generalizability: going beyond the error confusions observed in training -Only confusions observed in training can be corrected +Easily generalizable via confu- sion sets and features (3) Supervision/Annotation: role of learner data in training the system -Required +Not required (4) System flexibility: adapting knowledge sources per error phenomena -Not easy to integrate error- specific knowledge resources +Flexible; phenomenon-specific knowledge sources <ref type="table">Table 4</ref>: Summary of the key properties of the MT and the classifier-based approaches. We use + and − to indicate a positive or a negative value with respect to each factor.</p><p>Classifiers are trained individually for a specific error type. Because an error type needs to be de- fined, typically only well-defined mistakes can be addressed in a straightforward way. Given an error type, a confusion set is specified and includes a list of confusable words. For some errors, confusion sets are constructed using a closed list (e.g. prepo- sitions). For other error types, NLP tools are re- quired. To identify locations where an article was likely omitted incorrectly, for example, a phrase chunker is used. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task.</p><p>In the classifier paradigm, there are various al- gorithms -generative <ref type="bibr" target="#b10">(Gamon, 2010;</ref><ref type="bibr" target="#b29">Park and Levy, 2011</ref>), discriminative ( <ref type="bibr" target="#b13">Han et al., 2006;</ref><ref type="bibr" target="#b9">Gamon et al., 2008;</ref><ref type="bibr" target="#b5">Felice and Pulman, 2008;</ref>, and joint approaches <ref type="bibr">(Dahlmeier and Ng, 2012;</ref><ref type="bibr" target="#b35">Rozovskaya and Roth, 2013)</ref>. Earlier works trained on native data (due to lack of annotation). Later approaches incorpo- rated learner data in training in various ways <ref type="bibr" target="#b14">(Han et al., 2010;</ref><ref type="bibr" target="#b10">Gamon, 2010;</ref><ref type="bibr" target="#b32">Rozovskaya and Roth, 2010a;</ref><ref type="bibr">Dahlmeier and Ng, 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Error Analysis of MT and Classifiers</head><p>This section presents error analysis of the MT and classifier approaches. We begin by identify- ing several key properties that distinguish between MT systems and classifier systems and that we use to characterize the learning frameworks and the outputs of the systems: (1a) Error coverage denotes the ability of a sys- tem to identify and correct a variety of error types. (1b) Error complexity indicates the capacity of a system to address complex mistakes such as those where multiple errors interact.</p><p>(2) Generalizibility refers to the ability of a sys- tem to identify mistakes in new unseen contexts and propose corrections beyond those observed in training data. (3) The role of supervision or having annotated learner data for training. (4) System flexibility is a property of the system that allows it to adapt resources specially to correct various phenomena. The two paradigms are sum- marized in <ref type="table">Table 4</ref>. We use + and − to indicate whether a learning framework has desirable (+) or undesirable characteristic with regard to each fac- tor.</p><p>The first three properties characterize system output, while (3) and (4) arise from the system frameworks. Below we analyze the output of sev- eral state-of-the-art CoNLL-2014 systems in more detail. 1 Section 4 explores (3) and (4) that relate to the learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Error Coverage and Complexity</head><p>Error coverage To understand how systems differ with respect to error coverage, we consider recall of each system per error type. Error-type recall can be easily computed using error tags and is re- ported in the CoNLL overview paper.</p><p>The recall numbers show substantial variations among the systems. If we consider error cat- egories that have non-negligible recall numbers (higher than 10%), classifier-based approaches have a much lower proportion of error types for which 10% recall was achieved. Among the 28 er- ror types, the top classifier systems -Columbia University-University of Illinois (CUUI, top-2) and National Tsing Hua University (NTHU, top- 5) -have a recall higher than 10% for 8 and 9 error types, respectively. In contrast, the two MT- based systems -Cambridge University (CAMB, (1) It is a concern that will be with us *{during our whole life}/{for our entire life} . (2) The decision to inform relatives of *{such genetic disorder}/{such genetic disorders} will be dependent . . . (3) .. we need to respect it and we have no right *{in saying}/{to say} that he must tell his relatives about it . (4) ...and his family might be a *{genetically risked}/{genetic risk} family . (5) ...he was *diagnosis/{diagnosed with} a kind of genetic disease which is very serious . (6) The situation may become *worst/worse if the child has diseases like cancer or heart disease . . . <ref type="table">Table 5</ref>: Complex and interacting mistakes that MT successfully addresses. Output of the MT-based AMU system. top-1) and the Adam Mickiewicz University sys- tem (AMU, top-3) -have 15 and 17 error types, respectively, for which the recall is at least 10%.</p><p>These recall discrepancies indicate that the MT approach has a better overall coverage, which is intuitive given that all types of confusions are au- tomatically added through phrase-based transla- tion tables in MT, while classifiers must explicitly model each error type. Note, however, that these numbers do not necessarily indicate good type- based performance, since high recall may corre- spond to low precision. Error complexity In the MT approach, error con- fusions are learned automatically via the phrase translation tables extracted from the parallel train- ing data. Thus, an MT system can easily handle in- teracting and complex errors where replacements involve a sequence of words. <ref type="table">Table 5</ref> illustrates complex and interacting mistakes that the MT ap- proach is able to handle. Example (1) contains a phrase-level correction that includes both a prepo- sition replacement and an adjective change. (2) is an instance of an interacting mistake where there is a dependency between the article and the noun number, and a mistake can be corrected by chang- ing one of the properties but not both. (3), (4) and (5) require multiple simultaneous corrections on various words in a phrase. <ref type="formula">(6)</ref> is an example of an incorrect adjectival form, an error that is typically not modeled with standard classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalizability</head><p>Because MT systems extract error/correction pairs from phrase-translation tables, they can only iden- tify erroneous surface forms observed in training and propose corrections that occurred with the cor- responding surface forms. Crucially, in a standard MT scenario, any resulting translation consists of "matches" mined from the translation tables, so a standard MT model lacks lexical abstractions that might help generalize, thus out-of-vocabulary words is a well-known problem in MT ( <ref type="bibr" target="#b3">Daume and Jagarlamudi, 2011</ref>  With classifiers, it is easy to generalize using higher-level information that goes beyond surface form and to adjust the abstraction to the error type. Many grammatical errors may benefit from gener- alizations based on POS or parse information; we can thus expect that classifiers will do better on errors that require linguistic abstractions.</p><p>To validate this hypothesis, we evaluate type- based performance of two systems: a top-3 MT- based AMU system and a top-2 classifier-based CUUI; we do not include the top-1 system, since it is a hybrid system that also uses rules.</p><p>Unlike recall, estimating type-based precision requires knowing the type of the correction sup- plied by the system, which is not specified in the output. We thus manually analyze the output of the AMU and CUUI systems for seven common error categories and assign to each correction an appropriate type to estimate precision and F0.5 <ref type="table" target="#tab_5">(Table 6</ref>). The CUUI system addresses all of these errors, with the exception of mechanical (Mec), of which it handles a small subset. The AMU sys- tem does better on mechanical, preposition, word form, and noun number. CUUI does better on ar- ticles, verb agreement, and verb form.</p><p>We now consider examples of errors that are corrected by the classifier-based CUUI system in these three categories but are missed by the MT- based AMU system <ref type="table">(Table 7)</ref>. Examples (1) and Long-distance dependencies: verb agreement</p><formula xml:id="formula_2">(1)</formula><p>As a result , in the case that when one of the members *happen/happens to feel uncomfortable or abnormal , he or she should be aware that . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(2) A study of New York University in 2010 shown that patients with family members around generally *recovers/recover 2-4 days faster than those taken care by professional nurses .</head><p>Confusions not found in training: verb agreement and verb form (3) Hence , the social media sites *serves/serve as a platform for the connection . (4) After *came/coming back from the hospital , the man told his parents that the problem was that he carried . . . (5) social media is the only resource they can approach to know everything *happened/happening in their country . . .</p><p>Superfluous words: articles (6) For *an/∅ example , if exercising is helpful, we can always look for more chances for the family to go exercise . (7) . . . as soon a person is made aware of his or her genetic profile , he or she has *a/∅ knowledge about others .</p><p>Omissions: articles (8) In this case , if one of the family members or close relatives is found to carry *∅/a genetic risk . . . <ref type="table">Table 7</ref>: Generalizing beyond surface form: Examples of mistakes that classifiers successfully address. Output of the classifier-based CUUI system.</p><p>(2) illustrate verb errors with long-distance sub- jects ("one" and "patients"). This is handled in the classification approach via syntactic features. An MT system misses these errors because it is limited to edits within short spans. Examples (3), (4), and (5) illustrate verb mistakes for which the correct replacements were not observed in train- ing but that are nonetheless corrected by general- izing beyond surface form. Finally, (6) and <ref type="formula">(7)</ref> illustrate omission and insertion errors, a major- ity of article mistakes. The MT system is espe- cially bad at correcting such mistakes. Notably, the classifier-based CUUI system correctly identi- fied twice as many omitted articles and more than 20 times more superfluous articles than the MT- based AMU system. This happens because an MT system is restricted to suggesting deletions and in- sertions in those contexts that were observed in training, whereas a classifier uses shallow parse in- formation, which allows it to insert or delete an ar- ticle in front of every eligible noun phrase. These examples demonstrate that the ability of a system to generalize beyond the surface forms is indeed beneficial for long-distance dependencies, for ab- stracting away from surface forms when formu- lating confusion sets, and for mistakes involving omitting or inserting a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Learner Native CoNLL- train Lang-8 Eng.</p><p>Wiki. Web1T</p><formula xml:id="formula_3">1.2M 48M 2B 1T MT - Classif.</formula><p>- - MT and classifier components and show how to exploit the strengths of each framework in combi- nation. <ref type="table" target="#tab_6">Table 8</ref> summarizes the data used. Results are reported with respect to all errors in the test data. This is different from performance for indi- vidual errors in <ref type="table" target="#tab_5">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Machine Translation Systems</head><p>A key advantage of the MT framework is that, un- like with classifiers, error confusions are learned from parallel data automatically, without further (linguistic) input. We build two MT systems that differ only in the use of parallel data: the CoNLL- 2014 training data and Lang-8. Our MT systems are trained using Moses ( <ref type="bibr" target="#b21">Koehn et al., 2007)</ref> and follow the standard approach <ref type="bibr" target="#b18">(Junczys-Dowmunt and Grundkiewicz, 2014;</ref>). Both systems use two 5-gram language models -English Wikipedia and the corrected side of CoNLL-train -trained with <ref type="bibr">KenLM (Heafield et al., 2013)</ref>. <ref type="table">Table 9</ref> reports the performance of the systems. As shown, performance increases by more than 11 points when a larger parallel corpus is used. The best MT system outperforms the top CoNLL system by 2 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classifiers</head><p>We now present several classifier systems, explor- ing the two important properties of the classifica- tion framework -the ability to train without super-  <ref type="table">Table 9</ref>: MT systems trained in this work.</p><p>vision and system flexibility (see <ref type="table">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Supervision</head><p>Supervision in the form of annotated learner data plays an important role in developing an error cor- rection system but is expensive. Native data, in contrast, is cheap and available in large quantities. Therefore, the fact that, unlike with MT, it is pos- sible to build a classifier system without any anno- tated data, is a clear advantage of classifiers.</p><p>Training without supervision is possible in the classification framework, as follows. For a given mistake type, e.g. preposition, a classifier is trained on native data that is assumed to be cor- rect; the classifier uses context words around each preposition as features. The resulting model is then applied to learner prepositions and will pre- dict the most likely preposition in a given con- text. If the preposition predicted by the classi- fier is different from what the author used in text, this preposition is flagged as a mistake. We refer the reader to <ref type="bibr" target="#b33">Rozovskaya and Roth (2010b)</ref> and Rozovskaya and Roth (2011) for a description of training classifiers with and without supervision for error correction tasks. Below, we address two questions related to the use of supervision:</p><p>• Training with supervision: When training us- ing learner data, how does a classifier-based sys- tem compare against an MT system? • Training without supervision: How well can we do by building a classifier system with native data only, compared to MT and classifier-based systems that use supervision?</p><p>Our classifier system is based on the imple- mentation framework of the second CoNLL-2014 system ( ) and consists of classifiers for 7 most common grammatical errors in CoNLL-train: article; preposition; noun num- ber; verb agreement; verb form; verb tense; word form. All modules take as input the corpus doc- uments pre-processed with a POS tagger 3 <ref type="bibr">(EvenZohar and Roth, 2001</ref>), a shallow parser 4 (Pun-  <ref type="table">Table 10</ref>: Classifier systems trained with and without supervision. Learner data refers to CoNLL-train. Native data refers to Web1T. The MT system uses CoNLL-train for parallel data.</p><p>yakanok and Roth, 2001), a syntactic parser ( <ref type="bibr" target="#b19">Klein and Manning, 2003</ref>) and a dependency converter ( <ref type="bibr" target="#b23">Marneffe et al., 2006</ref>). Classifiers are trained either on learner data (CoNLL-train) or native data (Web1T). Classifiers built on CoNLL-train are trained discriminatively with the Averaged Perceptron algorithm ( <ref type="bibr" target="#b31">Rizzolo and Roth, 2010)</ref> and use rich POS and syntactic features tailored to specific error types that are standard for these tasks ( <ref type="bibr" target="#b22">Lee and Seneff, 2008;</ref><ref type="bibr" target="#b13">Han et al., 2006;</ref>); Na¨ıveNa¨ıve Bayes classifiers are trained on Web1T with word n-gram features. A detailed description of the classifiers and the fea- tures used can be found in . We also add several novel ideas that are described below. <ref type="table">Table 10</ref> shows the performance of two classi- fier systems, trained with supervision (on CoNLL- train) and without supervision on native data (Web1T), and compares these to an MT approach trained on CoNLL-train. The first classifier system performs comparably to the MT system <ref type="bibr">(27.76 vs. 28.25)</ref>, however, the native-trained classifier sys- tem outperforms both, and does not use any an- notated data. The native-trained classifier system would place fourth in CoNLL-2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Flexibility</head><p>We now explore another advantage of the classifier-based approach, that of allowing for a flexible architecture where we can tailor knowl- edge sources for individual phenomena. In Sec- tion 4.2.1, we already took advantage of the fact that in the classifier framework it is easy to in- corporate features suited to individual error types. We now show that by adding supervision in a way tailored toward specific errors we can further im- prove the classifier-based approach. Adding Supervision in a Tailored Way There is a trade-off between training on native and learner  data. The advantage of training on native data is clearly the size, which is important for estimating context parameters. Learner data provides addi- tional information, such as learner error patterns and the manner of non-native writing.</p><p>Instead of choosing to train on one data type, the classifier framework allows one to combine the two data sources in various ways: voting ), alternating structure op- timization ( <ref type="bibr">Dahlmeier and Ng, 2011)</ref>, training a meta-classifier <ref type="bibr" target="#b10">(Gamon, 2010)</ref>, and extracting er- ror patterns ( ). We compare two approaches of adding supervision: (1) Learner error patterns: Error patterns are ex- tracted from learner data and "injected" into mod- els trained on native data ( ). Learner data is used to estimate mistake pa- rameters; contextual cues are based on native data. (2) Learner error patterns+native predictions: Classifiers are trained on native data. Classifier predictions are used as features in models trained on learner data. Learner data thus contributes both the specific manner of learner writing and the mis- take parameters. The native data contributes con- textual information.</p><p>We found that (2) is superior to (1) for arti- cle, agreement, and preposition errors; (1) works better on verb form and word form errors; and noun number errors perform best when a classifier is trained on native data. (Learner error patterns were found not to be beneficial for correcting noun number errors ). Tai- lored supervision yields an improvement of almost 3 points over the system trained on native data and almost 9 points over the system trained on learner data <ref type="table" target="#tab_10">(Table 11)</ref>. Adding Mechanical Errors Finally, we add components for mechanical errors: punctuation, spelling, and capitalization. These are distin- guished from the grammatical mistakes, as they are not specific to GEC and can be handled with existing resources or simple methods.</p><p>For capitalization and missing commas, we  <ref type="bibr">60.79 19.93 43.11 CoNLL-2014</ref><ref type="bibr">top system 39.71 30.10 37.33 Susanto et al. (2014</ref> 53.55 <ref type="bibr">19.14 39.39 Miz. &amp; Mats. (2016)</ref> 45.80 26.60 40.00  compile a list of patterns using CoNLL training data. We also use an off-the-shelf speller <ref type="bibr" target="#b8">(Flor, 2012;</ref><ref type="bibr" target="#b7">Flor and Futagi, 2012)</ref>. Results are shown in <ref type="table" target="#tab_2">Table 12</ref>. Performance improves by almost 5 and 7 points for the native-trained system and for the best configuration of classifiers with supervision. Both systems also outperform the top CoNLL sys- tem, by 1 and 6 points, respectively. The result of 43.11 by the best classifier configuration substan- tially outperforms the existing state-of-the-art: a combination of two MT systems and two classi- fier systems, and MT with re-ranking ( <ref type="bibr" target="#b24">Mizumoto and Matsumoto, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Combining MT and Classifier Systems</head><p>Since MT and classifiers differ with respect to the types of errors they can better handle, we combine these systems in a pipeline architecture where the MT is applied to the output of classi- fiers. Classifiers are applied first, since MT is bet- ter at handling complex phenomena. First, we add the speller and those classifier components that perform substantially better than MT (articles and verb agreement), due to the ability of classifiers to generalize beyond lexical information. The added classifiers are part of the best system in <ref type="table" target="#tab_2">Table 12</ref>.</p><p>Results are shown in  <ref type="table" target="#tab_2">(Table 12)</ref> 60.79 19.93 43.11 Best class.+MT (CoNLL-train) 51.92 25.08 42.77 Best class.+MT <ref type="bibr">(Lang-8)</ref> 60.17 25.64 47.40  CoNLL-train and Lang-8, respectively. Notably, the CoNLL-train MT system especially benefits, which shows that when the parallel data is small, it is particularly worthwhile to add classifiers. It should be stressed that even with a smaller parallel corpus, when the three modules are added, the resulting system is very competitive with pre- vious state-of-the-art that uses a lot more super- vision:  and <ref type="bibr" target="#b24">Mizumoto and Matsumoto (2016)</ref> use Lang-8. These results show that when one has an MT system, it is possi- ble to improve by investing effort into building se- lect classifiers for phenomena that are most chal- lenging for MT.</p><p>Finally, <ref type="table" target="#tab_15">Table 14</ref> demonstrates that combining MT with the best classifier system improves the result further when the MT system is trained on Lang-8, but not when the MT system is trained on CoNLL-train. We also note that the CoNLL-train MT system also has a much lower precision than the other systems. We conclude that when only a limited amount of data is available, the classifier approach on its own performs better.</p><p>As a summary, <ref type="table" target="#tab_16">Table 15</ref> lists the best sys- tems developed in this work -a classifier sys- tem, a pipeline of select classifiers and MT, and a pipeline consisting of the best classifier and the MT systems -and compares to existing state-of- the-art. Our classifier system is a 3-point improve- ment over the existing state-of-the-art, while the best pipeline is a 7.4-point improvement (20% rel- ative improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusions</head><p>A recent surge in GEC research has produced two leading state-of-the-art approaches -machine learning classification and machine translation. Based on the analysis of the methods and an er- ror analysis on the outputs of state-of-the-art sys- tems that adopt these approaches, we explained the differences and the key advantages of each. With respect to error phenomena, we showed that while MT is better at handling complex mistakes, classifiers are better at correcting mistakes that re- quire abstracting beyond lexical context. We fur- ther showed that the key strengths of the classifi- cation framework are its flexibility and the ability to train without supervision.</p><p>We built several systems that draw on the strengths of each approach individually and in a pipeline. The best classifier system and the pipelines outperform reported best results on the task, often by a large margin.</p><p>The purpose of this work is to gain a better understanding of the advantages offered by each learning method in order to make further progress on the GEC task. We showed that the values pro- vided by each method can be exploited within each approach and in combination, depending on the re- sources available, such as annotated learner data (MT), and additional linguistic resources (clas- sifiers). As a result, we built robust systems and showed substantial improvement over existing state-of-the-art.</p><p>For future work, we intend to study the problem in the context of other languages. However, it is important to realize that the problem is far from being solved even in English, and the current work makes very significant progress on it. of 39th Annual Meeting of the Association for Computa- tional Linguistics, pages 26-33, Toulouse, France, July.</p><p>T. <ref type="bibr">Brants and A. Franz. 2006</ref>. Web 1T 5-gram Version 1.</p><p>Linguistic Data Consortium.</p><p>A. <ref type="bibr">Carlson and I. Fette. 2007</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) and</head><label>and</label><figDesc></figDesc><table>System 
Method 
Performance 
P 
R 
F0.5 
CoNLL-2014 top 3 
MT 
41.62 21.40 35.01 
CoNLL-2014 top 2 
Classif. 
41.78 24.88 36.79 
CoNLL-2014 top 1 
MT, rules 
39.71 30.10 37.33 
Susanto et al. (2014) MT, classif. 53.55 19.14 39.39 
Miz. &amp; Mats. (2016) MT 
45.80 26.60 40.00 
This work 
MT, classif. 60.17 25.64 47.40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example errors. In the parentheses, the error codes used in the shared task are shown. Errors 
exemplifying the relevant phenomena are marked; the sentences may contain other mistakes. 

Rank System F0.5 Approach 
External training data 
External 
name 
Native data 
Learner data 
error modules 

1 
CAMB 37.33 Rules and MT 
Microsoft Web LM 
Cambridge Corpus, Eng. 
Vocab Profile 
Cambridge "Write 
and Improve" 
2 
CUUI 
36.79 Classif.; patterns Web1T 
3 
AMU 
35.01 MT 
Wikipedia, CommonCrawl Lang-8 
4 
POST 
30.88 LM and rules 
Web1T 
PyEnchant Spell 

5 
NTHU 
29.92 
Rules, MT, clas-
sif. 
Web1T, Gigaword, BNC, 
Google Books 
Spellcheckers: As-
pell, GingerIt 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The top 5 systems in CoNLL-2014. The last column lists external proofing tools used. LM 
stands for language models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). While more advanced MT models can abstract by adding higher-level</figDesc><table>Error 
AMU (MT) 
CUUI (Classif.) 
type 
P 
R F0.5 
P 
R F0.5 
Orthog./punc. (Mec) 61.6 16.3 39.6 53.3 8.7 26.4 
Article (ArtOrDet) 
38.0 10.9 25.4 31.8 47.9 34.0 
Preposition (Prep) 
54.9 10.4 29.5 31.7 8.8 20.9 
Noun number (Nn) 
49.6 43.2 48.2 42.5 46.2 43.2 
Verb tense (Vt) 
30.2 9.3 20.8 61.1 5.4 19.9 
Subj.-verb agr. (SVA) 48.3 14.9 33.3 57.7 57.7 57.7 
Verb form (Vform) 
40.5 16.8 31.8 69.2 15.1 40.3 
Word form (Wform) 59.0 36.6 52.6 60.0 13.5 35.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Performance of MT and classifier sys- tems from CoNLL-2014 on common errors. features such as POS, previous attempt yielded only marginal improvements (Mizumoto and Mat- sumoto, 2016), since one typically needs different types of abstractions depending on the error type, as we show below.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Data used in the experiments. Corpora sizes are in the number of words.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>2211</head><label>2211</label><figDesc></figDesc><table>Training 

Performance 
data 
P 
R 
F0.5 
(1) Learner 
32.15 17.96 27.76 
(2) Native 
38.41 23.05 33.89 
(3) Tailored 
57.07 14.74 36.26 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Classifiers: supervision in a tailored 
way. Trained on (1) learner data (CoNLL-train); 
(2) native data (Web1T); (3) data sources tailored 
per error type. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table>Classifier systems in this work. Com-
parison to existing state-of-the-art. 

System 
Performance 
P 
R 
F0.5 
MT is trained on CoNLL-train 
MT 
43.34 11.81 28.25 
Spelling+MT 
49.86 16.36 35.37 
Article+MT 
45.11 13.99 31.22 
Verb agr.+MT 
46.36 14.63 32.33 
Art.+Verb agr.+Spell+MT 52.07 20.89 40.10 
MT is trained on Lang-8 
MT 
66.15 15.11 39.48 
Spelling+MT 
65.87 16.94 41.75 
Article+MT 
63.81 17.70 41.95 
Verb. agr.+MT 
66.09 18.01 43.08 
Art.+Verb agr.+Spell+MT 64.13 22.15 46.51 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 13 :</head><label>13</label><figDesc>Pipelines: select classifiers and MT.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 13 .</head><label>13</label><figDesc></figDesc><table>Adding classi-
fiers improves the performance, thereby demon-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>Table 14 :</head><label>14</label><figDesc></figDesc><table>Pipelines: the best classifier system 
and MT systems. 

System 
Performance 
P 
R 
F0.5 
Best classifier (Table 12) 
60.79 19.93 43.11 
Art.+Verb agr.+Spell+MT 64.13 22.15 46.51 
Best classifier+MT 
60.17 25.64 47.40 
CoNLL-2014 top system 
39.71 30.10 37.33 
Susanto et al. (2014) 
53.55 19.14 39.39 
Miz. &amp; Mats. (2016) 
45.80 26.60 40.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 15 :</head><label>15</label><figDesc></figDesc><table>Best systems in this work. Comparison 
to existing state-of-the-art. 

strating that the classifiers address a complemen-
tary set of mistakes. Adding all three modules im-
proves the results from 28.25 to 40.10 and from 
39.48 to 46.51 for the MT systems trained on 
</table></figure>

			<note place="foot" n="1"> Outputs are available on the CoNLL-2014 website.</note>

			<note place="foot" n="4"> Developing New State-of-the-Art MT and Classifier Systems In this section, we explore the advantages of each learning approach, as identified in the previous section, within each learning framework. To this end, drawing on the strengths of each framework, we develop new state-of-the-art MT and classifier systems. 2 In the next section, we will use these 2 Implementation details can be found at cogcomp.cs. illinois.edu/page/publication view/793</note>

			<note place="foot" n="3"> http://cogcomp.cs.illinois.edu/page/ software view/POS 4 http://cogcomp.cs.illinois.edu/page/ software view/Chunker</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Michael Flor for his help with running the spelling system on the data and John Wieting for sharing the English Wikipedia corpus. The authors are also grateful to Mark Sammons, Peter Chew, and the anonymous reviewers for the insightful comments on the paper. The work of Dan Roth on this project was supported by DARPA under agree-ment number FA8750-13-2-0008. Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the view of the agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scaling to very very large corpora for natural language disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings R. Dale and A. Kilgarriff</title>
		<meeting>R. Dale and A. Kilgarriff</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Proceedings of the 13th</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">European Workshop on Natural Language Generation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the NAACL Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation for machine translation by mining unseen words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jagarlamudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A sequential model for multi class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Even-Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A classifier-based approach to preposition and determiner error correction in L2 English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ø</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kochmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On using context for automatic correction of non-word misspellings in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Futagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics</title>
		<meeting>the 7th Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Four types of context for automatic spelling correction. Traitement Automatique des Langues (TAL). (Special Issue: Managing noise in the signal: error handling in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="61" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using contextual speller techniques and language modeling for ESL error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using mostly native data to correct errors in learners&apos; writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Applying Winnow to context-sensitive spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Winnow based approach to context-sensitive spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting errors in English article usage by non-native speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using an errorannotated learner corpus to develop and ESL/EFL error correction system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine translation system combination with flexible word ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hanneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Fourth Workshop on Statistical Machine Translation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic error detection in the Japanese learners&apos; English spoken data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Izumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Supnithi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast exact inference with a factored model for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical phrasebased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An analysis of grammatical errors in non-native speech in English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Spoken Language Technology Workshop</title>
		<meeting>the 2008 Spoken Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ch</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative reranking for grammatical error correction with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning SNS for automated japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The CoNLL-2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL: Shared Task</title>
		<meeting>CoNLL: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL: Shared Task</title>
		<meeting>CoNLL: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated whole sentence grammar correction using a noisy channel model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The use of classifiers in sequential inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Based Java for Rapid Development of NLP Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rizzolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating confusion sets for context-sensitive error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training paradigms for correcting errors in grammar and usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for ESL correction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint learning and inference for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building a State-of-theArt Grammatical Error Correction System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">University of Illinois system in HOO text correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gioja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Workshop on Natural Language Generation (ENLG)</title>
		<meeting>the European Workshop on Natural Language Generation (ENLG)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The University of Illinois and Columbia system in the CoNLL-2014 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL Shared Task</title>
		<meeting>CoNLL Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">System combination for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using parse features for preposition selection and error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
