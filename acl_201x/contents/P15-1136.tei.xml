<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity-Centric Coreference Resolution with Model Stacking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
							<email>kevclark@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Entity-Centric Coreference Resolution with Model Stacking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1405" to="1415"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Mention pair models that predict whether or not two mentions are coreferent have historically been very effective for coref-erence resolution, but do not make use of entity-level information. However, we show that the scores produced by such models can be aggregated to define powerful entity-level features between clusters of mentions. Using these features, we train an entity-centric coreference system that learns an effective policy for building up coreference chains incrementally. The mention pair scores are also used to prune the search space the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. Many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the corefer- ence decisions ( <ref type="bibr">Soon et al., 2001;</ref><ref type="bibr" target="#b27">Ng and Cardie, 2002;</ref><ref type="bibr" target="#b3">Bengtson and Roth, 2008)</ref>. However, such systems do not make use of entity-level informa- tion, i.e., features between clusters of mentions in- stead of pairs.</p><p>Using entity-level information is valuable be- cause it allows early coreference decisions to in- form later ones. For example, finding that Clin- ton and she corefer makes it more likely that Clin- ton corefers with Hillary Clinton than Bill Clin- ton due to gender agreement constraints. Such in- formation has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the in- formation from the partially completed corefer- ence chains produced so far to guide later deci- sions ( <ref type="bibr" target="#b31">Raghunathan et al., 2010;</ref><ref type="bibr" target="#b37">Stoyanov and Eisner, 2012;</ref><ref type="bibr" target="#b24">Ma et al., 2014</ref>).</p><p>However, defining useful features between clus- ters of mentions and learning an effective policy for incrementally building up clusters can be chal- lenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of men- tions ( <ref type="bibr" target="#b15">Fernandes et al., 2012;</ref><ref type="bibr" target="#b13">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b6">Chang et al., 2013)</ref>. In this paper we in- troduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture dif- ferent linguistic phenomena in coreference resolu- tion. We then describe how the probabilities pro- duced by these models can be used to generate expressive features between clusters of mentions. Using these features, we train an entity-centric in- cremental coreference system.</p><p>The entity-centric system builds up coreference chains with agglomerative clustering: each men- tion starts in its own cluster and then pairs of clus- ters are merged each step. We train an agent to determine whether it is desirable to merge a par- ticular pair of clusters using an imitation learning algorithm based on DAgger ( <ref type="bibr" target="#b35">Ross et al., 2011</ref>). Previous incremental coreference systems heuris- tically define which actions are beneficial for the agent to perform, but we instead propose a way of assigning exact costs to actions based on coref- erence evaluation metrics, adding a concept of the severity of a mistake. Furthermore, rather than considering all pairs of clusters as candidate merges, we use the scores of the pairwise mod- els to reduce the search space, first by providing an ordering over which merges are considered and secondly by discarding merges that are not likely to be good. This greatly reduces the time it takes to run the agent, making learning computationally feasible.</p><p>Imitation learning is challenging because it is a non-i.i.d. learning problem; the distribution of states seen by the agent depends on the agent's pa- rameters. Model stacking offers a way of decom- posing the learning problem by training pairwise models with many parameters in a straightforward supervised learning setting and using their outputs for training a much simpler model in the more difficult imitation learning setting. Furthermore, mention pair scores can produce powerful features for training the agent because the scores indicate which mention pairs between the clusters in ques- tion are relevant; high scoring and low scoring pairs can indicate when a merge should be forced or disallowed while other mention pairs may pro- vide little useful information.</p><p>We run experiments on the English portion of the 2012 CoNLL Shared Task dataset. The entity- centric clustering algorithm greatly outperforms commonly used heuristic methods for coordinat- ing pairwise scores to produce a coreference par- tition. We also show that combining the scores of different pairwise models designed to capture different aspects coreference results in significant gains in accuracy. Our final system gets a com- bined score of 63.02 on the dataset, substantially outperforming other state of the art systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mention Pair Models</head><p>Mention pair models predict whether or not a given pair of mentions belong in the same coref- erence cluster. We incorporate two different men- tion pair models into our system. However, other pairwise models could easily be added; one advan- tage of our model stacking approach is that it can combine different simple classifiers in a modular way.</p><p>Our two models are designed to capture dif- ferent aspects of coreference. The first one is built to predict coreference for all of the candi- date antecedents of a mention. This makes it use- ful for providing scores when the current mention has clear coreference links to many previous men- tions. For example President Clinton might be linked to the president, Bill Clinton, and Mr. Pres- ident.</p><p>However, mentions often only have one clear antecedent. This is especially common in pronom- inal anaphora resolution, such as in the sentence Bill arrived, but nobody saw him. The pronoun him is directly referring back to a previous part of the discourse, not some entity that other mentions may also refer to. However, there still might be coreference links between him and previous men- tions in the text because of transitivity: any other mention about Bill would be coreferent with him. For such mentions, there may be very little ev- idence in the discourse to suggest a coreference link, so attempting to train a model to predict these will bear little fruit. With this as motivation, we also train a model to predict only one correct an- tecedent of the current mention.</p><p>We found a classification model to be well suited for the first task and and a ranking model to be well suited for the second one. These two mod- els differ only in the training criteria used. Both models use a logistic classifier to assign a proba- bility to a mention m and candidate antecedent a representing the likelihood that the two mentions are coreferent. The candidate antecedent a may take on the value NA indicating that m has no an- tecedent. The probability of coreference takes the standard logistic form:</p><formula xml:id="formula_0">p θ (a, m) = (1 + e θ T f (a,m) ) −1</formula><p>where f (a, m) is a vector of feature functions on a and m and θ are the feature weights we wish to learn. Let M denote the set of all mentions in the training set, T (m) denote the set of true an- tecedents of a mention m (i.e., mentions that occur before m in the text that are coreferent with m or {NA} if m has no antecedent), and F(m) denote the set of false antecedents of m. We want to find a parameter vector θ that assigns high probabili- ties to the candidate antecedents in T (m) and low probabilities to the ones in F(m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification Model</head><p>For the classification model, we consider each pair of mentions independently with the goal of pre- dicting coreference correctly for as many of them as possible. The model is trained by minimiz- ing negative conditional log likelihood augmented with L1 regularization:</p><formula xml:id="formula_1">L c (θ c ) = − m∈M t∈T (m) log p θc (t, m) + f ∈F (m) log(1 − p θc (f, m)) + λ||θ c || 1</formula><p>By summing over all candidate antecedents, the objective encourages the model to produce good probabilities for all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking Model</head><p>For the ranking model, candidate antecedents for a mention are considered simultaneously and com- pete with each other to be matched with the cur- rent mention. This makes the model well suited to the task of finding a single best antecedent for a mention. A natural learning objective for such a model would be a max-margin training crite- ria that encourages separation between the highest scoring true antecedent and highest scoring false antecedent of the current mention. However, we found such models to be poor at producing scores useful for a downstream clustering model because a max-margin objective encourages scores for true antecedents to be high only relative to other can- didate antecedents. It is much more beneficial to have mention pair scores that are comparable across different mentions as well as different can- didate antecedents. For this reason, we instead train the model with an objective that maximizes the conditional log likelihood of the highest scor- ing true and false antecedents under the logistic model:</p><formula xml:id="formula_2">L r (θ r ) = − m∈M max t∈T (m) log p θr (t, m) + min f ∈F (m) log(1 − p θr (f, m)) + λ||θ r || 1</formula><p>For both models, we set λ = 0.001 and opti- mize their objectives using AdaGrad (Duchi et al., 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Features</head><p>Our mention pair models use a variety of common features for mention pair classification (for more details see <ref type="bibr" target="#b3">(Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b38">Stoyanov et al., 2010;</ref><ref type="bibr" target="#b21">Lee et al., 2011;</ref><ref type="bibr" target="#b34">Recasens et al., 2013)</ref>). These include</p><p>• Distance features, e.g., the distance between the two mentions in sentences or number of mentions.</p><p>• Syntactic features, e.g., number of embed- ded NPs under a mention, POS tags of the first, last, and head word.</p><p>• Semantic features, e.g., named entity type, speaker identification.</p><p>• Rule-based features, e.g., exact and partial string matching.</p><p>• Lexical Features, e.g., the first, last, and head word of the current mention.</p><p>We also employ a feature conjunction scheme sim- ilar to the one described by <ref type="bibr" target="#b13">Durrett and Klein (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Entity-Centric Coreference Model</head><p>Mention pair scores alone are not enough to pro- duce a final set of coreference clusters because they do not enforce transitivity: if the pair of men- tions (a, b) and the pair of mentions (b, c) are deemed coreferent by the model, there is no guar- antee that the model will also classify (a, c) as coreferent. Thus a second step is needed to co- ordinate the scores to produce a final coreference partition. A widely used approach for this is best- first clustering ( <ref type="bibr" target="#b27">Ng and Cardie, 2002</ref>). For each mention, the best-first algorithm assigns the most probable preceding mention classified as corefer- ent with it as the antecedent. The primary weakness of this approach is that it only relies on local information to make decisions, so it cannot consolidate information at the entity level. As a result, coreference chains produced by such algorithms can exhibit low coherency. For example, a cluster may consist of [Hillary Clin- ton, Clinton, he] because the coreference decision between Hillary Clinton and Clinton is made in- dependently of the one between Clinton and he.</p><p>To tackle this problem, we build an entity- centric model that operates between pairs of clus- ters instead of pairs of mentions, guided by scores produced by the pairwise models. It builds up clusters of mentions believed to refer to the same entity as it goes, relying on the partially formed clusters produced so far to make decisions. For example, the system could reject linking <ref type="bibr">[Hillary Clinton]</ref> with <ref type="bibr">[Clinton, he]</ref> because of the low score between the pair (Hillary Clinton, he).</p><p>Our entity-centric "agent" builds up coreference chains with agglomerative clustering. It begins in a start state where each mention is in a sepa- rate single-element cluster. At each step, it ob- serves the current state s, which consists of all par- tially formed coreference clusters produced so far, and selects some action a which merges two exist- ing clusters. The action will result in a new state with new candidate actions and the process is re- peated. The model is entity-centric in that it builds up clusters of mentions representing entities and merges clusters if it predicts they are representing the same one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test-time Inference</head><p>The agent assigns a score to each action a using a linear model with feature function f e and weight vector θ e : s θe (a) = θ T e f e (a). A particular setting of θ e defines a policy π that determines which ac- tion a = π(s) the agent will take in state s. This policy is to greedily take highest scoring candidate action available from the current state.</p><p>Rather than using all possible cluster merges as the candidate set of actions the agent selects from, we use the scores produced by mention pair mod- els to reduce the search space. First, we order all mention pairs in the document in descending or- der according to their pairwise scores. This causes clustering to occur in an easy-first fashion, where harder decisions are delayed until more informa- tion is available. Secondly, we discard all men- tion pairs that score below a threshold t under the assumption that the clusters containing these pairs are unlikely to be coreferent. In our experi- ments we were able able set t so that over 95% of pairs were removed with no decrease in accuracy. Lastly, we iterate through this list of pairs in or- der. For each pair, we make a binary decision on whether or not the clusters containing these pairs should be merged. This formulates the agent's task so it only has two actions to chose from instead of a number of actions proportional to the number of clusters squared. Algorithm 1 shows the full test- time procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning</head><p>Imitation Learning with DAgger We face a sequential prediction problem where fu- ture observations (visited states) depend on previ- ous actions. This is challenging because it violates the common i.i.d. assumptions made in statistical learning. Imitation learning, where expert demon- strations of good behavior are used to teach the agent, has proven very useful in practice for this sort of problem ( <ref type="bibr" target="#b0">Argall et al., 2009</ref>). We use imita- tion learning to set the parameters θ e of our agent by training it to classify whether a particular action is the one an expert policy would take in the cur- rent state. In particular, we use θ e as parameters for a binary logistic classifier that predicts which action (merge or do not merge) matches the expert policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Inference method: agglomerative clustering</head><p>Input: Set of mentions in document M, pair- wise classifier with parameters θ c , agent with parameters θ e , cutoff threshold t Output: Clustering C</p><p>Initialize list of mention pairs</p><formula xml:id="formula_3">P → [] for each pair (m i , m j ) ∈ M 2 with i &lt; j do if p θc (m i , m j ) &gt; t then P .append((m i , m j )) end if end for Sort P in descending order according to p θc Initialize C → initial clustering with each men- tion in M in its own cluster for (m i , m j ) ∈ P do if C[m i ] = C[m j ] and s θe (C[m i ], C[m j ]) &gt; 0 then DoMerge(C[m i ], C[m j ], C) end if end for</formula><p>We found the DAgger (Ross et al., 2011) imita- tion learning method (see Algorithm 2) to be effec- tive for this task. DAgger is an iterative algorithm that aggregates a dataset D consisting of states and the actions performed by the expert policy in those states. At each iteration, it first samples a trajec- tory of states visited by the current policy by run- ning the policy to completion from the start state. It then labels those states with the best action ac- cording to the expert policy, adds those labeled ex- amples to the dataset, and then trains a new clas- sifier over the dataset to get a new policy. When producing a trajectory to train on, the expert pol- icy is stochastically mixed with the current policy; with probability β i the expert's action is chosen in- stead of the current policy's. We set β so it decays exponentially as the iteration number increases.</p><p>By sampling trajectories under the current policy, DAgger exposes the system to states at train time similar to the ones it will face at test time. In contrast, training the agent on the gold labels alone would unrealistically teach it to make decisions under the assumption that all previous decisions were correct, potentially causing it to over-rely on information from past actions. This is especially problematic in coreference, where the error rate is quite high. Even when using DAgger, this problem could exist to a lesser degree if the model heavily overfits to the training data. How- ever, the agent has a small number of parameters thanks to our model stacking approach, reducing the risk of this happening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Learning method: DAgger</head><p>Input: initial policyˆπpolicyˆ policyˆπ 1 , expert policy π * Output:</p><formula xml:id="formula_4">final policyˆπpolicyˆ policyˆπ N Initialize D ← ∅ for i = 1 to N do Let π i = β i π * + (1 − β i )ˆ π i</formula><note type="other">Sample a trajectory under the current policy using π i Get dataset D i = (s, π * (s)) of states visited by π i and actions given by the expert Aggregate datasets: D ← D ∪ D i Train classifierˆπclassifierˆ classifierˆπ i+1 on D end for</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assigning Costs to Actions</head><p>A key aspect of incrementally building corefer- ence clusters is that some local decisions are much more important than others. For example, a merge between two large clusters influences the score far more than a merge between two small ones. Ad- ditionally, getting early decisions correct is crucial because later actions are dependent on early ones, causing errors to compound if mistakes are made early. To capture this, we take an approach in- spired by the SEARN learning algorithm <ref type="bibr" target="#b9">(Daumé et al., 2009)</ref> and add costs to the actions in the aggregated dataset. We then train the agent to do cost-sensitive classification. Using these costs, we simply define the expert policy as the policy that takes the action with the lowest cost at each step.</p><p>We want our costs to represent how a partic- ular local decision will affect the final score of the coreference system. Unfortunately, standard coreference evaluation metrics do not decompose over cluster merges. Instead, we compute the loss of an action by "rolling out" the current policy to completion. More concretely, let m be a function (such as a coreference evaluation metric) that as- signs scores to states; we are interested in reach- ing a final state for which m is high. Suppose we are assigning costs to the set of actions A(s) that can be taken from some state s. For each action a ∈ A(s), we apply that action to s to get a new state s , run the current policyˆπpolicyˆ policyˆπ i from s to com- pletion, and then compute the value of m on the resulting final state. This gives exactly the final score the system would get if it made the action a from state s and then continued under the current policy. Let f m (s, a) denote this value for a par- ticular metric, state, and action. We assign each action the regret r associated with taking that ac- tion under the current policy as a cost:</p><formula xml:id="formula_5">r(s, a) = max a ∈A(s) f m (s, a ) − f m (s, a)</formula><p>The "rolling out" procedure means we naively have to visit O(t 2 ) states each iteration instead of t, where t is the length of a trajectory. However, the highly constrained action space described in section 3.1 combined with the use of memoization allows the algorithm to still run efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving Runtime with Memoization</head><p>During training, the agent will see many of the same states and actions multiple times. We can ex- ploit this with memoization, significantly improv- ing the algorithm's runtime. In particular, we store the following values:</p><p>• Given a state s and action a, the value of the cost function, r(s, a).</p><p>• Given an action a, the score the model as- signs that action, s θe (a).</p><p>• Given an action a, the result of the feature function on that action, f e (a).</p><p>The first two values depend on the current model, so the saved values must be cleared between iter- ations of training. In experiments on the develop- ment set of the CoNLL 2012 corpus, these tables had 76%, 94%, and 93% hit rates respectively af- ter 50 passes over the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Features</head><p>Our agent uses features that are derived from the scores produced by the two mention pair mod- els. Although these scores only operate on men- tion pairs, they are combined to capture cluster- level interactions by being aggregated in differ- ent ways over pairs of mentions from the clus- ters. Mention pair scores can produce powerful features for training the agent because they show which mention pairs between the clusters in ques- tion are relevant, and often a small subset of the mention pairs provide far more information than the rest. For example, a strong negative pairwise • The minimum and maximum probability of coreference.</p><p>• The average probability and average log probability of coreference.</p><p>• The average probability and log proba- bility of coreference for a particular pair of grammatical types of mentions (either pronoun or non-pronoun).</p><p>For exam- ple, Avg-Prob non-pronoun pronoun gives the average probability of coreference when the candidate antecedent is not a pro- noun and the candidate anaphor is a pronoun.</p><p>Note that the averaged features have a natural probabilistic interpretation; the average probabil- ity corresponds to the expected number of coref- erence links between the involved mention pairs while the average log probability corresponds to the probability that all mention pairs will have a coreference link. All of these features are com- puted twice: once with the classification model and once with the ranking model.</p><p>We also compute the following features based on other aspects of the current state:</p><p>• Whether a preceding mention pair in the list of mention pairs has the same candidate anaphor as the current one.</p><p>• The index of the current mention pair in the list divided by the size of the list, i.e., what percentage of the list have we seen so far.</p><p>• The number of mentions in the current docu- ment.</p><p>• The probability of the first-occurring men- tion in the second-occurring cluster not be- ing anaphoric (i.e., p θc (NA, m)). This pre- vents producing clusters that, for example, start with a pronoun.</p><p>Lastly, we take one feature conjunction with a boolean representing whether both clusters are size 1. In total, there are only 56 features af- ter the feature conjunction. However, these fea- tures provide strong signal because they are di- rectly related to the probabilities of mentions be- ing coreferent. In contrast, the pairwise models use thousands of features (after feature conjunc- tions), including lexical features that are extremely sparse. The pairwise models can easily exploit this much bigger feature set because they oper- ate in a classic supervised learning setting. The entity-centric model, on the other hand, learns in a much more challenging non-i.i.d. setting. Model stacking avoids the difficulty of directly training the entity-centric model with a large set of weak features by decomposing the task into first learn- ing to produce good pairwise scores and then us- ing those scores to generate a manageable set of strong features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Details</head><p>Because the entity-centric agent relies on the out- put of pairwise classifiers, they should not be trained on the same data. Therefore we split the training set into two sections and use one for train- ing the pairwise models and the other for training the agent. When evaluating on the development set, we use 80% of the documents in the training set to train the mention pair models and the rest to train the entity-centric model. When evaluating on the test set we use the whole training set for the mention pair models and the development set for the entity-centric model. We also tried using cross-validation instead of a single split, but found this did not improve performance, which we be- lieve to be because this trains the agent with dif- ferent pairwise models than the ones used at test time.</p><p>For our initial policyˆπpolicyˆ policyˆπ 1 , we set the parame- ters of the agent so it operates with simple best- first clustering (initializing all feature weights to 0 except for the maximum-score, anaphor-seen, and bias features). For m, the performance met- ric determining the action costs, we use a linear combination of the B 3 (Bagga and Baldwin, 1998) and MUC ( <ref type="bibr" target="#b39">Vilain et al., 1995</ref>) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAF φ 4 ) ( <ref type="bibr" target="#b23">Luo, 2005)</ref>, was not used because it is expensive to compute. We found weighting B 3 three times as much as MUC to be effective on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We apply our model to the English portion of the CoNLL 2012 Shared Task data <ref type="bibr" target="#b30">(Pradhan et al., 2012)</ref>, which is derived from the OntoNotes corpus ( <ref type="bibr" target="#b19">Hovy et al., 2006</ref>). The data is split into a training set of 2802 documents, development set of 343 documents, and a test set of 345 documents. We use the provided preprocessing for parse trees, named entity tags, etc. The models are evaluated using three of the most popular metrics for coreference resolution: MUC, B 3 , and Entity-based CEAFE (CEAF φ 4 ). We also include the average F 1 score (CoNLL F 1 ) of these three metrics, as is commonly done in CoNLL Shared Tasks. We used the most recent version of the CoNLL scorer (version 8.01), which implements the original definitions of these metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Detection</head><p>Our experiments were run using system-produced predicted mentions. We used the rule-based MUC B 3 CEAF φ 4 Avg. Classification, B.F. 72.00 60.01 55.63 62.55</p><p>Ranking, B.F. 71.91 60.63 56.38 62.97 Classification, E.C. 72.34 61.46 57.16 63.65</p><p>Ranking, E.C. 72.37 61.34 57.13 63.61 Both, E.C.</p><p>72.52 62.02 57.69 64.08 <ref type="table">Table 1</ref>: Metric scores on the development set for the classification and ranking pairwise mod- els when using best-first clustering (B.F.) or the entity-centric model (E.C.).</p><p>mention detection algorithm from <ref type="bibr" target="#b31">Raghunathan et al. (2010)</ref>, which first extracts pronouns and maximal NP projections as candidate mentions and then filters this set with rules that remove spurious mentions such as numeric entities or pleonastic it pronouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Models</head><p>We compare the effectiveness of the entity-centric model with the commonly used best-first cluster- ing approach, which assigns mentions the high- est scoring previous mention as the antecedent. Unlike the entity-centric model, the best-first ap- proach only relies on local information to make decisions. We also compare the effectiveness of the ranking and classification pairwise models. <ref type="table">Table 1</ref> shows the results of these models on the development set.</p><p>The entity-centric model outperforms best- first clustering for both mention pair models, demonstrating the utility of a learned, incremental clustering algorithm. The improvement is much greater for the classification pairwise model, caus- ing it to outperform the ranking model with the entity-centric clustering algorithm even though it performs significantly worse than the ranking model with best-first clustering. This suggests that although the ranking model is better at finding a single correct antecedent for a mention, the classification model is more useful for producing cluster-level features. Incorporating probabilities from both pairwise models further improved scores over using either model alone, indicating that the mention pair classifiers were successful in learning scoring functions useful in different circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating other Entity-Level Features</head><p>Although the entity-centric model has so far only MUC B 3 CEAF φ 4 Avg. Scores Only 72.52 62.02 57.69 64.08 +Agreement 72.59 61.98 57.58 64.05 <ref type="table">Table 2</ref>: Metric scores on the development set for the entity-centric model with and without the ad- dition of entity-level agreement features. used features derived from the scores produced by mention pair models, other entity-level features could easily be added. We experiment with this by adding four cluster-level agreement features based on gender, number, animacy, and named entity type. Each of these features can take on three val- ues: "same" (e.g., both clusters have gender value feminine), "compatible" (e.g., one cluster has gen- der value feminine while the other has value un- known), or "incompatible" (one cluster has gender value feminine while the other has value mascu- line). The cluster-level value for a particular fea- ture is the most common value among mentions in that cluster (e.g., if a cluster has 2 masculine men- tions, 1 feminine mention, and 1 unknown men- tion) the value is considered masculine. <ref type="table">Table 2</ref> shows the results.</p><p>Adding the additional features had no substan- tial impact on scores, suggesting that features de- rived from pairwise scores are sufficient for cap- turing this kind of entity-level information. A disagreement between clusters necessarily means there will be disagreements between some of the involved mentions, so features like the average and minimum probability between mention pairs will have lower values when a disagreement is present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final System Performance</head><p>In <ref type="table" target="#tab_1">Table 3</ref> we compare the results of our system with the following state-of-the-art approaches: the JOINT and INDEP models of the Berkeley sys- tem <ref type="bibr" target="#b14">(Durrett and Klein, 2014</ref>) (the JOINT model jointly does NER and entity linking along with coreference); the Prune-and-Score system ( <ref type="bibr" target="#b24">Ma et al., 2014</ref>); the HOTCoref system <ref type="bibr" target="#b5">(Björkelund and Kuhn, 2014)</ref>; the CPL 3 M sytem ( <ref type="bibr" target="#b6">Chang et al., 2013)</ref>; and Fernandes et al. We use the full entity- centric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase ac- curacy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F 1 score. The largest improvement is in the B 3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAF φ 4 F 1 and second highest MUC F 1 scores among the other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Both mention pair ( <ref type="bibr">Soon et al., 2001;</ref><ref type="bibr" target="#b27">Ng and Cardie, 2002;</ref><ref type="bibr" target="#b3">Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b38">Stoyanov et al., 2010;</ref><ref type="bibr" target="#b4">Björkelund and Farkas, 2012)</ref> and mention ranking models <ref type="bibr" target="#b11">(Denis and Baldridge, 2007b;</ref><ref type="bibr" target="#b32">Rahman and Ng, 2009</ref>) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering <ref type="bibr">(Soon et al., 2001</ref>) and best-first clustering <ref type="bibr" target="#b27">(Ng and Cardie, 2002</ref>) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce tran- sitivity <ref type="bibr" target="#b10">(Denis and Baldridge, 2007a;</ref><ref type="bibr" target="#b16">Finkel and Manning, 2008)</ref>, graph partitioning algorithms <ref type="bibr" target="#b26">(McCallum and Wellner, 2005;</ref><ref type="bibr" target="#b28">Nicolae and Nicolae, 2006</ref>), the Dempster-Shafer rule <ref type="bibr" target="#b20">(Kehler, 1997;</ref><ref type="bibr" target="#b2">Bean and Riloff, 2004</ref>), or correlational clustering <ref type="bibr" target="#b25">(McCallum and Wellner, 2003;</ref><ref type="bibr" target="#b17">Finley and Joachims, 2005</ref>). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation met- ric, and can use the outputs of more than one men- tion pair model.</p><p>Recently, coreference models using latent an- tecedents have gained in popularity and achieved state-of-the-art results <ref type="bibr" target="#b15">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b13">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b6">Chang et al., 2013;</ref><ref type="bibr" target="#b5">Björkelund and Kuhn, 2014</ref>). These learn a scor- ing function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference.</p><p>Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster ( <ref type="bibr" target="#b22">Luo et al., 2004;</ref><ref type="bibr" target="#b40">Yang et al., 2008;</ref><ref type="bibr" target="#b33">Rahman and Ng, 2011</ref>). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. <ref type="bibr" target="#b31">Raghunathan et al. (2010)</ref> take this approach with a rule-based system that runs in multiple passes  and Stoyanov and Eisner (2012) train a classi- fier to do this with a structured perceptron algo- rithm. Entity-level information has also been suc- cessfully incorporated in coreference systems us- ing joint inference <ref type="bibr" target="#b25">(McCallum and Wellner, 2003;</ref><ref type="bibr" target="#b7">Culotta et al., 2006;</ref><ref type="bibr" target="#b29">Poon and Domingos, 2008;</ref><ref type="bibr" target="#b18">Haghighi and Klein, 2010)</ref>, but these approaches do not directly learn parameters tuned so the sys- tem runs effectively at test time, while our imita- tion learning approach does.</p><p>Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by <ref type="bibr" target="#b8">Daumé et al. (2005)</ref> and <ref type="bibr" target="#b24">Ma et al. (2014)</ref>. Other works use structured perceptron models for the same purpose <ref type="bibr" target="#b37">(Stoyanov and Eisner, 2012;</ref><ref type="bibr" target="#b15">Fernandes et al., 2012;</ref><ref type="bibr" target="#b5">Björkelund and Kuhn, 2014</ref>). These systems all heuristically determine which actions are desirable for the system to perform. In contrast, our approach directly computes a cost for actions based on coreference evaluation met- rics. This means our system directly learns which actions lead to good clusterings instead of which look good locally according to a heuristic. Fur- thermore, the costs provide our system a measure of the severity of a mistake, which we argue is very beneficial for the coreference task.</p><p>Our model stacking approach further distin- guishes this work by providing a new way of defin- ing cluster-level features. The majority of useful features for coreference systems operate on pairs of mentions (in one of our experiments we show the addition of classic entity-level features does not improve our system), but incremental corefer- ence systems must make decisions involving many mention pairs. Other incremental coreference sys- tems either incorporate features from a single pair <ref type="bibr" target="#b37">(Stoyanov and Eisner, 2012)</ref> or average features across all pairs in the involved clusters <ref type="bibr" target="#b24">(Ma et al., 2014</ref>). Our system instead combines informa- tion from the involved mention pairs in a variety of ways with with higher order features produced from the scores of mention pair models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a new approach to coreference res- olution that trains an entity-centric system using the scores produced by mention pair models as features. The brunt of task-specific learning oc- curs within the mention pair models, which are trained in a straightforward supervised manner. Guided by the pairwise scores, our entity-centric agent then learns an effective procedure for build- ing up coreference clusters incrementally, using previous decisions to inform later ones. The agent benefits from using multiple mention pair mod- els designed to capture different aspects of coref- erence. Experiments show that the agent, which learns how to coordinate mention pair scores, out- performs the commonly used best-first method. We evaluate our final system on the English por- tion of the CoNLL 2012 Shared Task and report a significant improvement over the current state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of features generated for a candidate cluster merge. Weights on edges are the probabilities of coreference produced by a mention pair model.</figDesc><graphic url="image-1.png" coords="6,82.77,62.81,432.00,139.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of this work with other state-of-the-art approaches on the test set. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their thoughtful comments. Stanford University grate-fully acknowledges the support of the Defense Ad-vanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of robot learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Brenna D Argall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="469" to="483" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of contextual role knowledge for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Bean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Datadriven multilingual coreference resolution using resolver stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A constrained latent variable model for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">First-order probabilistic models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A largescale exploration of effective global features for a joint entity detection and tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint determination of anaphoricity and coreference resolution using integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A ranking approach to pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1588" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero Nogueira Dos</forename><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy Luiz</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enforcing transitivity in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL), Short Paper</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised clustering with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLTNAACL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ontonotes: the 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic coreference in information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stanford&apos;s multi-pass sieve coreference resolution system at the conll-2011 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A mentionsynchronous coreference resolution algorithm based on the bell tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prune-and-score: Learning for greedy coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, J Walker</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Mannem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward conditional models of identity uncertainty with application to proper noun coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI Workshop on Information Integration on the Web</title>
		<meeting>the IJCAI Workshop on Information Integration on the Web</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional models of identity uncertainty with application to noun coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bestcut: A graph algorithm for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Nicolae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="275" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint unsupervised coreference resolution with markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning-Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multi-pass sieve for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Karthik Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Sudarshan Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supervised models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Narrowing the modeling gap: a cluster-ranking approach to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="469" to="521" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The life and death of discourse entities: Identifying singleton mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLTNAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A machine learning approach to coreference resolution of noun phrases</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Easy-first coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2519" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Reconcile: A coreference resolution research platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Buttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hysom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Technical Report, Cornell University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th conference on Message understanding</title>
		<meeting>the 6th conference on Message understanding</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
	<note>Dennis Connolly, and Lynette Hirschman</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An entity-mention model for coreference resolution with inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew</forename><surname>Lim Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
