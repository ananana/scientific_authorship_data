<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">UMIACS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">UMIACS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Colorado</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">Daumé</forename><surname>Iii</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">UMIACS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1681" to="1691"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their inputs , which requires many expensive computations. We present a simple deep neural network that competes with and, in some cases, outperforms such models on sentiment analysis and factoid question answering tasks while taking only a fraction of the training time. While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words models by deepening our network and applying a novel variant of dropout. Moreover , our model performs better than syntactic models on datasets with high syntactic variance. We show that our model makes similar errors to syntactically-aware models, indicating that for the tasks we consider , nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vector space models for natural language process- ing (NLP) represent words using low dimensional vectors called embeddings. To apply vector space models to sentences or documents, one must first select an appropriate composition function, which is a mathematical process for combining multiple words into a single vector.</p><p>Composition functions fall into two classes: un- ordered and syntactic. Unordered functions treat in- put texts as bags of word embeddings, while syntac- tic functions take word order and sentence structure into account. Previously published experimental results have shown that syntactic functions outper- form unordered functions on many tasks <ref type="bibr" target="#b35">(Socher et al., 2013b;</ref><ref type="bibr" target="#b20">Kalchbrenner and Blunsom, 2013)</ref>.</p><p>However, there is a tradeoff: syntactic functions require more training time than unordered compo- sition functions and are prohibitively expensive in the case of huge datasets or limited computing re- sources. For example, the recursive neural network (Section 2) computes costly matrix/tensor products and nonlinearities at every node of a syntactic parse tree, which limits it to smaller datasets that can be reliably parsed.</p><p>We introduce a deep unordered model that ob- tains near state-of-the-art accuracies on a variety of sentence and document-level tasks with just min- utes of training time on an average laptop computer. This model, the deep averaging network (DAN), works in three simple steps:</p><p>1. take the vector average of the embeddings associated with an input sequence of tokens 2. pass that average through one or more feed- forward layers 3. perform (linear) classification on the final layer's representation</p><p>The model can be improved by applying a novel dropout-inspired regularizer: for each training in- stance, randomly drop some of the tokens' embed- dings before computing the average. We evaluate DANs on sentiment analysis and fac- toid question answering tasks at both the sentence and document level in Section 4. Our model's suc- cesses demonstrate that for these tasks, the choice of composition function is not as important as ini- tializing with pretrained embeddings and using a deep network. Furthermore, DANs, unlike more complex composition functions, can be effectively trained on data that have high syntactic variance. A qualitative analysis of the learned layers suggests that the model works by magnifying tiny but mean- ingful differences in the vector average through multiple hidden layers, and a detailed error analy- sis shows that syntactically-aware models actually make very similar errors to those of the more na¨ıvena¨ıve DAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Unordered vs. Syntactic Composition</head><p>Our goal is to marry the speed of unordered func- tions with the accuracy of syntactic functions. In this section, we first describe a class of un- ordered composition functions dubbed "neural bag- of-words models" (NBOW). We then explore more complex syntactic functions designed to avoid many of the pitfalls associated with NBOW mod- els. Finally, we present the deep averaging network (DAN), which stacks nonlinear layers over the tradi- tional NBOW model and achieves performance on par with or better than that of syntactic functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Bag-of-Words Models</head><p>For simplicity, consider text classification: map an input sequence of tokens X to one of k labels. We first apply a composition function g to the sequence of word embeddings v w for w ∈ X. The output of this composition function is a vector z that serves as input to a logistic regression function.</p><p>In our instantiation of NBOW, g averages word embeddings 1</p><formula xml:id="formula_0">z = g(w ∈ X) = 1 |X| w∈X v w .<label>(1)</label></formula><p>Feeding z to a softmax layer induces estimated probabilities for each output labeî</p><formula xml:id="formula_1">labeî y = softmax(W s · z + b),<label>(2)</label></formula><p>where the softmax function is</p><formula xml:id="formula_2">softmax(q) = exp q k j=1 exp q j (3)</formula><p>W s is a k × d matrix for a dataset with k output labels, and b is a bias term.</p><p>We train the NBOW model to minimize cross- entropy error, which for a single training instance with ground-truth label y is</p><formula xml:id="formula_3">(ˆ y) = k p=1 y p log(ˆ y p ).<label>(4)</label></formula><p>Before we describe our deep extension of the NBOW model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed fur- ther in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Considering Syntax for Composition</head><p>Given a sentence like "You'll be more entertained getting hit by a bus", an unordered model like NBOW might be deceived by the word "entertained" to return a positive prediction. In contrast, syn- tactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficiency in the process. In subsequent sections, we argue that this complexity is not matched by a corresponding gain in performance.</p><p>Recursive neural networks (RecNNs) are syntac- tic functions that rely on natural language's inher- ent structure to achieve state-of-the-art accuracies on sentiment analysis tasks <ref type="bibr" target="#b38">(Tai et al., 2015)</ref>. As in NBOW, each word type has an associated embed- ding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children <ref type="figure">(Figure 1, left)</ref>. A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product <ref type="bibr" target="#b35">(Socher et al., 2013b)</ref>. While RecNNs can model complex linguistic phenomena like negation ( <ref type="bibr" target="#b13">Hermann et al., 2013)</ref>, they require much more training time than NBOW models. The nonlinearities and matrix/tensor prod- ucts at each node of the parse tree are expen- sive, especially as model dimensionality increases. RecNNs also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models <ref type="bibr" target="#b25">(Li, 2014)</ref>. Finally, RecNNs require rela- tively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. <ref type="bibr" target="#b23">Kim (2014)</ref> shows that some of these issues can be avoided by using a convolutional network in- stead of a RecNN, but the computational complex- ity increases even further (see Section 4 for runtime comparisons).</p><p>What contributes most to the power of syntactic <ref type="figure">Figure 1</ref>: On the left, a RecNN is given an input sentence for sentiment classification. Softmax layers are placed above every internal node to avoid vanishing gradient issues. On the right is a two-layer DAN taking the same input. While the RecNN has to compute a nonlinear representation (purple vectors) for every node in the parse tree of its input, this DAN only computes two nonlinear layers for every possible input.</p><formula xml:id="formula_4">Predator c 1 is c 2 a c 3 masterpiece c 4 z1 = f (W c3 c4 + b) z2 = f (W c2 z1 + b) z3 = f (W c1 z2 + b) softmax softmax softmax RecNN Predator c 1 is c 2 a c 3 masterpiece c 4 av = 4 i=1 ci 4 h 1 = f (W 1 · av + b 1 ) h 2 = f (W 2 · h 1 + b 2 ) softmax DAN</formula><p>functions: the compositionality or the nonlineari- ties? <ref type="bibr" target="#b35">Socher et al. (2013b)</ref> report that removing the nonlinearities from their RecNN models drops per- formance on the Stanford Sentiment Treebank by over 5% absolute accuracy. Most unordered func- tions are linear mappings between bag-of-words features and output labels, so might they suffer from the same issue? To isolate the effects of syn- tactic composition from the nonlinear transforma- tions that are crucial to RecNN performance, we investigate how well a deep version of the NBOW model performs on tasks that have recently been dominated by syntactically-aware models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Averaging Networks</head><p>The intuition behind deep feed-forward neural net- works is that each layer learns a more abstract rep- resentation of the input than the previous one <ref type="bibr" target="#b3">(Bengio et al., 2013</ref>). We can apply this concept to the NBOW model discussed in Section 2.1 with the ex- pectation that each layer will increasingly magnify small but meaningful differences in the word em- bedding average. To be more concrete, take s 1 as the sentence "I really loved Rosamund Pike's per- formance in the movie Gone Girl" and generate s 2 and s 3 by replacing "loved" with "liked" and then again by "despised". The vector averages of these three sentences are almost identical, but the aver- ages associated with the synonymous sentences s 1 and s 2 are slightly more similar to each other than they are to s 3 's average. Could adding depth to NBOW make small such distinctions as this one more apparent? In Equa- tion 1, we compute z, the vector representation for input text X, by averaging the word vectors v w∈X . Instead of directly passing this representation to an output layer, we can further transform z by adding more layers before applying the softmax. Suppose we have n layers, z 1...n . We compute each layer</p><formula xml:id="formula_5">z i = g(z i−1 ) = f (W i · z i−1 + b i )<label>(5)</label></formula><p>and feed the final layer's representation, z n , to a softmax layer for prediction ( <ref type="bibr">Figure 1, right)</ref>. This model, which we call a deep averaging net- work (DAN), is still unordered, but its depth allows it to capture subtle variations in the input better than the standard NBOW model. Furthermore, com- puting each layer requires just a single matrix multi- plication, so the complexity scales with the number of layers rather than the number of nodes in a parse tree. In practice, we find no significant difference between the training time of a DAN and that of the shallow NBOW model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Dropout Improves Robustness</head><p>Dropout regularizes neural networks by randomly setting hidden and/or input units to zero with some probability p ( <ref type="bibr" target="#b14">Hinton et al., 2012;</ref><ref type="bibr" target="#b36">Srivastava et al., 2014</ref>). Given a neural network with n units, dropout prevents overfitting by creating an ensem- ble of 2 n different networks that share parameters, where each network consists of some combination of dropped and undropped units. Instead of drop- ping units, a natural extension for the DAN model is to randomly drop word tokens' entire word embed- dings from the vector average. Using this method, which we call word dropout, our network theoreti- cally sees 2 |X| different token sequences for each input X.</p><p>We posit a vector r with |X| independent Bernoulli trials, each of which equals 1 with prob- ability p. The embedding v w for token w in X is dropped from the average if r w is 0, which expo- nentially increases the number of unique examples the network sees during training. This allows us to modify Equation 1:</p><formula xml:id="formula_6">r w ∼ Bernoulli(p) (6) ˆ X = {w|w ∈ X and r w &gt; 0} (7) z = g(w ∈ X) = w∈ˆXw∈ˆ w∈ˆX v w | ˆ X| .<label>(8)</label></formula><p>Depending on the choice of p, many of the "dropped" versions of an original training instance will be very similar to each other, but for shorter inputs this is less likely. We might drop a very important token, such as "horrible" in "the crab rangoon was especially horrible"; however, since the number of word types that are predictive of the output labels is low compared to non-predictive ones (e.g., neutral words in sentiment analysis), we always see improvements using this technique.</p><p>Theoretically, word dropout can also be applied to other neural network-based approaches. How- ever, we observe no significant performance differ- ences in preliminary experiments when applying word dropout to leaf nodes in RecNNs for senti- ment analysis (dropped leaf representations are set to zero vectors), and it slightly hurts performance on the question answering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare DANs to both the shallow NBOW model as well as more complicated syntactic mod- els on sentence and document-level sentiment anal- ysis and factoid question answering tasks. The DAN architecture we use for each task is almost identi- cal, differing across tasks only in the type of output layer and the choice of activation function. Our results show that DANs outperform other bag-of- words models and many syntactic models with very little training time. <ref type="bibr">2</ref> On the question-answering task, DANs effectively train on out-of-domain data, while RecNNs struggle to reconcile the syntactic differences between the training and test data. <ref type="bibr">2</ref> Code at http://github.com/miyyer/dan. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Analysis</head><p>Recently, syntactic composition functions have revolutionized both fine-grained and binary (pos- itive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rot- ten Tomatoes (RT) movie reviews dataset (Pang and <ref type="bibr" target="#b29">Lee, 2005</ref>) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) in- troduced by <ref type="bibr" target="#b35">Socher et al. (2013b)</ref>. Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). the word-representation restricted Boltzmann ma- chine ( <ref type="bibr">Dahl et al., 2012, WRRBM)</ref>, which only works on the document-level IMDB task. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Neural Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Non-Neural Baselines</head><p>We also compare to non-neural baselines, specif- ically the bigram na¨ıvena¨ıve Bayes (BINB) and na¨ıvena¨ıve Bayes support vector machine (NBSVM-BI) mod- els introduced by <ref type="bibr" target="#b40">Wang and Manning (2012)</ref>, both of which are memory-intensive due to huge feature spaces of size |V | 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">DAN Configurations</head><p>In <ref type="table">Table 1</ref>, we compare a variety of DAN and NBOW configurations 5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and re- stricting the model to only root-level labels affects performance. With this in mind, the NBOW-RAND and DAN-RAND models are initialized with ran- dom 300-dimensional word embeddings, while the other models are initialized with publicly-available 300-d GloVe vectors trained over the Common Crawl ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>). The DAN-ROOT model only has access to sentence-level labels for SST experiments, while all other models are trained on labeled phrases (if they exist) in addition to sen- tences. We train all NBOW and DAN models using AdaGrad ( <ref type="bibr" target="#b10">Duchi et al., 2011</ref>).</p><p>We apply DANs to documents by averaging the embeddings for all of a document's tokens and then feeding that average through multiple layers as before. Since the representations computed by DANs are always d-dimensional vectors regardless of the input size, they are efficient with respect to both memory and computational cost. We find that the hyperparameters selected on the SST also work well for the IMDB task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Dataset Details</head><p>We evaluate over both fine-grained and binary sentence-level classification tasks on the SST, and just the binary task on RT and IMDB. In the fine- grained SST setting, each sentence has a label from zero to five where two is the neutral class. For the binary task, we ignore all neutral sentences. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Results</head><p>The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN-MC model. It's also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive ( <ref type="bibr" target="#b31">Sayeed et al., 2012;</ref><ref type="bibr" target="#b19">Iyyer et al., 2014b</ref>), this result is promising for large or messy datasets where fine-grained annotation is infeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Timing Experiments</head><p>DANs require less time per epoch and-in general- require fewer epochs than their syntactic coun- terparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of <ref type="table">Table 1</ref>; the reported times are for a single epoch to control for hyper- parameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minutes on a single core of a laptop; when labeled phrases are added as separate training in- stances, training time jumps to twenty minutes. <ref type="bibr">7</ref> All timing experiments were performed on a single core of an Intel I7 processor with 8GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Factoid Question Answering</head><p>DANs work well for sentiment analysis, but how do they do on other NLP tasks? We shift gears to a paragraph-length factoid question answering task and find that our model outperforms other unordered functions as well as a more complex syntactic RecNN model. More interestingly, we find that unlike the RecNN, the DAN significantly benefits from out-of-domain Wikipedia training data.</p><p>Quiz bowl is a trivia competition in which play- ers are asked four-to-six sentence questions about entities (e.g., authors, battles, or events). It is an ideal task to evaluate DANs because there is prior test:1,821}. Split sizes increase by an order of magnitude when labeled phrases are added to the training set. For RT, we do 10-fold CV over a balanced binary dataset of 10,662 sentences. Similarly, for the IMDB experiments we use the provided balanced binary training set of 25,000 documents. <ref type="bibr">7</ref> We also find that DANs take significantly fewer epochs to reach convergence than syntactic models.  and achieves comparable accuracy to a state-of-the- art information retrieval baseline, which highlights a benefit of ignoring word order for this task.  work using both syntactic and unordered models for quiz bowl question answering. In Boyd- , na¨ıvena¨ıve Bayes bag-of-words models (BOW-DT) and sequential language models work well on easy questions but poorly on harder ones. A dependency-tree RecNN called QANTA proposed in <ref type="bibr" target="#b18">Iyyer et al. (2014a)</ref> shows substantial improve- ments, leading to the hypothesis that correctly mod- eling compositionality is crucial for answering hard questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pos 1 Pos 2 Full</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dataset and Experimental Setup</head><p>To test this, we train a DAN over the history ques- tions from <ref type="bibr" target="#b18">Iyyer et al. (2014a)</ref>. <ref type="bibr">8</ref> This dataset is aug- <ref type="bibr">8</ref> The training set contains 14,219 sentences over 3,761 questions. For more detail about data and baseline systems, mented with 49,581 sentence/page-title pairs from the Wikipedia articles associated with the answers in the dataset. For fair comparison with QANTA, we use a normalized tanh activation function at the last layer instead of ReLu, and we also change the output layer from a softmax to the margin rank- ing loss <ref type="bibr" target="#b41">(Weston et al., 2011</ref>) used in QANTA. We initialize the DAN with the same pretrained 100- d word embeddings that were used to initialize QANTA.</p><p>We also evaluate the effectiveness of word dropout on this task in <ref type="figure" target="#fig_1">Figure 2</ref>. Cross-validation indicates that p = 0.3 works best for question an- swering, although the improvement in accuracy is negligible for sentiment analysis. Finally, continu- ing the trend observed in the sentiment experiments, DAN converges much faster than QANTA. <ref type="table" target="#tab_3">Table 2</ref> shows that while DAN is slightly worse than QANTA when trained only on question-answer pairs, it improves when trained on additional out- of-domain Wikipedia data (DAN-WIKI), reaching performance comparable to that of a state-of-the-art information retrieval system (IR-WIKI). QANTA, in contrast, barely improves when Wikipedia data is added (QANTA-WIKI) possibly due to the syntactic differences between Wikipedia text and quiz bowl question text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">DANs Improve with Noisy Data</head><p>The most common syntactic structures in quiz bowl sentences are imperative constructions such as "Identify this British author who wrote Wuther- ing Heights", which are almost never seen in Wikipedia. Furthermore, the subject of most quiz bowl sentences is a pronoun or pronomial mention referring to the answer, a property that is not true of Wikipedia sentences (e.g., "Little of Emily's work from this period survives, except for poems spoken by characters."). Finally, many Wikipedia sentences do not uniquely identify the title of the page they come from, such as the following sen- tence from Emily Brontë's page: "She does not seem to have made any friends outside her family." While noisy data affect both DAN and QANTA, the latter is further hampered by the syntactic diver- gence between quiz bowl questions and Wikipedia, which may explain the lack of improvement in ac- curacy.  Figure 4: Two to three layers is optimal for the DAN on the SST binary sentiment analysis task, but adding any depth at all is an improvement over the shallow NBOW model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">How Do DANs Work?</head><p>In this section we first examine how the deep layers of the DAN amplify tiny differences in the vector av- erage that are predictive of the output labels. Next, we compare DANs to DRecNNs on sentences that contain negations and contrastive conjunctions and find that both models make similar errors despite the latter's increased complexity. Finally, we an- alyze the predictive ability of unsupervised word embeddings on a simple sentiment task in an effort to explain why initialization with these embeddings improves the DAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Perturbation Analysis</head><p>Following the work of ˙ <ref type="bibr" target="#b17">Irsoy and Cardie (2014)</ref>, we examine our network by measuring the response at each hidden layer to perturbations in an input sen- tence. In particular, we use the template the film's performances were awesome and replace the fi- nal word with increasingly negative polarity words (cool, okay, underwhelming, the worst). For each perturbed sentence, we observe how much the hid- den layers differ from those associated with the original template in 1-norm. <ref type="figure" target="#fig_3">Figure 3</ref> shows that as a DAN gets deeper, the dif- ferences between negative and positive sentences become increasingly amplified. While nonexistent in the shallow NBOW model, these differences are visible even with just a single hidden layer, thus explaining why deepening the NBOW improves sen- timent analysis as shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Handling Negations and "but": Where Syntax is Still Needed</head><p>While DANs outperform other bag-of-words mod- els, how can they model linguistic phenomena such as negation without considering word order? To evaluate DANs over tougher inputs, we collect 92 sentences, each of which contains at least one nega- tion and one contrastive conjunction, from the dev and test sets of the SST. 9 Our fine-grained accuracy is higher on this subset than on the full dataset, improving almost five percent absolute accuracy to 53.3%. The DRecNN model of ˙ Irsoy and Cardie (2014) obtains a similar accuracy of 51.1%, con- trary to our intuition that syntactic functions should outperform unordered functions on sentences that clearly require syntax to understand. <ref type="bibr">10</ref> Are these sentences truly difficult to classify? A close inspection reveals that both the DAN and the DRecNN have an overwhelming tendency to pre- dict negative sentiment (60.9% and 55.4% of the time for the DAN and DRecNN respectively) when they see a negation compared to positive sentiment (35.9% for DANs, 34.8% for DRecNNs). If we fur- ther restrict our subset of sentences to only those with positive ground truth labels, we find that while both models struggle, the DRecNN obtains 41.7% accuracy, outperforming the DAN's 37.5%.</p><p>To understand why a negation or contrastive con- junction triggers a negative sentiment prediction,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAN DRecNN</head><p>Ground Truth a lousy movie that's not merely unwatchable , but also unlistenable negative negative negative if you're not a prepubescent girl , you'll be laughing at britney spears ' movie-starring debut whenever it does n't have you impatiently squinting at your watch negative negative negative  <ref type="table">Table 3</ref>: Predictions of DAN and DRecNN models on real (top) and synthetic (bottom) sentences that contain negations and contrastive conjunctions. In the first column, words colored red individually predict the negative label when fed to a DAN, while blue words predict positive. The DAN learns that the negators not and n't are strong negative predictors, which means it is unable to capture double negation as in the last real example and the last synthetic example. The DRecNN does slightly better on the synthetic double negation, predicting a lower negative polarity.</p><p>we show six sentences from the negation subset and four synthetic sentences in <ref type="table">Table 3</ref>, along with both models' predictions. The token-level predictions in the table (shown as colored boxes) are computed by passing each token through the DAN as separate test instances. The tokens not and n't are strongly pre- dictive of negative sentiment. While this simplified "negation" works for many sentences in the datasets we consider, it prevents the DAN from reasoning about double negatives, as in "this movie was not bad". The DRecNN does slightly better in this case by predicting a lesser negative polarity than the DAN; however, we theorize that still more powerful syntactic composition functions (and more labelled instances of negation and related phenomena) are necessary to truly solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Unsupervised Embeddings Capture Sentiment</head><p>Our model consistently converges slower to a worse solution (dropping 3% in absolute accuracy on coarse-grained SST) when we randomly initialize the word embeddings. This does not apply to just DANs; both convolutional and recursive networks do the same <ref type="bibr" target="#b23">(Kim, 2014;</ref><ref type="bibr" target="#b17">˙ Irsoy and Cardie, 2014)</ref>. Why are initializations with these embeddings so crucial to obtaining good performance? Is it pos- sible that unsupervised training algorithms are al- ready capturing sentiment?</p><p>We investigate this theory by conducting a sim- ple experiment: given a sentiment lexicon contain- ing both positive and negative words, we train a logistic regression to discriminate between the asso- ciated word embeddings (without any fine-tuning). We use the lexicon created by <ref type="bibr" target="#b16">Hu and Liu (2004)</ref>, which consists of 2,006 positive words and 4,783 negative words. We balance and split the dataset into 3,000 training words and 1,000 test words. Using 300-dimensional GloVe embeddings pre- trained over the Common Crawl, we obtain over 95% accuracy on the unseen test set, supporting the hypothesis that unsupervised pretraining over large corpora can capture properties such as sentiment.</p><p>Intuitively, after the embeddings are fine-tuned during DAN training, we might expect a decrease in the norms of stopwords and an increase in the norms of sentiment-rich words like "awesome" or "horrible". However, we find no significant dif- ferences between the L 2 norms of stopwords and words in the sentiment lexicon of <ref type="bibr" target="#b16">Hu and Liu (2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality.</p><p>There are a variety of element-wise vector op- erations that could replace the average used in the DAN. <ref type="bibr" target="#b28">Mitchell and Lapata (2008)</ref> experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words ( <ref type="bibr" target="#b11">Erk and Padó, 2008;</ref><ref type="bibr" target="#b1">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b22">Kartsaklis and Sadrzadeh, 2013</ref>) and grammars ( <ref type="bibr" target="#b7">Coecke et al., 2010;</ref><ref type="bibr" target="#b12">Grefenstette and Sadrzadeh, 2011)</ref>. While the average works best for the tasks that we consider, <ref type="bibr" target="#b0">Banea et al. (2014)</ref> find that simply summing word2vec embeddings out- performs all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks.</p><p>Once we compute the embedding average in a DAN, we feed it to a deep neural network. In con- trast, most previous work on neural network-based methods for NLP tasks explicitly model word or- der. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing <ref type="bibr" target="#b34">(Socher et al., 2013a</ref>), machine trans- lation ( <ref type="bibr" target="#b26">Liu et al., 2014)</ref>, and paraphrase detec- tion ( <ref type="bibr" target="#b32">Socher et al., 2011a</ref>). Convolutional net- works also model word order in local windows and have achieved performance comparable to or bet- ter than that of RecNNs on many tasks <ref type="bibr" target="#b8">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Kim, 2014)</ref>. Meanwhile, feed- forward architectures like that of the DAN have been used for language modeling ( <ref type="bibr" target="#b2">Bengio et al., 2003)</ref>, selectional preference acquisition (Van de Cruys, 2014), and dependency parsing <ref type="bibr" target="#b5">(Chen and Manning, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>In Section 5, we showed that the performance of our DAN model worsens on sentences that con- tain lingustic phenomena such as double negation. One promising future direction is to cascade clas- sifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN's success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for clas- sification on newspaper reviews. Another poten- tially interesting application is to add gated units to a DAN,as has been done for recurrent and recur- sive neural networks <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Cho et al., 2014;</ref><ref type="bibr" target="#b38">Tai et al., 2015)</ref>, to drop useless words rather than randomly-selected ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we introduce the deep averaging net- work, which feeds an unweighted average of word vectors through multiple hidden layers before clas- sification. The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality. It is further strengthened by word dropout, a regu- larizer that reduces input redundancy. DANs ob- tain close to state-of-the-art accuracy on both sen- tence and document-level sentiment analysis and factoid question-answering tasks with much less training time than competing methods; in fact, all experiments were performed in a matter of min- utes on a single laptop core. We find that both DANs and syntactic functions make similar errors given syntactically-complex input, which motivates research into more powerful models of composi- tionality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Randomly dropping out 30% of words from the vector average is optimal for the quiz bowl task, yielding a gain in absolute accuracy of almost 3% on the quiz bowl question dataset compared to the same model trained with no word dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>see</head><label></label><figDesc>Iyyer et al. (2014a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perturbation response (difference in 1norm) at each layer of a 5-layer DAN after replacing awesome in the film's performances were awesome with four words of varying sentiment polarity. While the shallow NBOW model does not show any meaningful distinctions, we see that as the network gets deeper, negative sentences are increasingly different from the original positive sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>blessed with immense physical prowess he may well be, but ahola is simply not an actor positive neutral negative who knows what exactly godard is on about in this film , but his words and images do n't have to add up to mesmerize you. positive positive positive it's so good that its relentless , polished wit can withstand not only inept school productions , but even oliver parker 's movie adaptation negative positive positive too bad , but thanks to some lovely comedic moments and several fine performances , it's not a total loss negative negative positive this movie was not good negative negative negative this movie was good positive positive positive this movie was bad negative negative negative the movie was not bad negative negative positive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The DAN achieves slightly lower accu-
racies than the more complex QANTA in much 
less training time, even at early sentence posi-
tions where compositionality plays a bigger role. 
When Wikipedia is added to the training set (bot-
tom half of table), the DAN outperforms QANTA 
</table></figure>

			<note place="foot" n="1"> Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014).</note>

			<note place="foot" n="3"> PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data.</note>

			<note place="foot" n="4"> The WRRBM is trained using a slow Metropolis-Hastings algorithm. 5 Best hyperparameters chosen by cross-validation: three 300-d ReLu layers, word dropout probability p = 0.3, L2 regularization weight of 1e-5 applied to all parameters 6 Our fine-grained SST split is {train: 8,544, dev: 1,101, test: 2,210}, while our binary split is {train: 6,920, dev:872,</note>

			<note place="foot" n="9"> We search for non-neutral sentences containing not / n&apos;t, and but. 48 of the sentences are positive while 44 are negative. 10 Both models are initialized with pretrained 300-d GloVe embeddings for fair comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Ozan ˙ Irsoy not only for many insight-ful discussions but also for suggesting some of the experiments that we included in the paper. We also thank the anonymous reviewers, Richard Socher, Arafat Sultan, and the members of the UMD "Thinking on Your Feet" research group for their helpful comments. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also supported by NSF Grants CCF-1409287 and NCSE-1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simcompass: Using deep learning word embeddings to assess cross-level similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Besting the quiz master: Crowdsourcing incremental classification games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>He He, and Hal Daumé III</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Analysis (Lambek Festschirft)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines on word observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>George E Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A structured vector space model for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">not not bad&quot; is not &quot;bad&quot;: A distributional account of negation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the ACL Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">˙</forename><surname>Ozan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Political ideology detection using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Enns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prior disambiguation of word tensors for constructing sentence vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Feature weight tuning for recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1412.3714</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A recursive recurrent neural network for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grammatical structures for word-level sentiment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Rusk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parsing With Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Improved semantic representations from treestructured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A neural network approach to selectional preference acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
