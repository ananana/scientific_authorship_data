<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A joint inference of deep case analysis and zero subject generation for Japanese-to-English statistical machine translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ichikawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Japan</surname></persName>
						</author>
						<title level="a" type="main">A joint inference of deep case analysis and zero subject generation for Japanese-to-English statistical machine translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="557" to="562"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a simple joint inference of deep case analysis and zero subject generation for the pre-ordering in Japanese-to-English machine translation. The detection of subjects and objects from Japanese sentences is more difficult than that from English, while it is the key process to generate correct English word orders. In addition , subjects are often omitted in Japanese when they are inferable from the context. We propose a new Japanese deep syntactic parser that consists of pointwise proba-bilistic models and a global inference with linguistic constraints. We applied our new deep parser to pre-ordering in Japanese-to-English SMT system and show substantial improvements in automatic evaluations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Japanese to English translation is known to be one of the most difficult language pair for statistical machine translation (SMT). It has been widely be- lieved for years that the difference of word or- ders, i.e., Japanese is an SOV language, while En- glish is an SVO language, makes the English-to- Japanese and Japanese-to-English translation dif- ficult. However, simple, yet powerful pre-ordering techniques have made this argument a thing of the past ( <ref type="bibr" target="#b6">Isozaki et al., 2010b;</ref><ref type="bibr" target="#b8">Komachi et al., 2006;</ref><ref type="bibr" target="#b1">Fei and Michael, 2004;</ref><ref type="bibr" target="#b10">Lerner and Petrov, 2013;</ref><ref type="bibr" target="#b16">Wu et al., 2011;</ref><ref type="bibr" target="#b7">Katz-Brown and Collins, 2008;</ref><ref type="bibr" target="#b11">Neubig et al., 2012;</ref><ref type="bibr" target="#b3">Hoshino et al., 2013)</ref>. Pre- ordering processes the source sentence in such a way that word orders appear closer to their final positions on the target side.</p><p>While many successes of English-to-Japanese translation have been reported recently, the quality improvement of Japanese-to-English translation is still small even with the help of pre-ordering <ref type="bibr" target="#b2">(Goto et al., 2013</ref>). We found that there are two ma- jor issues that make Japanese-to-English transla- tion difficult. One is that Japanese subject and ob- ject cannot easily be identified compared to En- glish, while their detections are the key process to generate correct English word orders. Japanese surface syntactic structures are not always corre- sponding to their deep structures, i.e., semantic roles. The other is that Japanese is a pro-drop lan- guage in which certain classes of pronouns may be omitted when they are pragmatically inferable. In Japanese-to-English translation, these omitted pronouns have to be generated properly.</p><p>There are several researches that focused on the pre-ordering with Japanese deep syntactic analysis ( <ref type="bibr" target="#b8">Komachi et al., 2006;</ref><ref type="bibr" target="#b3">Hoshino et al., 2013</ref>) and zero pronoun generation ( <ref type="bibr" target="#b15">Taira et al., 2012)</ref> for Japanese-to-English translation. However, these two issues have been considered independently, while they heavily rely on one another.</p><p>In this paper, we propose a simple joint infer- ence which handles both Japanese deep structure analysis and zero pronoun generation. To the best of our knowledge, this is the first study that ad- dresses these two issues at the same time.</p><p>This paper is organized as follows. First, we de- scribe why Japanese-to-English translation is dif- ficult. Second, we show the basic idea of this work and its implementation based on pointwise probabilistic models and a global inference with an integer linear programming (ILP). Several ex- periments are employed to confirm that our new model can improve the Japanese to English trans- lation quality.</p><p>2 What makes Japanese-to-English translation difficult?</p><p>Japanese syntactic relations between arguments and predicates are usually specified by particles. There are several types of particles, but we focus on (ga), (wo) and (wa) for the sake of <ref type="table">Table 1</ref>: An example of difficult sentence for pars- ing Sentence: .</p><p>Gloss:</p><p>today wa TOP liquor ga NOM can drink. Translation: (I) can drink liquor today. simplicity 1 .</p><p>• ga is usually a subject marker. However, it becomes an object marker if the predicate has a potential voice type, which is usually trans- lated into can, be able to, want to, or would like to.</p><p>• wo is an object marker.</p><p>• wa is a topic case marker. The topic can be anything that a speaker wants to talk about. It can be subject, object, location, time or any other grammatical elements.</p><p>We cannot always identify Japanese subject and object only by seeing the surface case markers ga, wo and wa. Especially the topic case marker is problematic, since there is no concept of topic in English. It is necessary to get a deep interpretation of topic case markers in order to develop accurate Japanese-to-English SMT systems.</p><p>Another big issue is that Japanese subject (or even an object) can be omitted when they can pragmatically be inferable from the context. Such a pronoun-dropping is not a unique phenomenon in Japanese actually. For instance, Spanish also allows to omit pronouns. However, the inflec- tional suffix of Spanish verbs include a hint of the person of the subject. On the other hand, infer- ring Japanese subjects is more difficult than Span- ish, since Japanese verbs usually do not have any grammatical cues to tell the subject type. <ref type="table">Table 1</ref> shows an example Japanese sentence which cannot be parsed only with the surface structure. The second token wa specifies the rela- tion between (today) and (can drink). Human can easily tell that the relation of them is not a subject but an adverb (time). The topic case marker wa implies that the time when the speaker drinks liquor is the focus of this sentence. The 4th token ga indicates the relation between (liquor) and (can drink). Since the predi- cate has a potential voice (can drink), the ga par- ticle should be interpreted as an object here. In this sentence, the subject is omitted. In general, it is unknown who speaks this sentence, but the first person is a natural interpretation in this context.</p><p>Another tricky phenomenon is that detecting voice type is not always deterministic. There are several ways to generate a potential voice in Japanese, but we usually put the suffix word (reru) or (rareru) after predicates. How- ever, these suffix words are also used for a passive voice.</p><p>In summary, we can see that the following four factors are the potential causes that make the Japanese parsing difficult.</p><p>• Japanese voice type detection is not straight- forward. reru or rareru are used either for passive or potential voice.</p><p>• surface case ga changes its interpretation from subject to object when the predicate has a potential voice.</p><p>• topic case marker wa is used as a topic case marker which doesn't exist in English. Topic is either subject, object or any grammatical elements depending on the context.</p><p>• Japanese subject is often omitted when it is inferable from the context. There is no cue to tell the subject person in verb suffix (inflec- tion) like in Spanish verbs</p><p>We should note that they are not always inde- pendent issues. For instance, the deep case detec- tion helps to tell the voice type, and vice versa.</p><p>Another note is that they are unique issues observed only in Japanese-to-English translation. In English-to-Japanese translation, it is accept- able to generate Japanese sentences that do not use Japanese topic markers wa. Also, generating Japanese pronoun from English pronoun is accept- able, although it sounds redundant and unnatural for native speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A joint inference of deep case analysis</head><p>and zero subject generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic model over predicate-argument structures</head><p>Our deep parser runs on the top of a dependency parse tree. First, it extracts all predicates and their arguments from a dependency tree by using man- ual rules over POS tags. Since our pre-ordering system generates the final word orders from a labeled dependency tree, we formalize our deep parsing task as a simple labeling problem over de- pendency links, where the label indicates the deep syntactic roles between head and modifier. We here define a joint probability over a predi- cate and its arguments as follows:</p><formula xml:id="formula_0">P (p, z, v, A, S, D)<label>(1)</label></formula><p>where</p><p>• p: a predicate</p><p>• z: a zero subject candidate for p. z ∈ Z = {I, you, we, it, he/she, imperative, already exists}</p><p>• v: voice type of the predicate p. v ∈ V = {active, passive, potential }</p><p>• a k ∈ A: k-th argument which modifies or is modified by the predicate 2 .</p><p>• d k ∈ D: deep case label which represents a deep relation between a k and p. d ∈ { sub- ject, object, other }, where other means that deep case is neither subject nor object.</p><p>• s k ∈ S: surface relation (surface case marker) between a k and p.</p><p>We assume that a predicate p is independent from other predicates in a sentence. This assump- tion allows us to estimate the deep structures of p separately, with no regard to which decisions are made in other predicates.</p><p>An optimal zero subject label z, deep cases D, and voice type v for a given predicate p can be solved as the following optimization problem.</p><formula xml:id="formula_1">ˆ z, ˆ v, ˆ D = argmax z,v,D P (p, z, v, A, S, D)</formula><p>Since the inference of this joint probability is diffi- cult, we decompose P (p, z, v, A, S, D) into small independent sub models:</p><formula xml:id="formula_2">P (p, z, v, A, S, D) ≈ P z (z|p, A, S)P v (v|p, A, S) P d (D|p, v, A, S)P (p, A, S)<label>(2)</label></formula><p>We do not take the last term P (p, A, S) into con- sideration, since it is constant for the optimization.</p><p>In the next sections, we describe how these proba- bilities P z , P d , and P v are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Zero subject model: P z (z|p, A, S)</head><p>This model estimates the syntactic zero subject 3 of the predicate p. For instance, z= I means that the subject of p is omitted and its type is first person. z=imperative means that we do not need to aug- ment a subject because the predicate is imperative. z=already exists means that a subject already ap- pears in the sentence. A maximum entropy classi- fier is used in our zero subject model, which takes the contextual features extracted from p, A, and S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Voice type model: P v (v|p, A, S)</head><p>This model estimates the voice type of a predicate. We also use a maximum entropy classifier for this model. This classifier is used only when the predi- cate has the ambiguous suffix reru or rareru. If the predicate does not have any ambiguous suffix, this model returns pre-defined voice types with with very high probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Deep case model: P d (D|p, v, A, S)</head><p>This model estimates the deep syntactic role be- tween a predicate p and its arguments A. This model helps to resolve the deep cases when their surface cases are topic. We define P d as follows after introducing an independent assumption over predicate-argument structures:</p><formula xml:id="formula_3">P (D|p, v, A, S) ≈ i [max(p(d i |a i , p) − m(s i , d i , v), δ)].</formula><p>p(d|a, p) models the deep relation between p and a. We use a maximum likelihood estimation for p(d|a, p): where f req(s = ga, a, active form of p) is the frequency of how often an argument a and p ap- pears with the surface case ga. The frequencies are aggregated only when the predicate appear in active voice. If the voice type is active, we can safely assume that the surface cases ga and wo correspond to subject and object respectively. We compute the frequencies from a large amount of auto-parsed data. m(s, d, v) is a non-negative penalty variable de- scribing how the deep case d generates the sur- face case s depending on the voice type v. Since the number of possible surface cases, deep cases, and voice types are small, we define this penalty manually by referring to the Japanese grammar book (descriptive grammar research group, 2009). We use these manually defined penalties in order to put more importance on syntactic preferences rather than those of semantics. Even if a predicate- augment structure is semantically irrelevant, we take this structure as long as it is syntactically cor- rect in order to avoid SMT from generating liberal translations.</p><formula xml:id="formula_4">p(d = subj|a, p) = f req(s = ga,</formula><p>δ is a very small positive constant to avoid zero probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint inference with linguistic constraints</head><p>Our initial model (2) assumes that zero subjects and deep cases are generated independently. How- ever, this assumption does not always capture real linguistic phenomena. English is a subject- prominent language in which almost all sentences (or predicates) must have a subject. This implies that it is more reasonable to introduce strong lin- guistic constraints to the final solution for pre- ordering, which are described as follows:</p><p>• Subject is a mandatory role. A subject must be inferred either by zero subject or deep case model <ref type="bibr">4</ref> . When the voice type is passive, an object role in D is considered as a syntactic subject.</p><p>• A predicate can not have multiple subjects and objects respectively.</p><p>These two constraints avoid the model from in- ferring syntactically irrelevant solutions.</p><p>In order to find the result with the constraints above, we formalize our model as an integer lin- ear programming, ILP. Let {x 1 , , ..., x n } be bi- nary variables, i.e., x i ∈ {0, 1}. x i corresponds to the binary decisions in our model, e.g., x k = 1 if d i = subj and v = active. Let {p 1 , ..., p n } be probability vector corresponding to the binary de- cisions. ILP can be formalized as a mathematical problem, in which the objective function and the constraints are linear:</p><formula xml:id="formula_5">{ˆx{ˆx 1 , ..., ˆ x n } = argmax {x 1 ,...,xn}∈{0,1} n n i=1 log(p i )x i</formula><p>s.t. linear constraints over {x 1 , .., x n }.</p><p>After taking the log of (2), our optimization model can be converted into an ILP. Also, the constraints 4 imperative is also handled as an invisible subject described above can be represented as linear equa- tions over binary variables X. We leave the details of the representations to ( <ref type="bibr" target="#b14">Punyakanok et al., 2004</ref>; Iida and Poesio, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Japanese pre-ordering with deep parser</head><p>We use a simple rule-based approach to make pre- ordered Japanese sentences from our deep parse trees, which is similar to the algorithms described in ( <ref type="bibr" target="#b8">Komachi et al., 2006;</ref><ref type="bibr" target="#b7">Katz-Brown and Collins, 2008;</ref><ref type="bibr" target="#b3">Hoshino et al., 2013</ref>). First, we naively re- verse all the bunsetsu-chunks 5 . Then, we move a subject chunk just before its predicate. This process converts SOV to SVO. When the subject is omitted, we generate a subject with our deep parser and insert it to a subject position in the source sentence. There are three different ways to generate a subject.</p><p>1. Generate real Japanese words (Insert (I), (you).. etc)</p><p>2. Generate virtual seed Japanese words (Insert 1st person, 2nd person..., which are not in the Japanese lexicon.)</p><p>3. Generate only a single virtual seed Japanese word regardless of the subject type. (Insert zero subject) 1) is the most aggressive method, but it causes completely incorrect translations if the detection of subject type fails. 2) and 3) is rather conser- vative, since they leave SMT to generate English pronouns.</p><p>We decided to use the following hybrid ap- proach, since it shows the best performance in our preliminary experiments.</p><p>• In the training of SMT, use 3).</p><p>• In decoding, use 1) if the input sentence only has one predicate. Otherwise, use 3). <ref type="table">Table 2</ref> shows examples of our deep parser output. It can be seen that our parser can correctly identify the deep case of topic case markers wa.  <ref type="table">2</ref>: Examples of deep parser output (today wa) {d=other} (liquor ga) {d=obj} (can drink) {v=potential, z=I} (news ga) {d=subj} (was broadcast) {v=passive, z=already exist} (pasta wa) {d=obj} (ate+question) {v=active, z=you} (you wa) {d=subj} (ate+question) {v=active, z=already exist} 4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Examples of parsing results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>We carried out all our experiments using a state- of-the-art phrase-based statistical Japanese-to- English machine translation system <ref type="bibr" target="#b12">(Och, 2003)</ref> with pre-ordering. During the decoding, we use the reordering window (distortion limit) to 4 words. For parallel training data, we use an in- house collection of parallel sentences. These come from various sources with a substantial portion coming from the web. We trained our system on about 300M source words. Our test set contains about 10,000 sentences randomly sampled from the web.</p><p>The dependency parser we apply is an imple- mentation of a shift-reduce dependency parser which uses a bunsetsu-chunk as a basic unit for parsing ( <ref type="bibr" target="#b9">Kudo and Matsumoto, 2002</ref>).</p><p>The zero subject and voice type models were trained with about 20,000 and 5,000 manually an- notated web sentences respectively. In order to simplify the rating tasks for our annotators, we ex- tracted only one candidate predicate from a sen- tence for annotations.</p><p>We tested the following six systems.</p><p>• baseline: no pre-ordering.</p><p>• surface reordering : pre-ordering only with surface dependency relations.</p><p>• independent deep reordering: pre-ordering using deep parser without global linguistic constraints.</p><p>• independent deep reordering + zero sub- ject: pre-ordering using deep parser and zero subject generation without global linguistic constraints.</p><p>• joint deep reordering: pre-ordering using our new deep parser with global linguistic constraints.</p><p>• joint deep reordering + zero-subject: pre- ordering using deep parser and zero subject generation with global linguistic constraints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.90">61.89</head><p>As translation metrics, we used BLEU ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>), as well as RIBES ( <ref type="bibr" target="#b5">Isozaki et al., 2010a</ref>), which is designed for measuring the quality of distant language pairs in terms of word orders. <ref type="table" target="#tab_1">Table 3</ref> shows the experimental results for six pre- reordering systems. It can be seen that the pro- posed method with deep parser outperforms base- line and naive reordering with surface syntactic trees. The zero subject generation can also im- prove both BLEU and RIBES scores, but the im- provements are smaller than those with reordering. Also, joint inference with global linguistics con- straints outperforms the model which solves deep syntactic analysis and zero subject generation in- dependently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a simple joint inference of deep case analysis and zero subject generation for Japanese-to-English SMT. Our parser consists of pointwise probabilistic models and a global in- ference with linguistic constraints. We applied our new deep parser to pre-ordering in Japanese-to- English SMT system and showed substantial im- provements in automatic evaluations.</p><p>Our future work is to enhance our deep parser so that it can handle other linguistic phenomena, in- cluding causative voice, coordinations, and object ellipsis. Also, the current system is built on the top of a dependency parser. The final output of our deep parser is highly influenced by the parsing er- rors. It would be interesting to develop a full joint inference of dependency parsing and deep syntac- tic analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a, active form of p) f req(a, active form of p) p(d = obj|a, p) = f req(s = wo, a, active form of p) f req(a, active form of p) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for different reordering methods 
System 

BLEU RIBES 
baseline (no reordering) 

16.15 52.67 

surface reordering 

19.39 60.30 

independent deep reordering 

19.68 61.27 
independent deep reordering + zero subj. 19.81 61.67 

joint deep reordering 

19.76 61.43 

joint deep reordering + zero subj. 

</table></figure>

			<note place="foot" n="1"> Other case markers are less frequent than these three markers</note>

			<note place="foot" n="2"> Generally, an argument modifies a predicate, but in relative clauses, a predicate modifies an argument</note>

			<note place="foot" n="3"> Here syntactic subject means the subject which takes the voice type into account.</note>

			<note place="foot" n="5"> bunsetsu is a basic Japanese grammatical unit consisting of one content word and functional words.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Contemporary Japanese grammar book 2. Part 3. Case and Syntax</title>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Kuroshio Publishers</publisher>
		</imprint>
		<respStmt>
			<orgName>Japan descriptive grammar research group</orgName>
		</respStmt>
	</monogr>
	<note>Part 4. Voice</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving a statistical mt system with automatically learned rewrite patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mccord</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Overview of the patent machine translation task at the ntcir-10 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NTCIR</title>
		<meeting>of NTCIR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two-stage pre-ordering for japanese-to-english statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Hoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A cross-lingual ilp solution to zero anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic evaluation of translation quality for distant language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP. Association for Computational Linguistics</title>
		<meeting>of EMNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Head finalization: A simple reordering rule for sov languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Syntactic reordering in preprocessing for japanese english translation: Mit system description for ntcir7 patent translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Katz-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NTCIR-7 Workshop Meeting</title>
		<meeting>of the NTCIR-7 Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Phrase reordering for statistical machine translation based on predicate-argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Workshop on Spoken Language Translation</title>
		<meeting>of the International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Japanese dependency analysis using cascaded chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Source-side classifier preordering for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inducing a discriminative parser to optimize machine translation reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic role labeling via integer linear programming inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dav</forename><surname>Zimak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero pronoun resolution can improve the quality of je translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotoshi</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>of Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extracting pre-ordering rules from predicate-argument structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNLP</title>
		<meeting>of IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
