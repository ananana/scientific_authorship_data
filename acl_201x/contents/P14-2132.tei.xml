<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Observational Initialization of Type-Supervised Taggers</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
							<email>hzhang@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
							<email>denero@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Observational Initialization of Type-Supervised Taggers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="816" to="821"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work has sparked new interest in type-supervised part-of-speech tagging, a data setting in which no labeled sentences are available, but the set of allowed tags is known for each word type. This paper describes observational initializa-tion, a novel technique for initializing EM when training a type-supervised HMM tagger. Our initializer allocates probability mass to unambiguous transitions in an unlabeled corpus, generating token-level observations from type-level supervision. Experimentally, observational initializa-tion gives state-of-the-art type-supervised tagging accuracy, providing an error reduction of 56% over uniform initialization on the Penn English Treebank.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For many languages, there exist comprehensive dictionaries that list the possible parts-of-speech for each word type, but there are no corpora la- beled with the part-of-speech of each token in con- text. Type-supervised tagging <ref type="bibr" target="#b8">(Merialdo, 1994)</ref> explores this scenario; a model is provided with type-level information, such as the fact that "only" can be an adjective, adverb, or conjunction, but not any token-level information about which in- stances of "only" in a corpus are adjectives. Re- cent research has focused on using type-level su- pervision to infer token-level tags. For instance, <ref type="bibr" target="#b6">Li et al. (2012)</ref> derive type-level supervision from Wiktionary, <ref type="bibr" target="#b1">Das and Petrov (2011)</ref> and <ref type="bibr">Täckström et al. (2013)</ref> project type-level tag sets across lan- guages, and <ref type="bibr" target="#b2">Garrette and Baldridge (2013)</ref> solicit type-level annotations directly from speakers. In all of these efforts, a probabilistic sequence model is trained to disambiguate token-level tags that are * Research conducted during an internship at Google. constrained to match type-level tag restrictions. This paper describes observational initialization, a simple but effective learning technique for train- ing type-supervised taggers.</p><p>A hidden Markov model (HMM) can be used to disambiguate tags of individual tokens by max- imizing corpus likelihood using the expectation maximization (EM) algorithm. Our approach is motivated by a suite of oracle experiments that demonstrate the effect of initialization on the fi- nal tagging accuracy of an EM-trained HMM tag- ger. We show that initializing EM with accurate transition model parameters is sufficient to guide learning toward a high-accuracy final model.</p><p>Inspired by this finding, we introduce obser- vational initialization, which is a simple method to heuristically estimate transition parameters for a corpus using type-level supervision. Transi- tion probabilities are estimated from unambiguous consecutive tag pairs that arise when two consec- utive words each have only a single allowed tag. These unambiguous word pairs can be tagged cor- rectly without any statistical inference. Initializing EM with the relative frequency of these unambigu- ous pairs improves tagging accuracy dramatically over uniform initialization, reducing errors by 56% in English and 29% in German. This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of <ref type="bibr" target="#b9">Ravi and Knight (2009)</ref>, the Bayesian LDA-based model of <ref type="bibr" target="#b14">Toutanova and Johnson (2008)</ref>, and an HMM trained with language-specific initialization described by <ref type="bibr" target="#b3">Goldberg et al. (2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Type-Supervised Tagging</head><p>A first-order Markov model for part-of-speech tagging defines a distribution over sentences for which a single tag is given to each word token. Let w i ∈ W refer to the ith word in a sentence w, drawn from language vocabulary W . Likewise, t i ∈ T is the tag in tag sequence t of the ith word, drawn from tag inventory T . The joint probabil- ity of a sentence can be expressed in terms of two sets of parameters for conditional multinomial dis- tributions: φ defines the probability of a tag given its previous tag and θ defines the probability of a word given its tag.</p><formula xml:id="formula_0">P φ,θ (w, t) = |w| i=1 P φ (t i |t i−1 ) · P θ (w i |t i )</formula><p>Above, t 0 is a fixed start-of-sentence tag.</p><p>For a set of sentences S, the EM algorithm can be used to iteratively find a local maximum of the corpus log-likelihood:</p><formula xml:id="formula_1">(φ, θ; S) = w∈S ln t P φ,θ (w, t)</formula><p>The parameters φ and θ can then be used to predict the most likely sequence of tags for each sentence under the model:</p><formula xml:id="formula_2">ˆ t(w) = arg max t P φ,θ (w, t)</formula><p>Tagging accuracy is the fraction of these tags inˆt inˆinˆt(w) that match hand-labeled oracle tags t * (w).</p><p>Type Supervision. In addition to an unlabeled corpus of sentences, type-supervised models also have access to a tag dictionary D ⊆ W × T that contains all allowed word-tag pairs. For an EM- trained HMM, initially setting P θ (w|t) = 0 for all (w, t) / ∈ D ensures that all words will be labeled with allowed tags.</p><p>Tag dictionaries can be derived from various sources, such as lexicographic resources ( <ref type="bibr" target="#b6">Li et al., 2012</ref>) and cross-lingual projections ( <ref type="bibr" target="#b1">Das and Petrov, 2011)</ref>. In this paper, we will follow pre- vious work in deriving the tag dictionary from a labeled corpus <ref type="bibr" target="#b10">(Smith and Eisner, 2005)</ref>; this synthetic setting maximizes experiment repeata- bility and allows for direct comparison of type- supervised learning techniques.</p><p>Transductive Applications. We consider a transductive data setting in which the test set is available during training. In this case, the model is not required to generalize to unseen examples or unknown words, as in the typical inductive setting.</p><p>Transductive learning arises in document clus- tering and corpus analysis applications. For ex- ample, before running a document clustering al- gorithm on a fixed corpus of documents, it may be useful to tag each word with its most likely part- of-speech in context, disambiguating the lexical features in a bag-of-words representation. In cor- pus analysis or genre detection, it may be useful to determine for a fixed corpus the most common part-of-speech for each word type, which could be inferred by tagging each word with its most likely part-of-speech. In both cases, the set of sentences to tag is known in advance of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Initializing HMM Taggers</head><p>The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very differ- ent predictions. We explore this issue via exper- iments on the Wall Street Journal section of the English Penn Treebank ( <ref type="bibr" target="#b7">Marcus et al., 1993</ref>). We adopt the transductive data setting introduced by <ref type="bibr" target="#b10">Smith and Eisner (2005)</ref> and used by <ref type="bibr" target="#b4">Goldwater and Griffiths (2007)</ref>, <ref type="bibr" target="#b14">Toutanova and Johnson (2008)</ref> and <ref type="bibr" target="#b9">Ravi and Knight (2009)</ref>; models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs ap- pearing in the entire labeled corpus, and the tag- ging accuracy is evaluated on a 1005 sentence sub- set sampled from the corpus.</p><p>The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. UNIFORM initial- izes the model with uniform distributions over al- lowed outcomes:</p><formula xml:id="formula_3">P φ (t|t ) = 1 |T | P θ (w|t) = 1 |{w : (w, t) ∈ D}|</formula><p>SUPERVISED is an oracle setting that initializes the model with the relative frequency of observed pairs in a labeled corpus:</p><formula xml:id="formula_4">P φ (t|t ) ∝ (w,t * ) |w| i=1 δ((t * i , t * i−1 ), (t, t )) P θ (w|t) ∝ (w,t * ) |w| i=1 δ((w i , t * i ), (w, t))</formula><p>where the Kronecker δ(x, y) function is 1 if x and y are equal and 0 otherwise. <ref type="figure" target="#fig_0">Figure 1</ref> shows that while UNIFORM and SUPERVISED achieve nearly identical data log- likelihoods, their final tagging accuracy differs by    12%. Accuracy degrades somewhat from the SU- PERVISED initialization, since the data likelihood objective differs from the objective of maximizing tagging accuracy. However, the final SUPERVISED performance of 94.1% shows that there is substan- tial room for improvement over the UNIFORM ini- tializer. <ref type="figure" target="#fig_2">Figure 2</ref> compares two partially supervised ini- tializations. SUPERVISED TRANSITIONS initial- izes the transition model with oracle counts, but the emission model uniformly. Conversely, SU- PERVISED EMISSIONS initializes the emission pa- rameters from oracle counts, but initializes the transition model uniformly. There are many more emission parameters (57,390) than transition pa- rameters <ref type="bibr">(1,</ref><ref type="bibr">858)</ref>. Thus, it is not surprising that SUPERVISED EMISSIONS gives a higher initial likelihood. Again, both initializers lead to solu- tions with nearly the same likelihood as SUPER- VISED and UNIFORM. <ref type="figure" target="#fig_2">Figure 2</ref> shows that SUPERVISED TRANSI- TIONS outperforms SUPERVISED EMISSIONS in tagging accuracy, despite the fact that fewer pa- rameters are set with supervision. With fixed D, an accurate initialization of the transition distribu- tions leads to accurate tagging after EM training. We therefore concentrate on developing an effec- tive initialization for the transition distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Observational Initialization</head><p>The SUPERVISED TRANSITIONS initialization is estimated from observations of consecutive tags in a labeled corpus. Our OBSERVATIONAL initializer is likewise estimated from the relative frequency of consecutive tags, taking advantage of the struc- ture of the tag dictionary D. However, it does not require a labeled corpus.</p><p>Let D(w, ·) = {t : (w, t) ∈ D} denote the allowed tags for word w. The set</p><formula xml:id="formula_5">U = {w : |D(w, ·)| = 1}</formula><p>contains all words that have only one allowed tag. When a token of some w ∈ U is observed in a corpus, its tag is unambiguous. Therefore, its tag is observed as well, and a portion of the tag se- quence is known. When consecutive pairs of to- kens are both in U , we can observe a transition in the latent tag sequence. The OBSERVATIONAL ini- tializer simply estimates a transition distribution from the relative frequency of these unambiguous observations that occur whenever two consecutive tokens both have a unique tag.</p><p>We now formally define the observational ini- tializer. Let g(w, t) = δ(D(w, ·), {t}) be an indi- cator function that is 1 whenever w ∈ U and its single allowed tag is t, and 0 otherwise. Then, we initialize φ such that:</p><formula xml:id="formula_6">P φ (t|t ) ∝ w∈S |w| i=1 g(w i , t) · g(w i−1 , t )</formula><p>The emission parameters θ are set to be uniform over allowed words for each tag, as in UNIFORM initialization. <ref type="figure">Figure 3</ref> compares the OBSERVATIONAL ini- tializer to the SUPERVISED TRANSITIONS initial- izer, and the top of  <ref type="figure">Figure 3</ref>: The data log-likelihood (top) and tag- ging accuracy (bottom) of initializing with SU- PERVISED TRANSITIONS compared to the unsu- pervised OBSERVATIONAL initialization that re- quires only a tag dictionary and an unlabeled train- ing corpus.</p><p>English Penn Treebank. The OBSERVATIONAL initializer provides an error reduction over UNI- FORM of 56%, surpassing the performance of an initially supervised emission model and nearing the performance of a supervised transition model.</p><p>The bottom of <ref type="table" target="#tab_1">Table 1</ref> shows a similar compar- ison on the Tübingen treebank of spoken German ( <ref type="bibr" target="#b12">Telljohann et al., 2006</ref>). Both training and test- ing were performed on the entire treebank. The observational initializer provides an error reduc- tion over UNIFORM of 29%, and again outper- forms SUPERVISED EMISSIONS. On this dataset OBSERVATIONAL initialization matches the final performance of SUPERVISED TRANSITIONS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The fact that observations and prior knowledge are useful for part-of-speech tagging is well under- stood <ref type="bibr" target="#b0">(Brill, 1995)</ref>, but the approach of estimating an initial transition model only from unambiguous word pairs is novel.</p><p>Our experiments show that for EM-trained HMM taggers in a type-supervised transductive data setting, observational initialization is an ef- fective technique for guiding training toward high- accuracy solutions, approaching the oracle accu- racy of SUPERVISED TRANSITIONS initialization.</p><p>The fact that models with similar data likeli- hood can vary dramatically in accuracy has been observed in other learning problems. For instance, <ref type="bibr" target="#b13">Toutanova and Galley (2011)</ref>  <ref type="table" target="#tab_1">Table 1</ref>: Accuracy of English (top) and German (bottom) tagging models at initialization (left) and after 30 iterations of EM training (right) using var- ious initializers.</p><note type="other">show that optimal English Initial EM-trained UNIFORM 72.0 82.1 OBSERVATIONAL 89.2 92.1 SUP. EMISSIONS 92.8 91.0 SUP. TRANSITIONS 93.5 93.7 FULLY SUPERVISED 96.7 94.1 German Initial EM-trained UNIFORM 77.2 88.8 OBSERVATIONAL 92.7 92.1 SUP. EMISSIONS 90.7 89.0 SUP. TRANSITIONS 94.8 92.0 FULLY SUPERVISED 97.0 92.9</note><p>parameters for IBM Model 1 are not unique, and alignments predicted from different optimal pa- rameters vary significantly in accuracy. However, the effectiveness of observational ini- tialization is somewhat surprising because EM training includes these unambiguous tag pairs in its expected counts, even with uniform initializa- tion. Our experiments indicate that this signal is not used effectively unless explicitly encoded in the initialization.</p><p>In our English data, 48% of tokens and 74% of word types have only one allowed tag. 28% of pairs of adjacent tokens have only one allowed tag pair and contribute to observational initialization. In German, 49% of tokens and 87% of word types are unambiguous, and 26% of adjacent token pairs are unambiguous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>We now compare with several previous published results on type-supervised part-of-speech tagging trained using the same data setting on the English WSJ Penn Treebank, introduced by <ref type="bibr" target="#b10">Smith and Eisner (2005)</ref>.</p><p>Contrastive estimation <ref type="bibr" target="#b10">(Smith and Eisner, 2005</ref>) is a learning technique that approximates the par- tition function of the EM objective in a log-linear model by considering a neighborhood around ob- served training examples. The Bayesian HMM of <ref type="bibr" target="#b4">Goldwater and Griffiths (2007)</ref> is a second- order HMM (i.e., likelihood factors over triples of tags) that is estimated using a prior distribu- tion that promotes sparsity. Sparse priors have 45 tag set 17 tag set All train 973k train All train 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 Contrastive Estimation <ref type="bibr" target="#b10">(Smith and Eisner, 2005)</ref> - - 88.7 - Bayesian HMM <ref type="bibr" target="#b4">(Goldwater and Griffiths, 2007)</ref> 86.8 - 87.3 - Bayesian LDA-HMM ( <ref type="bibr" target="#b14">Toutanova and Johnson, 2008)</ref> - - 93.4 - Linguistic initialization ( <ref type="bibr" target="#b3">Goldberg et al., 2008)</ref> 91.4 - 93.8 - Minimal models <ref type="bibr" target="#b9">(Ravi and Knight, 2009)</ref> - 92.3 - 96.8 <ref type="table">Table 2</ref>: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k train describe models trained on the subset of 973k tokens used by <ref type="bibr" target="#b9">Ravi and Knight (2009)</ref>.</p><p>been motivated empirically for this task <ref type="bibr" target="#b5">(Johnson, 2007)</ref>. The Bayesian HMM model predicts tag se- quences via Gibbs sampling, integrating out model parameters. The Bayesian LDA-based model of Toutanova and Johnson (2008) models ambiguity classes of words, which allows information shar- ing among words in the tag dictionary. In addition, it incorporates morphology features and a sparse prior of tags for a word. Inference approximations are required to predict tags, integrating out model parameters. <ref type="bibr" target="#b9">Ravi and Knight (2009)</ref> employs integer linear programming to select a minimal set of parame- ters that can generate the test sentences, followed by EM to set parameter values. This technique requires the additional information of which sen- tences will be used for evaluation, and its scalabil- ity is limited. In addition, this work used a sub- set of the WSJ Penn Treebank for training and se- lecting a tag dictionary. This restriction actually tends to improve performance, because a smaller tag dictionary further constrains model optimiza- tion. We compare directly to their training set, kindly provided to us by the authors.</p><p>The linguistic initialization of <ref type="bibr" target="#b3">Goldberg et al. (2008)</ref> is most similar to the current work, in that it estimates maximum likelihood parameters of an HMM using EM, but starting with a well- chosen initialization with language specific lin- guistic knowledge. That work estimates emission distributions using a combination of suffix mor- phology rules and corpus context counts. <ref type="table">Table 2</ref> compares our results to these related techniques. Each column represents a variant of the experimental setting used in prior work. <ref type="bibr" target="#b10">Smith and Eisner (2005)</ref> introduced a mapping from the full 45 tag set of the Penn Treebank to 17 coarse tags. We report results on this coarse set by pro- jecting from the full set after learning and infer- ence. <ref type="bibr">1</ref> Using the full tag set or the full training data, our method offers the best published perfor- mance without language-specific assumptions or approximate inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>This paper has demonstrated a simple and effec- tive learning method for type-supervised, trans- ductive part-of-speech tagging. However, it is an open question whether the technique is as effec- tive for tag dictionaries derived from more natural sources than the labels of an existing treebank.</p><p>All of the methods to which we compare ex- cept <ref type="bibr" target="#b3">Goldberg et al. (2008)</ref> focus on learning and modeling techniques, while our method only ad- dresses initialization. We look forward to inves- tigating whether our technique can be used as an initialization or prior for these other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The data log-likelihood (top) and tagging accuracy (bottom) of two contrasting initializers, UNIFORM and SUPERVISED, compared on the Penn Treebank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The data log-likelihood (top) and tagging accuracy (bottom) of two partially supervised initializers, one with SUPERVISED TRANSITIONS and one with SUPERVISED EMISSIONS, compared on the Penn Treebank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 summarizes the perfor- mance of all initializers discussed so far for the</head><label>1</label><figDesc></figDesc><table>818 </table></figure>

			<note place="foot" n="1"> Training with the reduced tag set led to lower performance of 91.0% accuracy, likely because the coarse projection drops critical information about allowable English transitions, such as what verb forms can follow to be (Goldberg et al., 2008).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disambiguation rules for part of speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing Using Very Large Corpora</title>
		<imprint>
			<publisher>Kluwer Academic Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with bilingual graph-based projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Assocation for Computational Linguistics</title>
		<meeting>the Assocation for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a part-of-speech tagger from two hours of annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Assocation for Computational Linguistics</title>
		<meeting>the North American Chapter of the Assocation for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">EM can find pretty good HMM POS-taggers (when given a good start)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meni</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fully Bayesian approach to unsupervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Why doesnt EM nd good HMM POS-taggers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wiki-ly supervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tagging English text with a probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimized models for unsupervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stylebook for the tbingen treebank of written german</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Telljohann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Zinsmeister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why initialization matters for ibm model 1: Multiple optima and non-strict convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="461" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Bayesian LDA-based model for semi-supervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural and Information Processing Systems</title>
		<meeting>Neural and Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
