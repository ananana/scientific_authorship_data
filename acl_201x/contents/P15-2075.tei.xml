<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Lexical Embeddings with Syntactic and Lexicographic Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Lexical Embeddings with Syntactic and Lexicographic Knowledge</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="458" to="463"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose two improvements on lexical association used in embedding learning: factorizing individual dependency relations and using lexicographic knowledge from monolingual dictionaries. Both proposals provide low-entropy lexical co-occurrence information, and are empirically shown to improve embedding learning by performing notably better than several popular embedding models in similarity tasks. 1 Lexical Embeddings and Relatedness Lexical embeddings are essentially real-valued distributed representations of words. As a vector-space model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks. In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pen-nington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014). To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Eu-clidean similarity between the embeddings of related words. Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models. In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts. Although supported by the distributional hypothesis (Harris, 1954), this definition suffers from two major limitations. Firstly, the window frame size is usually rather small (for efficiency and sparsity considerations), which increases the false negative rate by missing long-distance dependencies. Secondly , a window frame can (and often does) span across different constituents in a sentence, resulting in an increased false positive rate by associating unrelated words. The problem is worsened as the size of the window increases since each false-positive n-gram will appear in two subsuming false-positive (n + 1)-grams. Several existing studies have addressed these limitations of window-based contexts. Nonetheless , we hypothesize that lexical embedding learning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable improvement in lexical similarity tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Lexical Embeddings and Relatedness</head><p>Lexical embeddings are essentially real-valued distributed representations of words. As a vector- space model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in var- ious NLP tasks. In recent years, researchers have developed efficient and effective algorithms for learning embeddings <ref type="bibr" target="#b15">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b21">Pennington et al., 2014</ref>) and extended model applica- tions from language modelling to various areas in NLP including lexical semantics ( <ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>) and parsing ( <ref type="bibr" target="#b2">Bansal et al., 2014</ref>).</p><p>To approximate semantic relatedness with ge- ometric distance, objective functions are usu- ally chosen to correlate positively with the Eu- clidean similarity between the embeddings of re- lated words. Maximizing such an objective func- tion is then equivalent to adjusting the embeddings so that those of the related words will be geomet- rically closer.</p><p>The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models. In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts. Al- though supported by the distributional hypothe- sis <ref type="bibr">(Harris, 1954)</ref>, this definition suffers from two major limitations. Firstly, the window frame size is usually rather small (for efficiency and sparsity considerations), which increases the false negative rate by missing long-distance dependencies. Sec- ondly, a window frame can (and often does) span across different constituents in a sentence, result- ing in an increased false positive rate by associ- ating unrelated words. The problem is worsened as the size of the window increases since each false-positive n-gram will appear in two subsum- ing false-positive (n + 1)-grams.</p><p>Several existing studies have addressed these limitations of window-based contexts. Nonethe- less, we hypothesize that lexical embedding learn- ing can further benefit from (1) factorizing syntac- tic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable im- provement in lexical similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Lexical embeddings have traditionally been used in language modelling as distributed representa- tions of words ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b19">Mnih and Hinton, 2009</ref>) and have only recently been used in other NLP tasks. <ref type="bibr" target="#b24">Turian et al. (2010)</ref>, for example, used embeddings from existing language models <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b18">Mnih and Hinton, 2007)</ref> as unsupervised lexical features to improve named entity recognition and chunking. Embed- ding models gained further popularity thanks to the simplicity and effectiveness of the word2vec model ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref>, which implicitly factorizes the point-wise mutual information ma- trix shifted by biases consisting of marginal counts of individual words ( <ref type="bibr" target="#b14">Levy and Goldberg, 2014b)</ref>. Efficiency is greatly improved by approximating the computationally costly softmax function with negative sampling (similar to that of Collobert and Weston 2008) or hierarchical softmax (similar to that of <ref type="bibr" target="#b18">Mnih and Hinton 2007)</ref>.</p><p>To address the limitation of contextual locality in many language models (including word2vec), <ref type="bibr" target="#b12">Huang et al. (2012)</ref> added a "global context score" to the local n-gram score <ref type="bibr" target="#b6">(Collobert and Weston, 2008)</ref>. The concatenation of word vectors and a "document vector" (centroid of the composing word vectors weighted by idf ) was used as model input. <ref type="bibr" target="#b21">Pennington et al. (2014)</ref> proposed to explic- itly factorize the global co-occurrence matrix be- tween words, and the resulting log bilinear model achieved state-of-the-art performance in lexical similarity, analogy, and named entity recognition.</p><p>Several later studies addressed the limitations of window-based co-occurrence by extending the word2vec model to predict words that are syn- tactically related to target words. <ref type="bibr" target="#b13">Levy and Goldberg (2014a)</ref> used syntactically related words non- discriminatively as syntactic context. <ref type="bibr" target="#b2">Bansal et al. (2014)</ref> used a training corpus consisting of se- quences of labels following certain manually com- piled patterns. <ref type="bibr" target="#b26">Zhao et al. (2014)</ref> employed coarse-grained classifications of contexts accord- ing to the hierarchical structures in a parse tree.</p><p>Semantic relations have also been explored as a form of lexical association. <ref type="bibr" target="#b7">Faruqui et al. (2015)</ref> proposed to retrofit pre-trained embeddings (de- rived using window-based contexts) to semantic lexicons. The goal is to derive a set of embeddings to capture relatedness suggested by semantic lex- icons while maintaining their resemblance to the corresponding window-based embeddings. <ref type="bibr" target="#b4">Bollegala et al. (2014)</ref> trained an embedding model with lexical, part-of-speech, and dependency patterns extracted from sentences containing frequently co- occurring word pairs. Each relation was repre- sented by a pattern representation matrix, which was combined and updated together with the word representation matrix (i.e., lexical embeddings) in a bilinear objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Factorizing Dependency Relations</head><p>One strong limitation of the existing dependency- based models is that no distinctions are made among the many different types of dependency re- lations. This is essentially a compromise to avoid issues in model complexity and data sparsity, and it precludes the possibility of studying individual or interactive effects of individual dependency re- lations on embedding learning.</p><p>Consequently, we propose a relation-dependent model to predict dependents given a governor un- der individual dependency relations. For example, given a nominal governor apple of the adjective modifier relation (amod), an embedding model will be trained to assign higher probability to ob- served adjectival dependents (e.g., red, sweet, etc.) than to rarely or never observed ones (e.g., pur- ple, savoury, etc.). If a model is able to accurately make such predictions, it can then be said to "un- derstand" the meaning of apple by possessing se- mantic knowledge about its certain attributes. By extension, similar models can be trained to learn the meaning of the governors in other dependency relations (e.g., adjectival governors in the inverse relation amod −1 , etc.).</p><p>The basic model uses an objective function sim- ilar to that of <ref type="bibr" target="#b15">Mikolov et al. (2013a)</ref>:</p><formula xml:id="formula_0">log σ (e T g e d ) + k ∑ i=1 E ˆ d i [log σ (−e T g e ˆ d i )],</formula><p>where e * and e * are the target and the output embeddings for the corresponding words, respec- tively, and σ is the sigmoid function. The sub- scripts g and d indicate whether an embedding cor- respond to the governor or the dependent of a de- pendency pair, andˆdandˆ andˆd * correspond to random sam- ples from the dependent vocabulary (drawn by un- igram frequency).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Incorporating Lexicographic Knowledge</head><p>Semantic information used in existing studies (Section 2) either relies on specialized lexical re- sources with limited availability or is obtained from complex procedures that are difficult to repli- cate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining rela- tion has been demonstrated to have good perfor- mance in various semantic tasks ( <ref type="bibr" target="#b5">Chodorow et al., 1985;</ref><ref type="bibr" target="#b0">Alshawi, 1987)</ref>. The inverse of the defining relation (also known as the Olney Concordance In- dex, <ref type="bibr" target="#b22">Reichert et al. 1969</ref>) has also been proven use- ful in building lexicographic taxonomies <ref type="bibr" target="#b1">(Amsler, 1980)</ref> and identifying synonyms ( <ref type="bibr" target="#b25">Wang and Hirst, 2011</ref>). Therefore, we use both the defining rela- tion and its inverse as sources of semantic associ- ation in the proposed embedding models.</p><p>Lexicographic knowledge is represented by adopting the same terminology used in syntactic dependencies: definienda as governors and defini- entia as dependents. For example, apple is related to fruit and rosaceous as a governor under def, or to cider and pippin as a dependent under def −1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Individual Knowledge Sources</head><p>Sparsity is a prominent issue in the relation- dependent models since each individual relation only receives a limited share of the overall co- occurrence information. We also propose a post- hoc, relation-independent model that combines the individual knowledge sources. The input of the model is the structured knowledge from relation- dependent models, for example, that something can be red or sweet, or it can ripen or fall, etc. The training objective is to predict the original word given the relation-dependent embeddings, with the intuition that if a model is trained to be able to "solve the riddle" and predict that this something is an apple, then the model is said to possess generic, relation-independent knowl- edge about the target word by learning from the relation-dependent knowledge sources. Given input word w I , its relation-independent embedding is derived by applying a linear model M on the concatenation of its relation-dependent embeddings (˜ e w I ). The objective function of a relation-independent model is then defined as</p><formula xml:id="formula_1">log σ (e T w I M˜eM˜e w I ) + k ∑ i=1 E ¯ w i [log σ (−e T ¯ w i M˜eM˜e w I )],</formula><p>where e * are the context embeddings for the corre- sponding words. Since˜eSince˜ Since˜e w I is a real-valued vector (instead of a 1-hot vector as in relation-dependent models), M can no longer be updated one column at a time. Instead, updates are defined as:</p><formula xml:id="formula_2">∂ ∂ M = [1 − σ (e T w O M˜eM˜e w I )]e w O ˜ e T w I − k ∑ i=1 [1 − σ (−e T w i M˜eM˜e w I )]e w i ˜ e T w I .</formula><p>Training is quite efficient in practice due to the low dimensionality of M; convergence is achieved af- ter very few epochs. 1 Note that this model is different from the non- factorized models that conflate multiple depen- dency relations because the proposed model is a deeper structure with pre-training on the factor- ized results (via the relation-dependent models) in the first layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data and Baselines</head><p>The Annotated English Gigaword ( <ref type="bibr" target="#b20">Napoles et al., 2012</ref>) is used as the main training corpus. It con- tains 4 billion words from news articles, parsed by the Stanford Parser. A random subset with 17 mil- lion words is also used to study the effect of train- ing data size (dubbed 17M).</p><p>Semantic relations are derived from the defini- tion text in the Online Plain Text English Dictio- nary 2 . There are approximately 806,000 definition pairs, 33,000 distinct definienda and 24,000 dis- tinct defining words. The entire corpus has 1.25 million words in a 7.1MB file.</p><p>Three baseline systems are used for compar- ison, including one non-factorized dependency- based model DEP ( <ref type="bibr" target="#b13">Levy and Goldberg, 2014a</ref>) and two window-based embedding models <ref type="bibr">w2v (or word2vec, Mikolov et al. 2013a</ref>) and GloVe ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>). Embedding dimension is 50 for all models (baselines as well as the pro- posed). Embeddings in the window-based mod- els are obtained by running the published software for each of these systems on the Gigaword corpus with default values for all hyper-parameters except for vector size (50) and minimum word frequency (100 for the entire Gigaword corpus; 5 for the 17M subset). For the w2v model, for example, we used the skip-gram model with the default value 5 as window size, negative sample size, and epoch size, and 0.025 as initial learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lexical Similarity</head><p>Relation-Dependent Models <ref type="table">Table 1</ref> shows the results on four similarity datasets: MC ( <ref type="bibr" target="#b17">Miller and</ref><ref type="bibr">Charles, 1991), RG (Rubenstein and</ref><ref type="bibr" target="#b23">Goodenough, 1965)</ref>, FG (or wordsim353, <ref type="bibr" target="#b8">Finkelstein et al. 2001), and</ref><ref type="bibr">SL (or SimLex, Hill et al. 2014b</ref>). The first three datasets consist of nouns, while the last one also includes verbs (SL v ) and adjectives (SL a ) in addition to nouns (SL n ). Semantically, FG contains many related pairs (e.g., movie-popcorn), whereas the other three datasets are purely similarity oriented.   <ref type="table">Table 1</ref>: Correlation between human judgement and cosine similarity of embeddings (trained on the Gigaword corpus) on six similarity datasets.</p><p>Performance is measured by Spearman's ρ be- tween system scores and human judgements of similarity between the pairs that accompany each dataset.</p><p>When dependency information is factorized into individual relations, models using the best- performing relation for each dataset 3 out-perform the baselines by large margins on 5 out of the 6 datasets. In comparison, the advantage of the syn- tactic information is not at all obvious when they are used in a non-factorized fashion in the DEP model; it out-performs the window-based meth- ods (below the dashed line) on only 3 datasets with limited margins. However, the window-based methods consistently outperform the dependency- based methods on the FG dataset, confirming our intuition that window-based methods are better at capturing relatedness than similarity.</p><p>When dependency relations are factorized into individual types, sparsity is a rather prominent is- sue especially when the training corpus is small. With sufficient training data, however, factorized models consistently outperform all baselines by very large margins on all but the FG dataset. Av- erage correlation (weighted by the size of each sub-dataset corresponding to the three POS's) on the SL dataset is 0.531, outperforming the best re- ported result on the dataset ( <ref type="bibr" target="#b10">Hill et al., 2014a</ref>  <ref type="table">Table 2</ref>: Lexical similarity performance of relation-independent models (trained on the 17M corpus) combining top two best-performing rela- tions for each POS.</p><p>Although the co-occurrence data is sparse, it is nonetheless highly "focused" ( <ref type="bibr" target="#b13">Levy and Goldberg, 2014a</ref>) with much lower entropy. As a result, convergence is much faster when compared to the non-factorized models such as DEP, which takes up to 10 times more iterations to converge.</p><p>Among the individual dependency relations, the most effective relations for nouns, adjectives, and verbs are amod, amod −1 , and nsubj, respec- tively. For nouns, we observed a notable gap in performance between amod and nn. Data inspec- tion reveals that a much higher proportion of nn modifiers are proper nouns (64.0% compared to about 0.01% in amod). The comparison suggests that, as noun modifiers, amod describes the at- tributes of nominal concepts while nn are more often instantiations, which apparently is semanti- cally less informative. On the other hand, nn is the better choice if the goal is to train embeddings for proper nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation-Independent Model</head><p>The relation-independent model (Section 3.3) is implemented by combining the top two best- performing relations for each POS: amod and dobj −1 for noun pairs, nsubj and dobj for verb pairs, and amod −1 and dobj −1 for adjective pairs.</p><p>Lexical similarity results on the 17M corpus are listed in <ref type="table">Table 2</ref>. The combined results improve over the best relation-dependent mod- els for all categories except for SL a (adjectives), where only the top-performing relation-dependent model (amod −1 ) yielded statistically significant results and thus, results are worsened by com- bining the second-best relation-dependent source dobj −1 (which is essentially noise). Compar- ing to baselines, the relation-independent model achieves better results in four out of the six cat-  <ref type="table">Table 3</ref>: Lexical similarity performance of mod- els using dictionary definitions and compared to word2vec trained on the Gigaword corpus.</p><p>egories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using Dictionary Definitions</head><p>Embeddings trained on dictionary definitions are also evaluated on the similarity datasets, and the results are shown in <ref type="table">Table 3</ref>. The individ- ual relations (defining and inverse) perform sur- prisingly well on the datasets when compared to word2vec. The relation-independent model brings consistent improvement by combining the relations, and the results compare favourably to word2vec trained on the entire Gigaword cor- pus. Similar to dependency relations, lexico- graphic information is also better at capturing sim- ilarity than relatedness, as suggested by the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This study explored the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. Compared to exist- ing syntax-based embedding models, the proposed embedding models benefits from factorizing syn- tactic information by individual dependency rela- tions. Empirically, syntactic information from in- dividual dependency types brings about notable improvement in model performance at a much higher rate of convergence. Lexicographic knowl- edge from monolingual dictionaries also helps im- prove lexical embedding learning. Embeddings trained on a compact, knowledge-intensive re- source rival state-of-the-art models trained on free texts thousands of times larger in size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Model 
MC 
RG 
FG SL n 
SL v SL a 
Rel. Dep. #1 .512 .486 .380 .354 
.222 .394 
Rel. Dep. #2 .390 .380 .360 .304 
.206 .236 

Rel. Indep. 

.570 .550 .392 .360 
.238 .338 

Baselines 

DEP 
.530 .558 .506 .346 
.138 .412 
w2v 
.563 .491 .562 .287 
.065 .379 
GloVe 
.306 .368 .308 .132 −.007 .254 

</table></figure>

			<note place="foot" n="1"> We also experimented with updating the relationdependent embeddings together with M, but this worsened evaluation performance.</note>

			<note place="foot" n="2"> http://www.mso.anu.edu.au/ ˜ ralph/ OPTED/</note>

			<note place="foot" n="3"> We did not hold out validation data to choose the bestperforming relations for each dataset. Our assumption is that the dominant part-of-speech of the words in each dataset is the determining factor of the top-performing syntactic relation for that dataset. Consequently, the choice of this relation should be relatively constant without having to rely on traditional parameter tuning. For the four noun datasets, for example, we observed that amod is consistently the topperforming relation, and we subsequently assumed similar consistency on the verb and the adjective datasets. The same observations and rationales apply for the relationindependent experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Gerald Penn, Ruslan Salakhutdinov, Suzanne Stevenson, and Xiaodan Zhu for their in-sightful comments, as well as the anonymous re-viewers for their valuable feedback. This study is financially supported by the Natural Sciences and Engineering Research Council of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Processing dictionary definitions with phrasal pattern hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The structure of the MerriamWebster Pocket Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Amsler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
		<respStmt>
			<orgName>The University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gauvain. Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2378</idno>
		<title level="m">Learning word representations from relational graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting semantic hierarchies from a large on-line dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Heidorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 23rd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="299" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on World Wide Web</title>
		<meeting>the 10th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Zellig Harris. Distributional structure. Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Embedding word similarity with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6448</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Coline Devin, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotated Gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two Dictionary Transcripts and Programs for Processing Them-The Encoding Scheme, Parsent and Conix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Olney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Research Report AD0691098</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring patterns in dictionary definitions for synonym extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning word embeddings from dependency relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2014 International Conference on Asian Language Processing (IALP)</title>
		<meeting>2014 International Conference on Asian Language Processing (IALP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="123" to="127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
