<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Helping Hand: Transfer Learning for Deep Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University New Brunswick</orgName>
								<address>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University New Brunswick</orgName>
								<address>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Helping Hand: Transfer Learning for Deep Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2524" to="2534"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2524</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different datasets in seven different languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past decades, sentiment analysis has grown from an academic endeavour to an essential analytics tool. Across the globe, people are voicing their opinion in online social media, product review sites, booking platforms, blogs, etc. Hence, it is important to keep abreast of ongoing developments in all pertinent markets, accounting for different domains as well as different languages. In recent years, deep neural architectures based on convolu- tional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification. At the same time, it is also frequently observed that deep neural networks tend to be particularly data-hungry. This is a problem in many real-world settings, where large amounts of training examples may be too costly to obtain for every target domain. A model trained on movie reviews, for instance, will fare very poorly on the task of assessing restaurant or hotel reviews, let alone tweets about politicians.</p><p>In this paper, we investigate how extrinsic sig- nals can be incorporated into deep neural networks for sentiment analysis. Numerous papers have found the use of regular pre-trained word vector representations to be beneficial for sentiment anal- ysis ( <ref type="bibr" target="#b28">Socher et al., 2013;</ref><ref type="bibr" target="#b16">Kim, 2014</ref>; dos Santos and de C. <ref type="bibr" target="#b27">Gatti, 2014)</ref>. In our paper, we instead consider word embeddings specifically specialized for the task of sentiment analysis, studying how they can lead to stronger and more consistent gains, despite the fact that the embeddings were obtained using out-of-domain data.</p><p>An intuitive solution would be to concatenate regular embeddings, which provide semantic re- latedness cues, with sentiment polarity cues that are captured in additional dimensions. We instead propose a bespoke convolutional neural network architecture with a separate memory module dedi- cated to the sentiment embeddings. Our empirical study shows that the sentiment embeddings can lead to consistent gains across different datasets in a diverse set of domains and languages if a suitable neural network architecture is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentiment Embedding Computation</head><p>Our goal is to incorporate external cues into a deep neural network such that the network is able to generalize better even when training data is scarce. While in computer vision, weights pre-trained on ImageNet are often used for transfer learning, the most popular way to incorporate external informa- tion into deep neural networks for text is to draw on word embeddings trained on vast amounts of word context information ( <ref type="bibr" target="#b23">Mikolov et al., 2013;</ref><ref type="bibr" target="#b24">Pennington et al., 2014;</ref><ref type="bibr" target="#b25">Peters et al., 2018)</ref>. Indeed, the semantic relatedness signals provided by such rep- resentations often lead to slightly improved results in polarity classification tasks <ref type="bibr" target="#b28">(Socher et al., 2013;</ref><ref type="bibr" target="#b16">Kim, 2014</ref>; dos Santos and de C. <ref type="bibr" target="#b27">Gatti, 2014)</ref>.</p><p>However, the co-occurrence-based objectives of word2vec and GloVe do not consider sentiment specifically. We thus seek to examine how com- plementary sentiment-specific information from an external source can give rise to further gains.</p><p>Transfer Learning. To this end, our goal is to in- duce sentiment embeddings that capture sentiment polarity signals in multiple domains and hence may be useful across a range of different sentiment anal- ysis tasks. The multi-domain nature of these dis- tinguish them from the kinds of generic polarity scores captured in sentiment polarity lexicons. We achieve this via transfer learning from trained mod- els, benefiting from supervision on a series of senti- ment polarity tasks from different domains. Given a training collection consisting of n binary clas- sification tasks (e.g., with documents in n differ- ent domains), we learn n corresponding polarity prediction models. From these, we then extract token-level scores that are tied to specific predic- tion outcomes. Specifically, we train n linear mod- els f i (x) = w i x + b i for tasks i = 1, . . . , n. Then, each vocabulary word index j is assigned a new n- dimensional word vector x j = (w 1,j , · · · , w n,j ) that incorporates the linear coefficients for that word across the different linear models.</p><p>A minor challenge is that na¨ıvelyna¨ıvely using bag-of- word features can lead to counter-intuitive weights. If a word such as "pleased" in one domain mainly occurs after the word "not", while the reviews in another domain primarily used "pleased" in its un- negated form, then "pleased" would be assessed as possessing opposite polarities in different domains. To avoid this, we assume that features are prepro- cessed to better reflect whether words occur in a negated context. In our experiments, we simply treat occurrences of "not word" as a single fea- ture "not word". Of course, one can replace this heuristic with much more sophisticated techniques that fully account for the scope of a wider range of negation constructions.</p><p>Graph-Based Extension. Most sentiment-related resources are available for the English language. To produce vectors for other languages in our experiments, we rely on cross-lingual projec- tion via graph-based propagation ( <ref type="bibr" target="#b6">de Melo, 2015;</ref><ref type="bibr" target="#b22">de Melo, 2017;</ref><ref type="bibr" target="#b7">Dong and de Melo, 2018)</ref>. At this point, we have a set of initial sentiment em- bedding vectors˜vvectors˜ vectors˜v x ∈ R n for words x ∈ V 0 . We assume that we have a lexical knowledge graph G L = (V, A L ) with a node set consist- ing of an extended multilingual vocabulary V ⊇ V 0 and a set of weighted directed arcs A L = {(x 1 , x 1 , w 1 ), . . . , (x m , x m , w m )}. Each such arc reflects a weighted semantic connection between two vocabulary items x, x ∈ V , where vocabulary items are words labeled with their respective lan- guage. Typically, many of the arcs in the G L would reflect translational equivalence, but in our experi- ments, we also include monolingual links between semantically related words. Given this data, we aim to minimize</p><formula xml:id="formula_0">− x∈V v x    1 (x,x ,w)∈A L w (x,x ,w)∈A L wv x    +C x∈V 0 v x − ˜ v x 2<label>(1)</label></formula><p>The first component of this objective seeks to en- sure that sentiment embeddings of words accord with those of their connected words, in terms of the dot product. The second part ensures that the devi- ation from any available initial word vectors˜vvectors˜ vectors˜v x is minimal (for some very high constant C). For opti- mization, we preinitialize v x = ˜ v x for all x ∈ V 0 , and then rely on stochastic gradient descent steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dual-Module Memory based CNNs</head><p>To feed this sentiment information into our archi- tecture, we propose a Dual-Module Memory based Convolutional Neural Network (DM-MCNN) ap- proach, which incorporates a dedicated memory module to process the sentiment embeddings, as illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. While the module with regular word embeddings enables the model to learn salient patterns and harness the nearest neighbour and lin- ear substructure properties of word embeddings, we conjecture that a separate sentiment memory module allows for better exploiting the information brought to the table by the sentiment embeddings.</p><p>Convolutional Module Inputs and Filters. The Convolutional Module input of the DM-MCNN is a sentence matrix S ∈ R s×d , the rows of which represent the words of the input sentence after to- kenization. In the case of S, i.e., in the regular module, each word is represented by its conven- tional word vector representation. Here, s refers to the length of a sentence, and d represents the dimensionality of the regular word vectors.</p><p>We perform convolutional operations on these matrices via linear filters. Given rows representing discrete words, we rely on weight matrices W ∈ R h×d with region size h. We use the notation S i:j to denote the sub-matrix of S from row i to row    <ref type="bibr" target="#b14">Kalchbrenner et al., 2014</ref>) is induced such that out-of-range submatrix values S i,j with i &lt; 1 or i &gt; s are taken to be zero. Thus, applying the filter on sub-matrices of S yields the output sequence o ∈ R s+h−1 as</p><formula xml:id="formula_1">sentence: ! " , ! # ,… ! $ , ! "% ! " ! # ! ' ! ( ! ) ! * ! + ! , ! $ ! "%</formula><formula xml:id="formula_2">o i = W S i:i+h−1 ,<label>(2)</label></formula><p>where the operator provides the sum of an element-wise multiplication. Wide convolutions ensure that filters can cover words at the margins of the normal weight matrix. Next, the c i in feature maps c ∈ R s+h−1 are computed as:</p><formula xml:id="formula_3">c i = f (o i + b)</formula><p>, where i = 1, . . . , s + h − 1, the parameter b ∈ R is a bias term, and f is an activation function.</p><p>Multiple Layers in Memory Module. The mem- ory module obtains as input a sequence of senti- ment embedding vectors for the input, and attempts to draw conclusions about the overall sentiment po- larity of the entire input sequence. Given a set of sentence words S = {w 1 , w 2 , w 3 , . . . , w n }, each word is mapped to its sentiment embedding vector of dimension d s and we denote this set of vectors as V s . The preliminary sentiment level v p is also a vector of dimensionality d s . We take the mean of all sentiment vectors v i for words w i ∈ S to initialize v p . Next, we compute a vector s of sim- ilarities s i between v p and each sentiment word vector v i , by taking the inner product, followed by 2 -normalization and a softmax:</p><formula xml:id="formula_4">s i = exp v p v i v p v i 2 i exp v p v i v p v i 2<label>(3)</label></formula><p>As the sentiment embeddings used in our paper are generated from a linear model, the degree of cor- respondence between v p and v i can adequately be assessed by the inner product. The resulting vector of scores s can be regarded as yielding sentiment weights for each word in the sentence. We apply 2 -normalization to ensure a more balanced weight distribution. The output sentiment level vector v o is then a sum over the sentiment inputs v i weighted by the 2 -normalized vector of similarities:</p><formula xml:id="formula_5">v o = i s i s 2 v i (4)</formula><p>This processing can be repeated in multiple passes, akin to how end-to-end memory networks for question answering often perform multiple hops ( <ref type="bibr" target="#b29">Sukhbaatar et al., 2015)</ref>. While in the first itera- tion, v p was set to the mean sentiment vector, sub- sequent passes may allow us to iteratively refine this vector. Assuming that v k o has been produced by the k-th pass, then the subsequent level v k+1 p in the next pass is:</p><formula xml:id="formula_6">v k+1 p = v k o + v k p (5)</formula><p>The intuition here is that multiple passes can enable the model to adaptively retrieve iterative sentiment level statistics beyond the initial average sentiment information.</p><p>Merging Layer and Prediction. Subsequently, for the convolutional module, 1d-max pooling is applied to c, which ought to capture the most promi- nent signals. In the memory module, the final sentiment vector is modulated by a weight matrix W s ∈ R l×ds to form a feature vector of dimension- ality l. In general, we can use multiple filters to obtain several features in the convolutional module, while the memory module allows for adjusting the number of passes over the memory. Finally, the outputs of these two modules are concatenated to form a fixed-length vector, which is passed to a fully connected softmax layer to obtain the final output probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function and Training. Our loss function is the cross-entropy function</head><formula xml:id="formula_7">L = − 1 n n i=1 c∈C y i,c lnˆylnˆ lnˆy i,c ,<label>(6)</label></formula><p>where n is the number of training examples, C is the set of (two) classes, y i,c are ground truth labels for a given training example and class c, andˆyandˆ andˆy i,c are corresponding label probabilities predicted by the model, as emitted by the softmax layer. We train our model using Adam optimization (Kingma and Ba, 2014) for better robustness across different datasets. Further details about our training regime follow in the Experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We now turn to our extensive empirical evaluation, which assesses the effectiveness of our novel archi- tecture with sentiment word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. For evaluation, we use real world datasets for 7 different languages, taken from a range of different sources that cover several do- mains. These are summarized in <ref type="table" target="#tab_1">Table 1</ref>, with ISO 639-3 language codes. In our experimental setup, these are all cast as binary polarity classification tasks, for which we use accuracy as our evaluation metric.</p><p>• • From TripAdvisor (TA), we crawled German, Russian, Italian, Czech, and Japanese reviews of restaurants and hotels. We removed three- star reviews, as these can be regarded as neu- tral ones, so reviews with a rating &lt; 3 are considered negative, while those with a rating &gt; 3 were deemed positive.</p><p>• The Amazon Fine Food Reviews AFF <ref type="bibr" target="#b21">(McAuley and Leskovec, 2013</ref>) dataset pro- vides food reviews left on Amazon. We chose a random subset of it with preprocessing as for TripAdvisor. As there was no test set provided for TripAdvisor or for the Amazon Fine Food Reviews data, we randomly partitioned this data into training, val- idation, and test splits with a 80%/10%/20% ra- tio. Additionally, 10% of the training sets from SE16-T5 were randomly extracted and reserved for validation, while SST provides its own validation set. The new datasets are available from http: //gerard.demelo.org/sentiment/.</p><p>Embeddings. The standard pre-trained word vec- tors used for English are the GloVe ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>) ones trained on 840 billion tokens of For our transfer learning approach, our experi- ments rely on the multi-domain sentiment dataset by <ref type="bibr" target="#b2">Blitzer et al. (2007)</ref>, collected from Amazon cus- tomers reviews. This dataset includes 25 categories of products and is used to generate our sentiment embeddings using linear models. Specifically, we train linear SVMs using scikit-learn to extract word coefficients in each domain and also for the union of all domains together, yielding a 26-dimensional sentiment embedding.</p><p>For comparison and analysis, we also consider several alternative forms of infusing external cues. Firstly, lexicon-driven methods have often been used for domain-independent sentiment analysis. We consider a recent sentiment lexicon called VADER ( <ref type="bibr" target="#b12">Hutto and Gilbert, 2014</ref>). The polar- ity scores assigned to words by the lexicon are taken as the components of a set of 1-dimensional word vectors (dividing the original scores by the difference between max and min polarity scores for normalization). Secondly, as another particu- larly strong alternative, we consider the SocialSent Reddit community-specific lexicons mined by the Stanford NLP group ( <ref type="bibr" target="#b8">Hamilton et al., 2016</ref>). These contain separate domain-specific scores for 250 different Reddit communities, and hence result in 250-dimensional embeddings.</p><p>For cross-lingual projection, we extract links between words from a 2017 dump of the English edition of Wiktionary. We restrict the vocabulary link set to include the languages in <ref type="table" target="#tab_1">Table 1</ref>, mining corresponding translation, synonymy, derivation, and etymological links from Wiktionary.</p><p>Neural Network Details. For CNNs, we make use of the well-known CNN-non-static architecture and hyperparameters proposed by <ref type="bibr" target="#b16">Kim (2014)</ref>, with a learning rate of 0.0006, obtained by tuning on the validation data. For our DM-MCNN models, the configuration of the convolutional module is the same as for CNNs, and the remaining hyperparam- eter values were as well tuned on the validation sets. An overview of the relevant network parame- ter values is given in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>For greater efficiency and better convergence properties, the training relies on mini-batches. Our implementation considers the maximal sentence length in each mini-batch and zero-pads all other sentences to this length under convolutional mod- ule, thus enabling uniform and fast processing of each mini-batch. All neural network architectures are implemented using the PyTorch framework 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>Baseline Results. Our main results are summa- rized in <ref type="table" target="#tab_4">Table 3</ref>. We compare both regular CNNs and our dual-module alternative DM-MCNNs un- der a variety of settings. A common approach is to use a CNN with randomly initialized word vec- tors. Comparing this to CNNs with GloVe/fastText embeddings, where GloVe is used for English, and fastText is used for all other languages, we observe substantial improvements across all datasets. This shows that word vectors do tend to convey perti- nent word semantics signals that enable models to generalize better. Note also that the accuracy us- ing GloVe on the English movies review dataset is consistent with numbers reported in previous work ( <ref type="bibr" target="#b35">Zhang and Wallace, 2015)</ref>. Dual-Module Architecture. Next, we consider our DM-MCNNs with their dual-module mecha- nism to take advantage of transfer learning. We ob- serve fairly consistent and sometimes quite substan- We report results for two different training con- ditions. In the first condition (with fine-tuning), the sentiment embedding matrix is preinitialized using the data from our transfer learning procedure, but the model is then able to modify these arbitrarily via backpropagation. In the second condition (no fine-tuning), we simply use our sentiment embed- ding matrix as is, and do not update it. Instead, the model is able to update its various other parame- ters, particularly its various weight matrices and bias vectors. While both training conditions outper- form the CNN baseline, there is no obvious winner among the two. When the training data set is very small and hence there is a significant risk of overfit- ting, one may be best advised to forgo fine-tuning. In contrast, when it is somewhat larger (as for our English datasets, which each have over 5,000 train- ing instances) or when the language is particularly idiosyncratic or not covered sufficiently well by our cross-lingual projection procedure (such as perhaps for Japanese), then fine-tuning is recommended. In this case, fine-tuning may allow the model to adjust the embeddings to cater to domain-specific mean- ings and corpus-specific correlations, while also overcoming possible sparsity of the cross-lingual vectors resulting from a lack of coverage of the translation dictionary.</p><p>It is important to note that many of the results in <ref type="table" target="#tab_4">Table 3</ref> stem from embeddings that were cre- ated automatically using cross-lingual projection. Our transfer learning embeddings were induced from entirely English data. Although the automati- cally projected cross-lingual embeddings are very noisy and limited in their coverage, particularly with respect to inflected forms, our model succeeds in exploiting them to obtain substantial gains in several different languages and domains.</p><p>Alternative Embedding Methods. For a more de- tailed analysis, we conducted additional experi- ments with alternative embedding conditions. In particular, as a simpler means of achieving gains over standard CNNs, we propose to use CNNs with word vectors augmented with sentiment cues. Given that regular word embeddings appear to be useful for capturing semantics, one may conjecture that extending these word vectors with additional dimensions to capture sentiment information can lead to improved results. For this, we simply con- catenate the regular word embeddings with differ- ent forms of sentiment embeddings that we have obtained, including those from the sentiment lexi- con VADER, from the Stanford SocialSent project, and from our transfer learning procedure via Ama- zon reviews. To conduct these experiments, we also produced cross-lingual projections of the VADER and SocialSent embedding data.</p><p>The results of using these embeddings as op- posed to regular ones are somewhat mixed. Con-catenating the VADER embeddings or our trans- fer learning ones leads to minor improvements on English, and our cross-lingual projection of them leads to occasional gains, but the results are far from consistent. Even on English, adding the 250- dimensional SocialSent embedding seems to de- grade the effectiveness of the CNN, although all input information that was previously there contin- ues to be provided to it. This suggests that a simple concatenation may harm the model's ability to har- ness the semantic information carried by regular word vectors. This risk seems more pronounced for larger-dimensional sentiment embeddings.</p><p>In contrast, with our DM-MCNNs approach, the sentiment information is provided to the model in a separate memory module that makes multiple passes over this data before combining it with the regular CNN module's signals. Thus, the model can exploit the two kinds of information indepen- dently, and learn a suitable way to aggregate them to produce an overall output classification.</p><p>This hence demonstrates not only that the senti- ment embeddings tend to provide important com- plementary signals but also that a dual-module ap- proach is best-suited to incorporate such signals into deep neural models.</p><p>We also analysed our DM-MCNNs with alterna- tive embeddings. When we feed random sentiment embeddings into them, not unexpectedly, in many cases the results do not improve much. This is be- cause our memory module has been designed to leverage informative prior information and to re- weight its signals based on this assumption. Hence, it is important to feed genuine sentiment cues into the memory module. Yet, on some languages, we nevertheless note improvements over the CNN baseline. In these cases, even if similarities be- tween pairs of sentiment vectors initially do not carry any significance, backpropagation may have succeeded in updating the sentiment embedding matrix such that eventually the memory module becomes able to discern salient patterns in the data.</p><p>We also considered our DM-MCNNs when feed- ing the VADER or SocialSent embeddings into the memory module. In this case, it also mostly suc- ceeded in outperforming the CNN baseline. In fact, on the Italian TripAdvisor dataset, the SocialSent embeddings yielded the overall strongest results. In all other cases, however, our transfer learning embeddings proved more effective. We believe that this is because they are obtained in a data-driven manner based on an objective that directly seeks to optimize for classification accuracy.</p><p>Influence of Training Set Size. To look into the effect of our approach with restricted training data, we first consider the SST dataset as an instructive example. We set the training set size to 20%, 50%, 100% of its original size and compared our full dual module model with sentiment embeddings against state-of-the-art methods.</p><p>The results are given in <ref type="table">Table 4</ref>. Our dual mod- ule CNN has a sizeable lead over other methods when only using 20% of SST training set. Given that we study how to incorporate extrinsic cues into a deep neural model, we consider CNN-Rule- q ( <ref type="bibr" target="#b10">Hu et al., 2016)</ref> and Gumbel Tree-LSTM ( <ref type="bibr" target="#b5">Choi et al., 2017)</ref> as the relevant baseline methods. The CNN-Rule-q method used an iterative distillation method that exploits structured information from logical rules, which for SST is based on the word but to determine the weights in the neural network. The Gumbel Tree-LSTM approach incorporates a Straight-Through Gumbel-Softmax into a tree- structured LSTM architecture that learns how to compose task-specific tree structures starting from plain raw text. They all require a large amount of data to pick up sufficient information during train- ing, while our method is able to efficiently capture sentiment information from our transfer learning even though the data is scarce.</p><p>For further analysis, we also artificially reduce the training set sizes to 50% of the original sizes given in <ref type="table" target="#tab_1">Table 1</ref> for our multilingual datasets. The results are plotted in <ref type="figure" target="#fig_2">Fig. 2</ref>. We compare: 1) the CNN model baseline, 2) the CNN model but con- catenating our sentiment embeddings from transfer learning, and 3) our full dual module model with these sentiment embeddings. We already saw in Ta- ble 3 that we obtain reasonable gains over generic embeddings when using the full training sets.</p><p>In <ref type="figure" target="#fig_2">Fig. 2</ref>, we additionally observe that the gains are overall much more remarkable on smaller train- ing sets. This shows that the sentiment embeddings are most useful when they are of high quality and domain-specific training data is scarce, although a modest amount of training data is still needed for the model to be able to adapt to the target domain.</p><p>Inspection of the DM-MCNN-learned Deep Sentiment Information. To further investigate what the model is learning, we examine the changes of weights of words on the English SST dataset when using the VADER sentiment embeddings <ref type="table">Table 4</ref>: Accuracy on SST with increasing training sizes Model 20% 50% 100% CNN <ref type="bibr" target="#b16">(Kim, 2014)</ref> 83.14 84.29 85.72 CNN-Rule-q ( <ref type="bibr" target="#b10">Hu et al., 2016)</ref> 83.75 85.45 86.49 Gumbel Tree-LSTM ( <ref type="bibr" target="#b5">Choi et al., 2017)</ref>   with DM-MCNNs. Although these are not as powerful as our transfer learning embeddings, the VADER embeddings are the most easily inter- pretable here, since they are one-dimensional, and thus can be regarded as word-specific weights. The result is visualized in <ref type="figure" target="#fig_3">Fig. 3</ref>. Here, the dark-shaded segments (in blue) refer to the original weights, while the light-shaded segments (in red) refer to the adjusted weights after training. The medium- shaded segments (in purple) reflect the overlap be- tween the two. Hence, whenever we observe a dark (blue) segment above a medium (purple) segment in a bar, we can infer that the fine-tuned weight for a word (e.g., for "plays" in <ref type="figure" target="#fig_3">Fig. 3</ref>) was lower than the original weight of that word. Conversely, whenever we observe a light (red) segment at the top, the weight increased during training (e.g., for hilarious). Generally, dark (blue) segments reflect decreased weight magnitudes and light (red) ones reflect increased weight magnitudes, both on the positive and on the negative side.</p><p>We consider in <ref type="figure" target="#fig_3">Fig. 3</ref> the top 50 weight changes only of words that were already covered by the original VADER sentiment embeddings. Here, it is worth noting that the weight magnitudes of positive words such as "laugh", "appealing" and negative words such as "lack", "missing" increase further, while words such as "damn", "interest", "war" see decreases in magnitude, presumably due to their ambiguity and context (e.g., "damn good", "lost the interest", descriptions of war movies). Hence, the figure confirms that our DM-MCNN approach is able to exploit and customize the provided sen- timent weights for the target domain. However, unlike the VADER data, our transfer learning ap- proach results in multi-dimensional sentiment em- beddings that can more easily capture multiple do- mains right from the start, thus making it possible to use them even without further fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Sentiment Mining and Embeddings. There is a long history of work on collecting word polarity scores manually ( <ref type="bibr" target="#b9">Hu and Liu, 2004</ref>) or via graph- based propagation from seeds ( <ref type="bibr" target="#b15">Kim and Hovy, 2004;</ref><ref type="bibr" target="#b0">Baccianella et al., 2010)</ref>. <ref type="bibr" target="#b20">Maas et al. (2011)</ref> present a probabilistic topic model that exploits sen- timent supervision during training, leading to rep- resentations that include sentiment signals. How- ever, in their experiments, the semantic-only mod- els mostly outperform the corresponding full mod- els with extra sentiment signals. <ref type="bibr" target="#b31">Tang et al. (2014)</ref> showed that one can acquire sentiment informa- tion by learning from millions of training examples via distant supervision. While prior work used such signals for rule-based sentiment analysis or for feature engineering in SVMs and other shallow models, our study examines how they are best be incorporated into deep neural models, as the base- line of na¨ıvelyna¨ıvely feeding them into the model does not work sufficiently well.</p><p>Neural Architectures. In terms of architectures, deep recursive neural networks <ref type="bibr" target="#b28">(Socher et al., 2013)</ref> were soon outperformed by deep convolutional and recurrent neural networks <ref type="bibr" target="#b13">( ˙ Irsoy and Cardie, 2014;</ref><ref type="bibr" target="#b16">Kim, 2014)</ref>. Recent work has investigated more involved models, with ingredients such as Tree-LSTMs ( <ref type="bibr" target="#b30">Tai et al., 2015;</ref><ref type="bibr" target="#b19">Looks et al., 2017)</ref>, hierarchical attention ( <ref type="bibr" target="#b34">Yang et al., 2016</ref>), user and product attention <ref type="bibr" target="#b4">(Chen et al., 2016)</ref>, aspect- specific modeling ( <ref type="bibr" target="#b32">Wang et al., 2015)</ref>, and part of speech-specific transition functions ( <ref type="bibr" target="#b11">Huang et al., 2017)</ref>. Large ensemble models also tend to outper- form individually trained sentiment analysis mod- els ( <ref type="bibr" target="#b19">Looks et al., 2017)</ref>. The goal of our study is not necessarily to devise the most sophisticated state- of-the-art neural architecture, but to demonstrate how external sentiment cues can be incorporated such architectures. Our initial explorations relied on a simple dual-channel convolutional neural net- work (Dong and de <ref type="bibr" target="#b7">Melo, 2018</ref>). The present work proposes a more sophisticated approach, drawing on ideas from attention mechanisms in machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) as well as from memory networks ( <ref type="bibr">Weston et al., 2014</ref>) and iter- ative attention ( <ref type="bibr" target="#b18">Kumar et al., 2015)</ref>, which have proven useful for tasks such as question answer- ing. We incorporate these ideas into a separate memory module that operates alongside the regular convolutional module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Deep neural networks are widely used in senti- ment polarity classification, but suffer from their dependence on very large annotated training cor- pora. In this paper, we study how to incorporate extrinsic cues into the network, beyond just generic word embeddings. We have found that this is best achieved using a dual-module approach that en- courages the learning of models with favourable generalization abilities. Our experiments show that this can lead to gains across a number of different languages and domains. Our embeddings and mul- tilingual datasets are freely available from http: //gerard.demelo.org/sentiment/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Dual-Module Memory based Convolutional Neural Network architecture. (b) Single layer in Memory Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effectiveness of three embedding alternatives on 6 languages at a reduced training size (comparing 50% and 100%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top 50 weight changes of words fine-tuned by the sentiment memory module of the DM-MCNN, using the one-dimensional VADER embeddings, but considering only words with non-zero values in the original VADER data. Here, the dark shade (blue) refers to the original value of word vectors, while the light shade (red) refers to their fine-tuned values after training. The medium intensity (purple) corresponds to the overlap between the original and fine-tuned word vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Dataset Descriptions</head><label>1</label><figDesc></figDesc><table>Language 
Source 
Domain 
train 
test 
en 
SST 
Movies 
6,920 1,821 
AFF 
Food 
5,945 1,189 
es 
SE16-T5 Restaurants 2,070 
881 
ru 
TA 
Hotels 
2,387 
682 
de 
TA 
Restaurants 1,687 
481 
cs 
TA 
Restaurants 1,722 
491 
it 
TA 
Hotels 
3,437 
982 
ja 
TA 
Restaurants 1,435 
411 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : DM-MCNN Model Parameter Settings. (a) General configuration.</head><label>2</label><figDesc></figDesc><table>Description 
Values 

Convol. Module 

filter region size 
(3,4,5) 
feature maps 
100 
pooling 
1d-max pooling 

Memory Module 
# passes (k) 
2 
feature vector size 
100 
dropout rate 
0.5 
optimizer 
Adam 
activation function 
ReLU 
batch size 
50 

(b) Learning rate α used in DM-MCNN un-
der 7 languages. 
en 
es 
de 
ru 
α 0.0004 0.0008 0.003 0.003 
cs 
it 
ja 
α 
0.003 
0.003 
0.003 

Common Crawl data 1 , while for other languages, 
we rely on the Facebook fastText Wikipedia em-
beddings (Bojanowski et al., 2016) as input repre-
sentations. All of these are 300-dimensional. The 
vectors are either fed to the CNN, or to the convo-
lutional module of the DM-MCNN during initial-
ization, while unknown words are initialized with 
zeros. All words, including the unknown ones, are 
fine-tuned during the training process. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Accuracy on several different English and non-English datasets from different domains, compar- ing our architecture against CNNs. Rest.: restaurants domain.</head><label>3</label><figDesc>tial gains over CNNs with just the GloVe/fastText vectors. We see that the sentiment embeddings provide important complementary signals beyond what is provided in regular word embeddings, and that our dual-module approach succeeds at exploit- ing these signals across a range of different do- mains and languages. Our transfer learning ap- proach leads to sentiment embeddings that capture signals from multiple domains. The model suc- cessfully picks the pertinent parts of this signal for datasets from domains as different as movie reviews and food reviews.</figDesc><table>Approach 
d 
en 
es 
ru 
de 
cs 
it 
ja 
Movies 
Food 
Rest. 
Hotels 
Rest. 
Rest. 
Hotels 
Rest. 
CNN 
-Random Init. 
300 
80.78 
86.63 
81.50 
90.18 
88.09 
90.00 
93.18 
78.59 
-Word Vec. Init. 
300 
85.72 
87.97 
85.13 
92.82 
92.10 
92.46 
96.20 
77.62 
Our Approach 
-With fine-tuning 
300/26 
86.99 
90.08 
85.02 
93.40 
93.14 
93.08 
95.50 
85.40 
-No fine-tuning 
300/26 
86.38 
88.81 
85.70 
94.87 
94.59 
93.48 
96.20 
77.62 
CNN with Concatenated Sentiment Embeddings 
-VADER 
301 
85.89 
88.39 
84.90 
92.31 
88.36 
93.08 
96.34 
77.62 
-SocialSent 
550 
84.90 
88.48 
82.63 
92.23 
91.48 
86.56 
94.51 
76.64 
-Our Embeddings 
326 
86.05 
89.07 
84.56 
92.72 
93.56 
91.24 
95.78 
77.62 
Our Model with Alternative Sentiment Embeddings 
-Random 
300/26 
86.16 
87.97 
85.24 
93.99 
93.14 
92.67 
96.20 
80.29 
-VADER 
300/1 
86.33 
88.39 
84.45 
94.18 
92.31 
92.87 
96.48 
75.43 
-SocialSent 
300/250 
86.38 
87.89 
83.09 
93.40 
92.31 
93.28 
96.62 
81.02 

</table></figure>

			<note place="foot" n="1"> https://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="2"> http://pytorch.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is funded in part by ARO grant no. W911NF-17-C-0098 as part of the DARPA Social-Sim program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. European Language Resources Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of task-specific tree structures with tree-lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02786</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wiktionary-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melo</forename><surname>Gerard De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit XV</title>
		<meeting>MT Summit XV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-lingual propagation for deep sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI 2018</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence (AAAI 2018</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inducing domain-specific sentiment lexicons from unlabeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD 2004: Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06318</idno>
		<title level="m">Harnessing deep neural networks with logic rules</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoding syntactic knowledge in neural networks for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICWSM-14</title>
		<meeting>ICWSM-14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">˙</forename><surname>Ozan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Determining the sentiment of opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Soo-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>COLING</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1367" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1506.07285</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep learning with dynamic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Looks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Herreshoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delesley</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<idno>abs/1702.02181</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="897" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inducing conceptual embedding spaces from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melo</forename><surname>Gerard De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW 2017</title>
		<meeting>WWW 2017</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orphée</forename><surname>De Clercq</surname></persName>
		</author>
		<title level="m">Semeval-2016 task 5: Aspect based sentiment analysis. Proceedings of SemEval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maíra</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sentiment-aspect extraction based on Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="616" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
