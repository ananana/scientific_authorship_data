<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sliding Alignment Windows for Real-Time Crowd Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Kazemi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering and Dept. of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahman</forename><surname>Lavaee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering and Dept. of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iftekhar</forename><surname>Naim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering and Dept. of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering and Dept. of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sliding Alignment Windows for Real-Time Crowd Captioning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="236" to="240"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The primary way of providing real-time speech to text captioning for hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists , each of whom is able to type only part of what they hear. In this paper, we extend the state of the art fixed-window alignment algorithm (Naim et al., 2013) for combining the individual captions into a final output sequence. Our method performs alignment on a sliding window of the input sequences, drastically reducing both the number of errors and the latency of the system to the end user over the previously published approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live televi- sion. Studies performed in the classroom set- ting show that the latency between when a word was said and when it is displayed must be under five seconds to maintain consistency between the captions being read and other visual cues <ref type="bibr" target="#b7">(Wald, 2005;</ref><ref type="bibr" target="#b2">Kushalnagar et al., 2014</ref>). The most com- mon approach to real-time captioning is to recruit a trained stenographer with a special purpose pho- netic keyboard, who transcribes the speech to text with less than five seconds of latency. Unfortu- nately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short <ref type="figure">Figure 1</ref>: General layout of crowd captioning sys- tems. Captionists (C1, C2, C3) submit partial cap- tions that are automatically combined into a high- quality output.</p><p>notice. Automatic speech recognition (ASR) sys- tems ( <ref type="bibr" target="#b6">Saraclar et al., 2002</ref>), on the other hand, at- tempts to provide a cheap and fully automated so- lution to this problem. However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker's voice, in a new environ- ment, or in the absence of a high quality micro- phone <ref type="bibr" target="#b8">(Wald, 2006</ref>). The accuracy of the ASR systems can be improved using the 're-speaking' technique, which requires a person that the ASR has been trained on to repeat the words said by a speaker as he hears them. Simultaneously hearing and speaking, however, is not straightforward, and requires some training.</p><p>An alternative approach is to combine the ef- forts of multiple non-expert captionists (anyone who can type), instead of relying on trained work- ers ( <ref type="bibr" target="#b3">Lasecki et al., 2012;</ref><ref type="bibr" target="#b5">Naim et al., 2013)</ref>. In this approach, multiple non-expert human work- ers transcribe an audio stream containing speech in real-time. Workers type as much as they can of the input, and, while no one worker's transcript is complete, the portions captured by various work- ers tend to overlap. For each input word, a time- stamp is recorded, indicating when the word is typed by a worker. The partial inputs are com- bined to produce a final transcript (see <ref type="figure">Figure 1</ref>). This approach has been shown to dramatically out- perform ASR in terms of both accuracy and Word Error Rate (WER) ( <ref type="bibr" target="#b3">Lasecki et al., 2012;</ref><ref type="bibr" target="#b5">Naim et al., 2013)</ref>. Furthermore, recall of individual words irrespective of their order approached and even ex- ceeded that of a trained expert stenographer with seven workers contributing, suggesting that the in- formation is present to meet the performance of a stenographer ( <ref type="bibr" target="#b3">Lasecki et al., 2012</ref>). However, aligning these individual words in the correct se- quential order remains a challenging problem. <ref type="bibr" target="#b3">Lasecki et al. (2012)</ref> addressed this alignment problem using off-the-shelf multiple sequence alignment tools, as well as an algorithm based on incrementally building a precedence graph over output words. Improved results for the alignment problem were shown using weighted A * search by <ref type="bibr" target="#b5">Naim et al. (2013)</ref>. To speed the search for the best alignment, <ref type="bibr" target="#b5">Naim et al. (2013)</ref> divided se- quences into chunks of a fixed time duration, and applied the A * alignment algorithm to each chunk independently. Although this method speeds the search for the best alignment, it introduces a sig- nificant number of errors to the output of the sys- tem due to inconsistency at the boundaries of the chunks. In this paper, we introduce a novel slid- ing window technique which avoids the errors pro- duced by previous systems at the boundaries of the chunks used for alignment. This technique produces dramatically fewer errors for the same amount of computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Overview and Background</head><p>The problem of aligning and combining multiple transcripts can be mapped to the well-studied Mul- tiple Sequence Alignment (MSA) problem (Edgar and <ref type="bibr" target="#b0">Batzoglou, 2006</ref>). Let S 1 , . . . , S K , K ≥ 2, be the K sequences over an alphabet Σ, and hav- ing length N 1 , . . . , N K . For the caption align- ment task, we treat each individual word as a sym- bol in our alphabet Σ. The special gap symbol '−' represents a missing word and does not be- long to Σ. Let A = (a ij ) be a K × N f matrix, where a ij ∈ Σ ∪ {−}, and the i th row has exactly (N f − N i ) gaps and is identical to S i if we ignore</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MSA-A * Algorithm</head><p>Require: K input sequences S = {S1, . . . , SK } having length N1, . . . , NK , heuristic weight w, beam size b input start ∈ N K , goal ∈ N k output an N × K matrix of integers indicating the index into each input sequence of each position in the output se-</p><formula xml:id="formula_0">quence 1: g(start) ← 0, f (start) ← w × h(start). 2: Q ← {start} 3: while Q = ∅ do 4: n ← EXTRACT-MIN(Q) 5: for all s ∈ {0, 1} K − {0 K } do 6: ni ← n + s 7:</formula><p>if ni = goal then 8:</p><p>Return the alignment matrix for the recon- structed path from start to ni 9:</p><p>else if ni ∈ Beam(b) then 10:</p><p>continue; 11:</p><formula xml:id="formula_1">else 12: g(ni) ← g(n) + c(n, ni) 13: f (ni) ← g(ni) + w × h(ni) 14: INSERT-ITEM(Q, ni, f (ni)) 15:</formula><p>end if 16:</p><p>end for 17: end while the gaps. Every column of A must have at least one non-gap symbol. Therefore, the j th column of A indicates an alignment state for the j th posi- tion, where the state can have one of the 2 K − 1 possible combinations. Our goal is to find the op- timum alignment matrix A OP T that minimizes the sum of pairs (SOP) cost function:</p><formula xml:id="formula_2">c(A) = 1≤i≤j≤K c(A ij ) (1)</formula><p>where c(A ij ) is the cost of the pairwise align- ment between S i and S j according to A. Formally,</p><formula xml:id="formula_3">c(A ij ) = N f l=1 sub(a il , a jl )</formula><p>, where sub(a il , a jl ) denotes the cost of substituting a jl for a il . If a il and a jl are identical, the substitution cost is zero. The substitution cost for two words is estimated based on the edit distance between two words. The exact solution to the SOP optimization problem is NP-Complete ( <ref type="bibr" target="#b9">Wang and Jiang, 1994)</ref>, but many methods solve it approximately. Our approach is based on weighted A * search for approximately solving the MSA problem <ref type="bibr" target="#b4">(Lermen and Reinert, 2000;</ref><ref type="bibr" target="#b5">Naim et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weighted A * Search for MSA</head><p>The problem of minimizing the SOP cost function for K sequences is equivalent to estimating the shortest path between a single source node and a single sink node in a K-dimensional mesh graph, where each node corresponds to a distinct position in the K sequences. The source node is [0, . . . , 0]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Fixed Window Algorithm</head><p>Require: K input sequences S = {S1, . . . , SK } having length N1, . . . , NK , window parameter chunk length.</p><formula xml:id="formula_4">1: start time ← 0 2: while goal ≺ [N1, . . . , NK ] do 3:</formula><p>for all i do 4:</p><formula xml:id="formula_5">start[i] ← closest word(i, start time) 5:</formula><p>end for 6:</p><p>end time ← start time + chunk length 7:</p><p>for all i do 8:</p><formula xml:id="formula_6">goal[i] ← closest word(i, end time) − 1 9:</formula><p>end for 10:</p><p>alignmatrix ←MSA-A * (start, goal) 11:</p><p>concatenate alignmatrix onto end of f inalmatrix 12:</p><p>start time ← end time 13: end while 14: Return f inalmatrix and the sink node is [N 1 , . . . , N K ]. The total num- ber of nodes in the lattice is (N 1 + 1) × (N 2 + 1) × · · ·×(N K +1), and each node has 2 K −1 possible successors and predecessors. The A * search algo- rithm treats each node position n = [n 1 , . . . , n K ] as a search state, and estimates the cost function g(n) and the heuristic function h(n) for each state. The cost function g(n) represents the exact min- imum SOP cost to align the K sequences from the beginning to the current position. The heuris- tic function represents the approximate minimum cost of aligning the suffixes of the K sequences, starting after the current position n. The com- monly used heuristic function is h pair (n):</p><formula xml:id="formula_7">h pair (n) = L(n → t) = 1≤i&lt;j≤K c(A * p (σ n i , σ n j ))<label>(2)</label></formula><p>where L(n → t) denotes the lower bound on the cost of the shortest path from n to destination t, A * p is the optimal pairwise alignment, and σ n i is the suffix of node n in the i-th sequence. The weighted A * search uses a priority queue Q to store the search states n. At each step of the A * search algorithm, the node with the smallest eval- uation function, f (n) = g(n) + wh pair (n) (where w ≥ 1), is extracted from the priority queue Q and expanded by one edge. The search continues un- til the goal node is extracted from Q. To further speed up the search, a beam constraint is applied on the search space using the timestamps of each individual input words. If the beam size is set to b seconds, then any state that aligns two words hav- ing more than b seconds time lag is ignored. The detailed procedure is shown in Algorithm 1. Af- ter the alignment, the captions are combined via majority voting at each position of the alignment matrix. We ignore the alignment columns where the majority vote is below a certain threshold t v (typically t v = 2), and thus filter out spurious er- rors and spelling mistakes.</p><p>Although weighted A * significantly speeds the search for the best alignment, it is still too slow for very long sequences. For this reason, <ref type="bibr" target="#b5">Naim et al. (2013)</ref> divided the sequences into chunks of a fixed time duration, and applied the A * align- ment algorithm to each chunk independently. The chunks were concatenated to produce the final out- put sequence, as shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Limitations of Fixed Window Algorithm</head><p>The fixed window based alignment has two key limitations. First, aligning disjoint chunks inde- pendently tends to introduce a large number of errors at the boundary of each chunk. This is because the chunk boundaries are defined with respect to the timestamps associated with each word in the captions, but the timestamps can vary greatly between words that should in fact be aligned. After all, if the timestamps corresponded precisely to the original time at which each word was spoken, the entire alignment problem would be trivial. The fact that the various instances of a single word in each transcription may fall on ei- ther side of a chunk boundary leads to errors where a word is either duplicated in the final output for more than one chunk, or omitted entirely. This problem also causes errors in ordering among the words remaining within one chunk, because there is less information available to constrain the order- ing relations between transcriptions. Second, the fixed window alignment algorithm requires longer chunks (≥ 10 seconds) to obtain reasonable accu- racy, and thus introduces unsatisfactory latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sliding Alignment Windows</head><p>In order to address the problems described above, we explore a technique based on a sliding align- ment window, shown in Algorithm 3. We start with alignment with a fixed chunk size. After aligning the first chunk, we use the information derived from the alignment to determine where the next chunk should begin within each transcrip- tion. We use a single point in the aligned output as the starting point for the next chunk, and de- termine the corresponding starting position within each original transcription. This single point is determined by a tunable parameter keep length</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Sliding Window Algorithm</head><p>Require: K input sequences S = {S1, . . . , SK } having length N1, . . . , NK , window parameters chunk length and keep length.</p><formula xml:id="formula_8">1: start ← 0 K , goal ← 0 K 2: while goal ≺ [N1, . . . , NK ] do 3: endtime ← chunk length + maxi time(start[i]) 4:</formula><p>for all i do 5:</p><p>goal (line 10 of Algorithm 3). The materials in the output alignment that follow this point is thrown away, and replaced with the output produced by aligning the next chunk starting from this point (line 8). The process continues iteratively, allow- ing us to avoid using the erroneous output align- ments in the neighborhood of the arbitrary end- points for each chunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate our system on a dataset of four 5- minute long audio clips of lectures in electrical engineering and chemistry lectures taken from MIT OpenCourseWare. The same dataset used by <ref type="bibr" target="#b3">(Lasecki et al., 2012)</ref> and <ref type="bibr" target="#b5">(Naim et al., 2013)</ref>. Each audio clip is transcribed by 10 non-expert human workers in real time. We measure the ac- curacy in terms of Word Error Rate (WER) with respect to a reference transcription.</p><p>We are interested in investigating how the three key parameters of the algorithm, i.e., the chunk size (c), the heuristic weight (w) and the keep- length (k), affect the system latency, the search speed, and the alignment accuracy. The chunk size directly determines the latency of the system to the end user, as alignment cannot begin until an entire chunk is captured. Furthermore, the chunk size, the heuristic weight, and the keep-length help us to trade-off speed versus accuracy. We also com- pare the performance of our algorithm with that of the most accurate fixed alignment window al- gorithm <ref type="bibr" target="#b5">(Naim et al., 2013</ref>). The performance in terms of WER for sliding and fixed alignment windows is presented in <ref type="figure" target="#fig_0">Figure 2</ref>. Out of the sys- tems in <ref type="figure" target="#fig_0">Figure 2</ref>, the first three systems consist of sliding alignment window algorithm with different values of keep-length parameter: (1) keep-length = 0.5; (2) keep-length = 0.67; and (3) keep-length = 0.85. The other systems are the graph-based al- gorithm of ( <ref type="bibr" target="#b3">Lasecki et al., 2012)</ref>, the MUSCLE algorithm of <ref type="bibr" target="#b1">(Edgar, 2004)</ref>, and the most accu-rate fixed alignment window algorithm of <ref type="bibr" target="#b5">(Naim et al., 2013)</ref>. We set the heuristic weight param- eter (w) to 3 and the chunk size parameter (c) to 5 seconds for all the three sliding window sys- tems and the fixed window system. Sliding align- ment window produces better results and outper- forms the other algorithms even for large values of the keep-length parameter. The sliding alignment window with keep-length 0.5 achieves 0.5679 av- erage accuracy in terms of (1-WER), providing a 18.09% improvement with respect to the most ac- curate fixed alignment window (average accuracy 0.4857). On the same dataset, <ref type="bibr" target="#b3">Lasecki et al. (2012)</ref> reported 36.6% accuracy using the Dragon Natu- rally Speaking ASR system (version 11.5 for Win- dows).</p><p>To show the trade-off between latency and ac- curacy, we fix the heuristic weight (w = 3) and plot the accuracy as a function of chunk size in <ref type="figure" target="#fig_1">Figure 3</ref>. We repeat this experiment for different values of keep-length. We observe that the slid- ing window approach dominates the fixed window approach across a wide range of chunk sizes. Fur- thermore, we can see that for smaller values of the chunk size parameter, increasing the keep-length makes the system less accurate. As the chunk size parameter increases, the performance of slid- ing window systems with different values of keep- length parameter converges. Therefore, at larger chunk sizes, for which there are smaller number of boundaries, the keep-length parameter has lower impact.</p><p>Next, we show the trade-off between computa- tion speed and accuracy in <ref type="figure" target="#fig_1">Figure 3</ref>, as we fix the heuristic weight and vary the chunk size over the range <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20,</ref><ref type="bibr">30]</ref> seconds. Larger chunks are more accurately aligned but require computa- tion time that grows as N K in the chunk size N in the worst case. Furthermore, smaller weights al- low faster alignment, but provide lower accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a novel sliding win- dow based text alignment algorithm for real-time crowd captioning. By effectively addressing the problem of alignment errors at chunk boundaries, our sliding window approach outperforms the ex- isting fixed window based system <ref type="bibr" target="#b5">(Naim et al., 2013</ref>) in terms of word error rate, particularly when the chunk size is small, and thus achieves higher accuracy at lower latency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evaluation of different systems on using WER metric for measuring transcription quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Tradeoff between speed and accuracy for different heuristic wights and keep-lengths</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple sequence alignment. Current opinion in structural biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serafim</forename><surname>Edgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batzoglou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="368" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MUSCLE: multiple sequence alignment with high accuracy and high throughput</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1792" to="1797" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accessibility evaluation of classroom captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raja S Kushalnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey P</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing (TACCESS)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time captioning by groups of non-experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Sadilek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abumoussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donato</forename><surname>Borrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Kushalnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25rd annual ACM symposium on User interface software and technology</title>
		<meeting>the 25rd annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The practical use of the A* algorithm for exact multiple sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lermen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Knut</forename><surname>Reinert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="655" to="671" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text alignment for real-time crowd captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iftekhar</forename><surname>Naim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-13)</title>
		<meeting>the 2013 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards automatic closed captioning: Low latency real time broadcast news transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraclar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Bocchieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Goffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Language Processing (ICSLP)</title>
		<meeting>the International Conference on Spoken Language Processing (ICSLP)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1741" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using automatic speech recognition to enhance education for all students: Turning a vision into reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Wald</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 35th Annual Conference on Frontiers in Education, 2005. FIE &apos;05</title>
		<meeting>35th Annual Conference on Frontiers in Education, 2005. FIE &apos;05</meeting>
		<imprint>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Creating accessible educational multimedia through editing automatic speech recognition captioning in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Wald</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactive Technology and Smart Education</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="141" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the complexity of multiple sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="337" to="348" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
