<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Soft Linear Constraints with Application to Citation Field Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Anzaroot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Soft Linear Constraints with Application to Citation Field Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="593" to="602"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Accurately segmenting a citation string into fields for authors, titles, etc. is a challenging task because the output typically obeys various global constraints. Previous work has shown that modeling soft constraints , where the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance. On the other hand, for imposing hard constraints, dual decomposition is a popular technique for efficient prediction given existing algorithms for uncon-strained inference. We extend dual decomposition to perform prediction subject to soft constraints. Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Citation field extraction, an instance of informa- tion extraction, is the task of segmenting and la- beling research paper citation strings into their constituent parts, including authors, editors, year, journal, volume, conference venue, etc. This task is important because citation data is often pro- vided only in plain text; however, having an ac- curate structured database of bibliographic infor- mation is necessary for many scientometric tasks, such as mapping scientific sub-communities, dis- covering research trends, and analyzing networks of researchers. Automated citation field extrac- tion needs further research because it has not yet reached a level of accuracy at which it can be prac- tically deployed in real-world systems.</p><p>Hidden Markov models and linear-chain condi- tional random fields (CRFs) have previously been applied to citation extraction <ref type="bibr" target="#b9">(Hetzner, 2008;</ref><ref type="bibr" target="#b14">Peng and McCallum, 2004</ref>) . These models support ef- ficient dynamic-programming inference, but only model local dependencies in the output label se- quence. However citations have strong global reg- ularities not captured by these models. For exam- ple many book citations contain both an author section and an editor section, but none have two disjoint author sections. Since linear-chain mod- els are unable to capture more than Markov depen- dencies, the models sometimes mislabel the editor as a second author. If we could enforce the global constraint that there should be only one author section, accuracy could be improved.</p><p>One framework for adding such global con- straints into tractable models is constrained infer- ence, in which at inference time the original model is augmented with restrictions on the outputs such that they obey certain global regularities. When hard constraints can be encoded as linear equa- tions on the output variables, and the underlying model's inference task can be posed as linear opti- mization, one can formulate this constrained infer- ence problem as an integer linear program (ILP) ( <ref type="bibr" target="#b15">Roth and Yih, 2004</ref>). Alternatively, one can em- ploy dual decomposition ( . Dual decompositions's advantage over ILP is is that it can leverage existing inference algorithms for the original model as a black box. Such a modular algorithm is easy to implement, and works quite well in practice, providing certificates of optimal- ity for most examples.</p><p>The above two approaches have previously been applied to impose hard constraints on a model's output. On the other hand, recent work has demon- strated improvements in citation field extraction by imposing soft constraints ( <ref type="bibr" target="#b3">Chang et al., 2012)</ref>. Here, the model is not required obey the global constraints, but merely pays a penalty for their vi-4 . ref-marker <ref type="bibr">[ J. first D. middle Monk ,last person ]</ref>authors <ref type="bibr">[ Cardinal Functions on Boolean Algebra , ]</ref>title [ Lectures in Mathematics , ETH Zurich , series Birkhause Verlag , publisher Basel , Boston , Berlin , address 1990 . year date ]venue This paper introduces a novel method for im- posing soft constraints via dual decomposition. We also propose a method for learning the penal- ties the prediction problem incurs for violating these soft constraints. Because our learning method drives many penalties to zero, it allows practitioners to perform 'constraint selection,' in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful con- straints, which can be run quickly at test time.</p><p>Using our new method, we are able to incor- porate not only all the soft global constraints of <ref type="bibr" target="#b3">Chang et al. (2012)</ref>, but also far more com- plex data-driven constraints, while also provid- ing stronger optimality certificates than their beam search technique. On a new, more broadly rep- resentative, and challenging citation field extrac- tion data set, we show that our methods achieve a 17.9% reduction in error versus a linear-chain con- ditional random field. Furthermore, we demon- strate that our inference technique can use and benefit from the constraints of <ref type="bibr" target="#b3">Chang et al. (2012)</ref>, but that including our data-driven constraints on top of these is beneficial. While this paper fo- cusses on an application to citation field extrac- tion, the novel methods introduced here would easily generalize to many problems with global output regularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structured Linear Models</head><p>The overall modeling technique we employ is to add soft constraints to a simple model for which we have an existing efficient prediction algorithm. For this underlying model, we employ a chain- structured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction <ref type="bibr" target="#b14">(Peng and McCallum, 2004</ref>). We produce a prediction by performing MAP inference <ref type="bibr" target="#b10">(Koller and Friedman, 2009</ref>).</p><p>The MAP inference task in a CRF be can ex- pressed as an optimization problem with a lin- ear objective <ref type="bibr" target="#b21">(Sontag, 2010;</ref><ref type="bibr" target="#b20">Sontag et al., 2011</ref>). Here, we define a binary indicator variable for each candidate setting of each factor in the graph- ical model. Each of these indicator variables is associated with the score that the factor takes on when it has the indictor variable's corresponding value. Since the log probability of some y in the CRF is proportional to sum of the scores of all the factors, we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MAP problem as</p><formula xml:id="formula_0">max. w, y s.t. y ∈ U,<label>(1)</label></formula><p>where the set U represents the set of valid config- urations of the indicator variables. Here, the con- straints are that all neighboring factors agree on the components of y in their overlap. Structured Linear Models are the general fam- ily of models where prediction requires solving a problem of the form (1), and they do not always correspond to a probabilistic model. The algo- rithms we present in later sections for handling soft global constraints and for learning the penal- ties of these constraints can be applied to gen- eral structured linear models, not just CRFs, pro- vided we have an available algorithm for perform- ing MAP inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dual Decomposition for Global Constraints</head><p>In order to perform prediction subject to various global constraints, we may need to augment the problem (1) with additional constraints. Dual De- composition is a popular method for performing MAP inference in this scenario, since it lever- ages known algorithms for MAP in the base prob- lem where these extra constraints have not been added ( <ref type="bibr" target="#b11">Komodakis et al., 2007;</ref><ref type="bibr" target="#b20">Sontag et al., 2011;</ref>. In this case, the MAP problem can be formulated as a structured linear model similar to equation (1), for which we have a MAP algorithm, but where we have im- posed some additional constraints Ay ≤ b that no longer allow us to use the algorithm. In other Algorithm 1 DD: projected subgradient for dual decomposition with hard constraints 1: while has not converged do 2:</p><formula xml:id="formula_1">y (t) = argmax y∈U w + A T λ, y 3: λ (t) = Π 0≤· λ (t−1) − η (t) (Ay − b)</formula><p>words, we consider the problem</p><formula xml:id="formula_2">max. w, y s.t. y ∈ U Ay ≤ b,<label>(2)</label></formula><p>for an arbitrary matrix A and vector b. We can write the Lagrangian of this problem as</p><formula xml:id="formula_3">L(y, λ) = w, y + λ T (Ay − b).<label>(3)</label></formula><p>Regrouping terms and maximizing over the primal variables, we have the dual problem</p><formula xml:id="formula_4">min. λ D(λ) = max y∈U w + A T λ, y − λ T b. (4)</formula><p>For any λ, we can evaluate the dual objective D(λ), since the maximization in (4) is of the same form as the original problem (1), and we assumed we had a method for performing MAP in this. Fur- thermore, a subgradient of D(λ) is Ay * − b, for an y * which maximizes this inner optimization prob- lem. Therefore, we can minimize D(λ) with the projected subgradient method <ref type="bibr" target="#b1">(Boyd and Vandenberghe, 2004</ref>), and the optimal y can be obtained when evaluating D(λ * ). Note that the subgradient of D(λ) is the amount by which each constraint is violated by λ when maximizing over y. Algorithm 1 depicts the basic projected subgra- dient descent algorithm for dual decomposition. The projection operator Π consists of truncating all negative coordinates of λ to 0. This is neces- sary because λ is a vector of dual variables for in- equality constraints. The algorithm has converged when each constraint is either satisfied by y (t) with equality or its corresponding component of λ is 0, due to complimentary slackness (Boyd and Van- denberghe, 2004).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Soft Constraints in Dual Decomposition</head><p>We now introduce an extension of Algorithm 1 to handle soft constraints. In our formulation, a soft-constrained model imposes a penalty for each unsatisfied constraint, proportional to the amount by which it is violated. Therefore, our derivation parallels how soft-margin SVMs are derived from hard-margin SVMs by introducing auxiliary slack variables <ref type="bibr" target="#b7">(Cortes and Vapnik, 1995)</ref>. Note that when performing MAP subject to soft constraints, optimal solutions might not satisfy some con- straints, since doing so would reduce the model's score by too much.</p><p>Consider the optimization problems of the form:</p><formula xml:id="formula_5">max. w, y − c, z s.t. y ∈ U Ay − b ≤ z −z ≤ 0,<label>(5)</label></formula><p>For positive c i , it is clear that an optimal z i will be equal to the degree to which a T i y ≤ b i is vio- lated. Therefore, we pay a cost c i times the degree to which the ith constraint is violated, which mir- rors how slack variables are used to represent the hinge loss for SVMs. Note that c i has to be pos- itive, otherwise this linear program is unbounded and an optimal value can be obtained by setting z i to infinity.</p><p>Using a similar construction as in section 2.2 we write the Lagrangian as:</p><formula xml:id="formula_6">(6) L(y, z, λ, µ) = w, y − c, z + λ T (Ay − b − z) + µ T (−z).</formula><p>The optimality constraints with respect to z tell us</p><formula xml:id="formula_7">that −c − λ − µ = 0, hence µ = −c − λ. Substi- tuting, we have L(y, λ) = w, y + λ T (Ay − b),<label>(7)</label></formula><p>except the constraint that µ = −c − λ implies that for µ to be positive λ ≤ c.</p><p>Since this Lagrangian has the same form as equation <ref type="formula" target="#formula_3">(3)</ref>, we can also derive a dual problem, which is the same as in equation <ref type="formula">(4)</ref>, with the ad- ditional constraint that each λ i can not be bigger than its cost c i . In other words, the dual problem can not penalize the violation of a constraint more than the soft constraint model in the primal would penalize you if you violated it.</p><p>This optimization problem can still be solved with projected subgradient descent and is depicted in Algorithm 2. The only modifications to Al- gorithm 1 are replacing the coordinate-wise pro- jection Π 0≤· with Π 0≤·≤c and how we check for convergence. Now, we check for the KKT con- ditions of <ref type="formula" target="#formula_5">(5)</ref>, where for every constraint i, either the constraint is satisfied with equality, λ i = 0, or λ i = c i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Soft-DD: projected subgradient for dual decomposition with soft constraints</head><p>1: while has not converged do 2:</p><formula xml:id="formula_8">y (t) = argmax y∈U w + A T λ, y 3: λ (t) = Π 0≤·≤c λ (t−1) − η (t) (Ay − b)</formula><p>Therefore, implementing soft-constrained dual decomposition is as easy as implementing hard- constrained dual decomposition, and the per- iteration complexity is the same. We encourage further applications of soft-constraint dual decom- position to existing and new NLP problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Penalties</head><p>One consideration when using soft v.s. hard con- straints is that soft constraints present a new train- ing problem, since we need to choose the vector c, the penalties for violating the constraints. An important property of problem <ref type="formula" target="#formula_5">(5)</ref> in the previous section is that it corresponds to a structured lin- ear model over y and z. Therefore, we can apply known training algorithms for estimating the pa- rameters of structured linear models to choose c.</p><p>All we need to employ the structured perceptron algorithm <ref type="bibr" target="#b6">(Collins, 2002</ref>) or the structured SVM algorithm ( <ref type="bibr" target="#b23">Tsochantaridis et al., 2004</ref>) is a black- box procedure for performing MAP inference in the structured linear model given an arbitrary cost vector. Fortunately, the MAP problem for (5) can be solved using Soft-DD, in Algorithm 2.</p><p>Each penalty c i has to be non-negative; other- wise, the optimization problem in equation <ref type="formula" target="#formula_5">(5)</ref> is ill-defined. This can be ensured by simple mod- ifications of the perceptron and subgradient de- scent optimization of the structured SVM objec- tive simply by truncating c coordinate-wise to be non-negative at every learning iteration.</p><p>Intuitively, the perceptron update increases the penalty for a constraint if it is satisfied in the ground truth and not in an inferred prediction, and decreases the penalty if the constraint is satisfied in the prediction and not the ground truth. Since we truncate penalties at 0, this suggests that we will learn a penalty of 0 for constraints in three cat- egories: constraints that do not hold in the ground truth, constraints that hold in the ground truth but are satisfied in practice by performing inference in the base CRF model, and constraints that are satisfied in practice as a side-effect of imposing non-zero penalties on some other constraints . A similar analysis holds for the structured SVM ap- proach.</p><p>Therefore, we can view learning the values of the penalties not just as parameter tuning, but as a means to perform 'constraint selection,' since con- straints that have a penalty of 0 can be ignored. This property allows us to consider large families of constraints, from which the useful ones are au- tomatically identified.</p><p>We found it beneficial, though it is not theoreti- cally necessary, to learn the constraints on a held- out development set, separately from the other model parameters, as during training most con- straints are satisfied due to overfitting, which leads to an underestimation of the relevant penalties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Citation Extraction Data</head><p>We consider the UMass citation dataset, first intro- duced in <ref type="bibr" target="#b0">Anzaroot and McCallum (2013)</ref>. It has over 1800 citation from many academic fields, ex- tracted from the arXiv. This dataset contains both coarse-grained and fine-grained labels; for exam- ple it contains labels for the segment of all authors, segments for each individual author, and for the first and last name of each author. There are 660 citations in the development set and 367 citation in the test set.</p><p>The labels in the UMass dataset are a con- catenation of labels from a hierarchically-defined schema. For example, a first name of an author is tagged as: authors/person/first. In addition, indi- vidual tokens are labeled using a BIO label schema for each level in the hierarchy. BIO is a commonly used labeling schema for information extraction tasks. BIO labeling allows individual labels on tokens to label segmentation information as well as labels for the segments. In this schema, labels that begin segments are prepended with a B, la- bels that continue a segment are prepended with an I, and tokens that don't have a labeling in this schema are given an O label. For example, in a hi- erarchical BIO label schema the first token in the first name for the second author may be labeled as: I-authors/B-person/B-first.</p><p>An example labeled citation in this dataset can be viewed in figure 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Global Constraints for Citation Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Constraint Templates</head><p>We now describe the families of global constraints we consider for citation extraction. Note these constraints are all linear, since they depend only on the counts of each possible conditional ran- dom field label. Moreover, since our labels are BIO-encoded, it is possible, by counting B tags, to count how often each citation tag itself appears in a sentence. The first two families of constraints that we describe are general to any sequence la- beling task while the last is specific to hierarchical labeling such as available in the UMass dataset.</p><p>Our sequence output is denoted as y and an ele- ment of this sequence is y k .</p><p>We</p><note type="other">denote [[y k = i]] as the function that outputs 1 if y k has a 1 at index i and 0 otherwise. Here, y k represents an output tag of the CRF, so if [[y k = i]] = 1, then we have that y k was given a label with index i.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Singleton Constraints</head><p>Singleton constraints ensure that each label can appear at most once in a citation. These are same global constraints that were used for citation field extraction in <ref type="bibr" target="#b3">Chang et al. (2012)</ref>. We define s(i) to be the number of times the label with index i is predicted in a citation, formally:</p><formula xml:id="formula_9">s(i) = y k ∈y [[y k = i]]</formula><p>The constraint that each label can appear at most once takes the form:</p><formula xml:id="formula_10">s(i) &lt;= 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pairwise Constraints</head><p>Pairwise constraints are constraints on the counts of two labels in a citation. We define z 1 (i, j) to be</p><formula xml:id="formula_11">z 1 (i, j) = y k ∈y [[y k = i]] + y k ∈y [[y k = j]]</formula><p>and z 2 (i, j) to be</p><formula xml:id="formula_12">z 2 (i, j) = y k ∈y [[y k = i]] − y k ∈y [[y k = j]]</formula><p>We consider all constraints of the forms: z(i, j) ≤ 0, 1, 2, 3 and z(i, j) ≥ 0, 1, 2, 3.</p><p>Note that some pairs of these constraints are re- dundant or logically incompatible. However, we are using them as soft constraints, so these con- straints will not necessarily be satisfied by the out- put of the model, which eliminates concern over enforcing logically impossible outputs. Further- more, in section 3.1 we described how our proce- dure for learning penalties will drive some penal- ties to 0, which effectively removes them from our set of constraints we consider. It can be shown, for example, that we will never learn non-zero penal- ties for certain pairs of logically incompatible con- straints using the perceptron-style algorithm de- scribed in section 3.1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Hierarchical Equality Constraints</head><p>The labels in the citation dataset are hierarchical labels. This means that the labels are the concate- nation of all the levels in the hierarchy. We can create constraints that are dependent on only one or couple of elements in the hierarchy.</p><p>We define C(x, i) as the function that returns 1 if the output x contains the label i in the hierarchy and 0 otherwise. We define e(i, j) to be</p><formula xml:id="formula_13">e(i, j) = y k ∈y [[C(y k , i)]] − y k ∈y [[C(y k , j)]]</formula><p>Hierarchical equality constraints take the forms:</p><formula xml:id="formula_14">e(i, j) ≥ 0<label>(8)</label></formula><formula xml:id="formula_15">e(i, j) ≤ 0<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Local constraints</head><p>We constrain the output labeling of the chain- structured CRF to be a valid BIO encoding. This both improves performance of the underly- ing model when used without global constraints, as well as ensures the validity of the global con- straints we impose, since they operate only on B labels. The constraint that the labeling is valid BIO can be expressed as a collection of pairwise constraints on adjacent labels in the se- quence. Rather than enforcing these constraints using dual decomposition, they can be enforced directly when performing MAP inference in the CRF by modifying the dynamic program of the Viterbi algorithm to only allow valid pairs of adja- cent labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Constraint Pruning</head><p>While the techniques from section 3.1 can easily cope with a large numbers of constraints at train- ing time, this can be computationally costly, spe- cially if one is considering very large constraint families. This is problematic because the size   <ref type="figure">Figure 2</ref>: Two examples where imposing soft global constraints improves field extraction errors. Soft- DD converged in 1 iteration on the first example, and 7 iterations on the second. When a reference is citing a book and not a section of the book, the correct labeling of the name of the book is title. In the first example, the baseline CRF incorrectly outputs booktitle, but this is fixed by Soft-DD, which penalizes outputs based on the constraint that booktitle should co-occur with an address label. In the second example, the unconstrained CRF output violates the constraint that title and status labels should not co-occur. The ground truth labeling also violates a constraint that title and language labels should not co-occur. At convergence of the Soft-DD algorithm, the correct labeling of language is predicted, which is possible because of the use of soft constraints. of some constraint families we consider grows quadratically with the number of candidate labels, and there are about 100 in the UMass dataset. Such a family consists of constraints that the sum of the counts of two different label types has to be bounded (a useful example is that there can't be more than one out of "phd thesis" and "jour- nal"). Therefore, quickly pruning bad constraints can save a substantial amount of training time, and can lead to better generalization. To do so, we calculate a score that estimates how useful each constraint is expected to be. Our score compares how often the constraint is vio- lated in the ground truth examples versus our pre- dictions. Here, prediction is done with respect to the base chain-structured CRF tagger and does not include global constraints. Note that it may make sense to consider a constraint that is sometimes vi- olated in the ground truth, as the penalty learning algorithm can learn a small penalty for it, which will allow it to be violated some of the time. Our importance score is defined as, for each constraint c on labeled set D,</p><formula xml:id="formula_16">imp(c) = d∈D [[max y w T d y]] c d∈D [[y d ]] c ,<label>(10)</label></formula><p>where <ref type="bibr">[[y]</ref>] c is 1 if the constraint is violated on out- put y and 0 otherwise. Here, y d denotes the ground truth labeling and w d is the vector of scores for the CRF tagger. We prune constraints by picking a cutoff value for imp(c). A value of imp(c) above 1 implies that the constraint is more violated on the pre- dicted examples than on the ground truth, and hence that we might want to keep it.</p><p>We also find that the constraints that have the largest imp values are semantically interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There are multiple previous examples of augment- ing chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs. In- ference in these models can be performed, for example, with loopy belief propagation ( <ref type="bibr" target="#b2">Bunescu and Mooney, 2004;</ref><ref type="bibr" target="#b22">Sutton and McCallum, 2004</ref>) or Gibbs sampling ( <ref type="bibr" target="#b8">Finkel et al., 2005</ref>). Be- lief propagation is prohibitively expensive in our model due to the high cardinalities of the out- put variables and of the global factors, which in- volve all output variables simultaneously. There are various methods for exploiting the combi- natorial structure of these factors, but perfor- mance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recogni- tion ( <ref type="bibr" target="#b8">Finkel et al., 2005</ref>), our previous experiments show that it does not work well for citation extrac- tion, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF.</p><p>Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP ( <ref type="bibr" target="#b12">Koo et al., 2010;</ref><ref type="bibr" target="#b13">Paul and Eisner, 2012;</ref><ref type="bibr" target="#b4">Chieu and Teow, 2012</ref>). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition-by in- troducing copies of output variables and an aux- iliary graphical model, as in . However, at every iteration of dual decomposition, MAP must be run in this auxiliary model. Further- more the copying of variables doubles the num- ber of iterations needed for information to flow between output variables, and thus slows conver- gence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard constraints, and is a very easy modifi- cation to existing hard constraint code.</p><p>Initial work in machine learning for citation ex- traction used Markov models with no global con- straints. Hidden Markov models (HMMs), were originally employed for automatically extracting information from research papers on the CORA dataset ( <ref type="bibr" target="#b19">Seymore et al., 1999;</ref><ref type="bibr" target="#b9">Hetzner, 2008)</ref>. Later, CRFs were shown to perform better on CORA, improving the results from the Hmm's token-level F1 of 86.6 to 91.5 with a CRF( <ref type="bibr" target="#b14">Peng and McCallum, 2004</ref>).</p><p>Recent work on globally-constrained inference in citation extraction used an HMM CCM , which is an HMM with the addition of global features that are restricted to have positive weights <ref type="bibr" target="#b3">(Chang et al., 2012</ref>). Approximate inference is performed using beam search. This method increased the HMM token-level accuracy from 86.69 to 93.92 on a test set of 100 citations from the CORA dataset. The global constraints added into the model are simply that each label only occurs once per citation. This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA ( <ref type="bibr" target="#b14">Peng and McCallum, 2004</ref>). In our ex- periments, we demonstrate that the specific global constraints used by <ref type="bibr" target="#b3">Chang et al. (2012)</ref> help on the UMass dataset as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>Our baseline is the one used in <ref type="bibr" target="#b0">Anzaroot and McCallum (2013)</ref>, with some labeling errors re- moved. This is a chain-structured CRF trained to maximize the conditional likelihood using L- BFGS with L2 regularization.</p><p>We use the same features as Anzaroot and <ref type="bibr" target="#b0">McCallum (2013)</ref>, which include word type, capital- ization, binned location in citation, regular expres- sion matches, and matches into lexicons. In addi- tion, we use a rule-based segmenter that segments the citation string based on punctuation as well as probable start or end segment words (e.g. 'in' and 'volume'). We add a binary feature to tokens that correspond to the start of a segment in the output of this simple segmenter. This final feature im- proves the F1 score on the cleaned test set from 94.0 F1 to 94.44 F1, which we use as a baseline score.</p><p>We then use the development set to learn the penalties for the soft constraints, using the percep- tron algorithm described in section 3.1. MAP in- ference in the model with soft constraints is per- formed using Soft-DD, shown in Algorithm 2.</p><p>We instantiate constraints from each template in section 5.1, iterating over all possible labels that contain a B prefix at any level in the hierarchy and pruning all constraints with imp(c) &lt; 2.75 cal- culated on the development set. We asses perfor- mance in terms of field-level F1 score, which is the harmonic mean of precision and recall for pre- dicted segments. <ref type="table">Table 1</ref> shows how each type of constraint fam- ily improved the F1 score on the dataset. Learning all the constraints jointly provides the largest im- provement in F1 at 95.39. This improvement in F1 over the baseline CRF as well as the improvement in F1 over using only-one constraints was shown to be statistically significant using the Wilcoxon signed rank test with p-values &lt; 0.05. In the all-constraints settings, 32.96% of the constraints have a learned parameter of 0, and therefore only  Since performing inference in the CRF is by far the most computationally intensive step in the iter- ative algorithm, this means our procedure requires approximately twice as much work as running the baseline CRF on the dataset. On examples where unconstrained inference does not satisfy the con- straints, Soft-DD converges after 4.52 iterations on average. For 11.99% of the examples, the Soft-DD algorithm satisfies constraints that were not satisfied during unconstrained inference, while in the remaining 11.72% Soft-DD converges with some constraints left unsatisfied, which is possible since we are imposing them as soft constraints. We could have enforced these constraints as hard constraints rather than soft ones. This exper- iment is shown in the last row of <ref type="table">Table 1</ref>, where F1 only improves to 94.6. In addition, running the DD algorithm with these constraints takes 5.21 iterations on average per example, which is 2.8 times slower than Soft-DD with learned penalties.</p><p>In <ref type="figure">Figure 2</ref>, we analyze the performance of Soft-DD when we don't necessarily run it to con- vergence, but stop after a fixed number of itera- tions on each test set example. We find that a large portion of our gain in accuracy can be obtained when we allow ourselves as few as 2 dual decom- position iterations. However, this only amounts to 1.24 times as much work as running the baseline CRF on the dataset, since the constraints are satis- fied immediately for many examples.</p><p>In <ref type="figure">Figure 2</ref> we consider two applications of our Soft-DD algorithm, and provide analysis in the caption.</p><p>We train and evaluate on the UMass dataset in- stead of CORA, because it is significantly larger, has a useful finer-grained labeling schema, and its annotation is more consistent. We were able to ob- tain better performance on CORA using our base- line CRF than the HM M CCM results presented in <ref type="bibr" target="#b3">Chang et al. (2012)</ref>, which include soft con- straints. Given this high performance of our base model on CORA, we did not apply our Soft-DD algorithm to the dataset. Furthermore, since the dataset is so small, learning the penalties for our large collection of constraints is difficult, and test set results are unreliable. Rather than compare our work to <ref type="bibr" target="#b3">Chang et al. (2012)</ref> via results on CORA, we apply their constraints on the UMass data us- ing Soft-DD and demonstrate accuracy gains, as discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Examples of learned constraints</head><p>We now describe a number of the useful con- straints that receive non-zero learned penalties and have high importance scores, defined in Sec- tion 5.6. The importance score of a constraint pro- vides information about how often it is violated by the CRF, but holds in the ground truth, and a non-zero penalty implies we enforce it as a soft constraint at test time.</p><p>The two singleton constraints with highest im- portance score are that there should only be at most one title segment in a citation and that there should be at most one author segment in a cita- tion. The only one author constraint is particu- larly useful for correctly labeling editor segments in cases where unconstrained inference mislabels them as author segments. As can be seen in <ref type="table">Table  3</ref>, editor fields are among the most improved with our new method, largely due to this constraint.</p><p>The two hierarchical constraints with the high- est importance scores with non-zero learned penalties constrain the output such that number of person segments does not exceed the number of first segments and vice-versa. Together, these constraints penalize outputs in which the number of person segments do not equal the number of first segments, i.e., every author should have a first name.</p><p>One important pairwise constraint penalizes outputs in which thesis segments don't co-occur with school segments. School segments label the name of the university that the thesis was submit- ted to. The application of this constraint increases the performance of the model on school segments   <ref type="table">Table 3</ref>: Labels with highest improvement in F1. U is in unconstrained inference. C is the results of constrained inference. + is the improvement in F1.</p><p>dramatically, as can be seen in table 3.</p><p>An interesting form of pairwise constraints pe- nalize outputs in which some labels do not co- occur with other labels. Some examples of con- straints in this form enforce that journal segments should co-occur with pages segments and that booktitle segments should co-occur with address segments. An example of the latter constraint be- ing employed during inference is the first example in <ref type="figure">Figure 2</ref>. Here, the constrained inference pe- nalizes output which contains a booktitle segment but no address segment. This penalization leads allows the constrained inference to correctly label the booktitle segment as a title segment.</p><p>The above example constraints are almost al- ways satisfied on the ground truth, and would be useful to enforce as hard constraints. However, there are a number of learned constraints that are often violated on the ground truth but are still use- ful as soft constraints. Take, for example, the con- straint that the number of number segments does not exceed the number of booktitle segments, as well as the constraint that it does not exceed the number of journal segments. These constraints are moderately violated on ground truth examples, however. For example, when booktitle segments co-occur with number segments but not with jour- nal segments, the second constraint is violated. It is still useful to impose these soft constraints, as strong evidence from the CRF allows us to violate them, and they can guide the model to good pre- dictions when the CRF is unconfident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduce a novel modification to the stan- dard projected subgradient dual decomposition al- gorithm for performing MAP inference subject to hard constraints to one for performing MAP in the presence of soft constraints. In addition, we offer an easy-to-implement procedure for learning the penalties on soft constraints. This method drives many penalties to zero, which allows users to auto- matically discover discriminative constraints from large families of candidates.</p><p>We show via experiments on a recent substantial dataset that using soft constraints, and selecting which constraints to use with our penalty-learning procedure, can lead to significant gains in accu- racy. We achieve a 17% gain in accuracy over a chain-structured CRF model, while only need- ing to run MAP in the CRF an average of less than 2 times per example. This minor incremen- tal cost over Viterbi, plus the fact that we obtain certificates of optimality on 100% of our test ex- amples in practice, suggests the usefulness of our algorithm for large-scale applications. We encour- age further use of our Soft-DD procedure for other structured prediction problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example labeled citation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Unconstrained</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Label</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>[ 17 ] ref-marker [ D. first Sivia ,last person J. first Skilling ,last person ]authors [ Data Analysis : A Bayesian Tutorial , booktitle Oxford University Press , publisher 2006 year date ]venue Constrained [17] ref-marker [ D. first Sivia ,last person J. first Skilling ,last person ]authors Data Analysis : A Bayesian Tutorial , title [ Oxford University Press , publisher 2006 year date ]venue</head><label>17</label><figDesc></figDesc><table>Unconstrained 
[ Sobol' ,last I. first M. middle person ]authors [ (1990) .year ]date [On sensitivity estimation for nonlinear mathe-
matical models .]title [ Matematicheskoe Modelirovanie , journal 2 volume (1) : number 112-118 .pages ( In Russian 
) . status ]venue 

Constrained 
[ Sobol' ,last I. first M. middle person ]authors [ (1990) .year ]date [On sensitivity estimation for nonlinear mathe-
matical models .]title [ Matematicheskoe Modelirovanie , journal 2 volume (1) : number 112-118 .pages ( In Russian 
) . language ]venue 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance from terminating Soft-DD 
early. Column 1 is the number of iterations we 
allow each example. Column 3 is the % of test 
examples that converged. Column 4 is the aver-
age number of necessary iterations, a surrogate for 
the slowdown over performing unconstrained in-
ference. 

421 constraints are active. Soft-DD converges, 
and thus solves the constrained inference prob-
lem exactly, for all test set examples after at most 
41 iterations. Running Soft-DD to convergence 
requires 1.83 iterations on average per example. 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Cen-ter for Intelligent Information Retrieval, in part by DARPA under agreement number FA8750-13-2-0020, in part by NSF grant #CNS-0958392, and in part by IARPA via DoI/NBC contract #D11PC20152. The U.S. Government is autho-rized to reproduce and distribute reprint for Gov-ernmental purposes notwithstanding any copy-right annotation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new dataset for fine-grained citation field extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Anzaroot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Peer Reviewing and Publishing Models</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poythress</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collective information extraction with relational markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 438</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 438</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Structured learning with constrained conditional models. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="399" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Combining local and non-local information with dual decomposition for named entity recognition from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Leong Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loo-Nin</forename><surname>Teow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Information Fusion (FUSION), 2012 15th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supportvector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple method for citation metadata extraction using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Hetzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM/IEEE-CS joint conference on Digital libraries</title>
		<meeting>the 8th ACM/IEEE-CS joint conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="280" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mrf optimization via dual decomposition: Message-passing revisited. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual decomposition for parsing with non-projective head automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1298" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Implicitly intersecting weighted automata using dual decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate information extraction from research papers using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004: Main Proceedings</title>
		<editor>Daniel Marcu Susan Dumais and Salim Roukos</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-05-02" />
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A linear programming formulation for global inference in natural language tasks. Defense Technical Information Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="305" to="362" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On dual decomposition and linear programming relaxations for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved parsing and pos tagging using inter-sentence consistency constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1434" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning hidden markov model structure for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-99 Workshop on Machine Learning for Information Extraction</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to dual decomposition for inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<editor>Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Approximate Inference in Graphical Models using LP Relaxations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Collective segmentation and labeling of distant entities in information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
