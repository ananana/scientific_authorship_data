<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fluency Boost Learning and Inference for Neural Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fluency Boost Learning and Inference for Neural Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1055" to="1065"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1055</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence&apos;s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally through multi-round seq2seq inference until the sentence&apos;s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence-to-sequence (seq2seq) models <ref type="bibr">(Cho et al., 2014;</ref><ref type="bibr" target="#b35">Sutskever et al., 2014</ref>) for grammati- cal error correction (GEC) have drawn growing at- tention ( <ref type="bibr" target="#b40">Xie et al., 2016;</ref><ref type="bibr" target="#b7">Ji et al., 2017;</ref><ref type="bibr" target="#b32">Schmaltz et al., 2017;</ref><ref type="bibr">Chollampatt and Ng, 2018</ref>) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sen- tence pairs like <ref type="figure">Figure 1</ref>(a). Limited by the size of training data, the models with millions of pa- rameters may not be well generalized. Thus, it is She see Tom is catched by policeman in park at last night.</p><p>She saw Tom caught by a policeman in the park last night.</p><p>She sees Tom is catched by policeman in park at last night.  Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sen- tence, but multi-round inference can.</p><p>common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by <ref type="figure">Figure  1</ref>(b). Second, the seq2seq models usually cannot perfectly correct a sentence with many grammati- cal errors through single-round seq2seq inference, as shown in <ref type="figure">Figure 1</ref>(b) and 1(c), because some errors in a sentence may make the context strange, which confuses the models to correct other errors.</p><p>To address the above-mentioned limitations in model learning and inference, this paper proposes a novel fluency boost learning and inference mech- anism, illustrated in <ref type="figure">Figure 2</ref>.</p><p>For fluency boosting learning, not only is a seq2seq model trained with original error- corrected sentence pairs, but also it generates less fluent sentences (e.g., from its n-best outputs) to establish new error-corrected sentence pairs by pairing them with their correct sentences during training, as long as the sentences' fluency 1 is be-She see Tom is catched by policeman in park at last night. She saw Tom caught by a policeman in the park last night.</p><p>She see Tom is caught by a policeman in park last night. She sees Tom caught by a policeman in the park last night. She saw Tom caught by a policeman in the park last night.</p><p>She saw Tom was caught by a policeman in the park last night.</p><p>She sees Tom is catched by policeman in park at last night. She sees Tom is catched by policeman in park at last night.  <ref type="figure">Figure 2</ref>: Fluency boost learning and inference: (a) given a training instance (i.e., an error-corrected sen- tence pair), fluency boost learning establishes multiple fluency boost sentence pairs from the seq2seq's n-best outputs during training. The fluency boost sentence pairs will be used as training instances in sub- sequent training epochs, which helps expand the training set and accordingly benefits model learning; (b) fluency boost inference allows an error correction model to correct a sentence incrementally through multi-round seq2seq inference until its fluency score stops increasing.</p><p>low that of their correct sentences, as <ref type="figure">Figure 2</ref>(a) shows. Specifically, we call the generated error- corrected sentence pairs fluency boost sentence pairs because the sentence in the target side al- ways improves fluency over that in the source side. The generated fluency boost sentence pairs dur- ing training will be used as additional training in- stances during subsequent training epochs, allow- ing the error correction model to see more gram- matically incorrect sentences during training and accordingly improving its generalization ability.</p><p>For model inference, fluency boost inference mechanism allows the model to correct a sentence incrementally with multi-round inference as long as the proposed edits can boost the sentence's flu- ency, as <ref type="figure">Figure 2</ref>(b) shows. For a sentence with multiple grammatical errors, some of the errors will be corrected first. The corrected parts will make the context clearer, which may benefit the model to correct the remaining errors.</p><p>Experiments demonstrate fluency boost learn- ing and inference enable neural seq2seq models to perform better for GEC and achieve state-of-the- art results on multiple GEC benchmarks.</p><p>Our contributions are summarized as follows:</p><p>• We present a novel learning and inference mechanism to address the limitations in pre- vious seq2seq models for GEC.</p><p>• We propose and compare multiple novel flu- ency boost learning strategies, exploring the learning methodology for neural GEC.</p><p>• Our approaches are proven to be effective to improve neural seq2seq GEC models to achieve state-of-the-art results on CoNLL- 2014 and JFLEG benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Neural grammatical error correction</head><p>As neural machine translation (NMT), a typical neural GEC approach uses a Recurrent Neural Network (RNN) based encoder-decoder seq2seq model ( <ref type="bibr" target="#b35">Sutskever et al., 2014;</ref><ref type="bibr">Cho et al., 2014</ref>) with attention mechanism ( <ref type="bibr">Bahdanau et al., 2014</ref>) to edit a raw sentence into the grammatically cor- rect sentence it should be, as <ref type="figure">Figure 1</ref>(a) shows. Given a raw sentence x r = (x r 1 , · · · , x r M ) and its corrected sentence x c = (x c 1 , · · · , x c N ) in which x r M and x c N are the M -th and N -th words of sentence x r and x c respectively, the er- ror correction seq2seq model learns a probabilis- tic mapping P (x c |x r ) from error-corrected sen- tence pairs through maximum likelihood estima- tion (MLE), which learns model parameters Θ crt to maximize the following equation:</p><formula xml:id="formula_0">Θ * crt = arg max Θ crt (x r ,x c )∈S * log P (x c |x r ; Θcrt) (1)</formula><p>where S * denotes the set of error-corrected sen- tence pairs. For model inference, an output sequence</p><formula xml:id="formula_1">x o = (x o 1 , · · · , x o i , · · · , x o L )</formula><p>is selected through beam search, which maximizes the following equation:  </p><formula xml:id="formula_2">P (x o |x r ) = L i=1 P (x o i |x r , x o &lt;i; Θcrt)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fluency boost learning</head><p>Conventional seq2seq models for GEC learns model parameters only from original error- corrected sentence pairs. However, such error- corrected sentence pairs are not sufficiently avail- able. As a result, many neural GEC models are not very well generalized. Fortunately, neural GEC is different from NMT. For neural GEC, its goal is improving a sentence's fluency 2 without changing its original meaning; thus, any sentence pair that satisfies this condition (we call it fluency boost condition) can be used as a training instance.</p><p>In this paper, we define f (x) as the fluency score of a sentence x:</p><formula xml:id="formula_3">f (x) = 1 1 + H(x)<label>(3)</label></formula><formula xml:id="formula_4">H(x) = − |x| i=1 log P (xi|x&lt;i) |x|<label>(4)</label></formula><p>where P (x i |x &lt;i ) is the probability of x i given context x &lt;i , computed by a language model, and |x| is the length of sentence x. H(x) is actually the cross entropy of the sentence x, whose range is</p><formula xml:id="formula_5">[0, +∞). Accordingly, the range of f (x) is (0, 1].</formula><p>The core idea of fluency boost learning is to generate fluency boost sentence pairs that satisfy the fluency boost condition during training, as Fig- ure 2(a) illustrates, so that these pairs can further help model learning.</p><p>In this section, we present three fluency boost learning strategies: back-boost, self-boost, and <ref type="bibr">2</ref> Fluency of a sentence in this paper refers to how likely the sentence is written by a native speaker. In other words, if a sentence is very likely to be written by a native speaker, it should be regarded highly fluent. dual-boost that generate fluency boost sentence pairs in different ways, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Back-boost learning</head><p>Back-boost learning borrows the idea from back translation ( <ref type="bibr" target="#b33">Sennrich et al., 2016</ref>) in NMT, refer- ring to training a backward model (we call it error generation model, as opposed to error correction model) that is used to convert a fluent sentence to a less fluent sentence with errors. Since the less flu- ent sentences are generated by the error generation seq2seq model trained with error-corrected data, they usually do not change the original sentence's meaning; thus, they can be paired with their cor- rect sentences, establishing fluency boost sentence pairs that can be used as training instances for er- ror correction models, as <ref type="figure" target="#fig_3">Figure 3</ref>(a) shows.</p><p>Specifically, we first train a seq2seq error gener- ation model Θ gen with S * which is identical to S * except that the source sentence and the target sen- tence are interchanged. Then, we use the model Θ gen to predict n-best outputs x o 1 , · · · , x on given a correct sentence x c . Given the fluency boost condition, we compare the fluency of each output x o k (where 1 ≤ k ≤ n) to that of its cor- rect sentence x c . If an output sentence's fluency score is much lower than its correct sentence, we call it a disfluency candidate of x c .</p><p>To formalize this process, we first define Y n (x; Θ) to denote the n-best outputs predicted by model Θ given the input x. Then, disfluency candidates of a correct sentence x c can be derived:</p><formula xml:id="formula_6">D back (x c ) = {x o k |x o k ∈ Yn(xc; Θgen) ∧ f (x c ) f (x o k ) ≥ σ} (5)</formula><p>Algorithm 1 Back-boost learning 1: Train error generation model Θgen with S * ; 2: for each sentence pair (x r , x c ) ∈ S do 3:</p><p>Compute D back (x c ) according to Eq (5); 4: end for 5: for each training epoch t do 6:</p><p>S ← ∅; 7:</p><p>Derive a subset St by randomly sampling |S * | ele- ments from S; 8:</p><p>for each (x r , x c ) ∈ St do 9:</p><p>Establish a fluency boost pair (x , x c ) by ran- domly sampling x ∈ D back (x c ); 10:</p><p>S ← S ∪ {(x , x c )}; 11:</p><p>end for 12:</p><p>Update error correction model Θcrt with S * ∪ S ; 13: end for where D back (x c ) denotes the disfluency candidate set for x c in back-boost learning. σ is a thresh- old to determine if x o k is less fluent than x c and it should be slightly larger 3 than 1.0, which helps fil- ter out sentence pairs with unnecessary edits (e.g., I like this book. → I like the book.).</p><p>In the subsequent training epochs, the error cor- rection model will not only learn from the original error-corrected sentence pairs (x r ,x c ), but also learn from fluency boost sentence pairs (</p><formula xml:id="formula_7">x o k ,x c ) where x o k is a sample of D back (x c ).</formula><p>We summarize this process in Algorithm 1 where S * is the set of original error-corrected sen- tence pairs, and S can be tentatively considered identical to S * when there is no additional native data to help model training (see Section 3.4). Note that we constrain the size of S t not to exceed |S * | (the 7th line in Algorithm 1) to avoid that too many fluency boost pairs overwhelm the effects of the original error-corrected pairs on model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-boost learning</head><p>In contrast to back-boost learning whose core idea is originally from NMT, self-boost learning is original, which is specially devised for neu- ral GEC. The idea of self-boost learning is il- lustrated by <ref type="figure" target="#fig_3">Figure 3</ref>(b) and was already briefly introduced in Section 1 and <ref type="figure">Figure 2(a)</ref>. Un- like back-boost learning in which an error gen- eration seq2seq model is trained to generate dis- fluency candidates, self-boost learning allows the error correction model to generate the candidates by itself. Since the disfluency candidates gener- ated by the error correction seq2seq model trained with error-corrected data rarely change the input Algorithm 2 Self-boost learning 1: for each sentence pair (x r , x c ) ∈ S do 2:</p><p>D self (x c ) ← ∅; 3: end for 4: S ← ∅ 5: for each training epoch t do 6:</p><p>Update error correction model Θcrt with S * ∪ S ; 7:</p><p>S ← ∅ 8:</p><p>Derive a subset St by randomly sampling |S * | ele- ments from S; 9:</p><p>for each (x r , x c ) ∈ St do 10:</p><p>Update D self (x c ) according to Eq (6); 11:</p><p>Establish a fluency boost pair (x , x c ) by ran- domly sampling x ∈ D self (x c ); 12:</p><p>S ← S ∪ {(x , x c )}; 13:</p><p>end for 14: end for sentence's meaning; thus, they can be used to es- tablish fluency boost sentence pairs.</p><p>For self-boost learning, given an error corrected pair (x r , x c ), an error correction model Θ crt first predicts n-best outputs x o 1 , · · · , x on for the raw sentence x r . Among the n-best outputs, any out- put that is not identical to x c can be considered as an error prediction. Instead of treating the error predictions useless, self-boost learning fully ex- ploits them. Specifically, if an error prediction x o k is much less fluent than that of its correct sentence x c , it will be added to x c 's disfluency candidate set D self (x c ), as Eq (6) shows:</p><formula xml:id="formula_8">D self (x c ) = D self (x c ) ∪ {x o k |x o k ∈ Yn(xr; Θcrt) ∧ f (x c ) f (x o k ) ≥ σ}<label>(6)</label></formula><p>In contrast to back-boost learning, self-boost generates disfluency candidates from a different perspective -by editing the raw sentence x r rather than the correct sentence x c . It is also notewor- thy that D self (x c ) is incrementally expanded be- cause the error correction model Θ crt is dynami- cally updated, as shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual-boost learning</head><p>As introduced above, back-and self-boost learn- ing generate disfluency candidates from different perspectives to create more fluency boost sentence pairs to benefit training the error correction model. Intuitively, the more diverse disfluency candidates generated, the more helpful for training an error correction model. Inspired by <ref type="bibr" target="#b5">He et al. (2016)</ref> and <ref type="bibr" target="#b46">Zhang et al. (2018)</ref>, we propose a dual-boost learning strategy, combining both back-and self- boost's perspectives to generate disfluency candi- dates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Dual-boost learning</head><p>1: for each (x r , x c ) ∈ S do 2:</p><p>D dual (x c ) ← ∅; 3: end for 4: S ← ∅; S ← ∅; 5: for each training epoch t do 6:</p><p>Update error correction model Θcrt with S * ∪ S ; <ref type="figure" target="#fig_3">Figure 3(c)</ref> shows, disfluency candidates in dual-boost learning are from both the error gener- ation model and the error correction model :</p><note type="other">7: Update error generation model Θgen with S * ∪ S ; 8: S ← ∅; S ← ∅; 9: Derive a subset St by randomly sampling |S * | ele- ments from S; 10: for each (x r , x c ) ∈ St do 11: Update D dual (x c ) according to Eq (7); 12: Establish a fluency boost pair (x , x c ) by ran- domly sampling x ∈ D dual (x c ); 13: S ← S ∪ {(x , x c )}; 14: Establish a reversed fluency boost pair (x c , x ) by randomly sampling x ∈ D dual (x c ); 15: S ← S ∪ {(x c , x )}; 16: end for 17: end for As</note><formula xml:id="formula_9">D dual (x c ) = D dual (x c ) ∪ {x o k |x o k ∈ Yn(xr; Θcrt) ∪ Yn(xc; Θgen) ∧ f (x c ) f (x o k ) ≥ σ}<label>(7)</label></formula><p>Moreover, the error correction model and the er- ror generation model are dual and both of them are dynamically updated, which improves each other: the disfluency candidates produced by er- ror generation model can benefit training the error correction model, while the disfluency candidates created by error correction model can be used as training data for the error generation model. We summarize this learning approach in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fluency boost learning with large-scale native data</head><p>Our proposed fluency boost learning strategies can be easily extended to utilize the huge volume of native data which is proven to be useful for GEC. As discussed in Section 3.1, when there is no additional native data, S in Algorithm 1-3 is iden- tical to S * . In the case where additional native data is available to help model learning, S becomes: S = S * ∪ C where C = {(x c , x c )} denotes the set of self- copied sentence pairs from native data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fluency boost inference</head><p>As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be per- fectly corrected through normal seq2seq inference We use CoNLL-2014 shared task dataset with original annotations ( ), which con- tains 1,312 sentences, as our main test set for eval- uation. We use MaxMatch (M 2 ) precision, recall and F 0.5 (Dahlmeier and Ng, 2012b) as our evalua- tion metrics. As previous studies, we use CoNLL- 2013 test data as our development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental setting</head><p>We set up experiments in order to answer the fol- lowing questions:  <ref type="table">Table 2</ref>: Performance of seq2seq for GEC with different learning (row) and inference (column) meth- ods on CoNLL-2014 dataset. (+LM) denotes decoding with the RNN language model through shallow fusion. The last 3 systems (with ) use the additional non-public Lang-8 data for training.</p><p>• Whether is fluency boost learning mechanism helpful for training the error correction model, and which of the strategies (back-boost, self- boost, dual-boost) is the most effective? • Whether does our fluency boost inference im- prove normal seq2seq inference for GEC? • Whether can our approach improve neural GEC to achieve state-of-the-art results?</p><p>The training details for our seq2seq error cor- rection model and error generation model are as follows: the encoder of the seq2seq models is a 2-layer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism ( <ref type="bibr" target="#b12">Luong et al., 2015)</ref>. Both the dimen- sionality of word embeddings and the hidden size of GRU cells are 500. The vocabulary sizes of the encoder and decoder are 100,000 and 50,000 re- spectively. The models' parameters are uniformly initialized in <ref type="bibr">[-0.1,0.1]</ref>. We train the models with an Adam optimizer with a learning rate of 0.0001 up to 40 epochs with batch size = 128. Dropout is applied to non-recurrent connections at a ratio of 0.15. For fluency boost learning, we generate dis- fluency candidates from 10-best outputs. During model inference, we set beam size to 5 and decode 1-best result with a 2-layer GRU RNN language model ( <ref type="bibr" target="#b14">Mikolov et al., 2010</ref>) through shallow fu- sion ( <ref type="bibr">Gülçehre et al., 2015</ref>) with weight β = 0.15. The RNN language model is trained from the na- tive data mentioned in Section 5.1, which is also used for computing fluency score in Eq (3). UNK tokens are replaced with the source token with the highest attention weight.</p><p>We resolve spelling errors with a public spell checker 4 as preprocessing, as <ref type="bibr" target="#b40">Xie et al. (2016)</ref> and  do. <ref type="table">Table 2</ref> compares the performance of seq2seq er- ror correction models with different learning and inference methods. By comparing by row, one can observe that our fluency boost learning approaches improve the performance over normal seq2seq learning, especially on the recall metric, since the fluency boost learning approaches generate a va- riety of grammatically incorrect sentences, allow- ing the error correction model to learn to correct much more sentences than the conventional learn- ing strategy. Among the proposed three fluency boost learning strategies, dual-boost achieves the best result in most cases because it produces more diverse incorrect sentences (average |D dual | ≈ 9.43) than either back-boost (avg |D back | ≈ 1.90) or self-boost learning (avg |D self | ≈ 8.10). With introducing large amounts of native text data, the performance of all the fluency boost learning ap- proaches gets improved. One reason is that our learning approaches produce more error-corrected sentence pairs to let the model be better general- ized. In addition, the huge volume of native data benefits the decoder to learn better to generate a fluent and error-free sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effectiveness of fluency boost learning</head><p>We test the effect of hyper-parameter σ in Eq (5-7) on fluency boost learning and show the re- sult in <ref type="table">Table 3</ref>. When σ is slightly larger than 1.0 (e.g., σ = 1.05), the model achieves the best per- formance because it effectively avoids generating sentence pairs with unnecessary or undesirable ed- its that affect the performance, as we discussed in Section 3.1. When σ continues increasing, the dis- fluency candidate set |D dual | drastically decreases, making the dual-boost learning gradually degrade to normal seq2seq learning. <ref type="table">Table 4</ref> shows some examples of disfluency  <ref type="table">Table 3</ref>: The effect of σ on dual-boost learning with normal seq2seq inference. |D dual | is the av- erage size of dual-boost disfluency candidate sets.</p><note type="other">σ 0 0.95 1.0 1.05 1.1 2.0 |Ddual| 41.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct sentence</head><p>How autism occurs is not well understood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disfluency candidates</head><p>How autism occurs is not good understood. How autism occur is not well understood. What autism occurs is not well understood. How autism occurs is not well understand. How autism occurs does not well understood. <ref type="table">Table 4</ref>: Examples of disfluency candidates for a correct sentence in dual-boost learning.</p><p>candidates 5 generated in dual-boost learning given a correct sentence in the native data. It is clear that our approach can generate less fluent sentences with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sen- tence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effectiveness of fluency boost inference</head><p>The effectiveness of various inference approaches can be observed by comparing the results in <ref type="table">Table  2</ref> by column. Compared to the normal seq2seq inference and seq2seq (+LM) baselines, fluency boost inference brings about on average 0.14 and 0.18 gain on F 0.5 respectively, which is a signif- icant 6 improvement, demonstrating multi-round edits by fluency boost inference is effective. Take our best system (the last row in <ref type="table">Table  2</ref>) as an example, among 1,312 sentences in the CoNLL-2014 dataset, seq2seq inference with shallow fusion LM edits 566 sentences. In con- trast, fluency boost inference additionally edits 23 sentences during the second round inference, im- proving F 0.5 from 52.59 to 52.72.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Towards the state-of-the-art for GEC</head><p>Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the state- of-the-art result.</p><p>We first compare our best models -dual-boost learning (+native) with fluency boost inference and shallow fusion LM -to top-performing GEC systems evaluated on CoNLL-2014 dataset: <ref type="bibr">5</ref> We give more details about disfluency candidates, in- cluding error type proportion, in the supplementary notes. <ref type="bibr">6</ref>   <ref type="table">Table 5</ref>: Performance of systems on CoNLL-2014 dataset. The system with bold fonts are based on seq2seq models. denotes the system uses the non-public error-corrected data from Lang-8.com.</p><p>• CAMB14, CAMB16 SMT , CAMB16 NMT and CAMB17: GEC systems ( <ref type="bibr" target="#b0">Felice et al., 2014;</ref><ref type="bibr" target="#b41">Yannakoudakis et al., 2017</ref>) developed by Cambridge University.</p><p>• AMU14 and AMU16: SMT-based GEC sys- tems <ref type="bibr">Grundkiewicz, 2014, 2016</ref>) developed by AMU.</p><p>• CUUI and VT16: the former system <ref type="bibr" target="#b25">(Rozovskaya et al., 2014</ref>) uses a classifier-based approach, which is improved by the latter sys- tem ( <ref type="bibr" target="#b28">Rozovskaya and Roth, 2016)</ref> through combining with an SMT-based approach.</p><p>• NUS14, NUS16 and NUS17: GEC systems ( <ref type="bibr">Chollampatt et al., 2016a;</ref><ref type="bibr">Chollampatt and Ng, 2017</ref>) that combine SMT with other techniques (e.g., classifiers).</p><p>• Char-seq2seq: a character-level seq2seq model <ref type="bibr" target="#b40">(Xie et al., 2016)</ref>. It uses a rule-based method to synthesize errors for data augmentation.</p><p>• Nested-seq2seq: a nested attention neural hy- brid seq2seq model ( <ref type="bibr" target="#b7">Ji et al., 2017</ref>).</p><p>• Adapt-seq2seq: a seq2seq model adapted to incorporate edit operations ( <ref type="bibr" target="#b32">Schmaltz et al., 2017</ref>). <ref type="table">Table 5</ref> shows the evaluation results on the CoNLL-2014 dataset. Without using the non- public training data from Lang-8.com, our sin-gle model obtains 50.04 F 0.5 , larlgely outperform- ing the other seq2seq models and only inferior to CAMB17 (AMU16 based) and NUS17. It should be noted, however, that the CAMB17 and NUS17 are actually re-rankers built on top of an SMT- based GEC system (AMU16's framework); thus, they are ensemble models. When we build our ap- proach on top of AMU16 (i.e., we take AMU16's outputs as the input to our GEC system to edit on top of its outputs), we achieve 53.30 F 0.5 score. With introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F 0.5 score respectively, which is a state- of-the-art result 7 on CoNLL-2014 dataset.</p><p>Moreover, we evaluate our approach on JFLEG corpus ( <ref type="bibr" target="#b19">Napoles et al., 2017)</ref>. JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach's generalization ability, we evaluate our single models used for CoNLL eval- uation (in <ref type="table">Table 5</ref>) on JFLEG without re-tuning. <ref type="table">Table 6</ref> shows the JFLEG leaderboard. Instead of M 2 score, JFLEG uses GLEU ( <ref type="bibr" target="#b17">Napoles et al., 2015</ref>) as its evaluation metric, which is a fluency- oriented GEC metric based on a variant of BLEU ( <ref type="bibr" target="#b22">Papineni et al., 2002</ref>) and has several advantages over M 2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-of- the-art result 8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Most of advanced GEC systems are classifier- based ( <ref type="bibr">Chodorow et al., 2007;</ref><ref type="bibr">De Felice and Pulman, 2008;</ref><ref type="bibr" target="#b4">Han et al., 2010;</ref><ref type="bibr" target="#b11">Leacock et al., 2010;</ref><ref type="bibr" target="#b37">Tetreault et al., 2010a;</ref><ref type="bibr">Dale and Kilgarriff, 2011)</ref> 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5=54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5=56.25), which are contemporaneous to this pa- per. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit opera- tion features, ensemble decoding and advanced model combi- nations). It should be noted that their approaches are orthog- onal to ours, making it possible to apply our fluency boost learning and inference mechanism to their models. <ref type="bibr">8</ref> The recently proposed SMT-NMT hybrid system ( <ref type="bibr" target="#b2">Grundkiewicz and Junczys-Dowmunt, 2018)</ref>, which is tuned towards GLEU on JFLEG Dev set, reports a higher result (GLEU=61.50 on JFLEG test set).  <ref type="table">Table 6</ref>: JFLEG Leaderboard. Ours denote the single dual-boost models in <ref type="table">Table 5</ref>. The systems with bold fonts are based on seq2seq models. * denotes the system is tuned on JFLEG.</p><p>or MT-based ( <ref type="bibr">Brockett et al., 2006;</ref><ref type="bibr">Dahlmeier and</ref><ref type="bibr">Ng, 2011, 2012a;</ref><ref type="bibr" target="#b42">Yoshimoto et al., 2013;</ref><ref type="bibr" target="#b45">Yuan and Felice, 2013;</ref><ref type="bibr">Behera and Bhattacharyya, 2013)</ref>. For example, top-performing systems <ref type="bibr" target="#b0">(Felice et al., 2014;</ref><ref type="bibr" target="#b25">Rozovskaya et al., 2014;</ref><ref type="bibr">JunczysDowmunt and Grundkiewicz, 2014</ref>) in CoNLL- 2014 shared task ( ) use either of the methods. Recently, many novel approaches ( <ref type="bibr">Chollampatt et al., 2016b,a;</ref><ref type="bibr" target="#b28">Rozovskaya and Roth, 2016;</ref><ref type="bibr" target="#b9">Junczys-Dowmunt and Grundkiewicz, 2016;</ref><ref type="bibr" target="#b16">Mizumoto and Matsumoto, 2016;</ref><ref type="bibr" target="#b6">Hoang et al., 2016;</ref><ref type="bibr" target="#b41">Yannakoudakis et al., 2017</ref>) have been proposed for GEC. Among them, seq2seq models <ref type="bibr" target="#b40">Xie et al., 2016;</ref><ref type="bibr" target="#b7">Ji et al., 2017;</ref><ref type="bibr" target="#b32">Schmaltz et al., 2017;</ref><ref type="bibr">Chollampatt and Ng, 2018)</ref> have caught much atten- tion. Unlike the models trained only with origi- nal error-corrected data, we propose a novel flu- ency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC <ref type="bibr">(Brockett et al., 2006;</ref><ref type="bibr" target="#b1">Foster and Andersen, 2009;</ref><ref type="bibr">Roth, 2010, 2011;</ref><ref type="bibr" target="#b29">Rozovskaya et al., 2012;</ref><ref type="bibr" target="#b0">Felice and Yuan, 2014;</ref><ref type="bibr" target="#b40">Xie et al., 2016;</ref>. More- over, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence's fluency can be improved.</p><p>To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT <ref type="bibr" target="#b39">(Xia et al., 2017</ref>).</p><p>In addition to the studies on GEC, there is also much research on grammatical error detection <ref type="bibr" target="#b11">(Leacock et al., 2010;</ref><ref type="bibr" target="#b24">Rei and Yannakoudakis, 2016;</ref><ref type="bibr" target="#b10">Kaneko et al., 2017)</ref> and GEC evaluation <ref type="bibr" target="#b38">(Tetreault et al., 2010b;</ref><ref type="bibr" target="#b13">Madnani et al., 2011;</ref><ref type="bibr">Dahlmeier and Ng, 2012c;</ref><ref type="bibr" target="#b17">Napoles et al., 2015;</ref><ref type="bibr">Bryant et al., 2017;</ref><ref type="bibr">Asano et al., 2017</ref>). We do not introduce them in detail because they are not much related to this paper's contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a novel fluency boost learning and inference mechanism to overcome the limitations of previous neural GEC models. Our proposed fluency boost learning fully exploits both error- corrected data and native data, largely improv- ing the performance over normal seq2seq learn- ing, while fluency boost inference utilizes the characteristic of GEC to incrementally improve a sentence's fluency through multi-round inference. The powerful learning and inference mechanism enables the seq2seq models to achieve state-of- the-art results on both CoNLL-2014 and JFLEG benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>She sees Tom caught by a policeman in the park last night. She sees Tom caught by a policeman in the park last night. She saw Tom caught by a policeman in the park last night.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Three fluency boost learning strategies: (a) back-boost, (b) self-boost, (c) dual-boost; all of them generate fluency boost sentence pairs (the pairs in the dashed boxes) to help model learning during training. The numbers in this figure are fluency scores of their corresponding sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>She sees Tom caught by a policeman in the park last night.</head><label></label><figDesc></figDesc><table>She saw Tom caught by a policeman in the park last night. 

She saw Tom caught by a policeman in the park last night. 

1st round seq2seq inference 

2nd round seq2seq inference 

3rd round seq2seq inference 

0.121 

0.144 

0.147 

0.147 

boost 

no boost 

(a) 
(b) 

boost 

sentence 
fluency 
fluency 
sentence 

seq2seq inference 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>18.49 41.81 61.56 18.85 42.37 61.75 23.30 46.42 61.94 23.70 46.83 back-boost 61.66 19.54 43.09 61.43 19.61 43.07 61.47 24.74 47.40 61.24 25.01 47.48 self-boost 61.64 19.83 43.35 61.50 19.90 43.36 62.13 24.45 47.49 61.67 24.76 47.51 dual-boost 62.03 20.82 44.44 61.64 21.19 44.61 62.22 25.49 48.30 61.64 26.45 48.69 back-boost (+native) 63.93 22.03 46.31 63.95 22.12 46.40 62.04 27.43 49.54 61.98 27.70 49.68 self-boost (+native) 64.33 22.10 46.54 64.14 22.19 46.54 62.18 27.59 49.71 61.64 28.37 49.93 dual-boost (+native) 65.77 21.92 46.98 65.82 22.14 47.19 62.64 27.40 49.83 62.70 27.69 50.04 back-boost (+native) 67.37 24.31 49.75 67.25 24.35 49.73 64.61 28.44 51.51 64.46 28.78 51.66 self-boost (+native) 66.52 25.13 50.03 66.78 25.33 50.31 63.82 30.15 52.17 63.34 31.63 52.21 dual-boost (+native) 66.34 25.39 50.16 66.45 25.51 50.30 64.72 30.06 52.59 64.47 30.48 52.72</figDesc><table>Model 
seq2seq 
fluency boost 
seq2seq (+LM) 
fluency boost (+LM) 
P 
R 
F0.5 
P 
R 
F0.5 
P 
R 
F0.5 
P 
R 
F0.5 
normal seq2seq 
61.06 </table></figure>

			<note place="foot" n="1"> A sentence&apos;s fluency score is defined to be inversely proportional to the sentence&apos;s cross entropy, as is in Eq (3).</note>

			<note place="foot" n="3"> In this paper, we set σ = 1.05 since the corrected sentence in our training data improves its corresponding raw sentence about 5% fluency on average.</note>

			<note place="foot" n="4"> https://azure.microsoft.com/en-us/services/cognitiveservices/spell-check/</note>

			<note place="foot">Mariano Felice and Zheng Yuan. 2014. Generating artificial errors for grammatical error correction. In Student Research Workshop at EACL.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their professional and constructive comments. We also thank Shujie Liu for his insightful discussions and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Hiroki Asano, Tomoya Mizumoto, and Kentaro Inui.</p><p>2017. Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical er- ror correction systems. In IJCNLP. Rachele De Felice and Stephen G Pulman. 2008. A classifier-based approach to preposition and deter- miner error correction in l2 english. In COLING.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL (Shared Task</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generrate: generating errors for use in grammatical error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Øistein E Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on innovative use of nlp for building educational applications</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Near human-level performance in grammatical error correction with hybrid machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin Junczys-Dowmunt</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05945</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using an error-annotated learner corpus to develop an esl/efl error correction system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na-Rae</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Hwa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting n-best hypotheses to improve an smt approach to grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The amu system in the conll-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL (Shared Task</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06353</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grammatical error detection using error-and grammaticality-specific word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Sakaizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automated grammatical error detection for language learners. Synthesis lectures on human language technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">They can help: Using crowdsourcing to improve the evaluation of grammatical error detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mining revision log of language learning sns for automated japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<editor>IJCNLP</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative reranking for grammatical error correction with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">There&apos;s no comparison: Referenceless evaluation metrics in grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Jfleg: A fluency corpus and benchmark for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04066</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL (Shared Task</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cambridge learner corpus: Error coding and analysis for lexicography and elt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Artificial error generation with machine translation and syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05236</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compositional sequence labeling models for error detection in learner writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The illinoiscolumbia system in the conll-2014 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL (Shared Task)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training paradigms for correcting errors in grammar and usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for esl correction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The ui system in the hoo 2012 shared task on error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Building Educational Applications Using NLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reassessing the goals of grammatical error correction: Fluency instead of grammaticality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grammatical error correction with neural reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adapting sequence models for sentence correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">System combination for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tense and aspect error correction for esl learners using global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshikazu</forename><surname>Tajiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using parse features for preposition selection and error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking grammatical error annotation and evaluation with the amazon mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Joel R Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09727</idno>
		<title level="m">Neural language correction with character-based attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural sequencelabelling models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Øistein E Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Naist at 2013 conll grammatical error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ippei</forename><surname>Yoshimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensuke</forename><surname>Mitsuzawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL (Shared Task)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Candidate re-ranking for smt-based grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Constrained grammatical error correction using statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL (Shared Task)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Joint training for neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00353</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
