<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Summary Prior Representation for Extractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<settlement>Xuzhou</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<settlement>Xuzhou</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Computing Department</orgName>
								<orgName type="institution">Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<settlement>Xuzhou</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Summary Prior Representation for Extractive Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="829" to="833"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose the concept of summary prior to define how much a sentence is appropriate to be selected into summary without consideration of its context. Different from previous work using manually compiled document-independent features, we develop a novel summary system called PriorSum, which applies the enhanced convolutional neu-ral networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concate-nated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models <ref type="bibr" target="#b12">(Osborne, 2002;</ref><ref type="bibr" target="#b7">Galley, 2006;</ref><ref type="bibr" target="#b3">Conroy et al., 2004;</ref><ref type="bibr" target="#b10">Li et al., 2007)</ref>, feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and document- independent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example:</p><p>1. Hurricane Emily slammed into Dominica on September 22, causing 3 deaths with its wind gusts up to 110 mph. * Contribution during internship at Microsoft Research 2. It was Emily, the hurricane which caused 3 deaths and armed with wind guests up to 110 mph, that slammed into Dominica on Tues- day.</p><p>The first sentence describes the major information of a hurricane. With similar meaning, the second sentence uses an emphatic structure and is some- what verbose. Obviously the first one should be preferred for a news summary. In this paper, we call such fact as summary prior nature 1 and learn document-independent features to reflect it. In previous summarization systems, though not well-studied, some widely-used sentence ranking features such as the length and the ratio of stop- words, can be seen as attempts to measure the summary prior nature to a certain extent. Notably, <ref type="bibr" target="#b8">Hong and Nenkova (2014)</ref> built a state-of-the-art summarization system through making use of ad- vanced document-independent features. However, these document-independent features are usually hand-crafted, difficult to exhaust each aspect of the summary prior nature. Meanwhile, items rep- resenting the same feature may contribute differ- ently to a summary. For example, "September 22" and "Tuesday" are both indicators of time, but the latter seldom occurs in a summary due to uncer- tainty. In addition, to the best of our knowledge, document-independent features beyond word level (e.g., phrases) are seldom involved in current re- search.</p><p>The CTSUM system developed by <ref type="bibr" target="#b16">Wan and Zhang (2014)</ref> is the most relevant to ours. It at- tempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization. To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR). How-ever, a low certainty score does not always rep- resent low quality of being a summary sentence. For example, the sentence below is from a topic about "Korea nuclear issue" in DUC 2004: Clin- ton acknowledged that U.S. is not yet certain that the suspicious underground construction project in North Korea is nuclear related. The under- lined phrases greatly reduce the certainty of this sentence according to <ref type="bibr" target="#b16">Wan and Zhang (2014)</ref>'s model. But, in fact, this sentence can summarize the government's attitude and is salient enough in the related documents. Thus, in our opinion, cer- tainty can just be viewed as a specific aspect of the summary prior nature.</p><p>To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the sum- mary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation <ref type="bibr" target="#b18">(Yih et al., 2014;</ref><ref type="bibr" target="#b13">Shen et al., 2014;</ref><ref type="bibr" target="#b19">Zeng et al., 2014</ref>), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent fea- tures derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling op- eration to associate these filters since phrases with different lengths may express the same as- pect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1).</p><p>We conduct extensive experiments on the <ref type="bibr">DUC 2001</ref><ref type="bibr">DUC , 2002</ref> and 2004 generic multi-document summarization datasets. The experimental results demonstrate that our model outperforms state- of-the-art extractive summarization approaches. Meanwhile, we analyze the different aspects sup- porting the summary prior in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our summarization system PriorSum follows the traditional extractive framework <ref type="bibr" target="#b1">(Carbonell and Goldstein, 1998;</ref><ref type="bibr" target="#b10">Li et al., 2007)</ref>. Specifically, the sentence ranking process scores and ranks the sen- tences from documents, and then the sentence se- lection process chooses the top ranked sentences to generate the final summary in accordance with the length constraint and redundancy among the selected sentences.</p><p>Sentence ranking aims to measure the saliency score of a sentence with consideration of both document-dependent and document-independent features. In this study, we apply an enhanced ver- sion of convolutional neural networks to automati- cally generate document-independent features ac- cording to the summary prior nature. Meanwhile, some document-dependent features are extracted. These two types of features are combined in the sentence regression step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Ranking</head><p>PriorSum improves the standard convolutional neural networks (CNNs) to learn the summary prior since CNN is able to learn compressed rep- resentation of n-grams effectively and tackle sen- tences with variable lengths naturally. We first introduce the standard CNNs, based on which we design our improved CNNs for obtaining document-independent features.</p><p>The standard CNNs contain a convolution oper- ation over several word embeddings, followed by a pooling operation. Let v i ∈ R k denote the k- dimensional word embedding of the i th word in the sentence. Assume v i:i+j to be the concatena- tion of word embeddings v i , · · · , v i+j . A convo- lution operation involves a filter W h t ∈ R l×hk , which operates on a window of h words to pro- duce a new feature with l dimensions:</p><formula xml:id="formula_0">c h i = f (W h t × v i:i+h−1 ) (1)</formula><p>where f is a non-linear function and tanh is used like common practice. Here, the bias term is ignored for simplicity. Then W h t is applied to each possible window of h words in the sentence of length N to produce a feature map:</p><formula xml:id="formula_1">C h = [c h 1 , · · · , c h N −h+1 ].</formula><p>Next, we adopt the widely- used max-over-time pooling operation <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>) to obtain the final featuresˆcfeaturesˆ featuresˆc h from C h . That is, ˆ c h = max{C h }. The idea behind this pooling operation is to capture the most im- portant features in a feature map.</p><p>In the standard CNNs, only the fixed-length windows of words are considered to represent a sentence. As we know, the variable-length phrases composed of a sentence can better express the sen- tence and disclose its summary prior nature. To make full use of the phrase information, we design an improved version of the standard CNNs, which use multiple filters for different window sizes as well as two max-over-time pooling operations to get the final summary prior representation. Specif- ically, let W 1 t , · · · , W m t be m filters for window sizes from 1 to m, and correspondingly we can obtain m feature maps C 1 , · · · , C m . For each fea- ture map C i , We first adopt a max-over-time pool- ing operation max{C i } with the goal of capturing the most salient features from each window size i. Next, a second max-over-time pooling operation is operated on all the windows to acquire the most representative features. To formulate, the docu- ment independent features x p can be generated by:</p><formula xml:id="formula_2">x p = max{max{C 1 }, · · · , max{C m }}.<label>(2)</label></formula><p>Kim (2014) also uses filters with varying win- dow sizes for sentence-level classification tasks. However, he reserves all the representations gen- erated by filters to a fully connected output layer. This practice greatly enlarges following parame- ters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these fil- ters.</p><p>Besides the features x p obtained through the CNNs, we also extract several document- dependent features notated as x e , shown in <ref type="table" target="#tab_0">Table  1</ref>. In the end, x p is combined with x e to con- duct sentence ranking. Here we follow the regres- sion framework of <ref type="bibr" target="#b10">Li et al. (2007)</ref>. The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to esti- mate this saliency.</p><formula xml:id="formula_3">φ = [x p , x e ]<label>(3)</label></formula><formula xml:id="formula_4">ˆ y = w T r × φ<label>(4)</label></formula><p>where w r ∈ R l+|xe| is the regression weights. We use linear transformation since it is convenient to compare with regression baselines (see Section 3.2).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence Selection</head><p>A summary is obliged to offer both informative and non-redundant content. Here, we employ a simple greedy algorithm to select sentences, simi- lar to the MMR strategy <ref type="bibr" target="#b1">(Carbonell and Goldstein, 1998</ref>). Firstly, we remove sentences less than 8 words (as in <ref type="bibr" target="#b5">Erkan and Radev (2004)</ref>) and sort the rest in descending order according to the estimated saliency scores. Then, we iteratively dequeue one sentence, and append it to the current summary if it is non-redundant. A sentence is considered non- redundant if it contains more new words compared to the current summary content. We empirically set the cut-off of new word ratio to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setup</head><p>In our work, we focus on the generic multi- document summarization task and carry out ex- periments on DUC 2001 2004 datasets. All the documents are from newswires and grouped into various thematic clusters. The summary length is limited to 100 words (665 bytes for DUC 2004).</p><p>We use DUC 2003 data as the development set and conduct a 3-fold cross-validation on <ref type="bibr">DUC 2001</ref><ref type="bibr">DUC , 2002</ref> and 2004 datasets with two years of data as training set and one year of data as test set.</p><p>We directly use the look-up table of 25- dimensional word embeddings trained by the model of <ref type="bibr" target="#b2">Collobert et al. (2011)</ref>. These small word embeddings largely reduces model param- eters. The dimension l of the hidden document- independent features is experimented in the range of <ref type="bibr">[1,</ref><ref type="bibr">40]</ref>, and the window sizes are experimented between 1 and 5. Through parameter experiments on development set, we set l = 20 and m = 3 for PriorSum. To update the weights W h t and w r , we apply the diagonal variant of AdaGrad with mini- batches ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>).</p><p>For evaluation, we adopt the widely-used auto- matic evaluation metric ROUGE <ref type="bibr" target="#b11">(Lin, 2004)</ref>, and take ROUGE-1 and ROUGE-2 as the main mea- sures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Baseline Methods</head><p>To evaluate the summarization performance of Pri- orSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in <ref type="table" target="#tab_2">Table 2</ref>) participat- ing DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC <ref type="bibr">(2001, 2002, and 2004</ref>) data. To our knowl- edge, the best reported results on <ref type="bibr">DUC 2001</ref><ref type="bibr">DUC , 2002</ref> and 2004 are from R2N2 ( <ref type="bibr" target="#b0">Cao et al., 2015)</ref>, ClusterCMRW ( <ref type="bibr" target="#b15">Wan and Yang, 2008)</ref> and REG- SUM 2 <ref type="bibr" target="#b8">(Hong and Nenkova, 2014</ref>) respectively. R2N2 applies recursive neural networks to learn feature combination. ClusterCMRW incorporates the cluster-level information into the graph-based ranking algorithm. REGSUM is a word regres- sion approach based on some advanced features such as word polarities <ref type="bibr" target="#b17">(Wiebe et al., 2005</ref>) and categories <ref type="bibr" target="#b14">(Tausczik and Pennebaker, 2010)</ref>. For these three systems, we directly cite their pub- lished results, marked with the sign "*" as in <ref type="table" target="#tab_2">Ta- ble 2</ref>. Meanwhile, <ref type="bibr">LexRank (Erkan and Radev, 2004</ref>), a commonly-used graph-based summariza- tion model, is introduced as an extra baseline. Comparing with this baseline can demonstrate the performance level of regression approaches. The baseline StandardCNN means that we adopt the standard CNNS with fixed window size for sum- mary prior representation.</p><p>To explore the effects of the learned summary prior representations, we design a baseline sys- tem named Reg Manual which adopts manually- compiled document-independent features such as NUMBER (whether number exist), NENTITY (whether named entities exist) and STOPRATIO (the ratio of stopwords). Then we combine these features with document-dependent features in Ta- ble 1 and tune the feature weights through LIB- LINEAR 3 support vector regression.</p><p>From <ref type="table" target="#tab_2">Table 2</ref>, we can see that PriorSum can achieve a comparable performance to the state- of-the-art summarization systems R2N2, Cluster- CMRW and REGSUM. With respect to baselines, PriorSum significantly 4 outperforms Reg Manual which uses manually compiled features and the graph-based summarization system LexRank. Meanwhile, PriorSum always enjoys a reasonable increase over StandardCNN, which verifies the ef- fects of the enhanced CNNs. It is noted that Stan- dardCNN can also achieve the state-of-the-art per- formance, indicating the summary prior represen- tation really works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>In this section, we explore what PriorSum learns according to the summary prior representations. Since the convolution layer follows a linear regres- sion output, we apply a simple strategy to measure how much the learned document-independent fea- tures contribute to the saliency estimation. Specif- ically, for each sentence, we ignore its document- dependent features through setting their values as   zeros and then apply a linear transformation using the weight w r to get a summary prior score x p . The greater the score, the more possible a sentence is to be included in a summary without context consideration. We analyze what intuitive features are hidden in the summary prior representation.</p><p>From <ref type="table" target="#tab_3">Table 3</ref>, first we find that high-scored sentences contains more named entities and num- bers, which conforms to human intuition. By contrast, the features NENTITY and NUMBER in Reg Manual hold very small weights, only 2%, 3% compared with the most significant fea- ture AVG-CF. One possible reason is that named entities or numbers are not independent features. For example, "month + number" is a common timestamp for an event whereas "number + a.m." is over-detailed and seldom appears in a summary. We can also see that low-scored sentences are rel- atively informal and fail to provide facts, which are difficult for human to generalize some spe- cific features. For instance, informal sentences seem to have more stopwords but the feature STO- PRATIO holds a relatively large positive weight in Reg Manual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>This paper proposes a novel summarization sys- tem called PriorSum to automatically learn sum- mary prior features for extractive summariza- tion. Experiments on the DUC generic multi- document summarization task show that our pro- posed method outperforms state-of-the-art ap- proaches. In addition, we demonstrate the dom- inant sentences discovered by PriorSum, and the results verify that our model can learn different as- pects of summary prior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Feature</head><label></label><figDesc>Description POSITION The position of the sentence. AVG-TF The averaged term frequency values of words in the sentence. AVG-CF The averaged cluster frequency values of words in the sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Extracted document-dependent features.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison results (%) on DUC datasets. 

Meanwhile, Yugoslavia's P.M. told an emer-
gency session Monday that the country is faced 
with war. 
high 
scored 
The rebels ethnic Tutsis, disenchanted members 
of President Laurent Kabila's army took up arms, 
creating division among Congo's 400 tribes. 
The blast killed two assailants, wounded 21 Is-
raelis and prompted Israel to suspend implemen-
tation of the peace accord with the Palestinians. 
The greatest need is that many, many of us have 
been psychologically traumatized, and very, very 
few are receiving help. 
low 
scored 
Ruben Rivera: An impatient hitter who will 
chase pitches out of the strike zone. 
I think we should worry about tuberculosis and 
the risk to the general population. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Example sentences selected by prior 
scores. 

</table></figure>

			<note place="foot" n="1"> In this paper, &quot;summary prior features&quot; and &quot;documentindependent features&quot; hold the same meaning.</note>

			<note place="foot" n="2"> REGSUM truncates a summary to 100 words.</note>

			<note place="foot" n="3"> http://www.csie.ntu.edu.tw/ ˜ cjlin/ liblinear/ 4 T-test with p-value ≤ 0.05</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their in-sightful comments. This work was partially sup-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ranking with recursive neural networks and its application to multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Left-brain/right-brain multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">D</forename><surname>John M Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne</forename><forename type="middle">P</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DUC</title>
		<meeting>DUC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lexrank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A skip-chain conditional random field for ranking meeting utterances by importance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="364" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving the estimation of word importance for news multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-document summarization using support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DUC</title>
		<meeting>DUC</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop</title>
		<meeting>the ACL Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using maximum entropy for sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Automatic Summarization</title>
		<meeting>ACL Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion publication of the 23rd international conference on World wide web companion</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The psychological meaning of words: Liwc and computerized text analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Tausczik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of language and social psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="54" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-document summarization using cluster-based link analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ctsum: extracting more certain summaries for news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
