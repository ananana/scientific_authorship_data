<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Opinion Targets and Opinion Words from Online Reviews with Graph Co-ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Opinion Targets and Opinion Words from Online Reviews with Graph Co-ranking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="314" to="324"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Extracting opinion targets and opinion words from online reviews are two fundamental tasks in opinion mining. This paper proposes a novel approach to collectively extract them with graph co-ranking. First, compared to previous methods which solely employed opinion relations among words, our method constructs a heterogeneous graph to model two types of relations, including semantic relations and opinion relations. Next, a co-ranking algorithm is proposed to estimate the confidence of each candidate, and the candidates with higher confidence will be extracted as opinion targets/words. In this way, different relations make cooperative effects on candidates&apos; confidence estimation. Moreover, word preference is captured and incorporated into our co-ranking algorithm. In this way, our co-ranking is personalized and each candi-date&apos;s confidence is only determined by its preferred collocations. It helps to improve the extraction precision. The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In opinion mining, extracting opinion targets and opinion words are two fundamental subtasks. Opinion targets are objects about which users' opinions are expressed, and opinion words are words which indicate opinions' polarities. Ex- tracting them can provide essential information for obtaining fine-grained analysis on customers' opinions. Thus, it has attracted a lot of attentions (Hu and Liu, 2004b; <ref type="bibr" target="#b12">Moghaddam and Ester, 2011;</ref><ref type="bibr" target="#b16">Mukherjee and Liu, 2012)</ref>.</p><p>To this end, previous work usually employed a collective extraction strategy ( <ref type="bibr" target="#b18">Qiu et al., 2009</ref>; Hu and Liu, 2004b; <ref type="bibr" target="#b10">Liu et al., 2013b</ref>). Their intuition is: opinion words usually co-occur with opinion targets in sentences, and there are strong modifi- cation relationship between them (called opinion relation in ( ). If a word is an opinion word, other words with which that word having opinion relations will have highly proba- bility to be opinion targets, and vice versa. In this way, extraction is alternatively performed and mu- tual reinforced between opinion targets and opin- ion words. Although this strategy has been widely employed by previous approaches, it still has sev- eral limitations.</p><p>1) Only considering opinion relations is in- sufficient. Previous methods mainly focused on employing opinion relations among words for opinion target/word co-extraction. They have in- vestigated a series of techniques to enhance opin- ion relations identification performance, such as nearest neighbor rules ( <ref type="bibr" target="#b7">Liu et al., 2005</ref>), syntactic patterns ( <ref type="bibr" target="#b17">Popescu and Etzioni, 2005)</ref>, word alignment models ( <ref type="bibr" target="#b10">Liu et al., 2013b;</ref><ref type="bibr" target="#b9">Liu et al., 2013a</ref>), etc. How- ever, we are curious that whether merely employ- ing opinion relations among words is enough for opinion target/word extraction? We note that there are additional types of relations among words. For example, "LCD" and "LED" both denote the same aspect "screen" in TV set domain, and they are topical related. We call such relations between homogeneous words as semantic relations. If we have known "LCD" to be an opinion target, "LED" is naturally to be an opinion target. Intuitively, besides opinion relations, semantic relations may provide additional rich clues for indicating opin- ion targets/words. Which kind of relations is more effective for opinion targets/words extraction? Is it beneficial to consider these two types of relations together for the extraction? To our best knowl-edge, these problems have seldom been studied before (see Section 2).</p><p>2) Ignoring word preference. When employ- ing opinion relations to perform mutual reinforc- ing extraction between opinion targets and opin- ion words, previous methods depended on opin- ion associations among words, but seldom consid- ered word preference. Word preference denotes a word's preferred collocations. Intuitively, the confidence of a candidate being an opinion tar- get (opinion word) should mostly be determined by its word preferences rather than all words hav- ing opinion relations with it. For example "This camera's price is expensive for me." "It's price is good." "Canon 40D has a good price." In these three sentences, "price" is modified by "good" more times than "expensive". In tradi- tional extraction strategy, opinion associations are usually computed based on the co-occurrence fre- quency. Thus, "good" has more strong opinion association with "price" than "expensive", and it would have more contributions on determining "price" to be an opinion target or not. It's un- reasonable. "Expensive" actually has more re- latedness with "price" than "good", and "expen- sive" is likely to be a word preference for "price". The confidence of "price" being an opinion target should be influenced by "expensive" in greater ex- tent than "good". In this way, we argue that the extraction will be more precise.</p><p>í µí±í µí±í µí±í µí± <ref type="bibr">4</ref> í µí±í µí±í µí±í µí± 6 í µí±í µí±í µí±í µí± 5 í µí±í µí±í µí±í µí± <ref type="bibr">1</ref> í µí±í µí±í µí±í µí± 3 í µí±í µí±í µí±í µí± 2 í µí±í µí±í µí±í µí± <ref type="bibr">2</ref> í µí±í µí±í µí±í µí± 4 í µí±í µí±í µí±í µí± <ref type="bibr">3</ref> í µí±í µí±í µí±í µí± 5 í µí±í µí±í µí±í µí± 6 í µí±í µí±í µí±í µí± 1 í µí°ºí µí°º í µí±¡í µí±¡í µí±¡í µí±¡ í µí°ºí µí°º í µí±í µí±í µí±í µí± í µí°ºí µí°º í µí±¡í µí±¡í µí±í µí±</p><p>Figure 1: Heterogeneous Graph: OC means opin- ion word candidates. T C means opinion target candidates. Solid curves and dotted lines respec- tively mean semantic relations and opinion rela- tions between two candidates.</p><p>Thus, to resolve these two problems, we present a novel approach with graph co-ranking. The col- lective extraction of opinion targets/words is per- formed in a co-ranking process. First, we oper- ate over a heterogeneous graph to model seman- tic relations and opinion relations into a unified model. Specifically, our heterogeneous graph is composed of three subgraphs which model differ- ent relation types and candidates, as shown in <ref type="figure">Fig- ure 1</ref>. The first subgraph G tt represents semantic relations among opinion target candidates, and the second subgraph G oo models semantic relations among opinion word candidates. The third part is a bipartite subgraph G to , which models opinion relations among different candidate types and con- nects the above two subgraphs together. Then we perform a random walk algorithm on G tt , G oo and G to separately, to estimate all candidates' confi- dence, and the entries with higher confidence than a threshold are correspondingly extracted as opin- ion targets/words. The results could reflect which type of relation is more useful for the extraction.</p><p>Second, a co-ranking algorithm, which incor- porates three separate random walks on G tt , G oo and G to into a unified process, is proposed to perform candidate confidence estimation. Differ- ent relations may cooperatively affect candidate confidence estimation and generate more global ranking results. Moreover, we discover each can- didate's preferences through topics. Such word preference will be different for different candi- dates. We add word preference information into our algorithm and make our co-ranking algorithm be personalized. A candidate's confidence would mainly absorb the contributions from its word preferences rather than its all neighbors with opin- ion relations, which may be beneficial for improv- ing extraction precision.</p><p>We perform experiments on real-world datasets from different languages and different domains. Results show that our approach effectively im- proves extraction performance compared to the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are many significant research efforts on opinion targets/words extraction (sentence level and corpus level). In sentence level extraction, previous methods ( <ref type="bibr" target="#b23">Wu et al., 2009;</ref><ref type="bibr" target="#b11">Ma and Wan, 2010;</ref><ref type="bibr" target="#b24">Yang and Cardie, 2013)</ref> mainly aimed to identify all opinion target/word mentions in sentences. They regarded it as a se- quence labeling task, where several classical mod- els were used, such as CRFs ( ) and SVM ( <ref type="bibr" target="#b23">Wu et al., 2009)</ref>. This paper belongs to corpus level extraction, and aims to generate a sentiment lexicon and a target list rather than to identify mentions in sen-tences. Most of previous corpus-level methods adopted a co-extraction framework, where opin- ion targets and opinion words reinforce each other according to their opinion relations. Thus, how to improve opinion relations identification perfor- mance was their main focus. (Hu and Liu, 2004a) exploited nearest neighbor rules to mine opinion relations among words. ( <ref type="bibr" target="#b17">Popescu and Etzioni, 2005</ref>) and ( <ref type="bibr" target="#b19">Qiu et al., 2011</ref>) designed syntactic patterns to perform this task. ( ) promoted Qiu's method. They adopted some spe- cial designed patterns to increase recall. ( <ref type="bibr" target="#b9">Liu et al., 2013a;</ref><ref type="bibr" target="#b10">Liu et al., 2013b</ref>) em- ployed word alignment model to capture opinion relations rather than syntactic parsing. The exper- imental results showed that these alignment-based methods are more effective than syntax-based ap- proaches for online informal texts. However, all aforementioned methods only employed opinion relations for the extraction, but ignore consider- ing semantic relations among homogeneous can- didates. Moreover, they all ignored word prefer- ence in the extraction process.</p><p>In terms of considering semantic relations among words, our method is related with sev- eral approaches based on topic model ( <ref type="bibr" target="#b26">Zhao et al., 2010;</ref><ref type="bibr" target="#b12">Moghaddam and Ester, 2011;</ref><ref type="bibr" target="#b13">Moghaddam and Ester, 2012a;</ref><ref type="bibr" target="#b15">Moghaddam and Ester, 2012b;</ref><ref type="bibr" target="#b16">Mukherjee and Liu, 2012)</ref>. The main goals of these methods weren't to extract opin- ion targets/words, but to categorize all given as- pect terms and sentiment words. Although these models could be used for our task according to the associations between candidates and topics, solely employing semantic relations is still one-sided and insufficient to obtain expected performance.</p><p>Furthermore, there is little work which consid- ered these two types of relations globally ( <ref type="bibr" target="#b20">Su et al., 2008;</ref><ref type="bibr" target="#b1">Hai et al., 2012;</ref><ref type="bibr" target="#b0">Bross and Ehrig, 2013)</ref>. They usually captured different relations using co- occurrence information. That was too coarse to obtain expected results ( ). In ad- dition, <ref type="bibr" target="#b1">(Hai et al., 2012</ref>) extracted opinion tar- gets/words in a bootstrapping process, which had an error propagation problem. In contrast, we per- form extraction with a global graph co-ranking process, where error propagation can be effec- tively alleviated. ( <ref type="bibr" target="#b20">Su et al., 2008</ref>) used heteroge- neous relations to find implicit sentiment associ- ations among words. Their aim was only to per- form aspect terms categorization but not to extract opinion targets/words. They extracted opinion tar- gets/words in advanced through simple phrase de- tection. Thus, the extraction performance is far from expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>In this section, we propose our method in detail. We formulate opinion targets/words extraction as a co-ranking task. All nouns/noun phrases are re- garded as opinion target candidates, and all ad- jectives/verbs are regarded as opinion word candi- dates, which are widely adopted by pervious meth- ods ( <ref type="bibr" target="#b3">Hu and Liu, 2004a;</ref><ref type="bibr" target="#b19">Qiu et al., 2011;</ref><ref type="bibr" target="#b21">Wang and Wang, 2008;</ref>). Then each can- didate will be assigned a confidence and ranked, and the candidates with higher confidence than a threshold will be extracted as the results.</p><p>Different from traditional methods, besides opinion relations among words, we additionally capture semantic relations among homogeneous candidates. To this end, a heterogeneous undi- rected graph G = (V, E) is constructed. V = V t ∪ V o denotes the vertex set, which includes opinion target candidates v t ∈ V t and opinion word candidates v o ∈ V o . E denotes the edge set, where e ij ∈ E means that there is a relation between two vertices. E tt ⊂ E represents the se- mantic relations between two opinion target candi- dates. E oo ⊂ E represents the semantic relations between two opinion word candidates. E to ⊂ E represents the opinion relations between opinion target candidates and opinion word candidates. Based on different relation types, we used three matrices M tt ∈ R |V t |×|V t | , M oo ∈ R |V o |×|V o | and M to ∈ R |V t |×|V o | to record the association weights between any two vertices, respectively. Section 3.4 will illustrate how to construct them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Only Considering Opinion Relations</head><p>To estimate the confidence of each candidate, we use a random walk algorithm on our graph to per- form co-ranking. Most previous methods (Hu and Liu, 2004a; Qiu et al., 2011; <ref type="bibr" target="#b21">Wang and Wang, 2008;</ref>) only considered opinion relations among words. Their basic assumption is as follows.</p><p>Assumption 1: If a word is likely to be an opinion word, the words which it has opinion relation with will have higher confidence to be opinion targets, and vice versa.</p><p>In this way, candidates' confidences (v t or v o ) are collectively determined by each other iteratively. It equals to making random walk on subgraph G to = (V, E to ) of G. Thus we have</p><formula xml:id="formula_0">C t = (1 − µ) × M to × C o + µ × I t C o = (1 − µ) × M T to × C t + µ × I o (1)</formula><p>where C t and C o respectively represent confi- dences of opinion targets and opinion words. m to i,j ∈ M to means the association weight between the ith opinion target and the jth opinion word ac- cording to their opinion relations.</p><p>It's worthy noting that I t and I o respectively de- note prior confidences of opinion target candidates and opinion word candidates. We argue that opin- ion targets are usually domain-specific, and there are remarkably distribution difference of them on</p><formula xml:id="formula_1">different domains (in-domain D in vs. out-domain D out ). If a candidate is salient in D in but common in D out , it's likely to be an opinion target in D in .</formula><p>Thus, we use a domain relevance measure (DR) <ref type="bibr" target="#b2">(Hai et al., 2013</ref>) to compute I t .</p><formula xml:id="formula_2">DR(t) = R(t, D in ) R(t, D out )<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">R(t, D) = ¯ wt st × N j=1 (w tj − 1 W j × W j k=1 w kj ) represents candidate relevance with domain D. w tj = (1 + logT F tj ) × log N DFt</formula><p>is a TF-IDF-like weight of candidate t in doc- ument j. T F tj is the frequency of the candi- date t in the jth document, and DF t is docu- ment frequency. N means the document num- ber in domain D. R(t, D) includes two mea- sures to reflect the salient of a candidate in D. 1)</p><formula xml:id="formula_4">w tj − 1 W j × W j k=1</formula><p>w kj reflects how frequently a term is mentioned in a particular document. W j denotes the word number in document j. 2) ¯ wt st quantifies how significantly a term is mentioned across all documents in D.</p><formula xml:id="formula_5">¯ w t = 1 N × N k=1</formula><p>w tk denotes average weight across all documents for</p><formula xml:id="formula_6">t. s t = 1 N × N j=1 (w tj − ¯ w j ) 2</formula><p>denotes the standard variance of term t. We use the given reviews as in-domain collection D in and Google n-gram corpus 1 as out-domain collection D out . Finally, each entry in I t is a normalized DR(t) score. In contrast, opinion words are usually domain-independent. Users may use same words to express theirs opinions, like "good", "bad", etc. But there are still some domain-dependent opinion <ref type="bibr">1</ref> http://books.google.com/ngrams/datasets words, like "delicious" in the restaurant domain, "powerful" in the car domain. It's difficult to dis- criminate them from other words by using statisti- cal information. So we simply set all entries in I o to be 1. µ ∈ [0, 1] in Eq.1 determines the impact of the prior confidence on results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Only Considering Semantic Relations</head><p>To estimate candidates' confidences by only con- sidering semantic relations among words, we make two separately random walks on the sub- graphs of G, G tt = (V, E tt ) and G oo = (V, E oo ). The basic assumption is as follows:</p><p>Assumption 2: If a word is likely to be an opinion target (opinion word), the words which it has strong semantic rela- tion with will have higher confidence to be opinion targets (opinion words).</p><p>In this way, the confidence of the candidate is determined only by its homogeneous neighbours. There is no mutual reinforcement between opinion targets and opinion words. Thus we have</p><formula xml:id="formula_7">C t = (1 − ν) × M tt × C t + ν × I t C o = (1 − ν) × M oo × C o + ν × I o (3)</formula><p>where ν has the same role as µ in Eq.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Considering Semantic Relations and Opinion Relations Together</head><p>To jointly model semantic relations and opinion relations for opinion targets/words extraction, we couple two random walking algorithms mentioned above together. Here, Assumption 1 and As- sumption 2 are both satisfied. Thus, an opinion target/word candidate's confidence is collectively determined by its neighbours according to differ- ent relation types. Meanwhile, each item may make influence on it's neighbours. It's an iterative reinforcement process. Thus, we have</p><formula xml:id="formula_8">C t = (1 − λ − µ) × M to × C o + λ × M tt × C t + µ × I t C o = (1 − λ − µ) × M T to × C t + λ × M oo × C o + µ × I o (4)</formula><p>where λ ∈ [0, 1] determines which type of rela- tions dominates candidate confidence estimation. λ = 0 means that each candidate's confidence is estimated by only considering opinion relations among words, which equals to Eq.1. Otherwise, when λ = 1, candidate confidence estimation only considers semantic relations among words, which equals to Eq.3. µ, I o and I t have the same meaning in Eq.1. Our algorithm will run iteratively until it converges or in a fixed iteration number Iter. In experiments, we set Iter = 200.</p><p>Obtaining</p><note type="other">Word Preference. The co-ranking algorithm in Eq.4 is based on a standard random walking algorithm, which randomly selects a link according to the association matrix M to , M tt and M oo , or jumps to a random node with prior confi- dence value. However, it generates a global rank- ing over all candidates without taking the node preference (word preference) into account. As mentioned in the first section, each opinion tar- get/word has its preferred collocations, it's reason- able that the confidence of an opinion target (opin- ion word) candidate should be preferentially de- termined by its preferences, rather than all of its neighbors with opinion relations.</note><p>To obtain the word preference, we resort to top- ics. We believe that if an opinion word v i o is topical related with a target word v j t , v i o can be regarded as a word preference for v j t , and vice versa. For example, "price" and "expensive" are topically related in phone's domain, so they are a word preference for each other.</p><p>Specifically, we use a vector</p><formula xml:id="formula_9">P T i = [P T i 1 , ..., P T i k , ..., P T i |V o | ] 1×|V o |</formula><p>to represent word preference of the ith opinion target candidate. P T i k means the preferred probability of the ith potential opinion target for the kth potential opinion words. To compute P T i k , we first use Kullback-Leibler divergence to measure the semantic distance between any two candidates on the bridge of topics. Thus, we have</p><formula xml:id="formula_10">D(v i , v j ) = 1 2 Σ z (KL z (v i ||v j ) + KL z (v j ||v i ))</formula><p>where</p><formula xml:id="formula_11">KL z (v i ||v j ) = p(z|v i )log p(z|v i ) p(z|v j ) means the KL-divergence from candidate v i to v j based on topic z. p(z|v) = p(v|z) p(z) p(v)</formula><p>, where p(v|z) is the probability of the candidate v to topic z (see Sec- tion 3.4). p(z) is the probability that topic z in reviews. p(v) is the probability that a candidate occurs in reviews. Then, a logistic function is used to map D(v i , v j ) into <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><formula xml:id="formula_12">SA(v i , v j ) = 1 1 + e D(v i ,v j )<label>(5)</label></formula><p>Then, we calculate</p><formula xml:id="formula_13">P T i k by normalize SA(v i , v j ) score, i.e. P T i k = SA(v t i ,v o k ) |V o | p=1 SA(v t i ,v o p )</formula><p>. For demon- stration, we give some examples in <ref type="table">Table 1</ref>, where each entry denotes a SA(v i , v j ) score between two candidates. We can see that using topics can suc- cessfully capture the preference information for each opinion target/word. to represent the preference information of the jth opin- ion word candidate.</p><p>Similarly, we have</p><formula xml:id="formula_14">P O j q = SA(v t q ,v o j ) |V t | k=1 SA(v t k ,v o j )</formula><p>.</p><p>Incorporating Word Preference into Co- ranking. To consider such word preference in our co-ranking algorithm, we incorporate it into the random walking on G to . Intuitively, prefer- ence vectors will be different for different can- didates. Thus, the co-ranking algorithm would be personalized. It allows that the candidate confidence propagates to other candidates only in its preference cluster. Specifically, we make modification on original transition matrix M to = (M to 1 , M to 2 , ..., M to |V t | ) and add each candidate's preference in it. LetˆMLetˆ LetˆM to = ( ˆ M to 1 , ˆ M to 2 , ..., ˆ M to |V t | ) be the modified transition matrix, which records the associations between opinion target candi- dates and opinion word candidates. Here M to k ∈ R 1×|V o | andˆMandˆ andˆM to k ∈ R 1×|V o | denotes the kth col- umn vector in M to andˆMandˆ andˆM to , respectively. And let Diag(P T k ) denote a diagonal matrix whose eigenvalue is vector P T k , we havê</p><formula xml:id="formula_15">havê M to k = M to k Diag(P T k )</formula><p>Similarly, let U to k ∈ R 1×|V t | andˆUandˆ andˆU to k ∈ R 1×|V t | denotes the kth row vector in M T to andˆMandˆ andˆM T to , re- spectively. Diag(P O k ) denote a diagonal matrix whose eigenvalue is vector P O k . Then we havê</p><formula xml:id="formula_15">havê M to k = M to k Diag(P T k )</formula><p>In this way, each candidate's preference is in- corporated into original associations based on opinion relation M to through Diag(P O k ) and Diag(P T k ). And candidates' confidences will mainly come from the contributions of its prefer- ences. Thus, C t and C o in Eq.4 become:</p><formula xml:id="formula_17">C t = (1 − λ − µ) × ˆ M to × C o + λ × M tt × C t + µ × I t C o = (1 − λ − µ) × ˆ M T to × C t + λ × M oo × C o + µ × I o (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Capturing Semantic and Opinion Relations</head><p>In this section, we explain how to capture seman- tic relations and opinion relations for constructing transition matrices M tt , M oo and M to . Capturing Semantic Relations: For captur- ing semantic relations among homogenous candi- dates, we employ topics. We believe that if two candidates share similar topics in the corpus, there is a strong semantic relation between them. Thus, we employ a LDA variation <ref type="bibr" target="#b16">(Mukherjee and Liu, 2012)</ref>, an extension of ( <ref type="bibr" target="#b26">Zhao et al., 2010)</ref>, to dis- cover topic distribution on words, which sampled all words into two separated observations: opinion targets and opinion words. It's because that we are only interested in topic distribution of opinion tar- gets/words, regardless of other useless words, in- cluding conjunctions, prepositions etc. This model has been proven to be better than the standard LDA model and other LDA variations for opinion mining <ref type="bibr" target="#b16">(Mukherjee and Liu, 2012)</ref>.</p><p>After topic modeling, we obtain the proba- bility of the candidates (v t and v o ) to topic z, i.e. p(z|v t ) and p(z|v o ), and topic distribution p(z). Then, a symmetric Kullback-Leibler diver- gence as same as Eq.5 is used to calculate the se- mantical associations between any two homoge- nous candidates. Thus, we obtain SA(v t , v t ) and SA(v o , v o ), which correspond to the entries in M tt and M oo , respectively.</p><p>Capturing Opinion Relations: To capture opinion relations among words and construct the transition matrix M to , we used an alignment- based method proposed in ( <ref type="bibr" target="#b10">Liu et al., 2013b)</ref>. This approach models capturing opinion relations as a monolingual word alignment process. Each opinion target can find its corresponding mod- ifiers in sentences through alignment, in which multiple factors are considered globally, such as co-occurrence information, word position in sen- tence, etc. Moreover, this model adopted a par- tially supervised framework to combine syntac- tic information with alignment results, which has been proven to be more precise than the state-of- the-art approaches for opinion relations identifica- tion ( <ref type="bibr" target="#b10">Liu et al., 2013b</ref>).</p><p>After performing word alignment, we obtain a set of word pairs composed of a noun (noun phrase) and its corresponding modified word. Then, we simply employ Pointwise Mutual Infor- mation (PMI) to calculate the opinion associations among words as the entries in</p><formula xml:id="formula_18">M to . OA(v t , v o ) = log p(v t ,v o ) p(v t )p(v o )</formula><p>, where v t and v o denote an opinion target candidate and an opinion word candidate, respectively. p(v t , v o ) is the co-occurrence prob- ability of v t and v o based on the opinion relation identification results. p(v t ) and p(v o ) give the in- dependent occurrence probability of of v t and v o , respectively</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Datasets: To evaluate the proposed method, we used three datasets. The first one is Customer Review Datasets (CRD), used in (Hu and Liu, 2004a), which contains reviews about five prod- ucts. The second one is COAE2008 dataset2 2 , which contains Chinese reviews about four prod- ucts. The third one is Large, also used in ( <ref type="bibr" target="#b22">Wang et al., 2011;</ref><ref type="bibr" target="#b9">Liu et al., 2013a)</ref>, where two domains are selected (Mp3 and Hotel). As mentioned in ( ), Large con- tains 6,000 sentences for each domain. Opinion targets/words are manually annotated, where three annotators were involved. Two annotators were required to annotate out opinion words/targets in reviews. When conflicts occur, the third annota- tor make final judgement. In total, we respectively obtain 1,112, 1,241 opinion targets and 334, 407 opinion words in Hotel, MP3.</p><p>Pre-processing: All sentences are tagged to obtain words' part-of-speech tags using Stanford NLP tool 3 . And noun phrases are identified using the method in ( <ref type="bibr" target="#b27">Zhu et al., 2009</ref>) before extraction.</p><p>Evaluation Metrics: We select precision(P), recall(R) and f-measure(F) as metrics. And a sig- nificant test is performed, i.e., a t-test with a de- fault significant level of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Our Method vs. The State-of-the-art Methods</head><p>To prove the effectiveness of the proposed method, we select some state-of-the-art methods for com- parison as follows:   Hu extracted opinion targets/words using asso- ciation mining rules ( <ref type="bibr" target="#b3">Hu and Liu, 2004a)</ref>.</p><formula xml:id="formula_19">Methods D1 D2 D3 D4 D5 Avg. P R F P R F P R F P R F P R F F</formula><p>DP used syntax-based patterns to capture opin- ion relations in sentences, and then used a boot- strapping process to extract opinion targets/words (Qiu et al., 2011),.</p><p>Zhang is proposed by ). They also used syntactic patterns to capture opin- ion relations between words. Then a HITS <ref type="bibr" target="#b5">(Kleinberg, 1999</ref>) algorithm is employed to extract opin- ion targets.</p><p>Liu is proposed by ( <ref type="bibr" target="#b9">Liu et al., 2013a</ref>), an ex- tension of ( ). They employed a word alignment model to capture opinion relations among words, and then used a random walking al- gorithm to extract opinion targets.</p><p>Hai is proposed by <ref type="bibr" target="#b1">(Hai et al., 2012)</ref>, which is similar to our method. They employed both of se- mantic relations and opinion relations to extract opinion words/targets in a bootstrapping frame- work. But they captured relations only using co- occurrence statistics. Moreover, word preference was not considered.</p><p>SAS is proposed by (Mukherjee and Liu, 2012), an extended lda-based model of ( <ref type="bibr" target="#b26">Zhao et al., 2010</ref>). The top K items for each aspect are ex- tracted as opinion targets/words. It means that only semantic relations among words are consid- ered in SAS. And we set aspects number to be 9 as same as <ref type="bibr" target="#b16">(Mukherjee and Liu, 2012)</ref>.</p><p>CR: is the proposed method in this paper by us- ing co-ranking, referring to Eq.4. CR doesn't con- sider word preference.</p><p>CR WP: is the full implementation of our method, referring to Eq.6.</p><p>Hu, DP, Zhang and Liu are the methods which only consider opinion relations among words. SAS is the methods which only consider seman- tic relations among words. Hai, CR and CR WP consider these two types of relations together. The parameter settings of state-of-the-art methods are same as their original paper. In CR and CR WP, we set λ = 0.4 and µ = 0.1. The experimental results are shown in <ref type="table" target="#tab_2">Table 2</ref>, 3, 4 and 5, where the last column presents the average F-measure scores for multiple domains. Since Liu and Zhang aren't designed for opinion words extraction, we don't present their results in <ref type="table" target="#tab_5">Table 4</ref> and 5. From exper- imental results, we can see.</p><p>1) Our methods (CR and CR WP) outperform other methods not only on opinion targets extrac- tion but on opinion words extraction in most do- mains. It proves the effectiveness of the proposed method.</p><p>2) CR and CR WP have much better perfor- mance than Liu and Zhang, especially on Recall. Liu and Zhang also use a ranking framework like ours, but they only employ opinion relations for extraction. In contrast, besides opinion relations, CR and CR WP further take semantic relations into account. Thus, more opinion targets/words can be extracted. Furthermore, we observe that CR and CR WP outperform SAS. SAS only ex- ploits semantic relations, but ignores opinion re- lations among words. Its extraction is performed separately and neglects the reinforcement between opinion targets and opinion words. Thus, SAS has worse performance than our methods. It demon- strates the usefulness of considering multiple rela- tion types.   3) CR and CR WP both outperform Hai. We believe the reasons are as follows. First, CR and CR WP considers multiple relations in a unified process by using graph co-ranking. In contrast, Hai adopts a bootstrapping framework which per- forms extraction step by step and may have the problem of error propagation. It demonstrates that our graph co-ranking is more suitable for this task than bootstrapping-based strategy. Second, our method captures semantic relations using topic modeling and captures opinion relations through word alignments, which are more precise than Hai which merely uses co-occurrence information to indicate such relations among words. In addition, word preference is not handled in Hai, but pro- cessed in CR WP. The results show the usefulness of word preference for opinion targets/words ex- traction.</p><formula xml:id="formula_20">Methods D1 D2 D3 D4 D5 Avg. P R F P R F P R F P R F P R F F Hu 0.57</formula><p>4) CR WP outperforms CR, especially on pre- cision. The only difference between them is that CR WP considers word preference when perform- ing graph ranking for candidate confidence esti- mation, but CR does not. Each candidate confi- dence estimation in CR WP gives more weights for this candidate's preferred words than CR. Thus, the precision can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Relation vs. Opinion Relation</head><p>In this section, we discuss which relation type is more effective for this task. For comparison, we design two baselines, called OnlySA and On- lyOA. OnlyOA only employs opinion relations among words, which equals to Eq.1. OnlySA only employs semantic relations among words, which equals to Eq.3. Moreover, Combine is our method which considers both of opinion relations and se- mantic relations together, referring to Eq.4 with  From results, we observe that OnlyOA outper- forms OnlySA in all domains. It demonstrates that employing opinion relations are more useful than semantic relations for co-extracting opinion targets/words. And it is necessary to utilize the mutual reinforcement relationship between opin- ion words and opinion targets. Moreover, Com- bine outperforms OnlySA and OnlyOA in all do- mains. It indicates that combining different rela- tions among words together is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Effectiveness of Considering Word Preference</head><p>In this section, we try to prove the necessity of considering word preference in Eq.6. Besides the comparison between CR and CR WP performed in the main experiment in Section 4.2, we fur- ther incorporate word preference in aforemen- tioned OnlyOA, named as OnlyOA WP, which only employs opinion relations among words and equals to Eq.6 with λ = 0. Experimental results are shown in <ref type="figure">Figure 3</ref>. Because of space limita- tion, we only show the results of the same domains in section 4.3, Form results, we observe that CR WP out- performs CR, and OnlyOA WP outperforms On- lyOA in all domains, especially on precision. These observations demonstrate that considering word preference is very important for opinion tar- gets/words extraction. We believe the reason is that exploiting word preference can provide more fine information for opinion target/word candi- dates' confidence estimation. Thus the perfor- mance can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MP3</head><p>Hotel  <ref type="figure">Figure 3</ref>: Experimental results when considering word preference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Sensitivity</head><p>In this subsection, we discuss the variation of ex- traction performance when changing λ and µ in Eq.6. Due to space limitation, we only show the F-measure of CR WP on four domains. Experi- mental results are shown in <ref type="figure">Figure 4</ref> and <ref type="figure" target="#fig_1">Figure  5</ref>. The left graphs in <ref type="figure">Figure 4</ref> and 5 present the performance variation of CR WP with varying λ from 0 to 0.9 and fixing µ = 0.1. The right graphs in <ref type="figure">Figure 4</ref> and 5 present the performance varia- tion of CR WP with varying µ from 0 to 0.6 and fixing λ = 0.4.</p><p>In the left graphs in <ref type="figure">Figure 4</ref> and 5, we observe the best performance is obtained when λ = 0.4. It indicates that opinion relations and semantic re- lations are both useful for extracting opinion tar- gets/words. The extraction performance is benefi- cial from their combination. In the right graphs in <ref type="figure">Figure 4</ref> and 5, the best performance is obtained when µ = 0.1. It indicates prior knowledge is useful for extraction. When µ increases, perfor- mance, however, decreases. It demonstrates that incorporating more prior knowledge into our al- gorithm would restrain other useful clues on esti- mating candidate confidence, and hurt the perfor- mance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presents a novel method with graph co- ranking to co-extract opinion targets/words. We model extracting opinion targets/words as a co- ranking process, where multiple heterogenous re- lations are modeled in a unified model to make co- operative effects on the extraction. In addition, we especially consider word preference in co-ranking process to perform more precise extraction. Com- pared to the state-of-the-art methods, experimental results prove the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Semantic Relations vs. Opinion Relations λ = 0.5. Figure 2 presents experimental results. The left graph presents opinion targets extraction results and the right graph presents opinion words extraction results. Because of space limitation, we only shown the results of four domains (MP3, Hotel, Laptop and Phone). From results, we observe that OnlyOA outperforms OnlySA in all domains. It demonstrates that employing opinion relations are more useful than semantic relations for co-extracting opinion targets/words. And it is necessary to utilize the mutual reinforcement relationship between opinion words and opinion targets. Moreover, Combine outperforms OnlySA and OnlyOA in all domains. It indicates that combining different relations among words together is effective.</figDesc><graphic url="image-2.png" coords="8,315.50,341.90,99.22,76.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Opinion words extraction results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Results of Opinion Targets Extraction on Customer Review Dataset</head><label>2</label><figDesc></figDesc><table>Methods 
Camera 
Car 
Laptop 
Phone 
Mp3 
Hotel 
Avg. 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 
F 
Hu 
0.63 
0.65 
0.64 
0.62 
0.58 
0.60 
0.51 
0.67 
0.58 
0.69 
0.60 
0.64 
0.61 
0.68 
0.64 
0.60 
0.65 
0.62 
0.587 
DP 
0.71 
0.70 
0.70 
0.72 
0.65 
0.68 
0.58 
0.69 
0.63 
0.78 
0.66 
0.72 
0.69 
0.70 
0.69 
0.67 
0.69 
0.68 
0.683 
Zhang 
0.71 
0.78 
0.74 
0.69 
0.68 
0.68 
0.57 
0.80 
0.67 
0.80 
0.71 
0.75 
0.67 
0.77 
0.72 
0.67 
0.76 
0.71 
0.712 
SAS 
0.72 
0.72 
0.72 
0.71 
0.64 
0.67 
0.59 
0.72 
0.65 
0.78 
0.69 
0.73 
0.69 
0.75 
0.72 
0.69 
0.74 
0.71 
0.700 
Liu 
0.75 
0.81 
0.78 
0.71 
0.71 
0.71 
0.61 
0.85 
0.71 
0.83 
0.74 
0.78 
0.70 
0.82 
0.76 
0.71 
0.80 
0.75 
0.749 
Hai 
0.68 
0.84 
0.76 
0.69 
0.75 
0.72 
0.58 
0.86 
0.72 
0.75 
0.76 
0.76 
0.65 
0.83 
0.74 
0.62 
0.82 
0.75 
0.742 
CR 
0.75 
0.83 
0.79 
0.72 
0.74 
0.73 
0.60 
0.85 
0.70 
0.83 
0.77 
0.80 
0.70 
0.84 
0.76 
0.71 
0.83 
0.77 
0.758 
CR WP 
0.78 
0.84 
0.81 
0.74 
0.75 
0.74 
0.64 
0.85 
0.73 
0.84 
0.76 
0.80 
0.74 
0.84 
0.79 
0.74 
0.82 
0.78 
0.773 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Results of Opinion Targets Extraction on COAE 2008 and Large</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Results of Opinion Words Extraction on Customer Review Dataset</head><label>4</label><figDesc></figDesc><table>Methods 
Camera 
Car 
Laptop 
Phone 
Mp3 
Hotel 
Avg. 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 
P 
R 
F 
F 
Hu 
0.72 
0.74 
0.73 
0.70 
0.71 
0.70 
0.66 
0.70 
0.68 
0.70 
0.70 
0.70 
0.48 
0.67 
0.56 
0.52 
0.69 
0.59 
0.660 
DP 
0.80 
0.73 
0.76 
0.79 
0.71 
0.75 
0.75 
0.69 
0.72 
0.78 
0.68 
0.73 
0.60 
0.65 
0.62 
0.61 
0.66 
0.63 
0.702 
SAS 
0.73 
0.70 
0.71 
0.75 
0.68 
0.71 
0.72 
0.68 
0.69 
0.71 
0.66 
0.68 
0.64 
0.62 
0.63 
0.66 
0.61 
0.63 
0.675 
Hai 
0.76 
0.74 
0.75 
0.72 
0.74 
0.73 
0.69 
0.72 
0.70 
0.72 
0.70 
0.71 
0.61 
0.69 
0.64 
0.59 
0.68 
0.64 
0.690 
CR 
0.80 
0.75 
0.77 
0.77 
0.74 
0.75 
0.73 
0.71 
0.72 
0.75 
0.71 
0.73 
0.63 
0.69 
0.64 
0.63 
0.68 
0.66 
0.710 
CR WP 
0.80 
0.75 
0.77 
0.80 
0.74 
0.77 
0.77 
0.71 
0.74 
0.78 
0.72 
0.75 
0.66 
0.68 
0.67 
0.67 
0.69 
0.68 
0.730 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 : Results of Opinion Words Extraction on COAE 2008 and Large</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> http://ir-china.org.cn/coae2008.html 3 http://nlp.stanford.edu/software/tagger.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic construction of domain and aspect specific sentiment 322 lexicons for customer review mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Bross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Ehrig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp;#38; knowledge management, CIKM &apos;13</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp;#38; knowledge management, CIKM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One seed to find them all: mining opinion features via association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying features in opinion mining via intrinsic and extrinsic domain relevance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
			<publisher>PrePrints</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mining opinion features in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Artificial Intelligence (AAAI)</title>
		<meeting>Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;04</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure-aware review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<editor>Chu-Ren Huang and Dan Jurafsky</editor>
		<imprint>
			<publisher>Tsinghua University Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="653" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Opinion observer: analyzing and comparing opinions on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Cheng</surname></persName>
		</author>
		<editor>Allan Ellis and Tatsuya Hagino</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Opinion target extraction using word-based translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1346" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Opinion target extraction using partially supervised word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Syntactic patterns versus word alignment: Extracting opinion targets from online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinion target extraction in chinese news comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<editor>ChuRen Huang and Dan Jurafsky</editor>
		<imprint>
			<publisher>Chinese Information Processing Society of China</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="782" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ilda: Interdependent lda model for learning latent aspects and their ratings from online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aspect-based opinion mining from product reviews</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12</title>
		<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1184" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the design of lda models for aspect-based opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Expanding domain sentiment lexicon through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Che</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu 0001</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hidden sentiment association in chinese web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Swen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
		<editor>Jinpeng Huai, Robin Chen, Hsiao-Wuen Hon, Yunhao Liu, Wei-Ying Ma, Andrew Tomkins, and Xiaodong Zhang 0001</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="959" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Latent aspect rating analysis without aspect keyword supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<editor>Chid Apt, Joydeep Ghosh, and Padhraic Smyth</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Phrase dependency parsing for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1533" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extracting and ranking product features in opinion documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk</forename><forename type="middle">Hwan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eamonn O&amp;apos;brien-Strain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<editor>ChuRen Huang and Dan Jurafsky</editor>
		<imprint>
			<publisher>Chinese Information Processing Society of China</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1462" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects and opinions with a maxent-lda hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-aspect opinion polling from textual reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<editor>David Wai-Lok Cheung, Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and Jimmy J. Lin</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1799" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
