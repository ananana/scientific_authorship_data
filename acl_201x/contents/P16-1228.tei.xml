<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harnessing Deep Neural Networks with Logic Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harnessing Deep Neural Networks with Logic Rules</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2410" to="2420"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the struc-tured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis , and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks provide a powerful mech- anism for learning patterns from massive data, achieving new levels of performance on image classification ( <ref type="bibr" target="#b23">Krizhevsky et al., 2012</ref>), speech recognition ( , machine trans- lation ( <ref type="bibr" target="#b8">Bahdanau et al., 2014</ref>), playing strategic board games <ref type="bibr" target="#b41">(Silver et al., 2016)</ref>, and so forth.</p><p>Despite the impressive advances, the widely- used DNN methods still have limitations. The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and some- times counter-intuitive results ( <ref type="bibr" target="#b43">Szegedy et al., 2014;</ref><ref type="bibr" target="#b35">Nguyen et al., 2015)</ref>. It is also difficult to encode human intention to guide the models to capture de- sired patterns, without expensive direct supervision or ad-hoc initialization.</p><p>On the other hand, the cognitive process of hu- man beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences <ref type="bibr" target="#b34">(Minksy, 1980;</ref><ref type="bibr" target="#b25">Lake et al., 2015)</ref>. Logic rules provide a flexible declarative language for communicating high-level cognition and expressing structured knowledge. It is there- fore desirable to integrate logic rules into DNNs, to transfer human intention and domain knowledge to neural models, and regulate the learning process.</p><p>In this paper, we present a framework capable of enhancing general types of neural networks, such as convolutional networks (CNNs) and recurrent networks (RNNs), on various tasks, with logic rule knowledge. Combining symbolic representations with neural methods have been considered in dif- ferent contexts. Neural-symbolic systems ( <ref type="bibr" target="#b15">Garcez et al., 2012</ref>) construct a network from a given rule set to execute reasoning. To exploit a priori knowl- edge in general neural architectures, recent work augments each raw data instance with useful fea- tures ), while network train- ing, however, is still limited to instance-label super- vision and suffers from the same issues mentioned above. Besides, a large variety of structural knowl- edge cannot be naturally encoded in the feature- label form.</p><p>Our framework enables a neural network to learn simultaneously from labeled instances as well as logic rules, through an iterative rule knowledge distillation procedure that transfers the structured information encoded in the logic rules into the net- work parameters. Since the general logic rules are complementary to the specific data labels, a natural "side-product" of the integration is the sup- port for semi-supervised learning where unlabeled data is used to better absorb the logical knowledge. Methodologically, our approach can be seen as a combination of the knowledge distillation ( <ref type="bibr" target="#b17">Hinton et al., 2015;</ref><ref type="bibr" target="#b9">Bucilu et al., 2006</ref>) and the posterior regularization (PR) method ( <ref type="bibr" target="#b14">Ganchev et al., 2010</ref>).</p><p>In particular, at each iteration we adapt the pos- terior constraint principle from PR to construct a rule-regularized teacher, and train the student net- work of interest to imitate the predictions of the teacher network. We leverage soft logic to support flexible rule encoding.</p><p>We apply the proposed framework on both CNN and RNN, and deploy on the task of sentiment analysis (SA) and named entity recognition (NER), respectively. With only a few (one or two) very intuitive rules, both the distilled networks and the joint teacher networks strongly improve over their basic forms (without rules), and achieve better or comparable performance to state-of-the-art models which typically have more parameters and compli- cated architectures.</p><p>To the best of our knowledge, this is the first work to integrate logic rules with general workhorse types of deep neural networks in a prin- cipled framework. The encouraging results indi- cate our method can be potentially useful for in- corporating richer types of human knowledge, and improving other application domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Combination of logic rules and neural networks has been considered in different contexts. Neural- symbolic systems ( <ref type="bibr" target="#b15">Garcez et al., 2012</ref>), such as KBANN ( <ref type="bibr" target="#b45">Towell et al., 1990</ref>) and CILP++ <ref type="bibr" target="#b13">(França et al., 2014</ref>), construct network architectures from given rules to perform reasoning and knowledge acquisition. A related line of research, such as Markov logic networks ( <ref type="bibr" target="#b40">Richardson and Domingos, 2006</ref>), derives probabilistic graphical models (rather than neural networks) from the rule set.</p><p>With the recent success of deep neural networks in a vast variety of application domains, it is in- creasingly desirable to incorporate structured logic knowledge into general types of networks to har- ness flexibility and reduce uninterpretability. Re- cent work that trains on extra features from do- main knowledge ), while producing improved results, does not go beyond the data-label paradigm. <ref type="bibr" target="#b24">Kulkarni et al. (2015)</ref> uses a specialized training procedure with careful order- ing of training instances to obtain an interpretable neural layer of an image network. <ref type="bibr" target="#b20">Karaletsos et al. (2016)</ref> develops a generative model jointly over data-labels and similarity knowledge expressed in triplet format to learn improved disentangled repre- sentations.</p><p>Though there do exist general frameworks that allow encoding various structured constraints on latent variable models ( <ref type="bibr" target="#b14">Ganchev et al., 2010;</ref><ref type="bibr" target="#b51">Zhu et al., 2014;</ref><ref type="bibr" target="#b29">Liang et al., 2009)</ref>, they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study. <ref type="bibr" target="#b28">Liang et al. (2008)</ref> transfers predictive power of pre-trained structured models to unstructured ones in a pipelined fashion.</p><p>Our proposed approach is distinct in that we use an iterative rule distillation process to effectively transfer rich structured knowledge, expressed in the declarative first-order logic language, into pa- rameters of general neural networks. We show that the proposed approach strongly outperforms an extensive array of other either ad-hoc or general integration methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we present our framework which en- capsulates the logical structured knowledge into a neural network. This is achieved by forcing the network to emulate the predictions of a rule- regularized teacher, and evolving both models it- eratively throughout training (section 3.2). The process is agnostic to the network architecture, and thus applicable to general types of neural models in- cluding CNNs and RNNs. We construct the teacher network in each iteration by adapting the posterior regularization principle in our logical constraint setting (section 3.3), where our formulation pro- vides a closed-form solution. <ref type="figure">Figure 1</ref> shows an overview of the proposed framework. Figure 1: Framework Overview. At each iteration, the teacher network is obtained by projecting the student network to a rule-regularized subspace (red dashed arrow); and the student network is updated to balance between emulating the teacher's output and predicting the true labels (black/blue solid ar- rows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Resources: Instances and Rules</head><p>Our approach allows neural networks to learn from both specific examples and general rules. Here we give the settings of these "learning resources".</p><p>Assume we have input variable x ∈ X and target variable y ∈ Y. For clarity, we focus on K-way classification, where Y = ∆ K is the K-dimensional probability simplex and y ∈ {0, 1} K ⊂ Y is a one-hot encoding of the class label. However, our method specification can straightforwardly be applied to other contexts such as regression and sequence learning (e.g., NER tagging, which is a sequence of classification deci- sions). The training data D = {(x n , y n )} N n=1 is a set of instantiations of (x, y).</p><p>Further consider a set of first-order logic (FOL) rules with confidences, denoted as</p><formula xml:id="formula_0">R = {(R l , λ l )} L l=1</formula><p>, where R l is the lth rule over the input-target space (X , Y), and λ l ∈ [0, ∞] is the confidence level with λ l = ∞ indicating a hard rule, i.e., all groundings are required to be true (=1). Here a grounding is the logic expression with all variables being instantiated. Given a set of examples (X, Y ) ⊂ (X , Y) (e.g., a minibatch from D), the set of groundings of R l are denoted as {r lg (X, Y )} G l g=1 . In practice a rule grounding is typically relevant to only a single or subset of examples, though here we give the most general form on the entire set.</p><p>We encode the FOL rules using soft logic ( <ref type="bibr" target="#b7">Bach et al., 2015</ref>) for flexible encoding and stable opti- mization. Specifically, soft logic allows continu- ous truth values from the interval [0, 1] instead of {0, 1}, and the Boolean logic operators are refor- mulated as:</p><formula xml:id="formula_1">A&amp;B = max{A + B − 1, 0} A ∨ B = min{A + B, 1} A1 ∧ · · · ∧ AN = i Ai/N ¬A = 1 − A (1)</formula><p>Here &amp; and ∧ are two different approximations to logical conjunction ( <ref type="bibr" target="#b12">Foulds et al., 2015)</ref>: &amp; is useful as a selection operator (e.g., A&amp;B = B when A = 1, and A&amp;B = 0 when A = 0), while ∧ is an averaging operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rule Knowledge Distillation</head><p>A neural network defines a conditional probabil- ity p θ (y|x) by using a softmax output layer that produces a K-dimensional soft prediction vector denoted as σ θ (x). The network is parameterized by weights θ. Standard neural network training has been to iteratively update θ to produce the correct labels of training instances. To integrate the information encoded in the rules, we propose to train the network to also imitate the outputs of a rule-regularized projection of p θ (y|x), de- noted as q(y|x), which explicitly includes rule con- straints as regularization terms. In each iteration q is constructed by projecting p θ into a subspace constrained by the rules, and thus has desirable properties. We present the construction in the next section. The prediction behavior of q reveals the information of the regularized subspace and struc- tured rules. Emulating the q outputs serves to trans- fer this knowledge into p θ . The new objective is then formulated as a balancing between imitating the soft predictions of q and predicting the true hard labels:</p><formula xml:id="formula_2">θ (t+1) = arg min θ∈Θ 1 N N n=1 (1 − π)(yn, σ θ (xn)) + π(s (t) n , σ θ (xn)),<label>(2)</label></formula><p>where denotes the loss function selected accord- ing to specific applications (e.g., the cross entropy loss for classification); s</p><formula xml:id="formula_3">(t)</formula><p>n is the soft prediction vector of q on x n at iteration t; and π is the imita- tion parameter calibrating the relative importance of the two objectives.</p><p>A similar imitation procedure has been used in other settings such as model compression ( <ref type="bibr" target="#b9">Bucilu et al., 2006;</ref><ref type="bibr" target="#b17">Hinton et al., 2015</ref>) where the pro- cess is termed distillation. Following them we call p θ (y|x) the "student" and q(y|x) the "teacher", which can be intuitively explained in analogous to human education where a teacher is aware of systematic general rules and she instructs students by providing her solutions to particular questions (i.e., the soft predictions). An important differ- ence from previous distillation work, where the teacher is obtained beforehand and the student is trained thereafter, is that our teacher and student are learned simultaneously during training.</p><p>Though it is possible to combine a neural net- work with rule constraints by projecting the net- work to the rule-regularized subspace after it is fully trained as before with only data-label in- stances, or by optimizing projected network di- rectly, we found our iterative teacher-student dis- tillation approach provides a much superior per- formance, as shown in the experiments. More- over, since p θ distills the rule information into the weights θ instead of relying on explicit rule rep- resentations, we can use p θ for predicting new ex- amples at test time when the rule assessment is expensive or even unavailable (i.e., the privileged information setting ( <ref type="bibr" target="#b30">Lopez-Paz et al., 2016)</ref>) while still enjoying the benefit of integration. Besides, the second loss term in Eq. <ref type="formula" target="#formula_2">(2)</ref> can be augmented with rich unlabeled data in addition to the labeled examples, which enables semi-supervised learning for better absorbing the rule knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Teacher Network Construction</head><p>We now proceed to construct the teacher network q(y|x) at each iteration from p θ (y|x). The itera- tion index t is omitted for clarity. We adapt the posterior regularization principle in our logic con- straint setting. Our formulation ensures a closed- form solution for q and thus avoids any significant increases in computational overhead.</p><p>Recall the set of FOL rules</p><formula xml:id="formula_4">R = {(R l , λ l )} L l=1</formula><p>. Our goal is to find the optimal q that fits the rules while at the same time staying close to p θ . For the first property, we apply a commonly-used strategy that imposes the rule constraints on q through an expectation operator. That is, for each rule (indexed by l) and each of its groundings (indexed by g)</p><formula xml:id="formula_5">on (X, Y ), we expect E q(Y |X) [r lg (X, Y )] = 1,</formula><p>with confidence λ l . The constraints define a rule- regularized space of all valid distributions. For the second property, we measure the closeness between q and p θ with KL-divergence, and wish to minimize it. Combining the two factors together and further allowing slackness for the constraints, we finally get the following optimization problem:</p><formula xml:id="formula_6">min q,ξ≥0 KL(q(Y |X)p θ (Y |X)) + C l,g l ξ l,g l s.t. λ l (1 − Eq[r l,g l (X, Y )]) ≤ ξ l,g l g l = 1, . . . , G l , l = 1, . . . , L,<label>(3)</label></formula><p>where ξ l,g l ≥ 0 is the slack variable for respec- tive logic constraint; and C is the regularization parameter. The problem can be seen as project- ing p θ into the constrained subspace. The problem is convex and can be efficiently solved in its dual form with closed-form solutions. We provide the detailed derivation in the supplementary materials and directly give the solution here:</p><formula xml:id="formula_7">q * (Y |X) ∝ p θ (Y |X) exp    − l,g l Cλ l (1 − r l,g l (X, Y ))    (4)</formula><p>Intuitively, a strong rule with large λ l will lead to low probabilities of predictions that fail to meet the constraints. We discuss the computation of the normalization factor in section 3.4.</p><p>Our framework is related to the posterior regular- ization (PR) method ( <ref type="bibr" target="#b14">Ganchev et al., 2010)</ref> which places constraints over model posterior in unsuper- vised setting. In classification, our optimization procedure is analogous to the modified EM algo- rithm for PR, by using cross-entropy loss in Eq. <ref type="formula" target="#formula_2">(2)</ref> and evaluating the second loss term on unlabeled data differing from D, so that Eq.(4) corresponds to the E-step and Eq. <ref type="formula" target="#formula_2">(2)</ref> is analogous to the M-step. This sheds light from another perspective on why our framework would work. However, we found in our experiments (section 5) that to produce strong performance it is crucial to use the same labeled data x n in the two losses of Eq.(2) so as to form a direct trade-off between imitating soft predictions and predicting correct hard labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementations</head><p>The procedure of iterative distilling optimization of our framework is summarized in Algorithm 1.</p><p>During training we need to compute the soft predictions of q at each iteration, which is straight- forward through direct enumeration if the rule con- straints in Eq.(4) are factored in the same way as the base neural model p θ (e.g., the "but"-rule of sentiment classification in section 4.1). If the con- straints introduce additional dependencies, e.g., bi- gram dependency as the transition rule in the NER task (section 4.2), we can use dynamic program- ming for efficient computation. For higher-order constraints (e.g., the listing rule in NER), we ap- proximate through Gibbs sampling that iteratively samples from q(y i |y −i , x) for each position i. If the constraints span multiple instances, we group the relevant instances in minibatches for joint in- ference (and randomly break some dependencies when a group is too large). Note that calculating the soft predictions is efficient since only one NN forward pass is required to compute the base dis- tribution p θ (y|x) (and few more, if needed, for calculating the truth values of relevant rules). p v.s. q at Test Time At test time we can use either the distilled student network p, or the teacher network q after a final projection. Our empirical re- sults show that both models substantially improve over the base network that is trained with only data- label instances. In general q performs better than p. Particularly, q is more suitable when the logic rules introduce additional dependencies (e.g., span- <ref type="formula" target="#formula_2">(2)</ref> balances between emulating the teacher soft predictions and predicting the true hard la- bels. Since the teacher network is constructed from p θ , which, at the beginning of training, would pro- duce low-quality predictions, we thus favor pre- dicting the true labels more at initial stage. As training goes on, we gradually bias towards emu- lating the teacher predictions to effectively distill the structured knowledge. Specifically, we define π (t) = min{π 0 , 1 − α t } at iteration t ≥ 0, where α ≤ 1 specifies the speed of decay and π 0 &lt; 1 is a lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Harnessing NN with Rules</head><formula xml:id="formula_8">Input: The training data D = {(xn, yn)} N n=1 , The rule set R = {(R l , λ l )} L l=1</formula><note type="other">, Parameters: π -imitation parameter C -regularization strength 1: Initialize neural network parameter θ 2: repeat 3: Sample a minibatch (X, Y ) ⊂ D 4: Construct teacher network q with Eq.(4) 5: Transfer knowledge into p θ by updating θ with Eq.(2) 6: until convergence Output: Distill student network p θ and teacher network q ning over multiple examples), requiring joint infer- ence. In contrast, as mentioned above, p is more lightweight and efficient, and useful when rule eval- uation is expensive or impossible at prediction time. Our experiments compare the performance of p and q extensively. Imitation Strength π The imitation parameter π in Eq.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications</head><p>We have presented our framework that is general enough to improve various types of neural networks with rules, and easy to use in that users are allowed to impose their knowledge and intentions through the declarative first-order logic. In this section we illustrate the versatility of our approach by ap- plying it on two workhorse network architectures, i.e., convolutional network and recurrent network, on two representative applications, i.e., sentence- level sentiment analysis which is a classification problem, and named entity recognition which is a sequence learning problem.</p><p>For each task, we first briefly describe the base neural network. Since we are not focusing on tuning network architectures, we largely use the same or similar networks to previous successful neural models. We then design the linguistically- motivated rules to be integrated. The CNN architecture for sentence-level sentiment analysis. The sentence representation vector is followed by a fully-connected layer with softmax output activation, to output sentiment pre- dictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sentiment Classification</head><p>Sentence-level sentiment analysis is to identify the sentiment (e.g., positive or negative) underlying an individual sentence. The task is crucial for many opinion mining applications. One challeng- ing point of the task is to capture the contrastive sense (e.g., by conjunction "but") within a sen- tence.</p><p>Base Network We use the single-channel convo- lutional network proposed in <ref type="bibr" target="#b21">(Kim, 2014</ref>). The sim- ple model has achieved compelling performance on various sentiment classification benchmarks. The network contains a convolutional layer on top of word vectors of a given sentence, followed by a max-over-time pooling layer and then a fully- connected layer with softmax output activation. A convolution operation is to apply a filter to word windows. Multiple filters with varying window sizes are used to obtain multiple features. <ref type="figure" target="#fig_1">Figure 2</ref> shows the network architecture.</p><p>Logic Rules One difficulty for the plain neural network is to identify contrastive sense in order to capture the dominant sentiment precisely. The con- junction word "but" is one of the strong indicators for such sentiment changes in a sentence, where the sentiment of clauses following "but" generally dominates. We thus consider sentences S with an "A-but-B" structure, and expect the sentiment of the whole sentence to be consistent with the sentiment of clause B. The logic rule is written as:</p><formula xml:id="formula_9">has-'A-but-B'-structure(S) ⇒ (1(y = +) ⇒ σ θ (B)+ ∧ σ θ (B)+ ⇒ 1(y = +)) ,<label>(5)</label></formula><p>where 1(·) is an indicator function that takes 1 when its argument is true, and 0 otherwise; class '+' represents 'positive'; and σ θ (B) + is the element of σ θ (B) for class '+'. By Eq.(1), when S has the 'A- but-B' structure, the truth value of the above logic rule equals to (1 + σ θ (B) + )/2 when y = +, and (2 − σ θ (B) + )/2 otherwise 1 . Note that here we assume two-way classification (i.e., positive and negative), though it is straightforward to design rules for finer grained sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named Entity Recognition</head><p>NER is to locate and classify elements in text into entity categories such as "persons" and "organiza- tions". It is an essential first step for downstream language understanding applications. The task as- signs to each word a named entity tag in an "X-Y" format where X is one of BIEOS (Beginning, In- side, End, Outside, and Singleton) and Y is the entity category. A valid tag sequence has to follow certain constraints by the definition of the tagging scheme. Besides, text with structures (e.g., lists) within or across sentences can usually expose some consistency patterns.</p><p>Base Network The base network has a similar architecture with the bi-directional LSTM recur- rent network (called BLSTM-CNN) proposed in ( <ref type="bibr" target="#b10">Chiu and Nichols, 2015)</ref> for NER which has out- performed most of previous neural models. The model uses a CNN and pre-trained word vectors to capture character-and word-level information, respectively. These features are then fed into a bi-directional RNN with LSTM units for sequence tagging. Compared to ( <ref type="bibr" target="#b10">Chiu and Nichols, 2015)</ref> we omit the character type and capitalization features, as well as the additive transition matrix in the out- put layer. <ref type="figure" target="#fig_2">Figure 3</ref> shows the network architecture.</p><p>Logic Rules The base network largely makes in- dependent tagging decisions at each position, ignor- ing the constraints on successive labels for a valid tag sequence (e.g., I-ORG cannot follow B-PER). In contrast to recent work (  which adds a conditional random field (CRF) to capture bi-gram dependencies between outputs, we instead apply logic rules which does not introduce extra parameters to learn. An example rule is:</p><formula xml:id="formula_10">equal(yi−1, I-ORG) ⇒ ¬ equal(yi, B-PER)<label>(6)</label></formula><p>1 <ref type="bibr">Replacing ∧ with &amp; in Eq.(5)</ref> leads to a probably more intuitive rule which takes the value σ θ (B)+ when y = +, and 1 − σ θ (B)+ otherwise.  The confidence levels are set to ∞ to prevent any violation.</p><p>We further leverage the list structures within and across sentences of the same documents. Specifi- cally, named entities at corresponding positions in a list are likely to be in the same categories. For instance, in "1. Juventus, 2. Barcelona, 3. ..." we know "Barcelona" must be an organization rather than a location, since its counterpart entity "Juven- tus" is an organization. We describe our simple procedure for identifying lists and counterparts in the supplementary materials. The logic rule is en- coded as:</p><p>is-counterpart(X, A) ⇒ 1 − c(ey) − c(σ θ (A))2, <ref type="formula">(7)</ref> where e y is the one-hot encoding of y (the class pre- diction of X); c(·) collapses the probability mass on the labels with the same categories into a single probability, yielding a vector with length equaling to the number of categories. We use 2 distance as a measure for the closeness between predictions of X and its counterpart A. Note that the distance takes value in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> which is a proper soft truth value. The list rule can span multiple sentences (within the same document). We found the teacher network q that enables explicit joint inference pro- vides much better performance over the distilled student network p (section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We validate our framework by evaluating its appli- cations of sentiment classification and named en- tity recognition on a variety of public benchmarks. By integrating the simple yet effective rules with  <ref type="bibr" target="#b48">Yin and Schutze, 2015)</ref> 89.4 - - 6 CNN-multichannel <ref type="bibr" target="#b21">(Kim, 2014)</ref> 88.1 81.1 85.0 7 Paragraph-Vec ( <ref type="bibr" target="#b27">Le and Mikolov, 2014)</ref> 87.8 - - 8 CRF-PR ( <ref type="bibr" target="#b47">Yang and Cardie, 2014)</ref> - - 82.7 9 RNTN <ref type="figure" target="#fig_1">(Socher et al., 2013)</ref> 85.4 - - 10 G-Dropout ( <ref type="bibr" target="#b46">Wang and Manning, 2013)</ref> - 79.0 82.1 <ref type="table">Table 1</ref>: Accuracy (%) of Sentiment Classification. <ref type="bibr">Row 1, CNN (Kim, 2014</ref>) is the base network corresponding to the "CNN-non-static" model in <ref type="bibr" target="#b21">(Kim, 2014</ref>). Rows 2-3 are the networks enhanced by our framework: CNN-Rule-p is the student network and CNN-Rule-q is the teacher network. For MR and CR, we report the average accuracy±one standard deviation using 10-fold cross validation.</p><p>the base networks, we obtain substantial improve- ments on both tasks and achieve state-of-the-art or comparable results to previous best-performing systems. Comparison with a diverse set of other rule integration methods demonstrates the unique effectiveness of our framework. Our approach also shows promising potentials in the semi-supervised learning and sparse data context. Throughout the experiments we set the regular- ization parameter to C = 400. In sentiment clas- sification we set the imitation parameter to π (t) = 1 − 0.9 t , while in NER π (t) = min{0.9, 1 − 0.9 t } to downplay the noisy listing rule. The confidence levels of rules are set to λ l = 1, except for hard constraints whose confidence is ∞. For neural network configuration, we largely followed the ref- erence work, as specified in the following respec- tive sections. All experiments were performed on a Linux machine with eight 4.0GHz CPU cores, one Tesla K40c GPU, and 32GB RAM. We imple- mented neural networks using Theano 2 , a popular deep learning platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sentiment Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Setup</head><p>We test our method on a number of commonly used benchmarks, including 1) SST2, Stanford Sentiment Treebank ( <ref type="bibr" target="#b42">Socher et al., 2013</ref>) which contains 2 classes (negative and positive), and 6920/872/1821 sentences in the train/dev/test sets respectively. Following <ref type="bibr" target="#b21">(Kim, 2014</ref>) we train mod- els on both sentences and phrases since all labels are provided. 2) MR (Pang and <ref type="bibr" target="#b36">Lee, 2005</ref>), a set of 10,662 one-sentence movie reviews with negative 2 http://deeplearning.net/software/theano or positive sentiment. 3) CR (Hu and Liu, 2004), customer reviews of various products, containing 2 classes and 3,775 instances. For MR and CR, we use 10-fold cross validation as in previous work. In each of the three datasets, around 15% sentences contains the word "but".</p><p>For the base neural network we use the "non- static" version in <ref type="bibr" target="#b21">(Kim, 2014</ref>) with the exact same configurations. Specifically, word vectors are ini- tialized using word2vec ( <ref type="bibr" target="#b33">Mikolov et al., 2013</ref>) and fine-tuned throughout training, and the neural pa- rameters are trained using SGD with the Adadelta update rule <ref type="bibr" target="#b49">(Zeiler, 2012)</ref>. <ref type="table">Table 1</ref> shows the sentiment classification per- formance. Rows 1-3 compare the base neural model with the models enhanced by our frame- work with the "but"-rule (Eq. <ref type="formula" target="#formula_9">(5)</ref>). We see that our method provides a strong boost on accuracy over all three datasets. The teacher network q fur- ther improves over the student network p, though the student network is more widely applicable in certain contexts as discussed in sections 3.2 and 3.4. Rows 4-10 show the accuracy of re- cent top-performing methods. On the MR and CR datasets, our model outperforms all the baselines. On SST2, MVCNN (Yin and Schutze, 2015) (Row 5) is the only system that shows a slightly better re- sult than ours. Their neural network has combined diverse sets of pre-trained word embeddings (while we use only word2vec) and contained more neural layers and parameters than our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Results</head><p>To further investigate the effectiveness of our framework in integrating structured rule knowl- edge, we compare with an extensive array of other Model Accuracy (%) 1 CNN (Kim, 2014) 87.2 2 -but-clause 87.3 3 -2-reg 87.5 4 -project 87.9 5 -opt-project 88.3 6 -pipeline 87.9</p><p>7 -Rule-p 88.8 8 -Rule-q 89.3 <ref type="table">Table 2</ref>: Performance of different rule integration methods on SST2. 1) CNN is the base network; 2) "-but-clause" takes the clause after "but" as input; 3) "-2 -reg" imposes a regularization term γσ θ (S) − σ θ (Y ) 2 to the CNN objective, with the strength γ selected on dev set; 4) "-project" projects the trained base CNN to the rule-regularized subspace with Eq.(3); 5) "-opt-project" directly optimizes the projected CNN; 6) "-pipeline" distills the pre-trained "-opt-project" to a plain CNN; 7-8) "-Rule-p" and "- Rule-q" are our models with p being the distilled stu- dent network and q the teacher network. Note that "-but-clause" and "-2 -reg" are ad-hoc methods ap- plicable specifically to the "but"-rule.</p><p>possible integration approaches. <ref type="table">Table 2</ref> lists these methods and their performance on the SST2 task. We see that: 1) Although all methods lead to differ- ent degrees of improvement, our framework outper- forms all other competitors with a large margin. 2) In particular, compared to the pipelined method in Row 6 which is in analogous to the structure com- pilation work ( <ref type="bibr" target="#b28">Liang et al., 2008)</ref>, our iterative dis- tillation (section 3.2) provides better performance. Another advantage of our method is that we only train one set of neural parameters, as opposed to two separate sets as in the pipelined approach.</p><p>3) The distilled student network "-Rule-p" achieves much superior accuracy compared to the base CNN, as well as "-project" and "-opt-project" which ex- plicitly project CNN to the rule-constrained sub- space. This validates that our distillation procedure transfers the structured knowledge into the neu- ral parameters effectively. The inferior accuracy of "-opt-project" can be partially attributed to the poor performance of its neural network part which achieves only 85.1% accuracy and leads to inaccu- rate evaluation of the "but"-rule in Eq.(5).</p><p>We next explore the performance of our frame- work with varying numbers of labeled instances as well as the effect of exploiting unlabeled data. In- tuitively, with less labeled examples we expect the Data size 5% 10% 30% 100%</p><p>1 CNN 79.9 81.6 83.6 87.2 2 -Rule-p 81.5 83.2 84.5 88.8 3 -Rule-q 82.5 83.9 85.6 89.3</p><p>4 -semi-PR 81.5 83.1 84.6 - 5 -semi-Rule-p 81.7 83.3 84.7 - 6 -semi-Rule-q 82.7 84.2 85.7 - <ref type="table">Table 3</ref>: Accuracy (%) on SST2 with varying sizes of labeled data and semi-supervised learning. The header row is the percentage of labeled examples for training. Rows 1-3 use only the supervised data. Rows 4-6 use semi-supervised learning where the re- maining training data are used as unlabeled exam- ples. For "-semi-PR" we only report its projected solution (in analogous to q) which performs better than the non-projected one (in analogous to p).</p><p>general rules would contribute more to the perfor- mance, and unlabeled data should help better learn from the rules. This can be a useful property espe- cially when data are sparse and labels are expensive to obtain. <ref type="table">Table 3</ref> shows the results. The subsam- pling is conducted on the sentence level. That is, for instance, in "5%" we first selected 5% training sentences uniformly at random, then trained the models on these sentences as well as their phrases. The results verify our expectations. 1) Rows 1-3 give the accuracy of using only data-label subsets for training. In every setting our methods consis- tently outperform the base CNN. 2) "-Rule-q" pro- vides higher improvement on 5% data (with margin 2.6%) than on larger data (e.g., 2.3% on 10% data, and 2.0% on 30% data), showing promising po- tential in the sparse data context. 3) By adding unlabeled instances for semi-supervised learning as in Rows 5-6, we get further improved accuracy. 4) Row 4, "-semi-PR" is the posterior regulariza- tion ( <ref type="bibr" target="#b14">Ganchev et al., 2010</ref>) which imposes the rule constraint through only unlabeled data during train- ing. Our distillation framework consistently pro- vides substantially better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Named Entity Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Setup</head><p>We evaluate on the well-established <ref type="bibr">CoNLL-2003 NER benchmark (Tjong Kim Sang and</ref><ref type="bibr" target="#b44">De Meulder, 2003)</ref>, which contains 14,987/3,466/3,684 sentences and 204,567/51,578/46,666 tokens in train/dev/test sets, respectively. The dataset in- cludes 4 categories, i.e., person, location, orga- nization, and misc. BIOES tagging scheme is used.  <ref type="formula" target="#formula_10">(6)</ref>) on the base BLSTM. Row 3, BLSTM- Rules further incorporates the list rule (Eq. <ref type="formula">(7)</ref>). We report the performance of both the student model p and the teacher model q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Around 1.7% named entities occur in lists.</head><p>We use the mostly same configurations for the base BLSTM network as in ( <ref type="bibr" target="#b10">Chiu and Nichols, 2015)</ref>, except that, besides the slight architecture difference (section 4.2), we apply Adadelta for pa- rameter updating. <ref type="bibr">GloVe (Pennington et al., 2014</ref>) word vectors are used to initialize word features. <ref type="table" target="#tab_2">Table 4</ref> presents the performance on the NER task. By incorporating the bi-gram transition rules (Row 2), the joint teacher model q achieves 1.56 improve- ment in F1 score that outperforms most previous neural based methods (Rows 4-7), including the BLSTM-CRF model ( ) which applies a conditional random field (CRF) on top of a BLSTM in order to capture the transition pat- terns and encourage valid sequences. In contrast, our method implements the desired constraints in a more straightforward way by using the declarative logic rule language, and at the same time does not introduce extra model parameters to learn. Further integration of the list rule (Row 3) provides a sec- ond boost in performance, achieving an F1 score very close to the best-performing systems including Joint-NER-EL ( ) (Row 8), a proba- bilistic graphical model optimizing NER and entity linking jointly with massive external resources, and BLSTM-CRF ( , a combination of BLSTM and CRF with more parameters than our rule-enhanced neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>From the table we see that the accuracy gap be- tween the joint teacher model q and the distilled student p is relatively larger than in the sentiment classification task <ref type="table">(Table 1)</ref>. This is because in the NER task we have used logic rules that introduce extra dependencies between adjacent tag positions as well as multiple instances, making the explicit joint inference of q useful for fulfilling these struc- tured constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Future Work</head><p>We have developed a framework which combines deep neural networks with first-order logic rules to allow integrating human knowledge and inten- tions into the neural models. In particular, we pro- posed an iterative distillation procedure that trans- fers the structured information of logic rules into the weights of neural networks. The transferring is done via a teacher network constructed using the posterior regularization principle. Our framework is general and applicable to various types of neu- ral architectures. With a few intuitive rules, our framework significantly improves base networks on sentiment analysis and named entity recogni- tion, demonstrating the practical significance of our approach.</p><p>Though we have focused on first-order logic rules, we leveraged soft logic formulation which can be easily extended to general probabilistic mod- els for expressing structured distributions and per- forming inference and reasoning <ref type="bibr" target="#b25">(Lake et al., 2015)</ref>. We plan to explore these diverse knowledge rep- resentations to guide the DNN learning. The pro- posed iterative distillation procedure also reveals connections to recent neural autoencoders <ref type="bibr" target="#b22">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b38">Rezende et al., 2014</ref>) where generative models encode probabilistic structures and neural recognition models distill the informa- tion through iterative optimization ( <ref type="bibr" target="#b39">Rezende et al., 2016;</ref><ref type="bibr" target="#b19">Johnson et al., 2016;</ref><ref type="bibr" target="#b20">Karaletsos et al., 2016)</ref>.</p><p>The encouraging empirical results indicate a strong potential of our approach for improving other application domains such as vision tasks, which we plan to explore in the future. Finally, we also would like to generalize our framework to automatically learn the confidence of different rules, and derive new rules from data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>loss labeled data logic rules í µí±(í µí±¦|í µí±¥) í µí± í µí¼ (í µí±¦|í µí±¥) projection unlabeled data teacher network construction rule knowledge distillation back propagation teacher í µí±(í µí±¦|í µí±¥) student í µí± í µí¼ (í µí±¦|í µí±¥)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The CNN architecture for sentence-level sentiment analysis. The sentence representation vector is followed by a fully-connected layer with softmax output activation, to output sentiment predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of the bidirectional LSTM recurrent network for NER. The CNN for extracting character representation is omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of NER on CoNLL-2003. 
Row 2, BLSTM-Rule-trans imposes the transition 
rules (Eq.</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able comments. This work is supported by NSF IIS1218282, NSF IIS1447676, Air Force FA8721-05-C-0003, and FA8750-12-2-0342.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blstm-Rule</surname></persName>
		</author>
		<idno>q: 91.11</idno>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">89</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nn-Lex (</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-Lstm (</forename><surname>Lample</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blstm-Lex</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">77</biblScope>
		</imprint>
		<respStmt>
			<orgName>Chiu and Nichols</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blstm-Crf1 (</forename><surname>Lample</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">94</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Ner-El (</forename><surname>Joint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blstm-Crf2 (</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hovy</forename></persName>
		</author>
		<idno>91.21</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Broecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04406</idno>
		<title level="m">Hinge-loss Markov random fields and probabilistic soft logic</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescumizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent topic networks: A versatile probabilistic programming framework for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast relational learning using bottom clause propositionalization with artificial neural networks. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Manoel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerson</forename><surname>França</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur S Davila</forename><surname>Zaverucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="81" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural-symbolic learning systems: foundations and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artur S D&amp;apos;avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krysia</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dov</forename><forename type="middle">M</forename><surname>Broda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabbay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Structured VAEs: Composing probabilistic graphical models and variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Matthew J Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian representation learning with oracle constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename><surname>Tech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">F</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2530" to="2538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structure compilation: trading structure for features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="592" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from measurements in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unifying distillation and privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prof. of ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint named entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Chin-Yew Lin, and Zaiqing Nie</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Minksy</surname></persName>
		</author>
		<idno>AI Lab Memo. Project MAC. MIT</idno>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Shakir Mohamed, and Daan Wierstra</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Refinement of approximate domain theories by knowledge-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Towell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noordewier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth National conference on Artificial intelligence</title>
		<meeting>the eighth National conference on Artificial intelligence<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="861" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context-aware learning for sentence-level sentiment analysis with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="325" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multichannel variable-size convolution for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CONLL</title>
		<meeting>of CONLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MGNCCNN: A simple approach to exploiting multiple word embeddings for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bayesian inference with posterior regularization and applications to infinite latent SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1799" to="1847" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
