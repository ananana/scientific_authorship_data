<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topically Driven Neural Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
						</author>
						<title level="a" type="main">Topically Driven Neural Language Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="355" to="365"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1033</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model out-performs a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models provide a powerful tool for extract- ing the macro-level content structure of a docu- ment collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP ( <ref type="bibr" target="#b12">Hall et al., 2008;</ref><ref type="bibr" target="#b25">Newman et al., 2010a</ref>; <ref type="bibr" target="#b35">Wang and McCallum, 2006)</ref>. A myriad of variants of the classical LDA method ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) have been proposed, including recent work on neural topic models ( <ref type="bibr" target="#b3">Cao et al., 2015;</ref><ref type="bibr" target="#b33">Wan et al., 2012;</ref><ref type="bibr" target="#b18">Larochelle and Lauly, 2012;</ref><ref type="bibr" target="#b13">Hinton and Salakhutdinov, 2009)</ref>.</p><p>Separately, language models have long been a foundational component of any NLP task involv- ing generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a span of text, traditionally at the sentence level, un- der the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences ( <ref type="bibr" target="#b34">Wang and Cho, 2016;</ref><ref type="bibr" target="#b15">Ji et al., 2016)</ref>.</p><p>In this paper, we combine the benefits of a topic model and language model in proposing a topically-driven language model, whereby we jointly learn topics and word sequence informa- tion. This allows us to both sensitise the predic- tions of the language model to the larger docu- ment narrative using topics, and to generate topics which are better sensitised to local context and are hence more coherent and interpretable.</p><p>Our model has two components: a language model and a topic model. We implement both components using neural networks, and train them jointly by treating each component as a sub-task in a multi-task learning setting. We show that our model is superior to other language models that leverage additional context, and that the generated topics are potentially more coherent than LDA topics. The architecture of the model provides an extra dimensionality of topic interpretability, in supporting the generation of sentences from a topic (or mix of topics). It is also highly flex- ible, in its ability to be supervised and incor- porate side information, which we show to fur- ther improve language model performance. An open source implementation of our model is avail- able at: https://github.com/jhlau/ topically-driven-language-model.  propose a model that learns topics and word dependencies using a Bayesian framework. Word generation is driven by either LDA or an HMM. For LDA, a word is generated based on a sampled topic in the document. For the A key difference over our model is that their lan- guage model is driven by an HMM, which uses a fixed window and is therefore unable to track long- range dependencies. <ref type="bibr" target="#b3">Cao et al. (2015)</ref> relate the topic model view of documents and words -documents having a multinomial distribution over topics and top- ics having a multinomial distributional over words -from a neural network perspective by embed- ding these relationships in differentiable functions. With that, the model lost the stochasticity and Bayesian inference of LDA but gained non-linear complex representations. The authors further pro- pose extensions to the model to do supervised learning where document labels are given. <ref type="bibr" target="#b34">Wang and Cho (2016)</ref> and <ref type="bibr" target="#b15">Ji et al. (2016)</ref> re- lax the sentence independence assumption in lan- guage modelling, and use preceeding sentences as additional context. By treating words in preceed- ing sentences as a bag of words, <ref type="bibr" target="#b34">Wang and Cho (2016)</ref> use an attentional mechanism to focus on these words when predicting the next word. The authors show that the incorporation of additional context helps language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>The architecture of the proposed topically-driven language model (henceforth "tdlm") is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. There are two components in tdlm: a language model and a topic model. The language model is designed to capture word relations in sen- tences, while the topic model learns topical infor- mation in documents. The topic model works like an auto-encoder, where it is given the document words as input and optimised to predict them.</p><p>The topic model takes in word embeddings of a document and generates a document vector us- ing a convolutional network. Given the document vector, we associate it with the topics via an atten- tion scheme to compute a weighted mean of topic vectors, which is then used to predict a word in the document.</p><p>The language model is a standard LSTM lan- guage model <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b23">Mikolov et al., 2010)</ref>, but it incorporates the weighted topic vector generated by the topic model to predict succeeding words.</p><p>Marrying the language and topic models allows the language model to be topically driven, i.e. it models not just word contexts but also the doc- ument context where the sentence occurs, in the form of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Model Component</head><p>Let x i ∈ R e be the e-dimensional word vector for the i-th word in the document. A document of n words is represented as a concatenation of its word vectors:</p><formula xml:id="formula_0">x 1:n = x 1 ⊕ x 2 ⊕ ... ⊕ x n</formula><p>where ⊕ denotes the concatenation operator. We use a number of convolutional filters to process the word vectors, but for clarity we will explain the network with one filter.</p><p>Let w v ∈ R eh be a convolutional filter which we apply to a window of h words to generate a fea- ture. A feature c i for a window of words x i:i+h−1 is given as follows:</p><formula xml:id="formula_1">c i = I(w v x i:i+h−1 + b v )</formula><p>where b v is a bias term and I is the identity func- tion. 1 A feature map c is a collection of features computed from all windows of words:</p><formula xml:id="formula_2">c = [c 1 , c 2 , ..., c n−h+1 ]</formula><p>where c ∈ R n−h+1 . To capture the most salient features in c, we apply a max-over-time pool- ing operation <ref type="bibr" target="#b7">(Collobert et al., 2011</ref>), yielding a scalar:</p><formula xml:id="formula_3">d = max i c i</formula><p>In the case where we use a filters, we have d ∈ R a , and this constitutes the vector represen- tation of the document generated by the convolu- tional and max-over-time pooling network.</p><p>The topic vectors are stored in two lookup tables A ∈ R k×a (input vector) and B ∈ R k×b (output vector), where k is the number of topics, and a and b are the dimensions of the topic vectors.</p><p>To align the document vector d with the topics, we compute an attention vector which is used to compute a document-topic representation: 2 p = softmax(Ad) (1)</p><formula xml:id="formula_4">s = B p<label>(2)</label></formula><p>where p ∈ R k and s ∈ R b . Intuitively, s is a weighted mean of topic vectors, with the weight- ing given by the attention p. This is inspired by the generative process of LDA, whereby documents are defined as having a multinomial distribution over topics. Finally s is connected to a dense layer with soft- max output to predict each word in the document, where each word is generated independently as a unigram bag-of-words, and the model is optimised using categorical cross-entropy loss. In practice, to improve efficiency we compute loss for pre- dicting a sequence of m 1 words in the document, where m 1 is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Model Component</head><p>The language model is implemented using LSTM units <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997</ref>):</p><formula xml:id="formula_5">i t = σ(W i v t + U i h t−1 + b i ) f t = σ(W f v t + U f h t−1 + b f ) o t = σ(W o v t + U o h t−1 + b i ) ˆ c t = tanh(W c v t + U c h t−1 + b c ) c t = f t c t−1 + i t ˆ c t h t = o t tanh(c t )</formula><p>where denotes element-wise product; i t , f t , o t are the input, forget and output activations respec- tively at time step t; and v t , h t and c t are the in- put word embedding, LSTM hidden state, and cell state, respectively. Hereinafter W, U and b are used to refer to the model parameters.</p><p>Traditionally, a language model operates at the sentence level, predicting the next word given its history of words in the sentence. The language model of tdlm incorporates topical information by assimilating the document-topic representation (s) with the hidden output of the LSTM (h t ) at each time step t. To prevent tdlm from memoris- ing the next word via the topic model network, we exclude the current sentence from the document context.</p><p>We use a gating unit similar to a GRU ( <ref type="bibr" target="#b6">Chung et al., 2014</ref>) to allow tdlm to learn the degree of influence of topical informa- tion on the language model:</p><formula xml:id="formula_6">z t = σ(W z s + U z h t + b z ) r t = σ(W r s + U r h t + b r ) ˆ h t = tanh(W h s + U h (r t h t ) + b h ) h t = (1 − z t ) h t + z t ˆ h t (3)</formula><p>where z t and r t are the update and reset gate acti- vations respectively at timestep t. The new hidden state h t is connected to a dense layer with linear transformation and softmax output to predict the next word, and the model is optimised using stan- dard categorical cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Regularisation</head><p>tdlm is trained using minibatches and SGD. 3 For the language model, a minibatch consists of a batch of sentences, while for the topic model it is a batch of documents (each predicting a sequence of m 1 words).</p><p>We treat the language and topic models as sub- tasks in a multi-task learning setting, and train them jointly using categorical cross-entropy loss. Most parameters in the topic model are shared by the language model, as illustrated by their scopes (dotted lines) in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Hyper-parameters of tdlm are detailed in Ta- ble 1. Word embeddings for the topic model and language model components are not shared, al- though their dimensions are the same (e). <ref type="bibr">4</ref> For m 1 , m 2 and m 3 , sequences/documents shorter than these thresholds are padded. Sentences longer than m 2 are broken into multiple se- quences, and documents longer than m 3 are trun- cated. Optimal hyper-parameter settings are tuned using the development set; the presented values are used for experiments in Sections 4 and 5.</p><p>To regularise tdlm, we use dropout regularisa- tion ( <ref type="bibr" target="#b28">Srivastava et al., 2014</ref>). We apply dropout to d and s in the topic model, and to the input word embedding and hidden output of the LSTM in the language model <ref type="bibr" target="#b27">(Pham et al., 2013;</ref><ref type="bibr" target="#b38">Zaremba et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Language Model Evaluation</head><p>We use standard language model perplexity as the evaluation metric. In terms of dataset, we use doc-ument collections from 3 sources: APNEWS, IMDB and BNC. APNEWS is a collection of Associated Press 5 news articles from 2009 to 2016. IMDB is a set of movie reviews collected by <ref type="bibr" target="#b21">Maas et al. (2011)</ref>. BNC is the written portion of the British National Corpus <ref type="bibr" target="#b2">(BNC Consortium, 2007)</ref>, which contains excerpts from journals, books, letters, es- says, memoranda, news and other types of text. For APNEWS and BNC, we randomly sub-sample a set of documents for our experiments.</p><p>For preprocessing, we tokenise words and sen- tences using Stanford CoreNLP ( <ref type="bibr" target="#b17">Klein and Manning, 2003</ref>). We lowercase all word tokens, filter word types that occur less than 10 times, and ex- clude the top 0.1% most frequent word types. <ref type="bibr">6</ref> We additionally remove stopwords for the topic model document context. <ref type="bibr">7</ref> All datasets are partitioned into training, development and test sets; prepro- cessed dataset statistics are presented in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>We tune hyper-parameters of tdlm based on development set language model perplexity. In general, we find that optimal settings are fairly ro- bust across collections, with the exception of m 3 , as document length is collection dependent; opti- mal hyper-parameter values are given in <ref type="table" target="#tab_0">Table 1</ref>. In terms of LSTM size, we explore 2 settings: a small model with 1 LSTM layer and 600 hidden units, and a large model with 2 layers and 900 hidden units. <ref type="bibr">8</ref> For the topic number, we experi- ment with 50, 100 and 150 topics. Word embed- dings are pre-trained 300-dimension word2vec Google News vectors. <ref type="bibr">9</ref> For comparison, we compare tdlm with: 10 vanilla-lstm: A standard LSTM language model, using the same tdlm hyper-parameters where applicable. This is the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lclm:</head><p>A larger context language model that incorporates context from preceding sentences ( <ref type="bibr" target="#b34">Wang and Cho, 2016)</ref>, by treating the preced- ing sentence as a bag of words, and using an  attentional mechanism when predicting the next word. An additional hyper-parameter in lclm is the number of preceeding sentences to incorpo- rate, which we tune based on a development set (to 4 sentences in each case). All other hyper- parameters (such as n batch , e, n epoch , k 2 ) are the same as tdlm.</p><note type="other">Hyper- Value Description parameter m 1 3 Output sequence length for topic model m 2 30 Sequence length for language model m 3 300,150,500 Maximum document length n batch 64 Minibatch size n layer 1,2 Number of LSTM layers n hidden 600,900 LSTM hidden size n epoch 10 Number of training epochs k 100,150,200 Number of topics e 300 Word embedding size h 2 Convolutional filter width a 20 Topic input vector size or number of features for convolutional filter b 50 Topic output vector size l 0.001 Learning rate of optimiser p 1 0.4 Topic model dropout keep probability p 2 0.6 Language model dropout keep probability</note><p>lstm+lda: A standard LSTM language model that incorporates LDA topic information. We first train an LDA model ( <ref type="bibr" target="#b1">Blei et al., 2003;</ref>) to learn 50/100/150 topics for APNEWS, IMDB and BNC. 11 For a document, the LSTM incorporates the LDA topic distribution (q) by concatenating it with the output hidden state (h t ) to predict the next word (i.e. h t = h t ⊕ q). That is, it incorporates topical information into the language model, but unlike tdlm the language model and topic model are trained separately.</p><p>We present language model perplexity perfor- mance in <ref type="table" target="#tab_3">Table 3</ref>. All models outperform the base- line vanilla-lstm, with tdlm performing the <ref type="bibr">11</ref> Based on Gibbs sampling; α = 0.1, β = 0.01. best across all collections. lclm is competitive over the BNC, although the superiority of tdlm for the other collections is substantial. lstm+lda performs relatively well over APNEWS and IMDB, but very poorly over BNC.</p><p>The strong performance of tdlm over lclm suggests that compressing document context into topics benefits language modelling more than us- ing extra context words directly. <ref type="bibr">12</ref> Overall, our re- sults show that topical information can help lan- guage modelling and that joint inference of topic and language model produces the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Topic Model Evaluation</head><p>We saw that tdlm performs well as a language model, but it is also a topic model, and like LDA it produces: (1) a probability distribution over topics for each document (Equation (1)); and (2) a prob- ability distribution over word types for each topic.    Recall that s is a weighted mean of topic vec- tors for a document (Equation <ref type="formula" target="#formula_4">(2)</ref>). Generating the vocabulary distribution for a particular topic is therefore trivial: we can do so by treating s as hav- ing maximum weight (1.0) for the topic of interest, and no weight (0.0) for all other topics. Let B t de- note the topic output vector for the t-th topic. To generate the multinomial distribution over word types for the t-th topic, we replace s with B t be- fore computing the softmax over the vocabulary.</p><p>Topic models are traditionally evaluated using model perplexity. There are various ways to es- timate test perplexity ( <ref type="bibr" target="#b31">Wallach et al., 2009</ref>), but <ref type="bibr" target="#b4">Chang et al. (2009)</ref> show that perplexity does not correlate with the coherence of the generated top- ics. <ref type="bibr" target="#b26">Newman et al. (2010b)</ref>; <ref type="bibr" target="#b24">Mimno et al. (2011)</ref>; <ref type="bibr" target="#b0">Aletras and Stevenson (2013)</ref> propose automatic approaches to computing topic coherence, and <ref type="bibr" target="#b20">Lau et al. (2014)</ref> summarises these methods to under- stand their differences. We propose using auto- matic topic coherence as a means to evaluate the topic model aspect of tdlm.</p><p>Following <ref type="bibr" target="#b20">Lau et al. (2014)</ref>, we compute topic coherence using normalised PMI ("NPMI") scores. Given the top-n words of a topic, co- herence is computed based on the sum of pair- wise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window <ref type="bibr" target="#b26">(Newman et al., 2010b;</ref><ref type="bibr" target="#b20">Lau et al., 2014)</ref>. <ref type="bibr">13</ref> Based on the findings of <ref type="bibr" target="#b19">Lau and Baldwin (2016)</ref>, we average topic coherence over the top- 5/10/15/20 topic words. To aggregate topic coher- ence scores for a model, we calculate the mean coherence over topics.</p><p>In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the lan- guage model experiments <ref type="bibr">(Section 4</ref>  ntm: ntm is a neural topic model proposed by <ref type="bibr" target="#b3">Cao et al. (2015)</ref>. The document-topic and topic- word multinomials are expressed from a neu- ral network perspective using differentiable func- tions. Model hyper-parameters are tuned using de- velopment loss.</p><p>Topic model performance is presented in <ref type="table" target="#tab_5">Ta- ble 4</ref>.</p><p>There are two models of tdlm (tdlm-small and tdlm-large), which spec- ify the size of its LSTM model (1 layer+600 hidden vs. 2 layers+900 hidden; see Section 4). tdlm achieves encouraging results: it has the best performance over APNEWS, and is compet- itive over IMDB. lda, however, produces more coherent topics over BNC. Interestingly, coher- ence appears to increase as the topic number in- creases for lda, but the trend is less pronounced for tdlm. ntm performs the worst of the 3 topic models, and manual inspection reveals that topics are in general not very interpretable. Overall, the results suggest that tdlm topics are competitive: at best they are more coherent than lda topics, and at worst they are as good as lda topics.</p><p>To better understand the spread of coherence scores and impact of outliers, we present box plots for all models (number of topics = 100) over the 3 domains in <ref type="figure" target="#fig_2">Figure 2</ref>. Across all domains, ntm has poor performance and larger spread of scores. The difference between lda and tdlm is small (tdlm &gt; lda in APNEWS, but lda &lt; tdlm in BNC), which is consistent with our previous observation that tdlm topics are competitive with lda topics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Extensions</head><p>One strength of tdlm is its flexibility, owing to it taking the form of a neural network. To show- case this flexibility, we explore two simple ex- tensions of tdlm, where we: (1) build a super- vised model using document labels (Section 6.1); and (2) incorporate additional document metadata (Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Supervised Model</head><p>In datasets where document labels are known, su- pervised topic model extensions are designed to leverage the additional information to improve modelling quality. The supervised setting also has an additional advantage in that model evaluation is simpler, since models can be quantitatively as- sessed via classification accuracy. To incorporate supervised document labels, we treat document classification as another sub-task in tdlm. Given a document and its label, we feed the document through the topic model network to generate the document-topic representation s, and connect it to another dense layer with softmax out- put to generate the probability distribution over classes.</p><p>During training, we have additional minibatches for the documents. We start the document classifi- cation training after the topic and language models have completed training in each epoch.</p><p>We use 20NEWS in this experiment, which is a popular dataset for text classification. 20NEWS is a collection of forum-like messages from 20 news- groups categories. We use the "bydate" version of the dataset, where the train and test partition is separated by a specific date. We sample 2K doc- uments from the training set to create the devel- opment set. For preprocessing we tokenise words and sentence using Stanford CoreNLP ( <ref type="bibr" target="#b17">Klein and Manning, 2003)</ref>, and lowercase all words. As with previous experiments (Section 4) we addi- tionally filter low/high frequency word types and stopwords. Preprocessed dataset statistics are pre- sented in <ref type="table" target="#tab_7">Table 5</ref>.</p><p>For comparison, we use the same two topic   models as in Section 5: ntm and lda. Both ntm and lda have natural supervised extensions ( <ref type="bibr" target="#b3">Cao et al., 2015;</ref><ref type="bibr" target="#b22">McAuliffe and Blei, 2008</ref>) for incorporating document labels. For this task, we tune the model hyper-parameters based on devel- opment accuracy. 14 Classification accuracy for all models is presented in <ref type="table" target="#tab_9">Table 6</ref>. We present tdlm results using only the small setting of LSTM (1 layer + 600 hidden), as we found there is little gain when using a larger LSTM.</p><p>ntm performs very strongly, outperforming both lda and tdlm by a substantial margin. Comparing lda and tdlm, tdlm achieves bet- ter performance, especially when there is a smaller number of topics. Upon inspection of the topics we found that ntm topics are much less coherent than those of lda and tdlm, consistent with our observations from Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Incorporating Document Metadata</head><p>In APNEWS, each news article contains addi- tional document metadata, including subject clas- sification tags, such as "General News", "Acci- dents and Disasters", and "Military and Defense". We present an extension to incorporate document metadata in tdlm to demonstrate its flexibility in integrating this additional information.</p><p>As some of the documents in our original AP- NEWS sample were missing tags, we re-sampled a set of APNEWS articles of the same size as our original, all of which have tags. In total, approxi- mately 1500 unique tags can be found among the training articles.</p><p>To incorporate these tags, we represent each of them as a learnable vector and concatenate it with the document vector before computing the attention distribution. Let z i ∈ R f denote the f -dimension vector for the i-th tag. For the j-th document, we sum up all tags associated with it:</p><formula xml:id="formula_7">e = ntags i=1 I(i, j)z i</formula><p>where n tags is the total number of unique tags, and function I(i, j) returns 1 is the i-th tag is in the j-th document or 0 otherwise. We compute d as before (Section 3.1), and concatenate it with the summed tag vector:</p><formula xml:id="formula_8">d = d ⊕ e.</formula><p>We train two versions of tdlm on the new AP- NEWS dataset: (1) the vanilla version that ignores the tag information; and (2) the extended version which incorporates tag information. <ref type="bibr">15</ref> We exper-Topic Generated Sentences protesters suspect gunman officers occupy gun arrests suspects shooting officer</p><p>• police say a suspect in the shooting was shot in the chest and later shot and killed by a police officer .</p><p>• a police officer shot her in the chest and the man was killed .</p><p>• police have said four men have been killed in a shooting in suburban london .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>film awards actress comedy music actor album show nominations movie</head><p>• it 's like it 's not fair to keep a star in a light , " he says .</p><p>• but james , a four-time star , is just a unk.</p><p>• a unk adaptation of the movie " the dark knight rises " won best picture and he was nominated for best drama for best director of " unk, " which will be presented sunday night . storm snow weather inches flooding rain service winds tornado forecasters</p><p>• temperatures are forecast to remain above freezing enough to reach a tropical storm or heaviest temperatures .</p><p>• snowfall totals were one of the busiest in the country .</p><p>• forecasters say tornado irene 's strong winds could ease visibility and funnel clouds of snow from snow monday to the mountains . virus nile flu vaccine disease outbreak infected symptoms cough tested</p><p>• he says the disease was transmitted by an infected person .</p><p>• unk says the man 's symptoms are spread away from the heat .</p><p>• meanwhile in the unk, the virus has been common in the mojave desert . imented with a few values for the tag vector size (f ) and find that a small value works well; in the following experiments we use f = 5. We evalu- ate the models based on language model perplex- ity and topic model coherence, and present the re- sults in <ref type="table" target="#tab_10">Table 7</ref>. <ref type="bibr">16</ref> In terms of language model perplexity, we see a consistent improvement over different topic set- tings, suggesting that the incorporation of tags improves modelling. In terms of topic coher- ence, there is a small but encouraging improve- ment (with one exception).</p><p>To investigate whether the vectors learnt for these tags are meaningful, we plot the top-14 most frequent tags in <ref type="figure" target="#fig_3">Figure 3</ref>. <ref type="bibr">17</ref> The plot seems reason- able: there are a few related tags that are close to each other, e.g. "State government" and "Govern- ment and politics"; "Crime" and "Violent Crime"; and "Social issues" and "Social affairs".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Topics generated by topic models are typically in- terpreted by way of their top-N highest probabil- ity words. In tdlm, we can additionally generate sentences related to the topic, providing another way to understand the topics. To do this, we can constrain the topic vector for the language model to be the topic output vector of a particular topic (Equation <ref type="formula">(3)</ref>).</p><p>We present 4 topics from a APNEWS model (k = 100; LSTM size = "large") and 3 ran- domly generated sentences conditioned on each <ref type="bibr">16</ref> As the vanilla tdlm is trained on the new APNEWS dataset, the numbers are slightly different to those in <ref type="table" target="#tab_3">Tables 3  and 4</ref>. <ref type="bibr">17</ref> The 5-dimensional vectors are compressed using PCA.</p><p>topic in <ref type="table" target="#tab_11">Table 8</ref>. <ref type="bibr">18</ref> The generated sentences high- light the content of the topics, providing another interpretable aspect for the topics. These results also reinforce that the language model is driven by topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We propose tdlm, a topically driven neural lan- guage model. tdlm has two components: a lan- guage model and a topic model, which are jointly trained using a neural network. We demonstrate that tdlm outperforms a state-of-the-art language model that incorporates larger context, and that its topics are potentially more coherent than LDA topics. We additionally propose simple extensions of tdlm to incorporate information such as docu- ment labels and metadata, and achieved encourag- ing results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of tdlm. Scope of the models are denoted by dotted lines: blue line denotes the scope of the topic model, red the language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Domain</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Boxplots of topic coherence of all models; number of topics = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatter plots of tag embeddings (model=150 topics)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>tdlm hyper-parameters; we experiment with 2 LSTM settings and 3 topic numbers, and m 3 
varies across the three domains (APNEWS, IMDB, and BNC). 

Collection 
Training 
Development 
Test 

#Docs #Tokens 
#Docs #Tokens 
#Docs #Tokens 

APNEWS 

50K 15M 
2K 
0.6M 
2K 
0.6M 

IMDB 

75K 20M 
12.5K 0.3M 
12.5K 0.3M 

BNC 

15K 18M 
1K 
1M 
1K 
1M 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Preprocessed dataset statistics.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Language model perplexity performance of all models over APNEWS, IMDB and BNC. Boldface 
indicates best performance in each row. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). We use the same hyper-parameter settings for tdlm and do not tune them. For comparison, we use the following topic models: lda: We use a LDA model as a baseline topic model. We use the same LDA models as were used to learn topic distributions for lstm+lda (Section 4).</figDesc><table>Topic No. 

System 
Coherence 

APNEWS IMDB BNC 

50 

lda 
.125 
.084 .106 
ntm 
.075 
.064 .081 
tdlm-small 
.149 
.104 .102 
tdlm-large 
.130 
.088 .095 

100 

lda 
.136 
.092 .119 
ntm 
.085 
.071 .070 
tdlm-small 
.152 
.087 .106 
tdlm-large 
.142 
.097 .101 

150 

lda 
.134 
.094 .119 
ntm 
.078 
.075 .072 
tdlm-small 
.147 
.085 .100 
tdlm-large 
.145 
.091 .104 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Mean topic coherence of all models over 
APNEWS, IMDB and BNC. Boldface indicates the 
best performance for each dataset and topic set-
ting. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 : 20NEWS preprocessed statistics.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>20NEWS classification accuracy. All 
models are supervised extensions of the original 
models. Boldface indicates the best performance 
for each topic setting. 

Topic No. Metadata Coherence Perplexity 

50 
No 
.128 
52.45 
Yes 
.131 
51.80 

100 
No 
.142 
52.14 
Yes 
.139 
51.76 

150 
No 
.135 
52.25 
Yes 
.143 
51.58 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Topic coherence and language model per-
plexity by incorporating classification tags on AP-
NEWS. Boldface indicates optimal coherence and 
perplexity performance for each topic setting. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 : Generated sentences for APNEWS topics.</head><label>8</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> A non-linear function is typically used here, but preliminary experiments suggest that the identity function works best for tdlm.</note>

			<note place="foot" n="2"> The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016). We explored various attention styles (including traditional schemes which use one vector for a topic), but found this approach to work best.</note>

			<note place="foot" n="3"> We use Adam as the optimiser (Kingma and Ba, 2014). 4 Word embeddings are updated during training.</note>

			<note place="foot" n="5"> https://www.ap.org/en-gb/. 6 For the topic model, we remove word tokens that correspond to these filtered word types; for the language model we represent them as unk tokens (as for unseen words in test). 7 We use Mallet&apos;s stopword list: https://github. com/mimno/Mallet/tree/master/stoplists. 8 Multi-layer LSTMs are vanilla stacked LSTMs without skip connections (Gers and Schmidhuber, 2000) or depthgating (Yao et al., 2015). 9 https://code.google.com/archive/p/ word2vec/. 10 Note that all models use the same pre-trained word2vec vectors.</note>

			<note place="foot" n="12"> The context size of lclm (4 sentences) is technically smaller than tdlm (full document), however, note that increasing the context size does not benefit lclm, as the context size of 4 gives the best performance.</note>

			<note place="foot" n="13"> We use this toolkit to compute topic coherence: https://github.com/jhlau/topic_ interpretability.</note>

			<note place="foot" n="14"> Most hyper-parameter values for tdlm are similar to those used in the language and topic model experiments; the only exceptions are: a = 80, b = 100, n epoch = 20, m3 = 150. The increase in parameters is unsurprising, as the additional supervision provides more constraint to the model.</note>

			<note place="foot" n="15"> Model hyper-parameters are the same as the ones used in the language (Section 4) and topic model (Section 5) experiments.</note>

			<note place="foot" n="18"> Words are sampled with temperature = 0.75. Generation is terminated when a special end symbol is generated or when sentence length is greater than 40 words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their in-sightful comments and valuable suggestions. This work was funded in part by the Australian Re-search Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10)</title>
		<meeting>the Tenth International Workshop on Computational Semantics (IWCS-10)<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The British National Corpus, version 3 (BNC XML Edition)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bnc Consortium</surname></persName>
		</author>
		<ptr target="http://www.natcorp.ox.ac.uk/" />
	</analytic>
	<monogr>
		<title level="m">Distributed by Oxford University Computing Services on behalf of the BNC Consortium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Conference on Artificial Intelligence (AAAI15</title>
		<meeting>the 29th Annual Conference on Artificial Intelligence (AAAI15<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2210" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS-09)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN&apos;2000)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN&apos;2000)<address><addrLine>Como, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="198" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>CoRR abs/1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17 (NIPS-05)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Studying the history of ideas using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="363" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS-09)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Document context language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR-16 Workshop</title>
		<meeting>ICLR-16 Workshop<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics (ACL<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The sensitivity of topic coherence evaluation to topic cardinality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT 2016)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT 2016)<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the EACL (EACL</title>
		<meeting>the 14th Conference of the EACL (EACL<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS-08)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>TERSPEECH 2010). Makuhari</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing document collections and search results using topic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="169" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010)</title>
		<meeting>Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<idno>CoRR abs/1312.4569</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28 (NIPS-15)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent memory networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT 2016)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics-Human Language Technologies (NAACL HLT 2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML-09)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML-09)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><surname>Montreal</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hybrid neural network-latent topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fifteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>La Palma, Canary Islands</addrLine></address></meeting>
		<imprint>
			<publisher>AISTATS-12</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1287" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Largercontext language modelling with recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1319" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Topics over time: a non-Markov continuous-time model of topical trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno>abs/1508.03790</idno>
		<title level="m">Depth-gated LSTM. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
