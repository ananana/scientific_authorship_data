<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectral Semi-Supervised Discourse Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fisher</surname></persName>
							<email>rwfisher@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Simmons</surname></persName>
							<email>reids@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spectral Semi-Supervised Discourse Relation Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="89" to="93"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Discourse parsing is the process of discovering the latent relational structure of a long form piece of text and remains a significant open challenge. One of the most difficult tasks in discourse parsing is the classification of implicit discourse relations. Most state-of-the-art systems do not leverage the great volume of unlabeled text available on the web-they rely instead on human annotated training data. By incorporating a mixture of labeled and unla-beled data, we are able to improve relation classification accuracy, reduce the need for annotated data, while still retaining the capacity to use labeled data to ensure that specific desired relations are learned. We achieve this using a latent variable model that is trained in a reduced dimensionality subspace using spectral methods. Our approach achieves an F 1 score of 0.485 on the implicit relation labeling task for the Penn Discourse Treebank.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse parsing is a fundamental task in natural language processing that entails the discovery of the latent relational structure in a multi-sentence piece of text. Unlike semantic and syntactic pars- ing, which are used for single sentence pars- ing, discourse parsing is used to discover inter- sentential relations in longer pieces of text. With- out discourse, parsing methods can only be used to understand documents as sequences of unrelated sentences.</p><p>Unfortunately, manual annotation of discourse structure in text is costly and time consuming. Multiple annotators are required for each relation to estimate inter-annotator agreement. The Penn Discourse Treebank (PDTB) ( <ref type="bibr" target="#b14">Prasad et al., 2008)</ref>.</p><p>is one of the largest annotated discourse parsing datasets, with 16,224 implicit relations. However, this pales in comparison to unlabeled datasets that can include millions of sentences of text. By aug- menting a labeled dataset with unlabeled data, we can use a bootstrapping framework to improve predictive accuracy, and reduce the need for la- beled data-which could make it much easier to port discourse parsing algorithms to new domains. On the other hand, a fully unsupervised parser may not be desirable because in many applications spe- cific discourse relations must be identified, which would be difficult to achieve without the use of la- beled examples.</p><p>There has recently been growing interest in a breed of algorithms based on spectral decomposi- tion, which are well suited to training with unla- beled data. Spectral algorithms utilize matrix fac- torization algorithms such as Singular Value De- composition (SVD) and rank factorization to dis- cover low-rank decompositions of matrices or ten- sors of empirical moments. In many models, these decompositions allow us to identify the subspace spanned by a group of parameter vectors or the actual parameter vectors themselves. For tasks where they can be applied, spectral methods pro- vide statistically consistent results that avoid lo- cal maxima. Also, spectral algorithms tend to be much faster-sometimes orders of magnitude faster-than competing approaches, which makes them ideal for tackling large datasets. These meth- ods can be viewed as inferring something about the latent structure of a domain-for example, in a hidden Markov model, the number of latent states and the sparsity pattern of the transition matrix are forms of latent structure, and spectral methods can recover both in the limit.</p><p>This paper presents a semi-supervised spectral model for a sequential relation labeling task for discourse parsing. Besides the theoretically desir- able properties mentioned above, we also demon-strate the practical advantages of the model with an empirical evaluation on the Penn Discourse Treebank (PDTB) ( <ref type="bibr" target="#b14">Prasad et al., 2008</ref>) dataset, which yields an F 1 score of 0.485. This accuracy shows a 7-9 percentage point improvement over approaches that do not utilize unlabeled training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been quite a bit of work concerning fully supervised relation classification with the PDTB ( <ref type="bibr" target="#b11">Lin et al., 2014;</ref><ref type="bibr" target="#b4">Feng and Hirst, 2012;</ref><ref type="bibr" target="#b16">Webber et al., 2012</ref>). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unla- beled data appears in <ref type="bibr" target="#b7">(Hernault et al., 2011</ref>), which showed that moderate classification accu- racy can be achieved with very small labeled datasets. However, this approach is not compet- itive with fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence ( <ref type="bibr" target="#b9">Joty et al., 2013;</ref><ref type="bibr" target="#b5">Feng and Hirst, 2014)</ref>, though this work has focused on the RST- DT corpus, rather than the PDTB.</p><p>In addition to requiring a fully supervised train- ing set, most existing discourse parsers use non- spectral optimization that is often slow and inex- act. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings <ref type="bibr" target="#b12">(Parikh et al., 2014;</ref>). Spectral methods have also been applied very successfully in many non-linguistic domains ( <ref type="bibr" target="#b8">Hsu et al., 2012;</ref><ref type="bibr" target="#b1">Boots and Gordon, 2010;</ref><ref type="bibr" target="#b6">Fisher et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition and Dataset</head><p>This section defines the discourse parsing prob- lem and discusses the characteristics of the PDTB. The PDTB consists of annotated articles from the Wall Street Journal and is used in our empiri- cal evaluations. This is combined with the New York Times Annotated Corpus <ref type="bibr" target="#b15">(Sandhaus, 2008)</ref>, which includes 1.8 million New York Times arti- cles printed between 1987 and 2007.</p><p>Discourse parsing can be reduced to three sepa- rate tasks. First, the text must be decomposed into elementary discourse units (EDUs), which may or may not coincide with sentence boundaries. The EDUs are often independent clauses that may be connected with conjunctions. After the text has been partitioned into EDUs, the discourse struc- ture must be identified. This requires us to iden- tify all pairs of EDUs that will be connected with some discourse relation. These relational links in- duce the skeletal structure of the discourse parse tree. Finally, each connection identified in the pre- vious step must be labeled using a known set of relations. Examples of these discourse relations include concession, causal, and instantiation rela- tions. In the PDTB, only adjacent discourse units are connected with a discourse relation, so with this dataset we are considering parse sequences rather than parse trees.</p><p>In this work, we focus on the relation labeling task, as fairly simple methods perform quite well at the other two tasks ( <ref type="bibr" target="#b16">Webber et al., 2012</ref>). We use the ground truth parse structures provided by the PDTB dataset, so as to isolate the error intro- duced by relation labeling in our results, but in practice a greedy structure learning algorithm can be used if the parse structures are not known a pri- ori.</p><p>Some of the relations in the dataset are induced by specific connective words in the text. For exam- ple, a contrast relation may be explicitly revealed by the conjunction but. Simple classifiers using only the text of the discourse connective with POS tags can find explicit relations with high accu- racy ( <ref type="bibr" target="#b11">Lin et al., 2014</ref>). The following sentence shows an example of a more difficult implicit re- lation. In this sentence, two EDUs are connected with an explanatory relation, shown in bold, al- though the connective word does not occur in the text.</p><p>"But a few funds have taken other defen- sive steps. Some have raised their cash positions to record levels.</p><p>[BECAUSE] High cash positions help buffer a fund when the market falls."</p><p>We focus on the more difficult implicit relations that are not induced by coordinating connectives in the text. The implicit relations have been shown to require more sophisticated feature sets includ- ing syntactic and linguistic information ( <ref type="bibr" target="#b10">Lin et al., 2009)</ref>. The PDTB dataset includes 16,053 exam- ples of implicit relations.</p><p>A full list of the PDTB relations is available in ( <ref type="bibr" target="#b14">Prasad et al., 2008</ref>). The relations are orga- nized hierarchically into top level, types, and sub- types. Our experiments focus on learning only up to level 2, as the level 3 (sub-type) relations are too specific and show only 80% inter-annotator agreement. There are 16 level 2 relations in the PDTB, but the 5 least common relations only ap- pear a handful of times in the dataset and are omit- ted from our tests, yielding 11 possible classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We incorporate unlabeled data into our spectral discourse parsing model using a bootstrapping framework. The model is trained over several iter- ations, and the most useful unlabeled sequences are added as labeled training data after each it- eration. Our method also utilizes Markovian la- tent states to compactly capture global informa- tion about a parse sequence, with one latent vari- able for each relation in the discourse parsing se- quence. Most discourse parsing frameworks will label relations independently of the rest of the ac- companying parse sequence, but this model allows for information about the global structure of the discourse parse to be used when labeling a rela- tion. A graphical representation of one link in the parsing model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Specifically, each potential relation r ij between elementary discourse units e i and e j is accompa- nied by a corresponding latent variable as h ij . Ac- cording to the model assumptions, the following equality holds: P (r ij = r|r 1,2 , r 2,3 ...r n+1,n ) = P (r ij = r|h ij )</p><p>To maintain notational consistency with other latent variable models, we will denote these re- lation variables as x 1 ...x n , keeping in mind that there is one possible relation for each adjacent pair of elementary discourse units.</p><p>For the Penn Discourse Treebank Dataset, the discourse parses behave like sequence of random variables representing the relations, which allows us to use an HMM-like latent variable model based on the framework presented in <ref type="figure" target="#fig_0">(Hsu et al., 2012)</ref>. If the discourse parses were instead trees, such as those seen in Rhetorical Structure Theory (RST) datasets, we can modify the standard model to in- clude separate parameters for left and right chil- dren, as demonstrated in <ref type="figure" target="#fig_0">(Dhillon et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spectral Learning</head><p>This section briefly describes the process of learn- ing a spectral HMM. Much more detail about the process is available in <ref type="figure" target="#fig_0">(Hsu et al., 2012)</ref>. Learn- ing in this model will occur in a subspace of di- mensionality m, but system dynamics will be the same if m is not less than the rank of the obser- vation matrix. If our original feature space has dimensionality n, we define a transformation ma- trix U ∈ R n×m , which can be computed using Singular Value Decomposition. Given the matrix U , coupled with the empirical unigram (P 1 ), bi- gram (P 2,1 ), and trigram matrices (P 3,x,1 ), we are able to estimate the subspace initial state distribu- tion (ˆ π U ) and observable operator ( ˆ A U ) using the following equalities (wherein the Moore-Penrose pseudo-inverse of matrix X is denoted by X + ):</p><formula xml:id="formula_0">ˆ π U = U T P 1 ˆ A U = U T P 3,x,1 (U T P 2,1 ) + ∀x</formula><p>For our original feature space, we use the rich linguistic discourse parsing features defined in <ref type="bibr" target="#b5">(Feng and Hirst, 2014)</ref>, which includes syn- tactic and linguistic features taken from depen- dency parsing, POS tagging, and semantic simi- larity measures. We augment this feature space with a vector space representation of semantics. A term-document co-occurrence matrix is computed using all of Wikipedia and Latent Dirichlet Anal- ysis was performed using this matrix. The top 200 concepts from the vector space representation for each pair of EDUs in the dataset are included in the feature space, with a concept regularization pa- rameter of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-Supervised Training</head><p>To begin semi-supervised training, we perform a syntactic parse of the unlabeled data and ex-tract EDU segments using the method described in <ref type="bibr" target="#b5">(Feng and Hirst, 2014)</ref>. The model is then trained using the labeled dataset, and the unlabeled re- lations are predicted using the Viterbi algorithm. The most informative sequences in the unlabeled training set are added to the labeled training set as labeled examples. To measure how informative a sequence of relations is, we use density-weighted certainty sampling (DCS). Specifically for a se- quence of relations r 1 ...r n taken from a document, d, we use the following formula:</p><formula xml:id="formula_1">DCS(d) = 1 n n i=1ˆp i=1ˆ i=1ˆp(r i ) H(r i )</formula><p>In this equation, H(r i ) represented the entropy of the distribution of label predictions for the rela- tion r i generated by the current spectral model, which is a measure of the model's uncertainty for the label of the given relation. Density is de- notedˆpnotedˆ notedˆp(r i ), and this quantity measures the extent to which the text corresponding to this relation is representative of the labeled corpus. To com- pute this measure, we create a Kernel Density Es- timate (KDE) over a 100 dimensional LDA vector space representation of all EDU's in the labeled corpus. We then compute the density of the KDE for the text associated with relation r i , which gives usˆpusˆ usˆp(r i ). All sequences of relations in the unla- beled dataset are ranked according to their aver- age density-weighted certainty score, and all se- quences scoring above a parameter ψ are added to the training set. The model is then retrained, the unlabeled data re-scored, and the process is repeated for several iterations. In iteration i, the labeled data in the training set is weighted w l i , and the unlabeled data is weighted w u i , with the unlabeled data receiving higher weight in subse- quent iterations. The KDE kernel bandwidth and the parameters ψ, w l i , w u i , and the number of hid- den states are chosen in experiments using 10-fold cross validation on the labeled training set, cou- pled with a subset of the unlabeled data. <ref type="figure" target="#fig_2">Figure 2</ref> shows the F 1 scores of the model using various sizes of labeled training sets. In all cases, the entirety of the unlabeled data is made avail- able, and 7 rounds of bootstrapping is conducted. Sections 2-22 of the PDTB are used for training, with section 23 being withheld for testing, as rec- ommended by the dataset guidelines (Prasad et al.,  2008). The results are compared against those re- ported in ( <ref type="bibr" target="#b11">Lin et al., 2014)</ref>, as well as a simple baseline classifier that labels all relations with the most common class, EntRel. Compared to the semi-supervised method described in <ref type="bibr" target="#b7">(Hernault et al., 2011</ref>), we show significant gains in accuracy at various sizes of dataset, although the unlabeled dataset used in our experiments is much larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>When the spectral HMM is trained using only the labeled dataset, with no unlabeled data, it pro- duces an F 1 score of 41.1%, which is comparable to the results reported in ( <ref type="bibr" target="#b11">Lin et al., 2014</ref>). By comparison, the semi-supervised classifier is able to obtain similar accuracy when using approxi- mately 50% of the labeled training data. When given access to the full labeled dataset, we see an improvement in the F 1 score of 7-9 percent- age points. Recent work has shown promising re- sults using CRFs for discourse parsing <ref type="bibr" target="#b9">(Joty et al., 2013;</ref><ref type="bibr" target="#b5">Feng and Hirst, 2014)</ref>, but the results re- ported in this work were taken from the RST-DT corpus and are not directly comparable. However, supervised CRFs and HMMs show similar accu- racy in other language tasks ( <ref type="bibr" target="#b13">Ponomareva et al., 2007;</ref><ref type="bibr" target="#b0">Awasthi et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we have shown that we are able to outperform fully-supervised relation classifiers by augmenting the training data with unlabeled text. The spectral optimization used in this ap- proach makes computation tractable even when using over one million documents. In future work, we would like to further improve the performance of this method when very small labeled training sets are available, which would allow discourse analysis to be applied in many new and interest- ing domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of the latent variable discourse parsing model taken from the Penn Discourse Treebank Dataset. The relation here is an example of a cause attribution relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Empirical results for labeling of implicit relations.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We give thanks to Carolyn Penstein Rosé and Geoff Gordon for their helpful discussions and suggestions. We also gratefully acknowledge the National Science Foundation for their support through EAGER grant number IIS1450543. This material is also based upon work supported by the Quality of Life Technology Center and the National Science Foundation under Cooperative Agreement EEC-0540865.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Part of speech tagging and chunking with hmm and crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranjal</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delip</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NLP Association of India (NLPAI) Machine Learning Contest</title>
		<meeting>NLP Association of India (NLPAI) Machine Learning Contest</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.0041</idno>
		<title level="m">Predictive state temporal difference learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable pcfgs: Algorithms and sample complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2399" to="2449" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral dependency parsing with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Paramveer S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="205" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Textlevel discourse parsing with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>The 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral machine learning for predicting power wheelchair exercise compliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Shiu</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Grindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annmarie</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu Kuang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Intelligent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="174" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised discourse relation classification with structural learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="340" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1480" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Shafiq R Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the penn discourse treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A pdtb-styled end-to-end discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spectral unsupervised parsing with additive tree metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional random fields vs. hidden markov models in a biomedical named entity recognition task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferrán</forename><surname>Pla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. Recent Advances in Natural Language Processing</title>
		<meeting>of Int. Conf. Recent Advances in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="479" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The new york times annotated corpus ldc2008t19. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discourse structure and language technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Egg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valia</forename><surname>Kordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="490" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
