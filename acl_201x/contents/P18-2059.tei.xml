<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse and Constrained Attention for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
							<email>cmalaviy@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Ferreira</surname></persName>
							<email>pedro.miguel.ferreira @tecnico.ulisboa.pt</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
							<email>andre.martins @unbabel.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Instituto Superior Técnico Universidade de Lisboa Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">TelecomunicaçTelecomunicaç˜Telecomunicações Lisbon</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse and Constrained Attention for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="370" to="376"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>370</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In NMT, words are sometimes dropped from the source or generated repeatedly in the translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differ-entiable and sparse. Empirical evaluation is provided in three languages pairs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) emerged in the last few years as a very successful paradigm <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b7">Gehring et al., 2017;</ref><ref type="bibr">Vaswani et al., 2017)</ref>. While NMT is generally more fluent than previous sta- tistical systems, adequacy is still a major con- cern ( <ref type="bibr" target="#b10">Koehn and Knowles, 2017)</ref>: common mis- takes include dropping source words and repeating words in the generated translation.</p><p>Previous work has attempted to mitigate this problem in various ways. <ref type="bibr" target="#b19">Wu et al. (2016)</ref> incor- porate coverage and length penalties during beam search-a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve ar- chitectural changes: providing coverage vectors to track the attention history ( <ref type="bibr" target="#b15">Mi et al., 2016;</ref><ref type="bibr">Tu et al., 2016)</ref>, using gating architectures and adap- tive attention to control the amount of source con- text provided ( <ref type="bibr">Tu et al., 2017a;</ref><ref type="bibr" target="#b11">Li and Zhu, 2017)</ref>, or adding a reconstruction loss ( <ref type="bibr">Tu et al., 2017b</ref>). <ref type="bibr" target="#b6">Feng et al. (2016)</ref> also use the notion of fertility * Work done during an internship at <ref type="bibr">Unbabel.</ref> implicitly in their proposed model. Their fertility conditioned decoder uses a coverage vector and an extract gate which are incorporated in the decod- ing recurrent unit, increasing the number of pa- rameters.</p><p>In this paper, we propose a different solution that does not change the overall architecture, but only the attention transformation. Namely, we replace the traditional softmax by other recently proposed transformations that either promote at- tention sparsity <ref type="bibr" target="#b13">(Martins and Astudillo, 2016)</ref> or upper bound the amount of attention a word can receive <ref type="bibr" target="#b14">(Martins and Kreutzer, 2017)</ref>. The bounds are determined by the fertility values of the source words. While these transformations have given encouraging results in various NLP problems, they have never been applied to NMT, to the best of our knowledge. Furthermore, we combine these two ideas and propose a novel attention transfor- mation, constrained sparsemax, which produces both sparse and bounded attention weights, yield- ing a compact and interpretable set of alignments. While being in-between soft and hard alignments <ref type="figure" target="#fig_2">(Figure 2</ref>), the constrained sparsemax transforma- tion is end-to-end differentiable, hence amenable for training with gradient backpropagation.</p><p>To sum up, our contributions are as follows: 1 • We formulate constrained sparsemax and de- rive efficient linear and sublinear-time algo- rithms for running forward and backward prop- agation. This transformation has two levels of sparsity: over time steps, and over the attended words at each step.</p><p>• We provide a detailed empirical comparison of various attention transformations, includ- ing softmax ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>), sparse-max <ref type="bibr" target="#b13">(Martins and Astudillo, 2016)</ref>, constrained softmax <ref type="bibr" target="#b14">(Martins and Kreutzer, 2017)</ref>, and our newly proposed constrained sparsemax. We provide error analysis including two new met- rics targeted at detecting coverage problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Our underlying model architecture is a standard at- tentional encoder-decoder ( <ref type="bibr" target="#b1">Bahdanau et al., 2014)</ref>. Let x := x 1:J and y := y 1:T denote the source and target sentences, respectively. We use a Bi-LSTM encoder to represent the source words as a matrix</p><formula xml:id="formula_0">H := [h 1 , . . . , h J ] ∈ R 2D×J</formula><p>. The conditional probability of the target sentence is given as</p><formula xml:id="formula_1">p(y | x) := T t=1 p(y t | y 1:(t−1) , x),<label>(1)</label></formula><p>where p(y t | y 1:(t−1) , x) is computed by a softmax output layer that receives a decoder state s t as in- put. This state is updated by an auto-regressive LSTM, s t = RNN(embed(y t−1 ), s t−1 , c t ), where c t is an input context vector. This vector is com- puted as c t := Hα t , where α t is a probability distribution that represents the attention over the source words, commonly obtained as</p><formula xml:id="formula_2">α t = softmax(z t ),<label>(2)</label></formula><p>where z t ∈ R J is a vector of scores. We follow <ref type="bibr" target="#b12">Luong et al. (2015)</ref> and define z t,j := s t−1 W h j as a bilinear transformation of encoder and de- coder states, where W is a model parameter. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sparse and Constrained Attention</head><p>In this work, we consider alternatives to Eq. 2. Since the softmax is strictly positive, it forces all words in the source to receive some probability mass in the resulting attention distribution, which can be wasteful. Moreover, it may happen that the decoder attends repeatedly to the same source words across time steps, causing repetitions in the generated translation, as <ref type="bibr">Tu et al. (2016)</ref> observed.</p><p>With this in mind, we replace Eq. 2 by α t = ρ(z t , u t ), where ρ is a transformation that may de- pend both on the scores z t ∈ R J and on upper bounds u t ∈ R J that limit the amount of atten- tion that each word can receive. We consider three alternatives to softmax, described next.</p><p>Sparsemax. The sparsemax transformation <ref type="bibr" target="#b13">(Martins and Astudillo, 2016</ref>) is defined as:</p><formula xml:id="formula_3">sparsemax(z) := arg min α∈∆ J α − z 2 , (3)</formula><p>where</p><formula xml:id="formula_4">∆ J := {α ∈ R J | α ≥ 0, j α j = 1}.</formula><p>In words, it is the Euclidean projection of the scores z onto the probability simplex. These projections tend to hit the boundary of the simplex, yielding a sparse probability distribution. This allows the de- coder to attend only to a few words in the source, assigning zero probability mass to all other words. <ref type="bibr" target="#b13">Martins and Astudillo (2016)</ref> have shown that the sparsemax can be evaluated in O(J) time (same asymptotic cost as softmax) and gradient back- propagation takes sublinear time (faster than soft- max), by exploiting the sparsity of the solution.</p><p>Constrained softmax. The constrained softmax transformation was recently proposed by <ref type="bibr" target="#b14">Martins and Kreutzer (2017)</ref> in the context of easy-first se- quence tagging, being defined as follows:</p><formula xml:id="formula_5">csoftmax(z; u) := arg min α∈∆ J KL(α softmax(z)) s.t. α ≤ u,<label>(4)</label></formula><p>where u is a vector of upper bounds, and KL(..) is the Kullback-Leibler divergence. In other words, it returns the distribution closest to softmax(z) whose attention probabilities are bounded by u. <ref type="bibr" target="#b14">Martins and Kreutzer (2017)</ref> have shown that this transformation can be evaluated in O(J log J) time and its gradients backpropagated in O(J) time.</p><p>To use this transformation in the attention mechanism, we make use of the idea of fertil- ity ( <ref type="bibr" target="#b3">Brown et al., 1993</ref>). Namely, let β t−1 := t−1 τ =1 α τ denote the cumulative attention that each source word has received up to time step t, and let f := (f j ) J j=1 be a vector containing fertil- ity upper bounds for each source word. The atten- tion at step t is computed as</p><formula xml:id="formula_6">α t = csoftmax(z t , f − β t−1 ).<label>(5)</label></formula><p>Intuitively, each source word j gets a credit of f j units of attention, which are consumed along the decoding process. If all the credit is exhausted, it receives zero attention from then on. Unlike the sparsemax transformation, which places sparse attention over the source words, the constrained softmax leads to sparsity over time steps. respectively. For constrained softmax/sparsemax, we set unit fertilities to every word; for each row the upper bounds (represented as green dashed lines) are set as the difference between these fertilities and the cumulative attention each word has received. The last row illustrates the cumulative attention for the three words after all rounds.</p><p>Constrained sparsemax. In this work, we pro- pose a novel transformation which shares the two properties above: it provides both sparse and bounded probabilities. It is defined as:</p><formula xml:id="formula_7">csparsemax(z; u) := arg min α∈∆ J α − z 2 s.t. α ≤ u.<label>(6)</label></formula><p>The following result, whose detailed proof we in- clude as supplementary material (Appendix A), is key for enabling the use of the constrained sparse- max transformation in neural networks.</p><p>Proposition 1 Let α = csparsemax(z; u) be the solution of Eq. 6, and define the sets • Gradient backpropagation. Backpropagation takes sublinear time</p><formula xml:id="formula_8">A = {j ∈ [J] | 0 &lt; α j &lt; u j }, A L = {j ∈ [J] | α j = 0}</formula><formula xml:id="formula_9">O(|A| + |A R |). Let L(θ)</formula><p>be a loss function, dα = α L(θ) be the out- put gradient, and dz = z L(θ) and du = u L(θ) be the input gradients. Then, we have:</p><formula xml:id="formula_10">dz j = 1(j ∈ A)(dα j − m) (7) du j = 1(j ∈ A R )(dα j − m),<label>(8)</label></formula><p>where m = 1 |A| j∈A dα j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fertility Bounds</head><p>We experiment with three ways of setting the fer- tility of the source words: CONSTANT, GUIDED, and PREDICTED. With CONSTANT, we set the fertilities of all source words to a fixed integer value f . With GUIDED, we train a word aligner based on IBM Model 2 (we used fast align in our experiments, <ref type="bibr" target="#b5">Dyer et al. (2013)</ref>) and, for each word in the vocabulary, we set the fertilities to the maximal observed value in the training data (or 1 if no alignment was observed  with fertility upper bounds and the word aligner may miss some word pairs, we found it beneficial to add a constant to this number (1 in our experi- ments). At test time, we use the expected fertilities according to our model.</p><p>Sink token. We append an additional &lt;SINK&gt; token to the end of the source sentence, to which we assign unbounded fertility (f J+1 = ∞). The token is akin to the null alignment in IBM mod- els. The reason we add this token is the following: without the sink token, the length of the generated target sentence can never exceed j f j words if we use constrained softmax/sparsemax. At train- ing time this may be problematic, since the target length is fixed and the problems in Eqs. 4-6 can become infeasible. By adding the sink token we guarantee j f j = ∞, eliminating the problem.</p><p>Exhaustion strategies. To avoid missing source words, we implemented a simple strategy to en- courage more attention to words with larger credit: we redefine the pre-attention word scores as z t = z t + cu t , where c is a constant (c = 0.2 in our experiments). This increases the score of words <ref type="bibr">which</ref> have not yet exhausted their fertility (we may regard it as a "soft" lower bound in Eqs. 4-6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluated our attention transformations on three language pairs. We focused on small datasets, as they are the most affected by cover- age mistakes. We use the IWSLT 2014 corpus for DE-EN, the KFTT corpus for JA-EN (Neu- big, 2011), and the WMT 2016 dataset for RO- EN. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our rea- son to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units ( <ref type="bibr" target="#b18">Sennrich et al., 2016</ref>) with a joint vocab- ulary and 32k merge operations. Our implemen- tation was done on a fork of the OpenNMT-py toolkit ( <ref type="bibr" target="#b9">Klein et al., 2017</ref>) with the default param- eters <ref type="bibr">4</ref> . We used a validation set to tune hyperpa- rameters introduced by our model. Even though our attention implementations are CPU-based us- ing NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices.</p><p>As baselines, we use softmax attention, as well as two recently proposed coverage models:</p><p>• COVPENALTY ( <ref type="bibr">Wu et al., 2016, §7)</ref>. At test time, the hypotheses in the beam are rescored with a global score that includes a length and a coverage penalty. <ref type="bibr">5</ref> We tuned α and β with grid search on {0.2k} 5 k=0 , as in <ref type="bibr" target="#b19">Wu et al. (2016)</ref>. • COVVECTOR ( <ref type="bibr">Tu et al., 2016)</ref>. At training and test time, coverage vectors β and additional pa- rameters v are used to condition the next atten- tion step. We adapted this to our bilinear atten- tion by defining z t,j = s t−1 (W h j + vβ t−1,j ). We also experimented combining the strategies above with the sparsemax transformation.</p><p>As evaluation metrics, we report tokenized BLEU, METEOR <ref type="bibr" target="#b4">(Denkowski and Lavie (2014)</ref>, as well as two new metrics that we describe next to account for over and under-translation. 6 <ref type="bibr">4</ref> We used a 2-layer LSTM, embedding and hidden size of 500, dropout 0.3, and the SGD optimizer for 13 epochs. <ref type="bibr">5</ref> Since our sparse attention can become 0 for some words, we extended the original coverage penalty by adding another parameter , set to 0.1: cp(x; y) := β J j=1 log max{, min{1, |y| t=1 αjt}}. 6 Both evaluation metrics are included in our software package at www.github.com/Unbabel/ sparse constrained attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>De-En</head><p>Ja-En Ro-En   REP-score: a new metric to count repetitions. Formally, given an n-gram s ∈ V n , let t(s) and r(s) be the its frequency in the model translation and reference. We first compute a sentence-level score</p><formula xml:id="formula_11">σ(t, r) = λ 1 s∈V n , t(s)≥2 max{0, t(s) − r(s)} + λ 2 w∈V max{0, t(ww) − r(ww)}.</formula><p>The REP-score is then given by summing σ(t, r) over sentences, normalizing by the number of words on the reference corpus, and multiplying by 100. We used n = 2, λ 1 = 1 and λ 2 = 2.</p><p>DROP-score: a new metric that accounts for possibly dropped words. To compute it, we first compute two sets of word alignments: from source to reference translation, and from source to the predicted translation. In our experiments, the alignments were obtained with fast align <ref type="bibr" target="#b5">(Dyer et al., 2013)</ref>, trained on the training partition of the data. Then, the DROP-score computes the percentage of source words that aligned with some word from the reference translation, but not with any word from the predicted translation. <ref type="table" target="#tab_2">Table 1</ref> shows the results. We can see that on average, the sparse models (csparsemax as well as sparsemax combined with coverage models) have higher scores on both BLEU and METEOR. Generally, they also obtain better REP and DROP scores than csoftmax and softmax, which suggests that sparse attention alleviates the problem of cov- erage to some extent.</p><p>To compare different fertility strategies, we ran experiments on the DE-EN for the csparsemax transformation <ref type="table" target="#tab_3">(Table 2)</ref>. We see that the PRE- DICTED strategy outperforms the others both in terms of BLEU and METEOR, albeit slightly. <ref type="figure" target="#fig_2">Figure 2</ref> shows examples of sentences for which the csparsemax fixed repetitions, along with the corresponding attention maps. We see that in the case of softmax repetitions, the decoder attends repeatedly to the same portion of the source sen- tence (the expression "letzten hundert" in the first sentence and "regierung" in the second sentence). Not only did csparsemax avoid repetitions, but it also yielded a sparse set of alignments, as ex- pected. Appendix B provides more examples of translations from all models in discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a new approach to address the cover- age problem in NMT, by replacing the softmax at- tentional transformation by sparse and constrained alternatives: sparsemax, constrained softmax, and the newly proposed constrained sparsemax. For the latter, we derived efficient forward and back- ward propagation algorithms. By incorporating a model for fertility prediction, our attention trans- formations led to sparse alignments, avoiding re- peated words in the translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the different attention transformations for a toy example with three source words. We show the attention values on the probability simplex. In the first row we assume scores z = (1.2, 0.8, −0.2), and in the second and third rows z = (0.7, 0.9, 0.1) and z = (−0.2, 0.2, 0.9), respectively. For constrained softmax/sparsemax, we set unit fertilities to every word; for each row the upper bounds (represented as green dashed lines) are set as the difference between these fertilities and the cumulative attention each word has received. The last row illustrates the cumulative attention for the three words after all rounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, and A R = {j ∈ [J] | α j = u j }. Then: • Forward propagation. α can be com- puted in O(J) time with the algorithm of Pardalos and Kovoor (1990) (Alg. 1 in Ap- pendix A). The solution takes the form α j = max{0, min{u j , z j − τ }}, where τ is a nor- malization constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention maps for softmax and csparsemax for two DE-EN sentence pairs (white means zero attention). Repeated words are highlighted. The reference translations are "This is Moore's law over the last hundred years" and "I am going to go ahead and select government."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>BLEU, METEOR, REP and DROP scores on the test sets for different attention transformations. 

BLEU METEOR 

CONSTANT, f = 2 
29.66 31.60 
CONSTANT, f = 3 
29.64 31.56 

GUIDED, 

29.56 31.45 
PREDICTED, c = 0 
29.78 31.60 
PREDICTED, c = 0.2 29.85 31.76 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Impact of various fertility strategies for 
the csparsemax attention model (DE-EN). 

</table></figure>

			<note place="foot" n="1"> Our software code is available at the OpenNMT fork www.github.com/Unbabel/OpenNMT-py/tree/dev and the running scripts at www.github.com/Unbabel/ sparse constrained attention.</note>

			<note place="foot" n="2"> This is the default implementation in the OpenNMT package. In preliminary experiments, feedforward attention (Bahdanau et al., 2014) did not show improvements.</note>

			<note place="foot" n="3"> A similar strategy was recently used by Gu et al. (2018) as a component of their non-autoregressive NMT model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the Unbabel AI Research team for numerous discussions, and the three anony-mous reviewers for their insightful comments. This work was supported by the European Re-search Council (ERC StG DeepSPIN 758969) and by the FundaçFundaç˜Fundação para a Ciência e Tec-nologia through contracts UID/EEA/50008/2013, PTDC/EEI-SII/7092/2014 (LearnBig), and CMU-PERI/TIC/0046/2014 (GoLocal).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Time bounds for selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaughan</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Ronald L Rivest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="448" to="461" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving attention modeling with implicit distortion and fertility for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3082" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>System Demonstrations</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning when to attend for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.11160</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning what&apos;s easy: Fully differentiable neural easy-first taggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kreutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="349" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Kyoto free translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="http://www.phontron.com/kftt" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Panos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naina</forename><surname>Pardalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kovoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
