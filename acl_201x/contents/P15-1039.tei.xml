<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating High Quality Proposition Banks for Multilingual Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
							<email>alan.akbik@tu-berlin.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Chiticariu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research -Almaden</orgName>
								<address>
									<addrLine>650 Harry Road</addrLine>
									<postCode>95120</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating High Quality Proposition Banks for Multilingual Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="397" to="407"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic role labeling (SRL) is crucial to natural language understanding as it identifies the predicate-argument structure in text with semantic labels. Unfortunately, resources required to construct SRL models are expensive to obtain and simply do not exist for most languages. In this paper, we present a two-stage method to enable the construction of SRL models for resource-poor languages by exploiting monolingual SRL and multilingual parallel data. Experimental results show that our method out-performs existing methods. We use our method to generate Proposition Banks with high to reasonable quality for 7 languages in three language families and release these resources to the research community.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic role labeling (SRL) is the task of automat- ically labeling predicates and arguments in a sen- tence with shallow semantic labels. This level of analysis provides a more stable semantic representa- tion across syntactically different sentences, thereby enabling a range of NLP tasks such as information extraction and question answering <ref type="bibr" target="#b22">(Shen and Lapata, 2007;</ref><ref type="bibr" target="#b13">Maqsud et al., 2014</ref>). Projects such as the Proposition Bank (PropBank) ( <ref type="bibr">Palmer et al., 2005</ref>) spent considerable effort to annotate corpora with semantic labels, in turn enabling supervised learn- ing of statistical SRL parsers for English. Unfor- * This work was conducted at IBM. tunately, due to the high costs of manual annota- tion, comparable SRL resources do not exist for most other languages, with few exceptions <ref type="bibr" target="#b10">(Hajič et al., 2009;</ref><ref type="bibr" target="#b8">Erk et al., 2003;</ref><ref type="bibr" target="#b31">Zaghouani et al., 2010;</ref><ref type="bibr" target="#b24">Vaidya et al., 2011)</ref>.</p><p>As a cost-effective alternative to manual annota- tion, previous work has investigated the direct pro- jection of semantic labels from a resource rich lan- guage (English) to a resource poor target language (TL) in parallel corpora <ref type="bibr" target="#b19">(Pado, 2007;</ref><ref type="bibr" target="#b26">Van der Plas et al., 2011</ref>). The underlying assumption is that orig- inal and translated sentences in parallel corpora are semantically broadly equivalent. Hence, if English sentences of a parallel corpus are automatically la- beled using an SRL system, these labels can be pro- jected onto aligned words in the TL corpus, thereby automatically labeling the TL corpus with seman- tic labels. This way, PropBank-like resources can automatically be created that enable the training of statistical SRL systems for new TLs.</p><p>However, as noted in previous work <ref type="bibr" target="#b19">(Pado, 2007</ref>  <ref type="figure">Figure 1</ref>: Pair of parallel sentences from Frenchgoldwith word alignments (dotted lines), SRL labels for the English sentence, and gold SRL labels for the French sentence. Only two of the seven English SRL labels should be projected here. shifts that go against this assumption. For example, in <ref type="figure">Fig. 1</ref>, the English sentence "We need to hold peo- ple responsible" is translated into a French sentence that literally reads as "There need to exist those re- sponsible". Hence, the predicate label of the English word "hold" should not be projected onto the French verb, which has a different meaning. As the exam- ple in <ref type="figure">Fig. 1</ref> shows, this means that only a subset of all SL labels can be directly projected.</p><p>In this paper, we aim to create PropBank-like re- sources for a range of languages from different lan- guage groups. To this end, we propose a two-stage approach to cross-lingual semantic labeling that ad- dresses such errors, shown in <ref type="figure" target="#fig_0">Fig. 2</ref>: Given a par- allel corpus in which the source language (SL) side is automatically labeled with PropBank labels and the TL side is syntactically parsed, we use a filtered projection approach that allows the projection only of high-confidence SL labels. This results in a TL corpus with low recall but high precision. In the second stage, we repeatedly sample a subset of com- plete TL sentences and train a classifier to iteratively add new labels, significantly increasing the recall in the TL corpus while retaining the improvement in precision.</p><p>Our contributions are: (1) We propose filtered projection focused specifically on raising the pre- cision of projected labels, based on a detailed anal- ysis of direct projection errors. (2) We propose a bootstrap learning approach to retrain the SRL to iteratively improve recall without a significant re- duction of precision, especially for arguments; <ref type="formula">(3)</ref> We demonstrate the effectiveness and generalizabil- ity of our approach via an extensive set of experi- ments over 7 different language pairs. (4) We gen- erate PropBanks for each of these languages and re- lease them to the research community. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Stage 1: Filtered Annotation Projection</head><p>Stage 1 of our approach <ref type="figure" target="#fig_0">(Fig. 2)</ref> is designed to create a TL corpus with high precision semantic labels.</p><p>Direct Projection The idea of direct annotation projection (Van der Plas et al., 2011) is to transfer semantic labels from SL sentences to TL sentences according to word alignments. Formally, for each pair of sentences s SL and s TL in the parallel corpus, the word alignment produces alignment pairs (w SL,i , w TL,i ), where w SL,i and w TL,i are words from s SL and s TL respectively. Under direct projection, if l SL,i is a predicate label for w SL,i and (w SL,i , w TL,i ) is an alignment pair, then l SL,i is transferred to w TL,i ; If l SL,j is a predicate-argument label for (w SL,i , w SL,j ), and (w SL,i , w TL,i ) and (w SL,j , w TL,j ) are alignment pairs, then l SL,j is transferred to (w TL,i , w TL,j ), as illustrated below.</p><p>Filtered Projection As discussed earlier, direct projection is vulnerable to errors stemming from issues such as translation shifts. We propose fil- tered projection focused specifically on improving the precision of projected labels. Specifically, for a pair of sentences s SL and s TL in the parallel corpus, we retain the semantic label l SL,i projected from w SL,i onto w TL,i if and only if it satisfies the filtering poli- cies. This results in a target corpus containing fewer labels but of higher precision compared to that ob- tained via direct projection.</p><p>In  <ref type="bibr" target="#b6">(Choi and McCallum, 2013)</ref>, a state-of-the-art SRL system, to produce se- mantic labels for English text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Error Analysis</head><p>We observe that direct projection labels have both low precision and low recall (see <ref type="bibr">Tab. 3 (Direct)</ref>).  • Translation Shift: Predicate Mismatch The most common predicate errors (37%) are translation shifts in which an English predicate is aligned to a French verb with a different meaning. <ref type="figure">Fig. 1</ref> illus- trates such a translation shift: label hold.01 of En- glish verb hold is wrongly projected onto the French verb ait, which is labeled as exist.01 in French gold .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of False Negatives</head><p>• Translation Shift: Verb→Non-Verb is another common predicate error (36%). English verbs may be aligned with TL words other than verbs, which is often indicative of translation shifts. For instance, in the following sentence pair</p><p>sSL We know what happened sFR On connait la suite</p><p>We know the result the English verb happen is aligned to the French noun suite (result), causing it to be wrongly pro- jected with the English predicate label happen.01.</p><p>• Non-Argument Head The most common argu- ment error (33%) is caused by the projection of ar- gument labels onto words other than the syntactic head of a target verb's argument. For example, in <ref type="figure">Fig. 1</ref> the label A1 on the English hold is wrongly transferred to the French ait, which is not the syn- tactic head of the complement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Filters</head><p>We consider the following filters to remove the most common types of false positives. Verb Filter (VF) targets Verb→Non-Verb transla- tion shift errors (Van der Plas et al., 2011). For- mally, if direct projection transfers predicate label l SL,i from w SL,i onto w TL,i , retain l SL,i only if both w SL,i and w TL,i are verbs. Translation Filter (TF) handles both Predicate Mismatch and Verb→Non-Verb translation shift er- rors. It makes use of a translation dictionary and allows projection only if the TL verb is a valid trans- lation of the SL verb. In addition, in order to ensure consistent predicate labels throughout the TL cor- pus, if a SL verb has several possible synonymous translations, it allows projection only for the most commonly observed translation. Formally, for an aligned pair (w SL,i , w TL,i ) where</p><formula xml:id="formula_0">w SL,i has predicate label l SL,i , if (w SL,i , w TL,i )</formula><p>is not a verb to verb translation from SL to TL, assign no label to w TL,i . Otherwise, split the set of SL trans- lations of w TL,i into synonym sets S 1 , S 2 , . . . ; For each k, let W k be the subset of S k most commonly aligned with w TL,i ; If w SL,i is in one of these W k , assign label l SL,i to w TL,i ; Otherwise assign no label to w TL,i . Reattachment Heuristic (RH) targets non- argument head errors that occur if a TL argument is not the direct child of a verb in the dependency parse tree of its sentence. <ref type="bibr">4</ref> Assume direct projection transfers the predicate-argument label l SL,j from (w SL,i , w SL,j ) onto (w TL,i , w TL,j ). Find the immedi- ate ancestor verb of w TL,j in the dependency parse tree. Denote as w TL,k its child that is an ancestor of w TL,j . Assign the label l SL,j to (w TL,i , w TL,k ) instead of (w TL,i , w TL,j ). An illustration is below:</p><p>RH ensures that labels are always attached to the syntactic heads of their respective arguments, as <ref type="bibr">de4</ref> In <ref type="bibr">(Padó and Lapata, 2009</ref>), a similar filtering method is defined over constituent-based trees to reduce the set of viable nodes for argument labels to all nodes that are not a child of some ancestor of the predicate.  termined by the dependency tree. An example of such reattachment is illustrated in <ref type="figure">Fig. 1</ref> (curved ar- row on TL sentence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Filter Effectiveness</head><p>We now present an initial validation on the effec- tiveness of the aforementioned filters by evaluating their contribution to annotation projection quality for French gold , as summarized in Tab. 3. VF reduces the number of wrongly projected predi- cate labels, resulting in an increase of predicate pre- cision to 59% (↑14 pp), without impact to recall. As a side effect, argument precision also increases to 53% (↑10 pp), since, if a predicate label cannot be projected, none of its arguments can be projected. TF 5 reduces the number of wrongly projected pred- icate labels even more significantly, increasing pred- icate precision to 88% (↑43 pp), at a small cost to re- call. Again, argument precision increases as a side effect. However, as expected, argument recall de- creases significantly (↓14 pp, to 17%), as many ar- guments can no longer be projected. RH targets argument labels directly (unlike VF and TF), significantly increasing argument precision and slightly increasing argument recall. In summary, initial experiments confirm that our proposed filters are effective in improving preci- sion of projected labels at a small cost in recall. In fact, TF+RH results in nearly 100% improvement in predicate and argument labels precision with a much smaller drop in recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Residual Errors</head><p>Filtered projection removes the most common errors discussed in Sec. 2.2. Most of the remaining errors come from the following sources. SRL Errors The most common residual errors in the remaining projected labels, especially for argu- ment labels, are caused by mistakes made by the En- glish SRL system. Any wrong label it assigns to an English sentence may be projected onto the TL sen- tence, resulting in false positives. No English Equivalent A small number of errors occur due to French particularities that do not exist in English. Such errors include certain French verbs for which no appropriate English PropBank labels exists, and French-specific syntactic particularities. <ref type="bibr">6</ref> Gold Data Errors Our evaluation so far relies on French gold as ground truth. Unfortunately, French gold does contain a small number of errors (e.g. missing argument labels). As a result, some correctly projected labels are being mistaken as false positives, causing a drop in both precision and recall. We therefore expect the true precision and recall of the approach to be somewhat higher than the estimate based on French gold .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Stage 2: Bootstrapped Training of SRL</head><p>As discussed earlier, the TL corpus generated via fil- tered projection suffers from low recall. We address this issue with the second stage of our method.</p><p>Relabeling The idea of relabeling (Van der Plas et al., 2011) is to first train an SRL system over a TL corpus labeled using direct projection (with VF filter) and then use this SRL to relabel the corpus, effectively overwriting the projected labels with po- tentially less noisy predicted labels.</p><p>We first present an analysis on relabeling in con- cert with our proposed filters (Sec. 3.1), which mo- tivates our bootstrap algorithm (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis of Relabeling Approach</head><p>We use the same experimental setup as in Sec. 2, and produce a labeled French corpus for each filtered an- notation method. We then train an off-the-shelf SRL system <ref type="bibr">(Björkelund et al., 2009</ref>) on each generated corpus and use it to relabel the corpus.</p><p>We measure precision and recall of each resulting TL corpus against French gold (see Tab. 4). Across all <ref type="bibr">6</ref> French negations, for instance, are split into a particle and a connegative. In the annotation scheme used in Frenchgold, particles and connegatives are labeled differently.  experiments, relabeling consistently improves recall over projection. The results also show how different factors affect the performance of relabeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplement vs. Overwrite Projected Labels</head><p>The labels produced by the trained SRL can be used to either overwrite projected labels as in (Van der Plas et al., 2011), or to supplement them (supply- ing labels only for words w/o projected labels). Whether to overwrite or supplement depends on whether labels produced by the trained SRL are of higher quality than the projected labels. We find that while predicted labels are of higher precision than directly projected labels, they are of lower precision than labels post filtered projection. Therefore, for filtered projection, it makes more sense to allow pre- dicted labels to only supplement projected labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Sampling Method</head><p>We are further in- terested in learning the impact of sampling the data on the quality of relabeling. For the best filter found earlier (TF+RH), we compare SRL trained on the entire data set (full data) with SRL trained only on the subset of completely annotated sentences (comp. sent.), where completeness is defined as: Definition 1. A direct component of a labeled sen- tence s TL is either a verb in s TL or a syntactic depen- dent of a verb. Then s TL is k-complete if s TL contains equal to or fewer than k unlabeled direct compo-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Bootstrap learning algorithm</head><p>Require: Corpus CTL with initial set of labels LTL, and resam- pling threshold function k(i); nents. 0-complete is abbreviated as complete.</p><formula xml:id="formula_1">for i = 1 to ∞ do Let ki = k(i); Let CTL comp = {w ∈ CTL : w ∈ sTL,</formula><p>We observe that for TF+RH, when new labels supplement projected labels, relabeling over com- plete sentences results in better recall at slightly re- duced precision, while including incomplete sen- tences into the training data reduces recall, but im- proves precision. While this finding may seem counterintuitive, it can be explained by how statis- tical SRL works. A densely labeled training data (such as comp. sent.) usually results in an SRL that gen- erates densely labeled sentences, resulting in better recall but poorer precision. On the other hand, train- ing data that is sparsely labeled results in an SRL that weighs the option of not assigning a label with higher probability, resulting in better precision and poorer recall. In short, one can control the trade- off between precision and recall of SRL output by manipulating the completeness of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bootstrap Learning</head><p>Building on the observation that we can sample data in such a way as to either favor precision or re- call, we propose a bootstrapping algorithm to train an SRL iteratively over k-complete subsets of the data which are supplemented by high precision la- bels produced from previous iteration. The detailed algorithm is depicted in Algorithm 1. Resampling Threshold Our goal is to use bootstrap learning to improve recall without sacrificing too much precision. Proposition 1. Under any resampling threshold, the set of labels L TL increases monotonically in each iteration of Algorithm 1. Since Prop. 1 guarantees the increase of the set of labels, we need to select a resampling function to favor precision while improving recall. Specifically, we use the formula k(i) = max(k 0 − i, 0), where k 0 is sufficiently large. Since the precision of labels generated by the SRL is lower than the precision of labels obtained from filtered projection, the preci- sion of the training data is expected to decrease with the increase in recall. Therefore, starting with a high k seeks to ensure high precision labels are added to the training data in the first iterations. Decreasing k in each iteration seeks to ensure that resampling is done in an increasingly restrictive way to ensure that only high-quality annotated sentences are added to the training data, thus maintaining a high confidence in the learned SRL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effectiveness of Bootstrapping</head><p>We experimentally evaluate the effectiveness of our model with k 0 = 9. 7 As shown in Tab 4, boot- strapping outperforms relabeling, producing labels with best overall quality in terms of F 1 measure and recall for both predicates and arguments, with a rel- atively small cost in precision.</p><p>While Algorithm 1 guarantees the increase of re- call (Prop. 1), it provides no such guarantee on pre- cision. Therefore, it is important to experimentally decide an early termination cutoff before the SRL gets overtrained. To do so, we evaluated the per- formance of the bootstrapping algorithm at each it- eration <ref type="figure" target="#fig_2">(Fig. 3)</ref>. We observe that for the first 3 it- erations, F 1 -measure for both predicates and argu- ments rises due to large increase in recall which offsets the smaller drop in precision. Then F 1 - measure remains stable, with recall rising and pre-   cision falling slightly at each iteration until conver- gence. To optimize precision and avoid overtrain- ing, we set an iteration cutoff of 3. This combina- tion of TF+RH filters, bootstrapping with k 0 = 9 and an iteration cutoff of 3 is used in the rest of our evaluation (Sec. 4), denoted as FB best .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multilingual Experiments</head><p>We use our method to generate Proposition Banks for 7 languages and evaluate the generated re- sources. We seek to answer the following ques- tions: (1) What is the estimated quality for the gen- erated PropBanks? How well does the approach work without language-specific adaptation? (2) Are there notable differences in quality from language to language; if so, why? We also present initial in- vestigations on how different factors affect the per- formance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Data Tab. 5 lists the 7 different TLs and resources used in our experiments. <ref type="bibr">8</ref> We chose these TLs be- cause (1) they are among top 10 most influential lan- guages in the world (Weber, 1997); and (2) we could find language experts to evaluate the results. English is used as SL in all our experiments.</p><p>Approach Tested For each TL, we used FB best (Sec. 3.3) to generate a corpus with semantic la- bels. From each TL corpus, we extracted all com- plete sentences to form the generated PropBanks.</p><p>8 From each parallel corpus, we only keep sentences that are considered well-formed based on a set of standard heuristics. For example, we require a well-formed sentence to end in punc- tuation and not to contain certain special characters. For Ara- bic, as the dependency parser we use has relatively poor parsing accuracy, we additionally require sentences to be shorter than 100 characters.  <ref type="table">Table 6</ref>: Estimated precision and recall over seven languages.</p><p>Manual Evaluation While a gold annotated cor- pus for French (French gold ) was available for our experiments in the previous Sections, no such re- sources existed for the other TLs we wished to eval- uate. We therefore chose to conduct a manual eval- uation for each TL, each executed identically: For each TL we randomly selected 100 complete sen- tences with their generated semantic labels and as- signed them to two language experts who were in- structed to evaluate the semantic labels (based on their English descriptions) for the predicates and their core arguments. For each label, they were asked to determine (1) whether the label is correct; (2) if yes, then whether the boundary of the labeled constituent is correct: If also yes, mark the label as fully correct, otherwise as partially correct.</p><p>Metrics We used the standard measures of preci- sion, recall, and F1 to measure the performance of the SRLs, with the following two schemes: (1) Ex- act: Only fully correct labels are considered as true positives; (2) Partial: Both fully and partially cor- rect matches are considered as true positives. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Tab. 6 summarizes the estimated quality of seman- tic labels generated by our method for all seven TL. As can be seen, our method performed well for all  seven languages and generated high quality seman- tics labels across the board. For predicate labels, the precision is over 95% and the recall is over 85% for all languages except for Hindi. For argument labels, when considering partially correct matches, the precision is at least 85% (above 90% for most languages) and the recall is between 66% to 83% for all the languages. These encouraging results obtained from a diverse set of languages implies the generalizability of our method. In addition, the inter-annotator agreement is very high for all the languages, indicating that the results obtained based on manual evaluation are very reliable. In addition, we make a number of interesting ob- servations: Dependency Parsing Accuracy The precision for exact argument labels is significantly below partial matches, particularly for Hindi (↓35 pp) and Ara- bic (↓19 pp). Since argument boundaries are deter- mined syntactically, such errors are caused by de- pendency parsing. The fact that Hindi and Arbic suffer the most from this issue is consistent with the poorer performance of their dependency parsers compared to other languages ( <ref type="bibr" target="#b17">Nivre et al., 2006;</ref><ref type="bibr" target="#b9">Green and Manning, 2010)</ref>. Hindi as the Main Outlier The results for Hindi are much worse than the results for other languages. Besides the poorer dependency parser performance, the size of the parallel corpus used could be a fac- tor: Hindencorp is one to two orders of magni- tude smaller than the other corpora. The quality of the parallel corpus could be a reason as well: Hindencorp was collected from various sources, while both UN and Europarl were extracted from governmental proceedings. Language-specific Errors Certain errors occur more frequently in some languages than others. An example are deverbal nouns in Chinese <ref type="bibr" target="#b28">(Xue, 2006</ref>) in formal passive constructions with support verb 受. Since we currently only consider verbs for pred-   icate labels, predicate labels are projected onto the support verbs instead of the deverbal nouns. Such errors appear for light verb constructions in all lan- guages, but particularly affect Chinese due to the high frequency of this passive construction in the UN corpus. Low Fraction of Complete Sentences As Tab. 7 shows, the fraction of complete sentences in the generated PropBanks is rather low, indicating the impact of moderate recall on the size of generated PropBanks. Especially for languages for which only small parallel corpora are available, such as Hindi, this points to the need to address recall issues in fu- ture work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Additional Experiments</head><p>The observations made in Sec. 4.2 suggests a few factors that may potentially affect the performance of our method. To better understand their impact, we conducted the following initial investigation. SRL models produced in this set of experiments were evaluated using French gold , sampled and eval- uated in the same way as other experiments in this section for comparability. Data Size We varied the data size for French by downsampling the UN corpus. As one can see from Tab. 8, downsampling the dataset by one order of magnitude (to 250k sentences) only slightly affects precision, while downsampling to 25k sentences has a more pronounced but still small impact on recall. It appears that data size does not have significant impact on the performance of our method.</p><p>Language-specific Customizations While our method is language-agnostic, intuitively language- specific customization can be helpful in address-ing language-specific errors. As an initial exper- iment, we added a simple heuristic to filter out French verbs that are commonly used for "existen- tial there" constructions, as one type of common errors for French involves the syntactic expletive il ( <ref type="bibr" target="#b7">Danlos, 2005</ref>) in "existential there" constructions such as il faut (see <ref type="figure">Fig. 1</ref> (TL sentence) for an ex- ample) wrongly labeled with with role information. As shown in Tab. 9, this simple customization re- sults in a small increase in precision, suggesting that language-specific customization can be helpful. Quality of English SRL As noted in Sec. 2.5, errors made by English SRL are often prorogated to the TL via projection. To assess the impact of English SRL quality, we used two different English SRL systems: CLEARNLP and MATE-SRL. As can be seen from Tab. 9, the impact of English SRL quality is sub- stantial on argument labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multilingual PropBanks</head><p>To facilitate future research on multilingual SRL, we release the created PropBanks for all 7 languages to the research community to encourage further re- search. Tab. 7 gives an overview over the resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Annotation Projection in Parallel Corpora to train monolingual tools for new languages was in- troduced in the context of learning a PoS tag- ger ( <ref type="bibr" target="#b30">Yarowsky et al., 2001</ref>). Similar in spirit to our approach of using filters to increase the precision of projected labels, recent <ref type="bibr">work (Täckström et al., 2013</ref>) uses token and type constraints to guide learn- ing in cross-lingual PoS tagging. Projection of Semantic Labels was considered for FrameNet ( <ref type="bibr" target="#b0">Baker et al., 1998</ref>) in <ref type="bibr">(Padó and Lapata, 2009;</ref><ref type="bibr" target="#b1">Basili et al., 2009)</ref>. Recently, however, most work in the area focuses on PropBank, which has been identified as a more suitable annotation scheme for joint syntactic-semantics settings due to broader coverage (Merlo and van der <ref type="bibr" target="#b14">Plas, 2009)</ref>, and was shown to be usable for languages other than En- glish ( <ref type="bibr" target="#b16">Monachesi et al., 2007)</ref>. Direct projection of PropBank annotations was considered in (Van der Plas et al., 2011). Our ap- proach significantly outperforms theirs in terms of recall and F 1 for both predicates and arguments (Section 3). A approach was proposed in (Van der <ref type="bibr" target="#b26">Plas et al., 2014</ref>) in which information is aggregated at the corpus level, resulting in a significantly bet- ter SRL corpus for French. However, this approach has several practical limitations: (1) it does not con- sider the problem of argument identification of SRL systems, treating arguments as already given; (2) it generates rules for the argument classification step preferably from manually annotated data; (3) it has been demonstrated for a single language (French), and was not applied to any other language. In con- trast, our approach trains an SRL system for both predicate and argument labels, in a completely au- tomatic fashion. Furthermore, we have applied our approach to generate PropBanks for 7 languages and conducted experiments that indicate a high F 1 mea- sure for all languages (Section 4). Other Related Work A number of approaches such as model transfer <ref type="bibr" target="#b12">(Kozhevnikov and Titov, 2013)</ref> and role induction <ref type="bibr" target="#b23">(Titov and Klementiev, 2012)</ref> exist for the argument classification step in the SRL pipeline. In contrast, our work addresses the full SRL pipeline and seeks to generate SRL resources for TLs with English PropBank labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a two-staged method to construct mul- tilingual SRL resources using monolingual SRL and parallel data and showed that our method outper- forms previous approaches in both precision and recall. More importantly, through comprehensive experiments over seven languages from three lan- guage families, we show that our proposed method works well across different languages without any language specific customization. Preliminary re- sults from additional experiments indicate that bet- ter English SRL and language-specific customiza- tion can further improve the results, which we aim to investigate in future work. A qualitative com- parison against existing or under-construction Prop- Banks for Chinese <ref type="bibr" target="#b29">(Xue, 2008)</ref>, <ref type="bibr">Hindi (Vaidya et al., 2011</ref>) or Arabic ( <ref type="bibr" target="#b31">Zaghouani et al., 2010</ref>) may be in- teresting, both for comparison of resources and for defining language-specific customizations. In ad- dition, we plan to expand our experiments both to more languages as well as NomBank ( <ref type="bibr" target="#b15">Meyers et al., 2004</ref>)-style noun labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the proposed two-stage approach for projecting English (EN) semantic role labels onto a TL corpus.</figDesc><graphic url="image-1.png" coords="2,63.64,82.54,478.21,201.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Values at each bootstrap iteration.</figDesc><graphic url="image-4.png" coords="6,325.29,91.27,187.20,131.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Dependency parsers:</head><label></label><figDesc>STANFORD: (Green and Manning, 2010), MATE-G: (Bohnet, 2010), MATE-T: (Bohnet and Nivre, 2012), MALT: (Nivre et al., 2006). Parallel corpora: UN: (Rafalovitch et al., 2009), Europarl: (Koehn, 2005), Hindencorp: (Bojar et al., 2014). Word alignment: The UN corpus is already word-aligned. For others, we use the Berkeley Aligner (DeNero and Liang, 2007).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Breakdown of error classes in argument projection. 

Analysis of False Positives While the recall pro-
duced by direct projection is close to the theoretical 
upper bound, the precision is far from the theoretical 
upper bound of 100%. To understand causes of false 
positives, we examine a random sample of 200 false 
positives, of which 100 are incorrect predicate la-
bels, and 100 are incorrect argument labels belong-
ing to correctly projected predicates. Tab. 1 and 2 
show the detailed breakdown of errors for predicates 
and arguments, respectively. We first analyze the 
most common types of errors and discuss the resid-
ual errors later in Sec. 2.5. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Quality of predicate and argument labels for different 
projection methods on Frenchgold, including upper bound. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Experiments on Frenchgold, with different projection and SRL training methods. SP=Supplement; OW=Overwrite.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Experimental setup . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Characteristics of the generated PropBanks. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Estimated impact of downsampling parallel corpus. 

PREDICATE 
ARGUMENT 

HEURISTIC 
P 
R 
F1 
P 
R 
F1 

none  *  
0.87 0.81 0.84 0.86 0.74 
0.8 
none  *  *  
0.88 
0.8 
0.84 0.76 0.65 
0.7 

customization  *  0.87 0.81 0.84 
0.9 
0.74 0.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Impact of English SRLs (  *  =CLEARNLP,  *  *  =MATE-
SRL) and language-spec. customization (filter synt. expletive). 

</table></figure>

			<note place="foot" n="1"> The resources are available on request.</note>

			<note place="foot" n="2"> For instance, the French verb sembler may be correctly labeled as either of the synonyms: seem.01 or appear.02. 3 This upper bound is different from the one reported in (Van der Plas et al., 2011) which corresponds to the interannotator agreement over manual annotation of 100 sentences.</note>

			<note place="foot" n="5"> In all experiments in this paper, we derived the translation dictionaries from the WIKTIONARY project and used VERBNET and WORDNET to find SL synonym groups.</note>

			<note place="foot" n="7"> We found that setting k0 to larger values had little impact on the final results .</note>

			<note place="foot" n="9"> Note that since the manually evaluated semantic labels are only a small fraction of the labels generated, the performance numbers obtained from manual evaluation is only an estimate of the actual quality for the generated resources.Thus the numbers obtained based on manual evaluation cannot be directly compared against the numbers computed over Frenchgold.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-language frame semantics transfer in bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
	<note>Computational Linguistics and Intelligent Text Processing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Bohnet and Nivre2012</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very high accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hindencorp-hindi-english and hindi-only corpus for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with selectional branching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic recognition of french expletive pronoun occurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Danlos</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/berkeleyaligner/" />
	</analytic>
	<monogr>
		<title level="m">Natural language processing. Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05)</title>
		<editor>Liang2007] John DeNero and Percy Liang</editor>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
	<note>The Berkeley Aligner</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards a resource for lexical semantics: A large german corpus with extensive semantic annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Better arabic parsing: Baselines, evaluations, and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2010] Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="394" to="402" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer of semantic role labeling models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1190" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nerdle: Topic-specific question answering using wikia seeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maqsud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Computational Linguistics, Proceedings of the Conference System Demonstrations</title>
		<editor>Lamia Tounsi and Rafal Rak</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abstraction and generalisation in semantic role labels: Propbank, verbnet or both</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Van Der Plas2009]</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lonneke</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Plas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2009</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annotating noun argument structure for nombank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="803" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adding semantic role annotation to a corpus of written dutch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Monachesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linguistic Annotation Workshop, LAW &apos;07</title>
		<meeting>the Linguistic Annotation Workshop, LAW &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maltparser: A data-driven parsergenerator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joakim Nivre, Johan Hall, and Jens Nilsson</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2216" to="2219" />
		</imprint>
	</monogr>
	<note>Proceedings of LREC</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-lingual annotation projection for semantic roles</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="340" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Sebastian Padó and Mirella Lapata</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-Lingual Annotation Projection Models for Role-Semantic Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<editor>MP. [Palmer et al.2005] Martha Palmer, Daniel Gildea, and Paul Kingsbury</editor>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Saarland University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>The proposition bank: An annotated corpus of semantic roles</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">United nations general assembly resolutions: A six-language parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rafalovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MT Summit</title>
		<meeting>the MT Summit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
	<note>Rafalovitch et al.2009</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Verbnet: A Broad-coverage, Comprehensive Verb Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapata2007] Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<editor>Citeseer. [Täckström et al.2013] Oscar Täckström, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre</editor>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Using semantic roles to improve question answering</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crosslingual induction of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev2012] Ivan Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klementiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="647" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analysis of the hindi proposition bank using dependency structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaidya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Linguistic Annotation Workshop</title>
		<meeting>the 5th Linguistic Annotation Workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-lingual validity of propbank in the manual annotation of french</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Der Plas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Linguistic Annotation Workshop</title>
		<meeting>the Fourth Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="113" to="117" />
		</imprint>
	</monogr>
	<note>Lonneke Van der Plas, Tanja Samardži´Samardži´c, and Paola Merlo. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Global methods for cross-lingual semantic role and predicate labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Der Plas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<editor>Marianna Apidianaki, Rue John von Neumann, and Chenhua Chen</editor>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1279" to="1290" />
		</imprint>
	</monogr>
	<note>Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Top languages: The world&apos;s 10 most influential languages. Language Today</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic role labeling of nominalized predicates in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="431" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Labeling chinese predicates with semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="255" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inducing multilingual text analysis tools via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international conference on Human language technology research</title>
		<meeting>the first international conference on Human language technology research</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The revised arabic propbank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zaghouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Linguistic Annotation Workshop</title>
		<meeting>the Fourth Linguistic Annotation Workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="222" to="226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
