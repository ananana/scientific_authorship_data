<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Neural Model for Temporal Information Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanliang</forename><surname>Meng</surname></persName>
							<email>ymeng@cs.uml.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory" key="lab1">Text Machine Lab for NLP</orgName>
								<orgName type="laboratory" key="lab2">Text Machine Lab for NLP Department of Computer Science University of Massachusetts Lowell</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory" key="lab1">Text Machine Lab for NLP</orgName>
								<orgName type="laboratory" key="lab2">Text Machine Lab for NLP Department of Computer Science University of Massachusetts Lowell</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Neural Model for Temporal Information Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="527" to="536"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>527</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a context-aware neural network model for temporal information extraction , with a uniform architecture for event-event, event-timex and timex-timex pairs. A Global Context Layer (GCL), inspired by the Neural Turing Machine (NTM), stores processed temporal relations in the narrative order, and retrieves them for use when the relevant entities are encountered. Relations are then classified in this larger context. The GCL model uses long-term memory and attention mechanisms to resolve long-distance dependencies that regular RNNs cannot recognize. GCL does not use postprocess-ing to resolve timegraph conflicts, outper-forming previous approaches that do so. To our knowledge, GCL is also the first model to use an NTM-like architecture to incorporate the information about global context into discourse-scale processing of natural text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting information about the order and timing of events from text is crucial to any system that attempts an in-depth natural language understand- ing, whether related to question answering, tempo- ral inference, or other related tasks. Earlier tempo- ral information extraction (TemporalIE) systems tended to rely on traditional statistical learning with feature-engineered task-specific models, typ- ically used in succession ( <ref type="bibr" target="#b20">Yoshikawa et al., 2009;</ref><ref type="bibr" target="#b10">Ling and Weld, 2010;</ref><ref type="bibr" target="#b15">Sun et al., 2013;</ref><ref type="bibr" target="#b0">Chambers et al., 2014;</ref><ref type="bibr" target="#b13">Mirza and Minard, 2015)</ref>.</p><p>Recently, there have been some attempts to ex- tract temporal relations with neural network mod- els, particularly with recurrent neural networks (RNN) models <ref type="bibr" target="#b11">(Meng et al., 2017;</ref><ref type="bibr" target="#b1">Cheng and Miyao, 2017;</ref><ref type="bibr" target="#b17">Tourille et al., 2017</ref>) and convolu- tional neural networks (CNN) ( <ref type="bibr" target="#b9">Lin et al., 2017</ref>). These models predominantly use token embed- dings as input, avoiding handcrafted features for each task. Typically, neural network models out- perform traditional statistical models. Some stud- ies also try to combine neural network models with rule-based information retrieval methods <ref type="bibr" target="#b2">(Fries, 2016)</ref>. These systems require different models for different pair types, so several models must be combined to fully process text.</p><p>A common disadvantage of all these models is that they build relations from isolated pairs of entities (events or temporal expressions). This context-blind, pairwise classification often gener- ates conflicts in the resulting timegraph. Common ways of ameliorating the conflicts is to apply some ad hoc constraints to account for basic properties of relations (e.g. transitivity), often without con- sidering the content of the text per se. For ex- ample, <ref type="bibr" target="#b10">Ling and Weld (2010)</ref> designed transitiv- ity formulae, used with local features. <ref type="bibr" target="#b14">Sun (2014)</ref> proposed a strategy that "prefers the edges that can be inferred by other edges in the graph and remove the ones that are least so". Another approach is to use the results from separate classifiers to rank re- sults according to their general confidence ( <ref type="bibr">Mani et al., 2007;</ref><ref type="bibr" target="#b0">Chambers et al., 2014</ref>). High-ranking results overwrite low-ranking ones. <ref type="bibr" target="#b11">Meng et al. (2017)</ref> used a greedy pruning algorithm to remove weak edges from the timegraph until it is coherent.</p><p>When humans read text, we certainly do not follow the procedure of interpreting interpret re- lations only locally first, and later come up with a compromise solution that involves all the entities. Instead, if local information is insufficient, we consider the relevant information from the wider context, and resolve the ambiguity as soon as pos- sible. The resolved relations are stored in our memory as "context" for further processing. If the later evidence suggests our early interpretation was wrong, we can correct it.</p><p>This paper proposes a model to simulate such mechanisms. Our model introduces a Global Con- text Layer (GCL), inspired by the Neural Turing Machine (NTM) architecture ( <ref type="bibr" target="#b3">Graves et al., 2014</ref>), to store processed relations in narrative order, and retrieve them for use when related entities are en- countered. The stored information can also be up- dated if necessary, allowing for self-correction. This paper's contributions are as follows. To our knowledge, this is the first attempt to use neu- ral network models with updateable external mem- ory to incorporate global context information for discourse-level processing of natural text in gen- eral and for temporal relation extraction in par- ticular. It gives a uniform treatment of all pairs of temporally relevant entities. We obtain state- of-the-art results on TimeBank-Dense, which is a standard benchmark for TemporalIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>We train and evaluate our model on TimeBank- Dense 1 <ref type="bibr" target="#b0">(Chambers et al., 2014</ref>). There are 6 classes of relations: SIMULTANEOUS, BEFORE, AFTER, IS INCLUDED, INCLUDES, and VAGUE TimeBank-Dense annotation aims to approximate a complete temporal relation graph by including all intra-sentential relations, all relations between adjacent sentences, and all relations with docu- ment creation time. TimeBank-Dense is one of the standard benchmarks for intrinsic evalution of TemporalIE systems. We follow the experimental setup in <ref type="bibr" target="#b0">Chambers et al. (2014)</ref>, which splits the corpus into training/validation/test sets of 22, 5, and 9 documents, respectively. Previous publica- tions often use the micro-averaged F1 score, which is equivalent to accuracy in this case. We also rely on the micro-averaged F1 score for model selec- tion and evaluation.</p><p>Following <ref type="bibr" target="#b11">Meng et al. (2017)</ref>, we augment the data by flipping all pairs, except for relations in- volving document creation time <ref type="bibr">(DCT)</ref>. In other words, if a pair (e i , e j ) exists, we add (e j , e i ) to the dataset with the opposite label (e.g. BEFORE becomes AFTER). The augmentation applies to the validation and test sets also. In the final evaluation, a double-checking technique picks one result from the two-way classification, based on output scores. The dataset is heavily imbalanced. The training set has as much as 44.1% VAGUE labels, whereas only 1.8% labels are SIMULTANEOUS. We did not do any up-sampling or down-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System</head><p>Our system has two main components. The first one is a pairwise relation classifier, and the other is the Global Context Layer (GCL). The pairwise re- lation classifier follows the architecture designed by <ref type="bibr" target="#b11">Meng et al. (2017)</ref>, which used the dependency paths to the least common ancestor (LCA) from each entity as input. We train the first component first, and then assemble them in a combined neu- ral network to continue training. <ref type="figure">Fig. 1</ref> gives an overview of the system. <ref type="figure">Figure 1</ref>: System overview. Originally, the pre-trained sys- tem has one more dense layer and an output layer, but they are truncated before combination. The max pooling layers on top of each Bi-LSTM layers are omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Global Context Layer</head><p>The Global Context Layer (GCL) we propose is inspired by the Neural Turing Machine (NTM) ar- chitecture, which is an extension of a recurrent neural network with external memory and an at- tention mechanism for reading and writing to that memory. NTM has been shown to perform ba- sic tasks such as copying, sorting, and associative recall ( <ref type="bibr" target="#b3">Graves et al., 2014</ref>). The external mem- ory not only enables a large (theoretically infinite) capacity for information storage, but also allows flexible access based on attention mechanisms.</p><p>Essentially, GCL is a specialized form of NTM, which eliminates some parameters to facilitate training, and specializes some functions to impose restrictions. While not as powerful as the canoni-cal NTM, it is more suitable for the task of retain- ing and updating global context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Motivation</head><p>Vanilla RNNs struggle with capturing long- distance dependencies. Gated RNNs such as LSTM have trainable gates to address the "van- ishing and exploding gradient" problem (Hochre- iter and Schmidhuber, 1997). At each time step, it chooses what to memorize and forget, so patterns over arbitrary time intervals can be recognized. However, the memory in LSTM is still short-term. No matter how long the cell states keep certain in- formation, once it is forgotten, it gets lost forever. Such a mechanism suffices for modeling contigu- ous sequences. For example, sentences are nat- urally fit units for such models, since a sentence starts only after the preceding sentence is finished, and LSTM may be an adequate tool to process sen- tences. However, when the sequences are not con- tiguous, as in temporal and other discourse-scale relations, LSTM models do not have the capabil- ity to look for input pieces across sequences.</p><p>When humans read text, discourse-level infor- mation is often distributed across the full scope of the text. To fully understand an article, we must be able to organize the processed information across sentences and paragraphs. In particular, to inter- pret temporal relations between entities in a sen- tence, sometimes we also look at relations with other entities elsewhere in the text. Such entities or relations form no regular sequences, and only a system with long-term memory as well as atten- tion mechanisms can process them. An NTM-like architecture has an external memory with attention mechanisms, so it is an ideal candidate for such tasks. Furthermore, unlike the models that use at- tention over inputs ( <ref type="bibr" target="#b18">Vinyals et al., 2015;</ref><ref type="bibr" target="#b8">Kumar et al., 2016</ref>), NTM-like models are capable of up- dating previously stored representations. We de- scribe below the GCL architecture that we use to store and update the global context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Reading</head><p>The input to the GCL layer is a concatenation of three layers from the pairwise neural network. Two of these are the entity context representation layers, encoded by the two LSTM branches. The other is the penultimate hidden layer before the output layer, which encodes the relation. We can write them as [e 1 , e 2 , x]. The context representa- tions are used as "keys" to uniquely identify the entities. Note that we use flat context embeddings, rather than dependency path embeddings, because dependency paths tend to be short and will also vary for the same entity, depending on the other entity in the pair. As such, they do not provide a unique way to represent an entity.</p><p>The original design of NTM has a complex addressing mechanism for reading, which also makes it difficult to train. An important difference in GCL is that we separate the "key" component from the "content" component of memory. Each</p><formula xml:id="formula_0">memory slot S[i] consists of [K[i]; M [i]]</formula><p>, where S is the whole memory with n slots, i ≤ n is the index, K is the key and M is the content. Ad- dressing is only performed on the key component.</p><p>The key component stores the representation of two entities, provided by the layers encoding the flat entity context.</p><formula xml:id="formula_1">K[i] = e M1 [i] ⊕ e M2 [i] (1)</formula><p>Here ⊕ is the concatenation operator. In the GCL model, the read head computes a reading weight W n×1 from the input entity representations e 1 , e 2 and the entity representations e M1 , e M2 in mem- ory (i.e., the keys in each memory slot). The first step is to compute the distance between current in- put and the memory columns, as shown in <ref type="bibr">Eq. 2. D[i]</ref> is the Euclidean distance between the input key and the memory key of slot</p><formula xml:id="formula_2">M [i]. D [i]</formula><p>is computed after flipping the two entities. We do so because the order of entities in a pair should not affect their relevance.</p><formula xml:id="formula_3">D[i] = 1 Z ||e 1 ⊕ e 2 − e M1 [i] ⊕ e M2 [i]|| 2 2 D [i] = 1 Z ||e 2 ⊕ e 1 − e M1 [i] ⊕ e M2 [i]|| 2 2 (2)</formula><p>where</p><formula xml:id="formula_4">Z = i D[i]</formula><p>is the normalization factor, and so is Z for the flipped case. The reading weight is then calculated as in Eq. 3, where 1 n×1 is a vector of all 1's.</p><formula xml:id="formula_5">W [i] = max(softmax(1 − D)[i], softmax(1 − D )[i])<label>(3)</label></formula><p>Every element of W represents the relevance of the corresponding memory slot (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Often it is still too blurred and needs to be further sharp- ened as in Eq. 4. Here β is a positive number. W β is a point-wise exponential function by power of β. A large β allows "winner takes all", so only the most relevant memory slots are read.</p><formula xml:id="formula_6">W read = softmax(W β )<label>(4)</label></formula><p>Parameter β could be a constant, or could be train- able. Our model computes it from the current in- put x t and the previous output h t−1 , and thus it varies in each time step. W sharp and b sharp are trainable weights and bias, c β is a constant, and ReLU is the rectified linear function.</p><formula xml:id="formula_7">β t = ReLU (W sharp [x t , h t−1 ]+b sharp )+c β (5)</formula><p>With the sharpened reading weight vector, we are able to obtain the read vector r 1×m from M as a weighted sum, as in Eq. 6.</p><formula xml:id="formula_8">r = i W read [i]M [i]<label>(6)</label></formula><p>Generally speaking, the depth of memory M should be large enough to allow sparse encoding, so that crucial information is not lost after the sum- mation. The read vector then contains contextual information relevant to current input. Both the read vector and the current input are fed to the con- troller, yielding GCL output. Unlike the canonical NTM, the CGL model does not have a trainable gate interpolating the W t computed at time t, with W t−1 computed at previous time t−1. The weight vector is not passed to next time step, so the atten- tion has no "inertia". We tried two variants of the controller: (a) state- tracking, with an LSTM layer, and (b) stateless, with a dense layer. An LSTM controller has an internal state, and also has gates to select input and output. If the input data and/or the read vec- tor from M have regular patterns with respect to time steps, an LSTM controller would be a better choice. For the specific task of temporal relation extraction, we saw no difference in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Writing</head><p>The controller produces an output h t , which is sent to the next layer and also used to update M . Similar to reading, the first step of writing pro- cedure is to compute an attention weight vector over the slots of M . As described above, the read- ing procedure computes a weighted sum over slots of M . The writing procedure writes a weighted h t to each slot. The attention mechanism here is de facto a soft addressing mechanism. The slots with a higher attention value will be the addresses which will get more of an update.</p><p>The same weight vector W computed as shown in Eq. 3 is used for writing. However, an addi- tional operation is introduced for writing. Recall that the weights are computed from entity repre- sentations. If the input entities are e 1 and e 2 , the weight vector should have high values in the slots corresponding to e 1 and/or e 2 . But we may not always want relevant memory slots to be overwrit- ten. Instead, additional information can be written to a different slot. Additionally, when M is rela- tively empty, as at the beginning, the addressing mechanism may treat all slots equally, and uni- formly update all slots in the same way. In this case we want the weight vector to shift each time, so M can diversify fast.</p><p>Therefore we use a shift function similar to the canonical NTM. The idea is to compute a shifted weight vector W by convolving W with a shift kernel s which maps a shift distance to a probabil- ity value. For example, s(−1) = 0.2, s(0) = 0.5, s(1) = 0.3 means the probabilities of shifting left, no shifting, and shifting right are 0.2, 0.5, 0.3, re- spectively. Generally speaking, we want s to give zeros for most shift distances, so the shifting oper- ation is limited to a small range.</p><formula xml:id="formula_9">W [i] = n−1 j=0 W [j]s[i − j]<label>(7)</label></formula><p>At each time step, the shift kernel depends on cur- rent input and output. If the allowed shift range is [-s/2, +s/2], we train a weight W s and bias b s to calculate the shift weights C s×1 ,</p><formula xml:id="formula_10">C t = softmax(W s [x t , h t ] + b s )<label>(8)</label></formula><p>Then the weights are mapped to a circulant ker- nel to perform the convolution in Eq. 7, the final output is W . Finally, the sharpening still needs to be applied. For the writing procedure, both addressing and shifting are "soft" in nature, and thus could yield a blurred outcome. Again, we train the weights to obtain a sharpening parameter γ each time, and perform softmax over W .</p><formula xml:id="formula_11">γ t = ReLU (W sharp [x t , h t ] + b sharp ) + c γ (9) W write = softmax( W γ )<label>(10</label></formula><p>) W γ is the point-wise exponential function, over the shifted weight vector. c γ is a positive constant.</p><p>The original NTM model has gates for interpo- lating W γ at the current time with the one com- puted at the previous time step, but we omit this operation. We also omit the erase vector and the add vector, so W write fully controls what to over- write in M and what to retain. As a result, the writing operation can be expressed as:</p><formula xml:id="formula_12">M t [i] = M t−1 [i]+W write [i](h t −M t−1 [i]) (11)</formula><p>The first term in Eq. 11 is the memory in the pre- vious time step, and the second term is the update. We update the keys in the same way. As we can see, the keys come from entity representations, but are not exactly the same, due to W write .</p><formula xml:id="formula_13">K t [i] = K t−1 [i] + W write [i](e 1 ⊕ e 2 − K t−1 [i])<label>(12)</label></formula><p>3.1.4 GCL vs. Canonical NTM We highlight below some major differences be- tween the canonical NTM and the GCL model. Typically, NTM computes the keys from input and output for accessing different memory addresses. In GCL, the keys are simply the entity representa- tions [e 1 , e 2 ] from input, in either order. The key function effectively involves slicing and flipping the input. Further discussion of the differences be- tween the GCL addressing mechanism and some of the other NTM variations is provided in Sec. 5.</p><p>Another major difference is that we do not use any gates to interpolate the attention vector at the current time step with the one from the previous time step. Instead, the previous attention vector is totally ignored. Since we do not compute the erase vector or the add vector, this allows the attention vector to fully control memory updates.</p><p>In addition, we unified the trainable weights for calculating β and γ at each time step. We found these parameters not to be crucial, and setting them to be constant does not affect the results. We also do not shift attention for reading. A possible advantage of shifting attention is that neighboring slots of the focus can also be accessed, providing a way to simulate associative recall. This is based on the fact that the writing procedure tends to write similar memories close to each other. However, in this study we want the reading procedure to be re- stricted. Associative recall can be realized from attention vector itself, without shifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pairwise Classification Model</head><p>The pairwise model classifies individual entity pairs, where entities are events and time expres- sions (timexes). In other words, for each pair, we only use the local context, and the relation of one pair does not affect the classification results for other pairs. We follow the architecture pro- posed in <ref type="bibr" target="#b11">Meng et al. (2017)</ref>, but with the follow- ing changes: (1) all three types of pairs are han- dled by the same neural network, rather than by three separately trained models; (2) the neighbor- ing words (a flat context) of entity mentions are used to generate input, in addition to words on syntactic dependency paths; (3) all timex-timex pairs are included as well, not only event-timex and event-event pairs; (4) every pair is assigned a 3-dimensional "time value", to approximate the rule-based approach when possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Event Pairs and Event-Timex Pairs</head><p>TimeBank-Dense dataset labels three types of pairs: intra-sentence, cross-sentence and docu- ment creation time (DCT). For intra-sentence pairs and cross-sentence pairs, we follow <ref type="bibr" target="#b11">Meng et al. (2017)</ref>. The shortest dependency path between the two entities is identified, and the word embeddings from the path to the least common ancestor for each entity are processed by two LSTM branches, with a separate max pooling layer for each branch. Path to the root is used for cross-sentence rela- tions. For relations with the DCT, we use a single word now as a placeholder for the DCT branch. Unlike <ref type="bibr" target="#b11">Meng et al. (2017)</ref>, we allow the model to accept all three pair types, with a "pair type" fea- ture as a component of input, defined as an integer with the value 1, -1 or 0, respectively.</p><p>In addition to the shortest dependency path, our model also uses a flat local context window, that is, the words around each entity mention, regard- less of syntactic structures. For an entity start- ing with word w i , the local context window is 5 words to the left and 10 words to its right i.e. w i−5 w i−4 ...w i w i+1 ...w i+10 . The windows are cut short at the edge of a sentence, or when the sec- ond entity in encountered. By using this context window, the words between two entities are of- ten used twice by the system, and thus given more consideration. To inform the system of other en- tity mentions, we also add special input tokens at the locations where events and timexes are tagged. The embeddings of the special tokens are uni- formly initialized, and automatically tuned during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Timex Pairs</head><p>The method described in <ref type="bibr" target="#b11">Meng et al. (2017)</ref> clas- sifies timex pairs by handcrafted rules and then adds them to the final results prior to postprocess- ing. Since timexes have concrete time values, a rule-based method would seem appropriate. How- ever, since our model uses global context to help classify relations and timex-timex pairs enrich the global context representation, we design a way for a common classifier model to handle such pairs.</p><p>When DCT is not involved, timex pairs are cre- ated the same way as cross-sentence pairs, that is, path to the root is used for each entity. DCT is represented by the placeholder word now. In ad- dition to the word-based representations, another input vector is used to simulate the rule-based ap- proach, to be explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Time Value Vectors</head><p>Every timex tag has a time value, following the ISO-8601 standard. Every value can be mapped to a 2D vector of real values (start, end). For a pair we use the subtraction of the vectors to represent the difference. Suppose we have timexes in below: THE HAGUE, Netherlands (AP)_ The World Court &lt;TIMEX3 tid="t21" type="DATE" value="1998-02-27" temporalFunction="true" functionInDocument="NONE" anchorTimeID="t0"&gt;Friday&lt;/TIMEX3&gt; rejected U.S. and British objections to a Libyan World Court case that has the trial of two Libyans suspected of blowing up a Pan Am jumbo jet over Scotland in &lt;TIMEX3 tid="t22" type="DATE" value="1988" temporalFunction="false" functionInDocument="NONE"&gt;1988&lt;/TIMEX3&gt;.  <ref type="bibr">= (1998.155, 1998.155)</ref>, and the second one <ref type="bibr">(1988, 1988 + 364/365) = (1988, 1988.997)</ref>. The difference of the values are put in the sign function, to ob- tain the representation: <ref type="bibr">(sign(1988 -1998.155</ref>), sign <ref type="bibr">(1988.997 -1998.155</ref>)) = (-1, -1). Vector (-1, -1) clearly indicates the AFTER relation between t21 and t22. We set the minimum interval to be a day, which is generally sufficient for our data. The DURATION timexes are not considered, and word- based input vectors are used to represent them.</p><p>In order to make all the input data have the same shape, we assign the time value vector to all pairs, even if a timex is not involved. For non-timex pairs, a vector (-1, 0, 0) is used. The first element -1 to indicate a "pseudo" time value. Real timex pairs have the first value of 1, so the example we just discussed would be assigned a vector <ref type="figure">(1, -1, - 1)</ref>. The time value vectors allow the model to take advantage of rule-based information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Two Components</head><p>We tried training the two components in a com- bined system, but found it slow to converge. In our experiments, we trained the pairwise model first, froze it, and then combined it with the GCL layer to train the GCL. This method also helps us ob- serve whether the GCL component alone improves results, given the same input.</p><p>We tried combining the systems in two ways. One is to connect the output layer of the pre- trained model to GCL, and the other is to slice the pre-trained model and connect its hidden layer to GCL. All the GCL layers are bi-directional, aver- aging forward and backward passes. By connect- ing the output layer, which has a softmax activa- tion, we hand the final decisions made by the pair- wise model to GCL. On the other hand, the hid- den layer provides higher layers with cruder but richer information. We found that the latter per- forms better. It is also possible to train the two components together from scratch. In this case, the learning rate has to be set much lower to as- sure convergence, and the training requires more epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For all the experiments, hyperparameters includ- ing the number of epochs are tuned with the val- idation set only. Training data is segmented into chunks. Each chunk contains relation pairs in the narrative order. The size of chunks is randomly chosen from <ref type="bibr">[40,</ref><ref type="bibr">60,</ref><ref type="bibr">80,</ref><ref type="bibr">120,</ref><ref type="bibr">160]</ref> at the begin- ning of each epoch of training. The GCL main- tains a memory for each chunk, and clears it at the end of a chunk. The idea here is to train the model on short paragraphs to avoid overfitting.</p><p>To introduce further randomness, the chunks are rotated for each epoch. For a specific training file, if chunk i starts with pair n i in epoch 1, in epoch 2, chunk i will start with pair n i +chunksize+11. 11 is a prime number we chose to assure each epoch observes different compositions of chunks. By doing the rotation, some pairs in the final chunk of epoch 1 will show up in the first chunk in epoch 2 as well. However, within each chunk, we do not randomize pairs, so narrative order is preserved at this level. We also do not shuffle the chunks, but only rotate them.</p><p>Evaluation on the test set uses only one chunk for each file (chunk size is the number of pairs). Each relation pair is only processed once, without "multiple rounds of reading". Thus, we essentially train the model to read shorter paragraphs (varied in length), but test it on long articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pairwise Model</head><p>As described in Section 3.2, the pairwise classi- fier has the following input vectors: left and right shortest path branches, two flat context vectors, a pair type flag, and a time value vector. Word em- beddings are initialized with glove.840B.300d word vectors 2 , and set to be trainable. The Bi- LSTM layers are followed by max-pooling. The two hidden layers have size 512 and 128, respec- tively. We train this model for 40 epochs, us- ing the RMSProp optimizer <ref type="bibr" target="#b16">(Tieleman and Hinton, 2012</ref>). The learning rate is scheduled as lr = 2 × 10 −3 × 2 − n 5 , where n is the number of epochs.</p><p>The middle block of <ref type="table">Table 1</ref> shows the per- formance of the pairwise model after applying double-checking. Since all pairs are flipped, double-checking combines results from (e i , e j ) and (e j , e i ), picking the label with the higher prob- ability score, which typically boosts performance. The results without double-checking show similar trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GCL model</head><p>After training the pairwise model, we combine it with GCL. Unless otherwise indicated, the results reported in this section use the model configura- tion that connects the hidden layer (rather than the output layer) of the pairwise model with a bidirec- tional GCL layer. The bidirectional GCL is real- ized as the average of a forward GCL and a back- ward GCL, each producing a sequence. Then two more hidden layers are put on top of it, followed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Micro-F1 Macro-F1 CAEVO (not NN model)</p><p>.507 CATENA (not NN model)</p><p>.511 <ref type="bibr" target="#b1">Cheng et al. 2017</ref> .520 3 <ref type="bibr" target="#b11">Meng et al. 2017</ref> .519 pairwise .535 .528 Two more hidden layers .539 .532 GCL w/ state-tracking controller .545 .538 GCL w/ stateless controller .546 .538 GCL w/ pre-trained output layer .541 .536 <ref type="table">Table 1</ref>: Results on the test set. The GCL models use the same hyperparameters, if possible. The two models on the top do not use neural networks. The results in the two lower blocks all use double-check. "Two more hidden lay- ers" means adding two dense layers on top of the pre-trained model without using GCL. The last row corresponds to con- necting the output layer of a pre-trained model to GCL layers with stateless controller.</p><p>by an output layer. All the layers in the pre-trained pairwise model are set to be untrainable. The two trainable hidden layers have sizes 512 and 128, re- spectively, with ReLU activtion and 0.3 dropout after each one. The GCLs have 128 memory slots. Learning rate is scheduled as lr = 2×10 −4 ×2 − n 2 . In the experiments, we found the models converge quite fast with respect to the number of epochs. It is not surprising because the lower layers are al- ready well trained, and frozen (no updating). Af- ter the 5th epoch, the training accuracy typically reaches 0.95. We stop training after 10 epochs.</p><p>The bottom block of <ref type="table">Table 1</ref> presents the re- sults, showing that all models from the present pa- per outperform existing models from the literature. One may argue the combined system adds more hidden layers over a pre-trained model, which con- tributes to the improvement in performance. We show a comparison to a baseline model which adds two dense layers on top of the pairwise model, without the GCL. The configuration of the two layers is the same as we used for the GCL models. The result shows that the performance is slightly higher than what we get from the pairwise model, but the difference is smaller than what we get from GCL models -suggesting that the performance improvement with GCL models is not just due to more parameters. We also tried adding an LSTM layer on top of the pre-trained model, and found the system cannot converge. It again confirms that GCL is more powerful than LSTM in handling ir- regular time series.</p><p>We found no difference in performance be- tween the stateless controller and state-tracking controller. Connecting the output layer of the pre-trained model to GCL seems to generate weaker results than connecting the hidden layer, although it also outperforms the pairwise model, and all pre- vious models in literature.</p><p>We performed significance testing to compare the pairwise model and the GCL-enabled model. A paired one-tailed t-test shows the results from the GCL model are significantly higher than re- sults from pairwise model (p-value 0.0015). While significant, the improvement is relatively small, we believe due in part to the small size of Timebank-Dense dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>To illustrate the difference in performance of the pairwise model and the GCL model, we created a sample paragraph in which long-distance depen- dencies and references to DCT are needed to re- solve some of the temporal relations:</p><p>John met Mary in Massachusetts when they attended the same university. They are getting married in 2019, 2 years after their graduation. But this year, they have relocated to New Hampshire.</p><p>We created the gold standard annotation for this text with 5 events, 2 timexes, and 24 TLINKs (see appendix) <ref type="bibr">4</ref> . We set the DCT to an arbitrary date "2018-04-01". There are no VAGUE or SIMULTA- NEOUS relations.</p><p>For this paragraph, the pairwise model yields an accuracy (i.e. micro-averaged F1) of 0.292, while the GCL-enabled model yields 0.417. Overall, the GCL-enabled model assigns 6 VAGUE labels while the pairwise assigns 11. It reflects the fact that GCL tries to infer relations from otherwise vague evidence. For example, it is difficult to infer the relation between met and 2019 from the local con- text (without DCT, particularly), so the pairwise model labels it as VAGUE, while the GCL-enabled model correctly assigns BEFORE.</p><p>Recall that the GCL is placed on top of a pre- trained pairwise model, so the mistakes made by the pairwise model propagate to GCL. For exam- ple, the pairwise model incorrectly classifies 2019 as BEFORE graduation -perhaps, due to a some- what unusual syntax. But the GCL-enabled sys- tem assigns it a VAGUE label, probably as a way to compromise. In the TimeBank-Dense test data, VAGUE cases dominate, which may have made it more difficult for GCL to assign proper labels. In the future, we believe it may be better to omit writing (and reading) the VAGUE relations to/from GCL.  We also break down the results according to the types of pairs. Compared to other systems, our approach has a big advantage for event-event (E- E) pairs, which is by far the most common (64%) relation pairs for all data, and also requires more complex natural language understanding. Com-   <ref type="bibr" target="#b12">Mirza and Tonelli (2016)</ref> paired to CAEVO, our performance on event-DCT (E-D) and event-timex (E-T) pairs is not that great. CAEVO uses engineered features such as entity at- tributes, temporal signals, and semantic informa- tion from WordNet, which seems to work well in these two cases. We took a closer look at our E-D results, and found that the relatively low perfor- mance is mainly caused by misclassifying VAGUE as AFTER. As <ref type="table" target="#tab_6">Table 4</ref> shows, among the 72  However, E-D is a relatively small category with only 311 instances in the test set, so it is difficult to draw any a substantive conclusion in this case. Recall that our model has a uniform architec- ture for all input types and is trained on event- event, event-timex and event-DCT pairs simulta- neously. As a result, its performance is not optimal for some lower-frequency pair types. Tuning the model for each pair type separately, as well as re- sampling to deal with class imbalance would, per- haps, improve performance. However, the point of these experiments was not to get the largest im- provement, but to show that the GCL mechanism can replace heuristic-based timegraph conflict res- olution, improving the performance of an other- wise very similar model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Error Analysis</head><formula xml:id="formula_14">Systems E-D E-E E-T Overall</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>While the GCL model is inspired by NTM, other NTM variants have also been proposed recently. <ref type="bibr" target="#b21">Zhang et al. (2015)</ref> proposed structured memory architectures for NTMs, and argue they could alle- viate overfitting and increase predictive accuracy. <ref type="bibr" target="#b4">Graves et al. (2016)</ref> proposed a memory access mechanism on top of NTM, which they call Differ- entiable Neural Computer (DNC). DNC can store the transitions between memory locations it ac- cesses, and thus can model some structured data. <ref type="bibr">Gülçehre et al. (2016)</ref> proposed a Dynamic Neural Turing Machine (D-NTM) model, which allows discrete access to memory. <ref type="bibr">Gülçehre et al. (2017)</ref> further simplified the addressing algorithm, so a single trainable matrix is used to get locations for read and write. Both models separate the ad- dress section from the content section of memory, as do we. We came up with the idea indepen- dently, noting that the content-based addressing in the canonical NTM model is difficult to train. A crucial difference between GCL and these mod- els is that they use input "content" to compute keys. In GCL, the addressing mechanism fully depends on the entity representations, which are provided by the context encoding layers and not computed by the GCL controller. Addressing then involves matching the input entities and the enti- ties in memory.</p><p>Other than NTM-based approaches, there are models that use an attention mechanism over ei- ther input or external memory. For instance, the Pointer Networks ( <ref type="bibr" target="#b18">Vinyals et al., 2015</ref>) uses at- tention over input timesteps. However, it has no power to rewrite information for later use, since they have no "memory" except for the RNN states. The Dynamic Memory Networks ( <ref type="bibr" target="#b8">Kumar et al., 2016)</ref> has an "episodic memory" module which can be updated at each timestep. However, the memory there is a vector ("episode") with- out internal structure, and the attention mechanism works on inputs, just as in Pointer Networks. Our GCL model and other NTM-based models have a memory with multiple slots, and the addressing function (attention) dictates writing and reading to/from certain slots in the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed the first context-aware neural model for temporal information extraction using an external memory to represent global context. Our model introduces a Global Context Layer which is able to save and retrieve processed tem- poral relations, and then use this global context to infer new relations from new input. The memory can be updated, allowing self-correction. Experi- mental results show that the proposed model beats previous results without resorting to ad-hoc reso- lution of timegraph conflicts in postprocessing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: GCL computing attention weights. Input entity representations are compared to the Key section of GCL memory. Slots with the same or similar entities get more attention.</figDesc><graphic url="image-2.png" coords="3,307.28,62.81,218.27,158.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows the overall performance for each relation using the GCL system with the stateless controller.</head><label>2</label><figDesc></figDesc><table>Since we flip pairs and use double-
checking to pick one result for each pair, BE-
FORE/AFTER and IS INCLUDED/INCLUDES are 
actually treated in the same way, respectively. 
Here we map the results to original pairs, in order 
to compare to other systems. 

Predicted labels 
SIMUL BEF AFT IS INCL INCL VAG Total 
SIMUL 
10 
0 
9 
2 
1 
17 
39 
BEF 
0 
327 
27 
15 
5 
215 589 
AFT 
1 
26 
208 
4 
5 
184 428 
IS INCL 
1 
27 
3 
59 
2 
67 
159 
INCL 
0 
16 
9 
2 
19 
70 
116 
VAG 
1 
171 
87 
28 
17 
596 900 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Overall results per relation. As the table shows, the VAGUE relation causes the most trouble. It is not only because VAGUE is the largest class, but also because it is often semantically ambiguous, so even human experts have low inter-annotator agreement. If we allow a relatively sparse labeling of data, and use other evaluation methods (e.g. question answering), the VAGUE class is not likely to have similar effects.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on the E-D, E-D and E-T pairs. GCL 
stands for the GCL-enabled system with a stateless controller. 
Frequencies are percentages in the test set. T-T pairs are 
not shown here. CAEVO is from Chambers et al. (2014). 
CATENA is from </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Test results from event and document creation time 
(E-D) pairs. The rows are true labels and the columns are 
predicted labels. 

VAGUE relations in E-D pairs, 20 are labeled AF-
TER by our system. In a news article, most events 
occur before the DCT i.e. the time when the arti-
cle was written. If the temporal relation is vague, 
our system tends to guess that the event occurs af-
ter the DCT. It is interesting because AFTER only 
accounts for 16% of all E-D pairs in test data (and 
about the same in training data), behind BEFORE 
(41%), VAGUE (21%), and IS INCLUDED (18%). 
</table></figure>

			<note place="foot" n="1"> https://www.usna.edu/Users/cs/ nchamber/caevo/#corpus</note>

			<note place="foot" n="2"> https://nlp.stanford.edu/projects/glove/ 3 This result does not include timex-timex pairs, which is 3% of total test instances.</note>

			<note place="foot" n="4"> Note that in TimeBank-Dense, no TLINKS are associated DURATION timexes, so 2 years is not annotated</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project is funded in part by an NSF CAREER award to Anna Rumshisky (IIS-1652742).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense event ordering with a multi-pass architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying temporal relations by bidirectional lstm over dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Brundlefly at semeval-2016 task 12: Recurrent neural networks vs. joint inference for clinical temporal information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Alan</forename><surname>Fries</surname></persName>
		</author>
		<idno>abs/1606.01433</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>abs/1410.5401</idno>
		<title level="m">Neural turing machines. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adrì A Puigdomènech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
			<publisher>Koray Kavukcuoglu, and Demis Hassabis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1701.08718</idno>
		<title level="m">Memory augmented neural networks with wormhole connections. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dynamic neural turing machine with soft and hard addressing schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1607.00036</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Representations of time expressions for temporal relation extraction with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08-04" />
			<biblScope unit="page" from="322" to="327" />
			<pubPlace>Vancouver, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Three approaches to learning tlinks in timeml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
		<idno>CS-07- 268</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence, AAAI 2010</title>
		<editor>Inderjeet Mani, Ben Wellner, Marc Verhagen, and James Pustejovsky</editor>
		<meeting>the TwentyFourth AAAI Conference on Artificial Intelligence, AAAI 2010<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Temporal information extraction</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temporal information extraction for question answering using syntactic dependencies in an lstm-based architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanliang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>of the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Catena: Causal and temporal relation extraction from natural language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 26th International Conference on Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hlt-fbk: a complete temporal processing system for qa tempeval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Lyse</forename><surname>Minard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>of the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="801" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Time Well Tell: Temporal Reasoning in Clinical Narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University at Albany, SUNY</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD dissertation. Department of Informatics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating temporal relations in clinical text: 2012 i2b2 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Uzuner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="806" to="813" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural architecture for temporal relation extraction: A bi-lstm approach for detecting narrative containers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Tourille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelie</forename><surname>Neveol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="230" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<editor>C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jointly identifying temporal relations with markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumasa</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="405" to="413" />
		</imprint>
	</monogr>
	<note>ACL &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Structured memory for neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1510.03931</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
