<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUSEEC: A Multilingual Text Summarization Tool</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Litvak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering</orgName>
								<orgName type="institution">Shamoon College of Engineering</orgName>
								<address>
									<addrLine>Beer Sheva</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Vanetik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering</orgName>
								<orgName type="institution">Shamoon College of Engineering</orgName>
								<address>
									<addrLine>Beer Sheva</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Last</surname></persName>
							<email>mlast@bgu.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Systems Engineering</orgName>
								<orgName type="institution">Ben Gurion University of the Negev</orgName>
								<address>
									<settlement>Beer Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Churkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering</orgName>
								<orgName type="institution">Shamoon College of Engineering</orgName>
								<address>
									<addrLine>Beer Sheva</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MUSEEC: A Multilingual Text Summarization Tool</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="73" to="78"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The MUSEEC (MUltilingual SEntence Extraction and Compression) summariza-tion tool implements several extractive summarization techniques-at the level of complete and compressed sentences-that can be applied, with some minor adaptations , to documents in multiple languages. The current version of MUSEEC provides the following summarization methods: (1) MUSE-a supervised summa-rizer, based on a genetic algorithm (GA), that ranks document sentences and extracts top-ranking sentences into a summary , (2) POLY-an unsupervised sum-marizer, based on linear programming (LP), that selects the best extract of document sentences, and (3) WECOM-an un-supervised extension of POLY that compiles a document summary from compressed sentences. In this paper, we provide an overview of MUSEEC methods and its architecture in general.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High quality summaries can significantly reduce the information overload of many professionals in a variety of fields. Moreover, the publication of information on the Internet in an ever-increasing variety of languages dictates the importance of de- veloping multi-lingual summarization tools that can be readily applied to documents in multiple languages.</p><p>There is a distinction between extractive sum- marization that is aimed at the selection of a sub- set of the most relevant fragments -mostly com- plete sentences -from a source text, and abstrac- tive summarization that generates a summary as a reformulated synopsis expressing the main idea of the input documents.</p><p>Unlike the abstractive summarization methods, which require natural language processing oper- ations, language-independent summarizers work in an extractive manner, usually via ranking frag- ments of a summarized text by a relevance score and selecting the top-ranked fragments (e.g., sen- tences) into a summary. Because sentence scor- ing methods, like MUSE (MUltilingual Sentence Extractor) <ref type="bibr" target="#b7">(Last and Litvak, 2012)</ref>, use a greedy approach, they cannot necessarily find the best ex- tract out of all possible combinations of sentences.</p><p>Another approach, based on the maximum cov- erage principle <ref type="bibr" target="#b13">(McDonald, 2007;</ref><ref type="bibr" target="#b3">Gillick and Favre, 2009)</ref>, tries to find the best subset of ex- tracted sentences. This problem is known as NP- hard ( <ref type="bibr" target="#b4">Khuller et al., 1999</ref>), but an approximate so- lution can be found by the POLY algorithm <ref type="bibr" target="#b9">(Litvak and Vanetik, 2013</ref>) in polynomial time.</p><p>Given the tight length constraints, extractive systems that select entire sentences are quite lim- ited in the quality of summaries they can produce. Compressive summarization seeks to overcome this limitation by compiling summaries from com- pressed sentences that are composed of strictly rel- evant information <ref type="bibr" target="#b5">(Knight and Marcu, 2002</ref>). WE- COM (Weighted COMpression) summarization approach ( <ref type="bibr" target="#b14">Vanetik et al., 2016</ref>) combines methods for term weighting and sentence compression into a weighted compression model. WECOM extends POLY by utilizing the choice of POLY's objective functions for the term-weighting model.</p><p>In this paper, we present MUSEEC, a multi- lingual text summarization platform, which cur- rently implements three single-document sum- marization algorithms: MUSE <ref type="bibr" target="#b7">(Last and Litvak, 2012</ref>), POLY algorithm ( <ref type="bibr" target="#b9">Litvak and Vanetik, 2013)</ref>, and WECOM ( <ref type="bibr" target="#b14">Vanetik et al., 2016</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MUSEEC: Overview</head><p>MUSEEC can be applied to documents in multi- ple languages. The current version was tested on nine languages: English, Hebrew, Arabic, Persian, Russian, Chinese, German, French, and Spanish, and its summarization quality was evaluated on three languages: English, Hebrew and Arabic. <ref type="bibr">1</ref> The sections below provide brief descriptions of the system architecture and its main components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MUSEEC Architecture</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, MUSEEC runs a pipeline that is composed of the following components: 1. Preprocessing. MUSEEC can work with docu- ments written in any language by treating the text as a sequence of UTF-8 characters. It performs the following pre-processing operations: (1) sentence segmentation, (2) word segmentation, (3) stem- ming, and (4) stop-word removal. The last two operations are skipped if they are unavailable for a given language. Some optional, linguistic fea- tures require Part-of Speech (POS) tagging as a pre-processing step as well. 2. Training. This stage is optional and it is rel- evant only for the supervised MUSE algorithm. Given a set of training parameters, MUSE finds the best vector of weights for a linear combination of chosen sentence features. The resulting vector (trained model) can be saved and used for future summarization of documents in the same or any other language. 3. Ranking. At this stage, entire sentences or their parts (in case of compressive summarization) are ranked. 4. Sentence Compression. This stage is also op- tional and it is relevant only for compressive sum- marization performed by WECOM. Given ranked sentence parts, new, shorter sentences are com- piled and ranked.</p><p>5. Extraction. Complete sentences are selected in the case of MUSE and POLY, and compressed sentences in the case of WECOM. 6. Postprocessing. The generated summaries can be post-processed by anaphora resolution (AR) and named entity (NE) tagging operations, if the corresponding tools are provided for a given language. MUSEEC utilizes Stanford CoreNLP package for English. 7. Results Presentation. Summaries are pre- sented in two formats: sentences highlighted in the original document, selected by the user from a list of input documents, and a list of extracted sentences shown in their original order. The user can also sort sentences by their rank and see their scores. MUSEEC allows the user to setup various sum- marization parameters, general and specific for a chosen algorithm, which are listed in <ref type="table">Table 1</ref>. The table does not contain explicit WECOM settings because running WECOM equivalent to running POLY with "compressive" choice for the summary type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MUltilingual Sentence Extractor (MUSE)</head><p>MUSE implements a supervised learning ap- proach to extractive summarization, where the best set of weights for a linear combination of sen- tence scoring metrics is found by a GA trained on a collection of documents and their gold stan- dard summaries. MUSE training can be performed from the MUSEEC tool. The obtained weighting vector is used for sentence scoring in future sum- marizations. Since most sentence scoring methods have a linear computational complexity, only the training phase of MUSE, which may be applied in advance, is time-consuming. In MUSEEC, one can use ROUGE-1 and ROUGE-2, Recall (Lin and Hovy, 2003) 2 as fitness functions for measuring summarization quality-similarity with gold stan-  <ref type="table">Table 1</ref>: MUSEEC general and method-specific parameters.</p><p>dard summaries, which should be maximized dur- ing the training. The reader is referred to <ref type="bibr" target="#b10">(Litvak et al., 2010</ref>) for a detailed description of the opti- mization procedure implemented by MUSE.</p><p>The user can choose a subset of sentence met- rics that will be included by MUSE in the lin- ear combination. By default, MUSEEC will use the 31 language-independent metrics presented in <ref type="bibr" target="#b7">(Last and Litvak, 2012)</ref>. MUSEEC also allows the user to employ additional, linguistic features, which are currently available only for the English language. These features are based on lemmatiza- tion, multi-word expressions (MWE), NE recogni- tion (NER), and POS tagging, all performed with Stanford CoreNLP package. The list of linguistic features is available in <ref type="bibr" target="#b2">(Dlikman, 2015)</ref>.</p><p>The training time of the GA is proportional to the number of GA iterations 3 multiplied by the number of individuals in a population, times the fitness (ROUGE) evaluation time. The summa- rization time (given a model) is linear in number of terms for all basic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">POLYnomial summarization with</head><p>POLYtopes (POLY)</p><p>Following the maximum coverage principle, the goal of POLY, which is an unsupervised summa- rizer, is to find the best subset of sentences that, under length constraints, can be presented as a summary. POLY uses an efficient text represen- tation model with the purpose of representing all possible extracts 4 without computing them explic- itly, that saves a great portion of computation time. Each sentence is represented by a hyperplane, and all sentences derived from a document form hyper- plane intersections (polytope). Then, all possible extracts can be represented by subplanes of hyper- plane intersections that are not located far from the boundary of the polytope. POLY is aimed at find- ing the extract that optimizes the chosen objective function. MUSEEC provides the following categories of objective functions, described in detail in <ref type="bibr" target="#b9">(Litvak and Vanetik, 2013</ref>). 1. Maximal weighted term sum, that maximizes the information coverage as a weighted term sum with following weight options supported:</p><p>1. Term sum: all terms get weight 1;</p><p>2. POS F: terms appearing earlier in the text get higher weight;</p><p>3. POS L: terms appearing close to the end of the text get higher weight;</p><p>4. POS B: terms appearing closer to text bound- aries (beginning or end) get higher weight;</p><p>5. TF: weight of a term is set to its frequency in the document;</p><p>6. TF IDF: weight of a term is set to its tf*idf value;</p><p>2. McDonald -maximal sentence coverage and minimal sentence overlap, that maximizes the summary similarity to the text and minimizes the similarity between sentences in a summary, based on the Jaccard similarity measure (based on (Mc- Donald, 2007)); 3. Gillick -maximal bigram sum and minimal sentence overlap, that maximizes the information coverage as a bigram sum while minimizing the similarity between sentences (based on <ref type="bibr" target="#b3">(Gillick and Favre, 2009)</ref>). All functions produce term weights in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> that are then used for calculating the importance scores of each sentence. Like in MUSE, the sentences with the highest score are added to the summary in a greedy man- ner. The overall complexity of POLY is polyno- mial in number of sentences. Further details about the POLY algorithm can be found in ( <ref type="bibr" target="#b9">Litvak and Vanetik, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">WEighted Compression (WECOM)</head><p>In WECOM ( <ref type="bibr" target="#b14">Vanetik et al., 2016</ref>), we shorten sentences by iteratively removing Elementary Discourse Units (EDUs), which were defined as grammatically independent parts of a sentence in (Marcu, 1997). We preserve the important content by optimizing the weighting function that measures cumulative importance and preserve a valid syntax by following the syntactic structure of a sentence. The implemented approach consists of the following steps: Term weight assignment. We apply a weighting model (using one of the options available for POLY) that assigns a non-negative weight to each occurrence of every term in all sentences of the document. EDU selection and ranking. At this stage, we prepare a list of candidate EDUs for re- moval. First, we generate the list of EDUs from constituency-based syntax trees <ref type="bibr" target="#b11">(Manning and Schütze, 1999</ref>) of sentences. Then, we omit from the list those EDUs that may create a grammatically incorrect sentence if they were to be removed. Finally, we compute weights for all remaining EDU candidates from term weights obtained in the first stage and sort them by increasing weight. Budgeted sentence compression and selection. We define a summary cost as its length measured in words or characters <ref type="bibr">5</ref> . We are given a budget for the summary cost, for example, the maximal number of words in a summary. The compressive part of WECOM is responsible for selecting EDUs in all sentences such that (1) the weight to cost ratio of the summary is maximal; and (2) the summary length does not exceed a given budget.</p><p>The compressed sentences are expected to be more succinct than the originals, to contain the important content from the originals, and to be grammatically correct. The compressed sentences are selected to a summary by the greedy manner. The overall complexity of WECOM is bound <ref type="figure">by  N log(N )</ref>, where N is a number of terms in all sentences. <ref type="table" target="#tab_2">Tables 2, 3</ref>, and 4 contain the summarized re- sults of automated evaluations for the MultiL- ing 2015, single-document summarization (MSS) task. The quality of the summaries is measured by ROUGE-1 (Recall, Precision, and F-measure), <ref type="bibr">(C.-Y, 2004</ref>). We also demonstrate the absolute ranks of each submission-P-Rank, R-Rank, and F-Rank-with their scores sorted by Precision, Re- call, and F-measure, respectively. Only the best submissions (in terms of F-measure) for each par- ticipating system are presented and sorted in de- scending order of their F-measure scores. Two systems-Oracles and Lead-were used as top-line and baseline summarizers, respectively. Oracles compute summaries for each article using the combinatorial covering algorithm in ( <ref type="bibr" target="#b1">Davis et al., 2012</ref>)-sentences were selected from a text to max- imally cover the tokens in the human summary. Since the Oracles system can actually "see" the human summaries, it is considered as the optimal algorithm and its scores are the best scores that ex- tractive approaches can achieve. The Lead system simply extracts the leading substring of the body text of the articles having the same length as the human summary of the article.  As can be seen, MUSE outperformed all other participating systems except for CCS in Hebrew. CCS (the CCS-5 submission, to be precise) uses the document tree structure of sections, subsec- tions, paragraphs, and sentences, and compiles a summary from the leading sentences of recursive system P score R score F score P-Rank R-Rank F-Rank    bottom-up interweaving of the node leading sen- tences, starting from leaves (usually, paragraphs in a section). POLY got very close scores, though it is an unsupervised approach and its comparison to a supervised summarizer is not fair. MUSEEC also participated in the multi- document summarization (MMS) task, on En- glish, Hebrew and Arabic. MUSE got first place on Hebrew, and 2 nd places on English and Ara- bic languages, out of 9 participants. POLY got third place on Hebrew, 4 th place on English, and 5 th place on Arabic, out of 9 participants. We explain the differences between scores in Hebrew and other languages by the lack of NLP tools for this language. For example, none of the competing systems performed stemming for Hebrew. Also, it is possible that the quality of the gold standard summaries or the level of agreement between an- notators in Hebrew was lower than in other lan- guages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>WECOM was evaluated in <ref type="bibr" target="#b14">(Vanetik et al., 2016</ref>) on three different datasets <ref type="bibr">(DUC 2002</ref><ref type="bibr">, DUC 2004</ref><ref type="bibr">, and DUC 2007</ref>) using automated and human ex- periments. Both automated and human scores have shown that compression significantly im- proves the quality of generated summaries. Ta- ble 5 contains results for POLY and WECOM summarizers on the DUC 2002 dataset. Statis- tical testing (using a paired T-test) showed that there is a significant improvement in ROUGE- 1 recall between ILP concept-based extraction method of <ref type="bibr" target="#b3">Gillick and Favre (2009)</ref> and WECOM with weights generated by Gillick and Favre's method. Another significant improvement is be- tween ILP extraction method of <ref type="bibr" target="#b13">McDonald (2007)</ref> and WECOM with weights generated by McDon- ald's method.  Practical running times for MUSE (summariza- tion) and POLY are tens of milliseconds per a text document of a few thousand words. WECOM running time is strictly dependent on the running time of dependency parsing performed by Stan- ford CoreNLP package, which takes 2 − 3 seconds per sentence. Given pre-saved pre-processing re- sults, WECOM takes tens of milliseconds per doc- ument as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System R-1 R R-1 P R-1 F R-2 R R-2 P R-2 F POLY + Gillick</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Possible Extensions</head><p>MUSEEC functionality can be easily extended us- ing its API. New algorithms can be added by im- plementing new ranking and/or compression mod- ules of the pipeline. The pipeline is dynamically built before running a summarization algorithm, and it can be configured by a programmer 6 . The currently implemented algorithms can also be ex- tended. For example, a new sentence feature for MUSE can be implemented by preparing one con- crete class implementing a predefined interface. Using Java reflection, it does not require changes in any other code. New objective functions can be provided for POLY by implementation of one concrete class implementing the predefined inter- face and adding a few rows in the objective func- tions factory for creation instances of a new class (using factory method design pattern). Using de- pendency injections design pattern, MUSEEC can switch from Stanford CoreNLP package to any other tool for text preprocessing. MUSEEC is totally language-independent and works for any language with input texts provided in UTF-8 en- coding. If no text processing tools for a given language are provided, MUSEEC skips the rele- vant stages in its pipeline (for example, it does not perform stemming for Chinese). Providing new NLP tools can improve MUSEEC summa- rization quality on additional languages. The sub- sequent stages in the MUSEEC pipeline (sentence ranking and compression) are totally language- independent and work with structured data gener- ated during pre-processing. The optional capabil- ities of NE tagging and AR in the post-processing stage may be also extended with additional NLP tools for specific languages.</p><p>The programmer and user guidelines for ex- tending and using MUSEEC can be provided upon request.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Final Remarks</head><p>In this paper, we present MUSEEC -a plat- form for summarizing documents in multiple lan- guages. MUSEEC implements several variations of three single-document summarization methods: MUSE, POLY, and WECOM. The big advantage of MUSEEC is its multilinguality. The system has been successfully evaluated on benchmark docu- ment collections in three languages (English, Ara- bic, and Hebrew) and tested on six more lan- guages. Also, MUSEEC has a flexible architec- ture and API, and it can be extended to other algo- rithms and languages.</p><p>However, MUSEEC has the following limita- tions: all its methods, especially compressive, are dependent on the pre-processing tools, in terms of summarization quality and performance. In order to improve coherency of the generated summaries, the MUSEEC user can apply AR as well as NE tagging to the generated summaries. More sophis- ticated post-processing operations performed on the extracted text in MUSEEC can further improve the user experience.</p><p>The MUSEEC tool, along with its code, is available under a BSD license on https://bitbucket.org/elenach/ onr_gui/wiki/Home. In the future, we intend to prepare a Web application allowing users to apply MUSEEC online.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MUSEEC pipeline</figDesc><graphic url="image-1.png" coords="2,117.36,62.81,362.85,83.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>CCS</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : MSS task. English.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>MSS task. Hebrew. 

system 
P score R score F score P-Rank R-Rank F-Rank 

Oracles 
0.630 0.658 0.644 1 
1 
1 
MUSE 
0.562 0.569 0.565 2 
4 
2 
CCS 
0.554 0.571 0.562 4 
3 
3 
EXB 
0.546 0.571 0.558 8 
2 
7 
POLY 
0.545 0.560 0.552 10 
9 
9 
LCS-IESI 0.540 0.527 0.531 11 
13 
12 
Lead 
0.524 0.535 0.529 13 
12 
13 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : MSS task. Arabic.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>ROUGE-1 and -2 scores. DUC 2002. 

</table></figure>

			<note place="foot" n="1"> MUSEEC also participated in MultiLing 2011, 2013, and 2015 contests on English, Hebrew and Arabic, and demonstrated excellent results.</note>

			<note place="foot" n="2"> We utilized the language-independent implementation of ROUGE that operates Unicode characters (Krapivin, 2014)</note>

			<note place="foot" n="3"> On average, in our experiments the GA performed 5 − 6 iterations of selection and reproduction before reaching convergence.</note>

			<note place="foot" n="4"> exponential in the number of sentences</note>

			<note place="foot" n="5"> depends on the user&apos;s choice of a summary maximal length</note>

			<note place="foot" n="6"> Because building pipeline requires programming skills, this option cannot be applied from GUI.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially funded by the U.S. De-partment of the Navy, Office of Naval Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004)</title>
		<meeting>the Workshop on Text Summarization Branches Out (WAS 2004)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">OCCAMS-An Optimal Combinatorial Covering Algorithm for Multi-document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Schlesinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 12th International Conference on Data Mining Workshops</title>
		<meeting>the IEEE 12th International Conference on Data Mining Workshops</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Linguistic features and machine learning methods in single-document extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dlikman</surname></persName>
		</author>
		<ptr target="http://www.ise.bgu.ac.il/faculty/mlast/papers/Thesis-ver7.pdf" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>Beer-Sheva</publisher>
		</imprint>
		<respStmt>
			<orgName>BenGurion University of the Negev</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing</title>
		<meeting>the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The budgeted maximum coverage problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Precessing Letters</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: A probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="91" to="107" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">JRougeJava ROUGE Implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krapivin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-lingual training of summarization systems using annotated corpora in a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litvak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using N-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL &apos;03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining the gaps: Towards polynomial summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vanetik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing</title>
		<meeting>the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="655" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new approach to improving multilingual summarization using a Genetic Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;10: Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="927" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">999</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From discourse structures to text summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="82" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An unsupervised constrained optimization approach to compressive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vanetik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Churkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Manuscript submitted for publication</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
