<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Machine Learning: Integrating Language, Vision and Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
						</author>
						<title level="a" type="main">Multimodal Machine Learning: Integrating Language, Vision and Speech</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Tutorial Abstracts</title>
						<meeting>ACL 2017, Tutorial Abstracts <address><addrLine>Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="3" to="5"/>
							<date type="published">July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-5002</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multimodal machine learning is a vibrant multidisciplinary research field which addresses some of the original goals of artificial intelligence by integrating and mod-eling multiple communicative modalities, including linguistic, acoustic and visual messages. With the initial research on audiovisual speech recognition and more recently with language &amp; vision projects such as image and video captioning and visual question answering, this research field brings some unique challenges for multi-modal researchers given the heterogene-ity of the data and the contingency often found between modalities.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tutorial overview</head><p>The present tutorial will review fundamental con- cepts of machine learning and deep neural net- works before describing the five main challenges in multimodal machine learning:</p><p>1. Representation: A first fundamental chal- lenge is to learn how to represent and summa- rize the multimodal data to highlight the com- plementarity and synchrony between modal- ities. The heterogeneity of multimodal data makes it particularly challenging for coordi- nated and joint representations. For example, language is often seen as symbolic while au- dio and visual modalities will be represented as signals.</p><p>2. Translation: A second challenge is how to translate data from one modality to another. Not only is the data heterogeneous, but the re- lationship between modalities is often open- ended or subjective. For example, when de- scribing a specific image verbally, more than one description can be correct. The evalua- tion and characterization of the multimodal translation may be subjective.</p><p>3. Alignment: A third challenge is to identify the direct relations between elements from two or more different modalities. For exam- ple, when analyzing the speech and gestures of a human subject, how can we align spe- cific gestures with the spoken words or ut- terances? This alignment between modali- ties may be based on long-range dependen- cies and the segmentation is often ambiguous (e.g., words or utterances).</p><p>4. Fusion: A fourth challenge is to join in- formation from two or more modalities to perform a prediction, discrete or continuous. For example, for audio-visual speech recog- nition, the visual descriptoin of the lip mo- tion is fused with the speech signal to predict spoken words. The information coming from different modalities may have varying predic- tive power and noise topology. With possibly missing data in at least one of the modalities. Multimodal fusion needs to handle such vari- ations.</p><p>5. Co-learning: A fifth challenge is to transfer knowledge between modalities and their rep- resentations. Exemplified by algorithms of co-training, conceptual grounding and zero shot learning, how does knowledge learn- ing from one modality (e.g., predicted labels or representation) can help a computational model trained on a different modality? This challenge is particularly relevant when one of the modalities has limited resources (e.g., an- notated data).</p><p>The tutorial will also present state-of-the-art al- gorithms that were recently proposed to solve mul-timodal applications such as image captioning, video descriptions and visual question-answer. We will also discuss the current and upcoming chal- lenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure</head><p>We plan to follow a similar structure to our ICMI 2016 tutorial which was 3 hours long:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Intruduction</head><p>• What is Multimodal?</p><p>-Historical view, multimodal vs mul- timedia  his Ph.D. and Master degrees from MIT Com- puter Science and Artificial Intelligence Labora- tory. In 2008, Dr. Morency was selected as one of "AI's 10 to Watch" by IEEE Intelligent Sys- tems. He has received 7 best paper awards in multiple ACM-and IEEE-sponsored conferences for his work on context-based gesture recogni- tion, multimodal probabilistic fusion and compu- tational models of human communication dynam- ics. Dr. Morency was General Chair for the International Conference on Multimodal Interac- tion (ICMI 2012) and the NIPS 2010 workshop on Modeling Human Communication Dynamics. He was Program Chair for <ref type="bibr">ICMI 2011</ref><ref type="bibr">ICMI , 2014</ref><ref type="bibr">ICMI and 2016</ref>, as well as the Tenth International Confer- ence on Creating, Connecting and Collaborating through Computing in January 2012. Tadas Baltrušaitis (http://www.cl.cam. ac.uk/ ˜ tb346/) is a post-doctoral associate at the Language Technologies Institute, Carnegie Mellon University. Before this, he was a post- doctoral research at the University of Cambridge, where he also received his PhD degree in 2014. His primary research interests lie in the automatic understanding of non-verbal human behaviour, computer vision, and multimodal machine learn- ing. His papers have won a number of awards for his work on non-verbal human behavior analysis, including ICMI 2014 best student paper award, and ETRA 2016 emerging investigator award. He is also a winner of several challenges in computer vision and multi-modal machine learning, includ- ing <ref type="bibr">FERA 2015, and</ref><ref type="bibr">AVEC 2011.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Core technical challenges -Representation learning, translation, alignment, fusion and co-learning 2. Basic concepts -Part 1 • Linear models -Score and loss functions, regulariza- tion • Neural networks -Activation functions, multi-layer perceptron • Optimization -Stochastic gradient descent, back- propagation 3. Unimodal representations • Language representations -Distributional hypothesis and word embedding • Visual representations -Convolutional neural networks • Acoustic representations -Spectrograms, auto-encoders 4. Multimodal representations • Joint representations -Visual semantic spaces, multimodal auto-encoder • Coordinated representations -Component analysis -Similarity metrics, canonical corre- lation analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Language models -Unigrams, bigrams, skip-grams, skip-thought • Unimodal sequence modeling -Recurrent neural networks, LSTMs • Optimization -Backpropagation through time 2. Multimodal translation and mapping • Encoder-decoder models -Machine translation, image caption- ing • Generative vs example based ap- proaches -Viseme generation, visual puppetry -Model evaluation 3. Modality alignment • Latent alignment approaches -Attention models, multi instance learning • Explicit alignment -Dynamic time warping 4. Multimodal fusion and co-learning • Model free approaches -Early and late fusion, hybrid models • Kernel-based fusion -Multiple kernel learning • Multimodal graphical models -Factorial HMM, Multi-view Hidden CRF • Co-learning -Parallel, non-parallel and hybrid data 5. Future directions and concluding remarks About the speakers Louis-Philippe Morency (https://www.cs. cmu.edu/ ˜ morency/) is Assistant Professor in the Language Technology Institute at the Carnegie Mellon University where he leads the Multimodal Communication and Machine Learn- ing Laboratory (MultiComp Lab). He received</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
