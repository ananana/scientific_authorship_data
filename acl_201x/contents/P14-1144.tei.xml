<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Prompt Adherence in Student Essays</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Human Language Technology Research Institute University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75083-0688</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Prompt Adherence in Student Essays</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1534" to="1543"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and prompt adherence. The work on modeling prompt adherence, however, has been focused mainly on whether individual sentences adhere to the prompt. We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension. Our approach significantly outper-forms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated essay scoring, the task of employing computer technology to evaluate and score writ- ten text, is one of the most important educational applications of natural language processing (NLP) (see <ref type="bibr" target="#b20">Shermis and Burstein (2003)</ref> and <ref type="bibr" target="#b21">Shermis et al. (2010)</ref> for an overview of the state of the art in this task). A major weakness of many ex- isting scoring engines such as the Intelligent Es- say Assessor TM ( <ref type="bibr" target="#b12">Landauer et al., 2003</ref>) is that they adopt a holistic scoring scheme, which summa- rizes the quality of an essay with a single score and thus provides very limited feedback to the writer. In particular, it is not clear which dimension of an essay (e.g., style, coherence, relevance) a score should be attributed to. Recent work addresses this problem by scoring a particular dimension of es- say quality such as coherence <ref type="bibr" target="#b15">(Miltsakaki and Kukich, 2004</ref>), technical errors, organization <ref type="bibr" target="#b18">(Persing et al., 2010)</ref>, and thesis clarity <ref type="bibr" target="#b17">(Persing and Ng, 2013)</ref>. Essay grading software that provides feedback along multiple dimensions of essay qual- ity such as E-rater/Criterion ( ) has also begun to emerge.</p><p>Our goal in this paper is to develop a com- putational model for scoring an essay along an under-investigated dimension -prompt adher- ence. Prompt adherence refers to how related an essay's content is to the prompt for which it was written. An essay with a high prompt adherence score consistently remains on the topic introduced by the prompt and is free of irrelevant digressions.</p><p>To our knowledge, little work has been done on scoring the prompt adherence of student essays since <ref type="bibr" target="#b6">Higgins et al. (2004)</ref>. Nevertheless, there are major differences between Higgins et al.'s work and our work with respect to both the way the task is formulated and the approach. Regarding task formulation, while Higgins et al. focus on classi- fying each sentence as having either good or bad adherence to the prompt, we focus on assigning a prompt adherence score to the entire essay, al- lowing the score to range from one to four points at half-point increments. As far as the approach is concerned, Higgins et al. adopt a knowledge- lean approach to the task, where almost all of the features they employ are computed based on a word-based semantic similarity measure known as Random Indexing ( <ref type="bibr" target="#b9">Kanerva et al., 2000</ref>). On the other hand, we employ a large variety of fea- tures, including lexical and knowledge-based fea- tures that encode how well the concepts in an es- say match those in the prompt, LDA-based fea- tures that provide semantic generalizations of lex- ical features, and "error type" features that encode different types of errors the writer made that are related to prompt adherence.</p><p>In sum, our contributions in this paper are two- fold. First, we develop a scoring model for the prompt adherence dimension on student essays us- ing a feature-rich approach. Second, in order to stimulate further research on this task, we make our data set consisting of prompt adherence an-Topic Languages Essays Most university degrees are the- oretical and do not prepare stu- dents for the real world. They are therefore of very little value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">131</head><p>The prison system is outdated.</p><p>No civilized society should pun- ish its criminals: it should reha- bilitate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">80</head><p>In his novel Animal Farm, George Orwell wrote "All men are equal but some are more equal than others." How true is this today? 10 64 <ref type="table">Table 1</ref>: Some examples of writing topics.</p><p>notations of 830 essays publicly available. Since progress in prompt adherence modeling is hin- dered in part by the lack of a publicly annotated corpus, we believe that our data set will be a valu- able resource to the NLP community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Corpus Information</head><p>We use as our corpus the 4.5 million word Interna- tional Corpus of Learner English (ICLE) ( <ref type="bibr" target="#b4">Granger et al., 2009)</ref>, which consists of more than 6000 es- says written by university undergraduates from 16 countries and 16 native languages who are learn- ers of English as a Foreign Language. 91% of the ICLE texts are argumentative. We select a subset consisting of 830 argumentative essays from the ICLE to annotate for training and testing of our essay prompt adherence scoring system. <ref type="table">Table 1</ref> shows three of the 13 topics selected for annota- tion. Fifteen native languages are represented in the set of annotated essays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus Annotation</head><p>We ask human annotators to score each of the 830 argumentative essays along the prompt adherence dimension. Our annotators were selected from over 30 applicants who were familiarized with the scoring rubric and given sample essays to score. The six who were most consistent with the ex- pected scores were given additional essays to an- notate. Annotators evaluated how well each es- say adheres to its prompt using a numerical score from one to four at half-point increments (see <ref type="table">Ta- ble 2</ref> for a description of each score). This con- trasts with previous work on prompt adherence es- say scoring, where the corpus is annotated with a binary decision (i.e., good or bad) (e.g., <ref type="bibr" target="#b6">Higgins et al. (2004;</ref>), <ref type="bibr" target="#b13">Louis and Higgins (2010)</ref>). Hence, our annotation scheme not only provides <ref type="table" target="#tab_0">Score Description of Prompt Adherence  4  essay fully addresses the prompt and consis- tently stays on topic  3  essay mostly addresses the prompt or occasion- ally wanders off topic  2  essay does not fully address the prompt or con- sistently wanders off topic  1</ref> essay does not address the prompt at all or is completely off topic <ref type="table">Table 2</ref>: Descriptions of the meaning of scores.</p><p>a finer-grained distinction of prompt adherence (which can be important in practice), but also makes the prediction task more challenging.</p><p>To ensure consistency in annotation, we ran- domly select 707 essays to have graded by mul- tiple annotators. Analysis reveals that the Pear- son's correlation coefficient computed over these doubly annotated essays is 0.243. Though annota- tors exactly agree on the prompt adherence score of an essay only 38% of the time, the scores they apply fall within 0.5 points in 66% of essays and within 1.0 point in 89% of essays. For the sake of our experiments, whenever annotators disagree on an essay's prompt adherence score, we assign the essay the average of all annotations rounded to the nearest half point. <ref type="table" target="#tab_0">Table 3</ref> shows the number of essays that receive each of the seven scores for prompt adherence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Score Prediction</head><p>In this section, we describe in detail our system for predicting essays' prompt adherence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Training and Application</head><p>We cast the problem of predicting an essay's prompt adherence score as 13 regression prob- lems, one for each prompt. Each essay is repre- sented as an instance whose label is the essay's true score (one of the values shown in <ref type="table" target="#tab_0">Table 3</ref>) with up to seven types of features including base- line (Section 4.2) and six other feature types pro- posed by us (Section 4.3). Our regressors may as- sign an essay any score in the range of 1.0−4.0. Using regression captures the fact that some pairs of scores are more similar than others (e.g., an essay with a prompt adherence score of 3.5 is more similar to an essay with a score of 4.0 than it is to one with a score of 1.0). A classification sys-tem, by contrast, may sometimes believe that the scores 1.0 and 4.0 are most likely for a particu- lar essay, even though these scores are at opposite ends of the score range.</p><p>Using a different regressor for each prompt cap- tures the fact that it may be easier for an essay to adhere to some prompts than to others, and com- mon problems students have writing essays for one prompt may not apply to essays written in re- sponse to another prompt. For example, in essays written in response to the prompt "Marx once said that religion was the opium of the masses. If he was alive at the end of the 20th century, he would replace religion with television," students some- times write essays about all the evils of television, forgetting that their essay is only supposed to be about whether it is "the opium of the masses". Stu- dents are less likely to make an analogous mistake when writing for the prompt "Crime does not pay."</p><p>After creating training instances for prompt p i , we train a linear regressor, r i , with regularization parameter c i for scoring test essays written in re- sponse to p i using the linear SVM regressor imple- mented in the LIBSVM software package <ref type="bibr" target="#b2">(Chang and Lin, 2001</ref>). All SVM-specific learning param- eters are set to their default values except c i , which we tune to maximize performance on held-out val- idation data.</p><p>After training the classifiers, we use them to classify the test set essays. The test instances are created in the same way as the training instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Features</head><p>Our baseline system for score prediction employs various features based on Random Indexing.</p><p>1. Random Indexing Random Indexing (RI) is "an efficient, scalable and incremental alterna- tive" <ref type="bibr" target="#b19">(Sahlgren, 2005</ref>) to Latent Semantic Index- ing <ref type="bibr" target="#b3">(Deerwester et al., 1990;</ref><ref type="bibr" target="#b11">Landauer and Dutnais, 1997</ref>) which allows us to automatically gen- erate a semantic similarity measure between any two words. We train our RI model on over 30 mil- lion words of the English Gigaword corpus <ref type="bibr" target="#b16">(Parker et al., 2009</ref>) using the S-Space package <ref type="bibr" target="#b8">(Jurgens and Stevens, 2010)</ref>. We expect that features based on RI will be useful for prompt adherence scor- ing because they may help us find text related to the prompt even if some of its concepts have have been rephrased (e.g., an essay may talk about "jail" rather than "prison", which is mentioned in one of the prompts), and because they have al- ready proven useful for the related task of deter- mining which sentences in an essay are related to the prompt ( <ref type="bibr" target="#b6">Higgins et al., 2004</ref>).</p><p>For each essay, we therefore attempt to adapt the RI features used by <ref type="bibr" target="#b6">Higgins et al. (2004)</ref> to our problem of prompt adherence scoring. We do this by generating one feature encoding the entire essay's similarity to the prompt, another encoding the essay's highest individual sentence's similarity to the prompt, a third encoding the highest entire essay similarity to one of the prompt sentences, another encoding the highest individual sentence similarity to an individual prompt sentence, and fi- nally one encoding the entire essay's similarity to a manually rewritten version of the prompt that ex- cludes extraneous material (such as "In his novel Animal Farm, George Orwell wrote," which is in- troductory material from the third prompt in Ta- ble 1). Our RI feature set necessarily excludes those features from Higgins et al. that are not easily translatable to our problem since we are concerned with an entire essay's adherence to its prompt rather than with each of its sentences' re- latedness to the prompt. Since RI does not pro- vide a straightforward way to measure similar- ity between groups of words such as sentences or essays, we use <ref type="bibr" target="#b5">Higgins and Burstein's (2007)</ref> method to generate these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Novel Features</head><p>Next, we introduce six types of novel features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">N-grams</head><p>As our first novel feature, we use the 10,000 most important lemmatized unigram, bigram, and trigram features that occur in the es- say. N-grams can be useful for prompt adherence scoring because they can capture useful words and phrases related to a prompt. For example, words and phrases like "university degree", "student", and "real world" are relevant to the first prompt in <ref type="table">Table 1</ref>, so it is more likely that an essay adheres to the prompt if they appear in the essay.</p><p>We determine the "most important" n-gram fea- tures using information gain computed over the training data <ref type="bibr" target="#b22">(Yang and Pedersen, 1997)</ref>. Since the essays vary greatly in length, we normalize each essay's set of n-gram features to unit length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Thesis Clarity Keywords</head><p>Our next set of fea- tures consists of the keyword features we intro- duced in our previous work on essay thesis clarity scoring <ref type="bibr" target="#b17">(Persing and Ng, 2013</ref>). Below we give an overview of these keyword features and motivate why they are potentially useful for prompt adher- ence scoring.</p><p>The keyword features were formed by first ex- amining the 13 essay prompts, splitting each into its component pieces. As an example of what is meant by a "component piece", consider the first prompt in <ref type="table">Table 1</ref>. The components of this prompt would be "Most university degrees are theoreti- cal", "Most university degrees do not prepare stu- dents for the real world", and "Most university de- grees are of very little value."</p><p>Then the most important (primary) and second most important (secondary) words were selected from each prompt component, where a word was considered "important" if it would be a good word for a student to use when stating her thesis about the prompt. So since the lemmatized version of the third component of the second prompt in <ref type="table">Table 1</ref> is "it should rehabilitate they", "rehabilitate" was selected as a primary keyword and "society" as a secondary keyword.</p><p>Features are then computed based on these key- words. For instance, one thesis clarity keyword feature is computed as follows. The RI similarity measure is first taken between the essay and each group of the prompt's primary keywords. The fea- ture then gets assigned the lowest of these values. If this feature has a low value, that suggests that the student ignored the prompt component from which the value came when writing the essay.</p><p>To compute another of the thesis clarity key- word features, the numbers of combined primary and secondary keywords the essay contains from each component of its prompt are counted. These numbers are then divided by the total count of pri- mary and secondary features in their respective components. The greatest of the fractions gener- ated in this way is encoded as a feature because if it has a low value, that indicates the essay's thesis may not be very relevant to the prompt. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Prompt Adherence Keywords</head><p>The thesis clarity keyword features described above were in- tended for the task of determining how clear an essay's thesis is, but since our goal is instead to de- termine how well an essay adheres to its prompt, it makes sense to adapt keyword features to our task rather than to adopt keyword features ex-actly as they have been used before. For this reason, we construct a new list of keywords for each prompt component, though since prompt ad- herence is more concerned with what the student says about the topics than it is with whether or not what she says about them is stated clearly, our keyword lists look a little different than the ones discussed above. For an example, we ear- lier alluded to the problem of students merely dis- cussing all the evils of television for the prompt "Marx once said that religion was the opium of the masses. If he was alive at the end of the 20th cen- tury, he would replace religion with television." Since the question suggests that students discuss whether television is analogous to religion in this way, our set of prompt adherence keywords for this prompt contains the word "religion" while the previously discussed keyword sets do not. This is because a thesis like "Television is bad" can be stated very clearly without making any reference to religion at all, and so an essay with a thesis like this can potentially have a very high thesis clarity score. It should not, however, have a very high prompt adherence score, as the prompt asked the student to discuss whether television is like reli- gion in a particular way, so religion should be at least briefly addressed for an essay to be awarded a high prompt adherence score.</p><p>Additionally, our prompt adherence keyword sets do not adopt the notions of primary and sec- ondary groups of keywords for each prompt com- ponent, instead collecting all the keywords for a component into one set because "secondary" key- words tend to be things that are important when we are concerned with what a student is saying about the topic rather than just how clearly she said it.</p><p>We form two types of features from prompt ad- herence keywords. While both types of features measure how much each prompt component was discussed in an essay, they differ in how they en- code the information. To obtain feature values of the first type, we take the RI similarities between the whole essay and each set of prompt adherence keywords from the prompt's components. This results in one to three features, as some prompts have one component while others have up to three.</p><p>We obtain feature values of the second type as follows. For each component, we count the num- ber of prompt adherence keywords the essay con- tains. We divide this number by the number of prompt adherence keywords we identified from the component. This results in one to three fea- tures since a prompt has one to three components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">LDA Topics</head><p>A problem with the features we have introduced up to this point is that they have trouble identifying topics that are not mentioned in the prompt, but are nevertheless related to the prompt. These topics should not diminish the es- say's prompt adherence score because they are at least related to prompt concepts. For example, consider the prompt "All armies should consist en- tirely of professional soldiers: there is no value in a system of military service." An essay contain- ing words like "peace", "patriotism", or "training" are probably not digressions from the prompt, and therefore should not be penalized for discussing these topics. But the various measures of keyword similarities described above will at best not notice that anything related to the prompt is being dis- cussed, and at worst, this might have effects like lowering some of the RI similarity scores, thereby probably lowering the prompt adherence score the regressor assigns to the essay. While n-gram fea- tures do not have exactly the same problem, they would still only notice that these example words are related to the prompt if multiple essays use the same words to discuss these concepts. For this reason, we introduce Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref> features.</p><p>In order to construct our LDA features, we first collect all essays written in response to each prompt into its own set. Note that this feature type exploits unlabeled data: it includes all essays in the ICLE responding to our prompts, not just those in our smaller annotated 830 essay dataset. We then use the MALLET <ref type="bibr" target="#b14">(McCallum, 2002</ref>) imple- mentation of LDA to build a topic model of 1,000 topics around each of these sets of essays. This results in what we can think of as a soft clustering of words into 1,000 sets for each prompt, where each set of words represents one of the topics LDA identified being discussed in the essays for that prompt. So for example, the five most impor- tant words in the most frequently discussed topic for the military prompt we mentioned above are "man", "military", "service", "pay", and "war".</p><p>We also use the MALLET-generated topic model to tell us how much of each essay is spent discussing each of the 1,000 topics. The model might tell us, for example, that a particular essay written on the military prompt spends 35% of the time discussing the "man", "military", "service", "pay", and "war" topic and 65% of the time dis- cussing a topic whose most important words are "fully", "count", "ordinary", "czech", and "day". Since the latter topic is discussed so much in the essay and does not appear to have much to do with the military prompt, this essay should probably get a bad prompt adherence score. We construct 1,000 features from this topic model, one for each topic. Each feature's value is obtained by using the topic model to tell us how much of the essay was spent discussing the feature's corresponding topic. From these features, our regressor should be able to learn which topics are important to a good prompt adherent essay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Manually Annotated LDA Topics</head><p>A weak- ness of the LDA topics feature type is that it may result in a regressor that has trouble distinguishing between an infrequent topic that is adherent to the prompt and one that just represents an irrelevant digression. This is because an infrequent topic may not appear in the training set often enough for the regressor to make this judgment. We introduce the manually annotated LDA topics feature type to address this problem.</p><p>In order to construct manually annotated LDA topic features, we first build 13 topic models, one for each prompt, just as described in the section on LDA topic features. Rather than requesting models of 1,000 topics, however, we request mod- els of only 100 topics 2 . We then go through all 13 lists of 100 topics as represented by their top ten words, manually annotating each topic with a number from 0 to 5 representing how likely it is that the topic is adherent to the prompt. A topic labeled 5 is very likely to be related to the prompt, where a topic labeled 0 appears totally unrelated.</p><p>Using these annotations alongside the topic dis- tribution for each essay that the topic models pro- vide us, we construct ten features. The first five features encode the sum of the contributions to an essay of topics annotated with a number ≥ 1, the sum of the contributions to an essay of topics an- notated with a number ≥ 2, and so on up to 5.</p><p>The next five features are similar to the last, with one feature taking on the sum of the contri- butions to an essay of topics annotated with the number 0, another feature taking on the sum of the contributions to an essay of topics annotated with the number 1, and so on up to 4. We do not include a feature for topics annotated with the number 5 because it would always have the same value as the feature for topics ≥ 5.</p><p>Features like these should give the regressor a better idea how much of an essay is composed of prompt-related arguments and discussion and how much of it is irrelevant to the prompt, even if some of the topics occurring in it are too infrequent to judge just from training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Predicted Thesis Clarity Errors</head><p>In our pre- vious work on essay thesis clarity scoring (Persing and Ng, 2013), we identified five classes of errors that detract from the clarity of an essay's thesis: Confusing Phrasing. The thesis is phrased oddly, making it hard to understand the writer's point. Incomplete Prompt Response. The thesis leaves some part of a multi-part prompt unaddressed. Relevance to Prompt. The apparent thesis's weak relation to the prompt causes confusion. Missing Details. The thesis leaves out an impor- tant detail needed to understand the writer's point. Writer Position. The thesis describes a position on the topic without making it clear that this is the position the writer supports.</p><p>We hypothesize that these errors, though orig- inally intended for thesis clarity scoring, could be useful for prompt adherence scoring as well. For instance, an essay that has a Relevance to Prompt error or an Incomplete Prompt Response error should intuitively receive a low prompt ad- herence score. For this reason, we introduce fea- tures based on these errors to our feature set for prompt adherence scoring <ref type="bibr">3</ref> .</p><p>While each of the essays in our data set was pre- viously annotated with these thesis clarity errors, in a realistic setting a prompt adherence scoring system will not have access to these manual error labels. As a result, we first need to predict which of these errors is present in each essay. To do this, we train five maximum entropy classifiers for each prompt, one for each of the five thesis clarity er- rors, using MALLET's <ref type="bibr" target="#b14">(McCallum, 2002</ref>) imple- mentation of maximum entropy classification. In- stances are presented to classifier for prompt p for error e in the following way. If a training essay is written in response to p, it will be used to gen-erate a training instance whose label is 1 if e was annotated for it or 0 otherwise. Since error pre- diction and prompt adherence scoring are related problems, the features we associate with this in- stance are features 1−6 which we have described earlier in this section. The classifier is then used to generate probabilities telling us how likely it is that each test essay has error e.</p><p>Then, when training our regressor for prompt adherence scoring, we add the following features to our instances. We add a binary feature indicat- ing the presence or absence of each error. Or in the case of test essays, the feature takes on a real value from 0 to 1 indicating how likely the classi- fier thought it was that the essay had each of the errors. This results in five additional features, one for each error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we evaluate our system for prompt adherence scoring. All the results we report are obtained via five-fold cross-validation exper- iments. In each experiment, we use 3 5 of our la- beled essays for model training, another 1 5 for pa- rameter tuning, and the final 1 5 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Scoring Metrics</head><p>We employ four evaluation metrics. As we will see below, S1, S2, and S3 are error metrics, so lower scores imply better performance. In contrast, P C is a correlation metric, so higher correlation im- plies better performance. The simplest metric, S1, measures the fre- quency at which a system predicts the wrong score out of the seven possible scores. Hence, a system that predicts the right score only 25% of the time would receive an S1 score of 0.75.</p><p>The S2 metric measures the average distance between a system's score and the actual score. This metric reflects the idea that a system that pre- dicts scores close to the annotator-assigned scores should be preferred over a system whose predic- tions are further off, even if both systems estimate the correct score at the same frequency.</p><p>The S3 metric measures the average square of the distance between a system's score predic- tions and the annotator-assigned scores. The in- tuition behind this system is that not only should we prefer a system whose predictions are close to the annotator scores, but we should also prefer one whose predictions are not too frequently very far away from the annotator scores. These three scores are given by:</p><formula xml:id="formula_0">1 N A j =E j 1, 1 N N i=1 |A j − E j |, 1 N N i=1 (A j − E j ) 2</formula><p>where A j , E j , and E j are the annotator assigned, system predicted, and rounded system predicted scores 4 respectively for essay j, and N is the num- ber of essays.</p><p>The last metric, P C, computes Pearson's cor- relation coefficient between a system's predicted scores and the annotator-assigned scores. P C ranges from −1 to 1. A positive (negative) P C implies that the two sets of predictions are posi- tively (negatively) correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Parameter Tuning</head><p>As mentioned earlier, for each prompt p i , we train a linear regressor r i using LIBSVM with regular- ization parameter c i . To optimize our system's performance on the three error measures described previously, we use held-out validation data to in- dependently tune each of the c i values 5 . Note that each of the c i values can be tuned independently because a c i value that is optimal for predicting scores for p i essays with respect to any of the error performance measures is necessarily also the opti- mal c i when measuring that error on essays from all prompts. However, this is not case with Pear- son's correlation coefficient, as the P C value for essays from all 13 prompts cannot be simplified as a weighted sum of the P C values obtained on each individual prompt. In order to obtain an optimal result as measured by P C, we jointly tune the c i parameters to optimize the P C value achieved by our system on the same held-out validation data. However, an exact solution to this optimization problem is computationally expensive, as there are too many <ref type="bibr">(7 13</ref> ) possible combinations of c values to exhaustively search. Consequently, we find a local maximum by employing the simulated an- <ref type="bibr">4</ref> Since our regressor assigns each essay a real value rather than an actual valid score, it would be difficult to obtain a reasonable S1 score without rounding the system estimated score to one of the possible values. For that reason, we round the estimated score to the nearest of the seven scores the hu- man annotators were permitted to assign (1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0) only when calculating S1. For other scoring metrics, we only round the predictions to 1.0 or 4.0 if they fall outside the 1.0−4.0 range. <ref type="bibr">5</ref> For parameter tuning, we employ the following values. ci may be assigned any of the values 10 0 10 1 , 10 2 , 10 3 , 10 4 , 10 5 , or 10 6 . System S1 S2 S3 P C Baseline . <ref type="bibr">517 .368 .234 .233 Our System .488 .348 .197</ref> .360 <ref type="table">Table 4</ref>: Five-fold cross-validation results for prompt adherence scoring. nealing algorithm <ref type="bibr" target="#b10">(Kirkpatrick et al., 1983)</ref>, alter- ing one c i value at a time to optimize P C while holding the remaining parameters fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>Five-fold cross-validation results on prompt ad- herence score prediction are shown in <ref type="table">Table 4</ref>. On the first line, this table shows that our baseline sys- tem, which recall uses only various RI features, predicts the wrong score 51.7% of the time. Its predictions are off by an average of .368 points, and the average squared distance between its pre- dicted score and the actual score is .234. In addi- tion, its predicted scores and the actual scores have a Pearson correlation coefficient of 0.233.</p><p>The results from our system, which uses all seven feature types described in Section 4, are shown in row 2 of the table. Our system obtains S1, S2, S3, and P C scores of . <ref type="bibr">488, .348, .197,</ref> and .360 respectively, yielding a significant im- provement over the baseline with respect to S2, S3, and P C with p &lt; 0.05, p &lt; 0.01, and p &lt; 0.06 respectively 6 . While our system yields improve- ments by all four measures, its improvement over the baseline S1 score is not significant. These re- sults mean that the greatest improvements our sys- tem makes are that it ensures that our score pre- dictions are not too often very far away from an essay's actual score, as making such predictions would tend to drive up S3, yielding a relative er- ror reduction in S3 of 15.8%, and it also ensures a better correlation between predicted and actual scores, thus yielding the 16.6% improvement in P C. <ref type="bibr">7</ref> It also gives more modest improvements in how frequently exactly the right score is predicted (S1) and is better at predicting scores closer to the actual scores (S2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Feature Ablation</head><p>To gain insight into how much impact each of the feature types has on our system, we perform fea-ture ablation experiments in which we remove the feature types from our system one-by-one.</p><p>Results of the ablation experiments when per- formed using the four scoring metrics are shown in <ref type="table">Table 5</ref>. The top line of each subtable shows what our system's score would be if we removed just one of the feature types from our system. So to see how our system performs by the S1 metric if we remove only predicted thesis clarity error features, we would look at the first row of results of Ta- ble 5(a) under the column headed by the number 7 since predicted thesis clarity errors are the seventh feature type introduced in Section 4. The number here tells us that our system's S1 score without this feature type is .502. Since <ref type="table">Table 4</ref> shows that when our system includes this feature type (along with all the other feature types), it obtains an S1 score of .488, this feature type's removal costs our system .014 S1 points, and thus its inclusion has a beneficial effect on the S1 score.</p><p>From row 1 of <ref type="table">Table 5</ref>(a), we can see that re- moving feature 4 yields a system with the best S1 score in the presence of the other feature types in this row. For this reason, we permanently remove feature 4 from the system before we generate the results on line 2. Thus, we can see what happens when we remove both feature 4 and feature 5 by looking at the second entry in row 2. And since removing feature 6 harms performance least in the presence of row 2's other feature types, we perma- nently remove both 4 and 6 from our feature set when we generate the third row of results. We it- eratively remove the feature type that yields a sys- tem with the best performance in this way until we get to the last line, where only one feature type is used to generate each result.</p><p>Since the feature type whose removal yields the best system is always the rightmost entry in a line, the order of column headings indicates the rela- tive importance of the feature types, with the left- most feature types being most important to per- formance and the rightmost feature types being least important in the presence of the other fea- ture types. This being the case, it is interesting to note that while the relative importance of differ- ent feature types does not remain exactly the same if we measure performance in different ways, we can see that some feature types tend to be more im- portant than others in a majority of the four scor- ing metrics. Features 2 (n-grams), 3 (thesis clarity keywords), and 6 (manually annotated LDA top- (a) Results using the S1 metric   <ref type="bibr">201 .197 .197 .197 .197 .196 .215 .201 .197 .196 .196 .196 .212 .203 .199 .197 .196 .212 .203 .199 .197 .212 .203 .199 .223</ref>   <ref type="table">Table 5</ref>: Feature ablation results. In each subtable, the first row shows how our system would perform if each feature type was removed. We remove the least important feature type, and show in the next row how the adjusted sys- tem would perform without each remaining type. For brevity, a feature type is referred to by its feature number: (1) RI; <ref type="formula">(2)</ref> n-grams; (3) thesis clarity keywords; (4) prompt adherence keywords; (5) LDA topics; (6) manually annotated LDA top- ics; and (7) predicted thesis clarity errors. ics) tend to be the most important feature types, as they tend to be the last feature types removed in the ablation subtables. Features 1 (RI) and 5 (LDA topics) are of middling importance, with neither ever being removed first or last, and each tending to have a moderate effect on performance. Finally, while features 4 (prompt adherence key- words) and 7 (predicted thesis clarity errors) may by themselves provide useful information to our system, in the presence of the other feature types they tend to be the least important to performance as they are often the first feature types removed.</p><p>While there is a tendency for some feature types to always be important (or unimportant) regardless of which scoring metric is used to measure per-  <ref type="table">Table 6</ref>: Regressor scores for our system.</p><p>formance, the relative importance of different fea- ture types does not always remain consistent if we measure performance in different ways. For ex- ample, while we identified feature 3 (thesis clar- ity keywords) as one of the most important fea- ture types generally due to its tendency to have a large beneficial impact on performance, when we are measuring performance using S3, it is the least useful feature type. Furthermore, its removal in- creases the S3 score by a small amount, meaning that its inclusion actually makes our system per- form worse with respect to S3. Though feature 3 is an extreme example, all feature types fluctuate in importance, as we see when we compare their or- ders of removal among the four ablation subtables. Hence, it is important to know how performance is measured when building a system for scoring prompt adherence. Feature 3 is not the only feature type whose re- moval sometimes has a beneficial impact on per- formance. As we can see in <ref type="table">Table 5</ref>(b), the re- moval of features 4, 5, and 7 improves our sys- tem's S2 score by .001 points. The same effect occurs in <ref type="table">Table 5</ref>(c) when we remove features 4, 7, and 3. These examples illustrate that under some scoring metrics, the inclusion of some fea- ture types is actively harmful to performance. For- tunately, this effect does not occur in any other cases than the two listed above, as most feature types usually have a beneficial or at least neutral impact on our system's performance.</p><p>For those feature types whose effect on perfor- mance is neutral in the first lines of ablation results (feature 4 in S1, features 3, 5, and 7 in S2, and fea- tures 1, 4, 5, and 7 in S3), it is important to note that their neutrality does not mean that they are unimportant. It merely means that they do not im- prove performance in the presence of other feature types. We can see this is the case by noting that they are not all the least important feature types in their respective subtables as indicated by column order. For example, by the time feature 1 gets per- manently removed in <ref type="table">Table 5</ref>(c), its removal harms performance by .002 S3 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of Predicted Scores</head><p>To more closely examine the behavior of our sys- tem, in <ref type="table">Table 6</ref> we chart the distributions of scores it predicts for essays having each gold standard score. As an example of how to read this table, consider the number 3.06 appearing in row 2.0 in the .25 column of the S3 region. This means that 25% of the time, when our system with parameters tuned for optimizing S3 is presented with a test es- say having a gold standard score of 2.0, it predicts that the essay has a score less than or equal to 3.06.</p><p>From this table, we see that our system has a strong bias toward predicting more frequent scores as there are no numbers less than 3.0 in the table, and about 93.7% of all essays have gold standard scores of 3.0 or above. Nevertheless, our system does not rely entirely on bias, as evidenced by the fact that each column in the table has a tendency for its scores to ascend as the gold standard score increases, implying that our system has some suc- cess at predicting lower scores for essays with lower gold standard prompt adherence scores.</p><p>Another interesting point to note about this ta- ble is that the difference in error weighting be- tween the S2 and S3 scoring metrics appears to be having its desired effect, as every entry in the S3 subtable is less than its corresponding entry in the S2 subtable due to the greater penalty the S3 met- ric imposes for predictions that are very far away from the gold standard scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a feature-rich approach to the under- investigated problem of predicting essay-level prompt adherence scores on student essays. In an evaluation on 830 argumentative essays selected from the ICLE corpus, our system significantly outperformed a Random Indexing based baseline by several evaluation metrics. To stimulate further research on this task, we make all our annotations, including our prompt adherence scores, the LDA topic annotations, and the error annotations pub- licly available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 : Distribution of prompt adherence scores.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Space limitations preclude a complete listing of the thesis clarity keyword features. See our website at http: //www.hlt.utdallas.edu/ ˜ persingq/ICLE/ for the complete list.</note>

			<note place="foot" n="2"> We use 100 topics for each prompt in the manually annotated version of LDA features rather than the 1,000 topics we use in the regular version of LDA features because 1,300 topics are not too costly to annotate, but manually annotating 13,000 topics would take too much time.</note>

			<note place="foot" n="3"> See our website at http://www.hlt.utdallas. edu/ ˜ persingq/ICLE/ for the complete list of error annotations.</note>

			<note place="foot" n="6"> All significance tests are paired t-tests. 7 These numbers are calculated B−O B−P where B is the baseline system&apos;s score, O is our system&apos;s score, and P is a perfect score. Perfect scores for error measures and P C are 0 and 1 respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated essay scoring with E-rater v.2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning, and Assessment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Journal of Technology</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/˜cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indexing by latent smeantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">International Corpus of Learner English (Version 2). Presses universitaires de Louvain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylviane</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estelle</forename><surname>Dagneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magali</forename><surname>Paquot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentence similarity measures for essay coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Computational Semantics</title>
		<meeting>the 7th International Workshop on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating multiple aspects of coherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2004 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying off-topic student essays without topicspecific training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The S-Space package: An open source package for word space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
		<meeting>the ACL 2010 System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random indexing of text samples for latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pentti</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kristoferson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of the Cognitive Science Society</title>
		<meeting>the 22nd Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="103" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A solution to plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dutnais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automated scoring and annotation of essays with the Intelligent Essay Assessor TM ˙ In Automated Essay Scoring: A Cross-Disciplinary Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Laham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
			<biblScope unit="page" from="87" to="112" />
			<pubPlace>Mahwah, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Off-topic essay detection using short prompt texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="92" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://mallet.cs.umass.edu" />
		<title level="m">MALLET: A Machine Learning for Language Toolkit</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of text coherence for electronic essay scoring systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="55" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">English Gigaword Fourth Edition. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling thesis clarity in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling organization in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An introduction to random indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automated Essay Scoring: A Cross-Disciplinary Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><forename type="middle">C</forename><surname>Shermis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
			<pubPlace>Mahwah, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated essay scoring: Writing assessment and instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Shermis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Encyclopedia of Education</title>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Machine Learning</title>
		<meeting>the 14th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
