<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparser, Better, Faster GPU Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparser, Better, Faster GPU Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="208" to="217"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Due to their origin in computer graphics , graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per second-more than a 2x speedup-on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning-nearly a 6x speedup.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Because NLP models typically treat sentences in- dependently, NLP problems have long been seen as "embarrassingly parallel" -large corpora can be processed arbitrarily fast by simply sending dif- ferent sentences to different machines. However, recent trends in computer architecture, particularly the development of powerful "general purpose" GPUs, have changed the landscape even for prob- lems that parallelize at the sentence level. First, classic single-core processors and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine. Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for the kinds of sparse computations that most current CPU-based NLP algorithms rely on.</p><p>Recently, <ref type="bibr" target="#b0">Canny et al. (2013)</ref> proposed a GPU implementation of a constituency parser that sac- rifices all sparsity in exchange for the sheer horse- power that GPUs can provide. Their system uses a grammar based on the Berkeley parser <ref type="bibr" target="#b8">(Petrov and Klein, 2007</ref>) (which is particularly amenable to GPU processing), "compiling" the grammar into a sequence of GPU kernels that are applied densely to every item in the parse chart. Together these kernels implement the Viterbi inside algorithm. On a mid-range GPU, their system can compute Viterbi derivations at 164 sentences per second on sentences of length 40 or less (see timing details below).</p><p>In this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-to- fine pruning to a GPU setting. On a CPU, pruning methods can give speedups of up to 100x. Such extreme speedups over a dense GPU baseline cur- rently seem unlikely because fine-grained sparsity appears to be directly at odds with dense paral- lelism. However, in this paper, we present a sys- tem that finds a middle ground, where some level of sparsity can be maintained without losing the parallelism of the GPU. We use a coarse-to-fine approach as in <ref type="bibr" target="#b8">Petrov and Klein (2007)</ref>, but with only one coarse pass. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of the approach: we first parse densely with a coarse grammar and then parse sparsely with the fine grammar, skipping symbols that the coarse pass deemed sufficiently unlikely. Using this ap- proach, we see a gain of more than 2x over the dense GPU implementation, resulting in overall speeds of up to 404 sentences per second. For comparison, the publicly available CPU imple- mentation of <ref type="bibr" target="#b8">Petrov and Klein (2007)</ref> parses ap- proximately 7 sentences per second per core on a modern CPU.</p><p>A further drawback of the dense approach in <ref type="bibr" target="#b0">Canny et al. (2013)</ref> is that it only computes Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of <ref type="bibr" target="#b8">Petrov and Klein (2007)</ref> only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn Treebank ( <ref type="bibr" target="#b5">Marcus et al., 1993)</ref>. To that end, we extend our coarse-to- fine GPU approach to computing marginals, along the way proposing a new way to exploit the coarse pass to avoid expensive log-domain computations in the fine pass. We then implement minimum- Bayes-risk parsing via the max recall algorithm of <ref type="bibr" target="#b2">Goodman (1996)</ref>. Without the coarse pass, the dense marginal computation is not efficient on a GPU, processing only 32 sentences per second. However, our approach allows us to process over 190 sentences per second, almost a 6x speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Note on Experiments</head><p>We build up our approach incrementally, with ex- periments interspersed throughout the paper, and summarized in <ref type="table">Tables 1 and 2</ref>. In this paper, we focus our attention on current-generation NVIDIA GPUs. Many of the ideas described here apply to other GPUs (such as those from AMD), but some specifics will differ. All experiments are run with an NVIDIA GeForce GTX 680, a mid-range GPU that costs around $500 at time of writing. Unless otherwise noted, all experiments are conducted on sentences of length ≤ 40 words, and we estimate times based on batches of 20K sentences. <ref type="bibr">1</ref> We should note that our experimental condition dif- fers from that of <ref type="bibr" target="#b0">Canny et al. (2013)</ref>: they evaluate on sentences of length ≤ 30. Furthermore, they <ref type="bibr">1</ref> The implementation of <ref type="bibr" target="#b0">Canny et al. (2013)</ref> cannot han- dle batches so large, and so we tested it on batches of 1200 sentences. Our reimplementation is approximately the same speed for the same batch sizes. For batches of 20K sentences, we used sentences from the training set. We verified that there was no significant difference in speed for sentences from the training set and from the test set. use two NVIDIA GeForce GTX 690s-each of which is essentially a repackaging of two 680s- meaning that our system and experiments would run approximately four times faster on their hard- ware. (This expected 4x factor is empirically con- sistent with the result of running their system on our hardware.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sparsity and CPUs</head><p>One successful approach for speeding up con- stituency parsers has been to use coarse-to-fine inference <ref type="bibr" target="#b1">(Charniak et al., 2006</ref>). In coarse-to- fine inference, we have a sequence of increasingly complex grammars G . Typically, each succes- sive grammar G is a refinement of the preceding grammar G −1 . That is, for each symbol A x in the fine grammar, there is some symbol A in the coarse grammar. For instance, in a latent variable parser, the coarse grammar would have symbols like N P , V P , etc., and the fine pass would have refined symbols N P 0 , N P 1 , V P 4 , and so on.</p><p>In coarse-to-fine inference, one applies the grammars in sequence, computing inside and out- side scores. Next, one computes (max) marginals for every labeled span (A, i, j) in a sentence. These max marginals are used to compute a prun- ing mask for every span (i, j). This mask is the set of symbols allowed for that span. Then, in the next pass, one only processes rules that are licensed by the pruning mask computed at the previous level.</p><p>This approach works because a low quality coarse grammar can still reliably be used to prune many symbols from the fine chart without loss of accuracy. <ref type="bibr" target="#b8">Petrov and Klein (2007)</ref> found that over 98% of symbols can be pruned from typical charts using a simple X-bar grammar without any loss of accuracy. Thus, the vast majority of rules can be skipped, and therefore most computation can be avoided. It is worth pointing out that although 98% of labeled spans can be skipped due to X-bar pruning, we found that only about 79% of binary rule applications can be skipped, because the un- pruned symbols tend to be the ones with a larger grammar footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GPU Architectures</head><p>Unfortunately, the standard coarse-to-fine ap- proach does not na¨ıvelyna¨ıvely translate to GPU archi- tectures. GPUs work by executing thousands of threads at once, but impose the constraint that large blocks of threads must be executing the same  <ref type="formula">(2013)</ref>'s system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass. The original system of <ref type="bibr" target="#b0">Canny et al. (2013)</ref> only used the fine pass, with no pruning. instructions in lockstep, differing only in their in- put data. Thus sparsely skipping rules and sym- bols will not save any work. Indeed, it may ac- tually slow the system down. In this section, we provide an overview of GPU architectures, focus- ing on the details that are relevant to building an efficient parser.</p><p>The large number of threads that a GPU exe- cutes are packaged into blocks of 32 threads called warps. All threads in a warp must execute the same instruction at every clock cycle: if one thread takes a branch the others do not, then all threads in the warp must follow both code paths. This situa- tion is called warp divergence. Because all threads execute all code paths that any thread takes, time can only be saved if an entire warp agrees to skip any particular branch.</p><p>NVIDIA GPUs have 8-15 processors called streaming multi-processors or SMs. <ref type="bibr">2</ref> Each SM can process up to 48 different warps at a time: it interleaves the execution of each warp, so that when one warp is stalled another warp can exe- cute. Unlike threads within a single warp, the 48 warps do not have to execute the same instruc- tions. However, the memory architecture is such that they will be faster if they access related mem- ory locations.</p><p>A further consideration is that the number of registers available to a thread in a warp is rather limited compared to a CPU. On the 600 series, maximum occupancy can only be achieved if each thread uses at most 63 registers ( <ref type="bibr" target="#b7">Nvidia, 2008)</ref>. <ref type="bibr">3</ref> Registers are many times faster than variables lo- cated in thread-local memory, which is actually the same speed as global memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Anatomy of a Dense GPU Parser</head><p>This architecture environment puts very different constraints on parsing algorithms from a CPU en- vironment. <ref type="bibr" target="#b0">Canny et al. (2013)</ref> proposed an imple- mentation of a PCFG parser that sacrifices stan- dard sparse methods like coarse-to-fine pruning, focusing instead on maximizing the instruction and memory throughput of the parser. They as- sume that they are parsing many sentences at once, with throughput being more important than la- tency. In this section, we describe their dense algo- rithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow.</p><p>At the top level, the CPU and GPU communi- cate via a work queue of parse items of the form (s, i, k, j), where s is an identifier of a sentence, i is the start of a span, k is the split point, and j  <ref type="table">Table 1</ref>: Performance numbers for computing Viterbi inside charts on 20,000 sentences of length ≤40 from the Penn Treebank. All times are measured on an NVIDIA GeForce GTX 680. 'Reimpl' is our reimplementation of their ap- proach. Speedups are measured in reference to this reimplementation. See Section 7 for discussion of the clustering algorithms and Section 6 for a de- scription of the pruning methods. The <ref type="bibr" target="#b0">Canny et al. (2013)</ref> system is benchmarked on a batch size of 1200 sentences, the others on 20,000.</p><p>is the end point. The GPU takes large numbers of parse items and applies the entire grammar to them in parallel. These parse items are enqueued in or- der of increasing span size, blocking until all items of a given length are complete. This approach is diagrammed in <ref type="figure">Figure 2</ref>. Because all rules are applied to all parse items, all threads are executing the same sequence of in- structions. Thus, there is no concern of warp di- vergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Grammar Compilation</head><p>One important feature of <ref type="bibr" target="#b0">Canny et al. (2013)</ref>'s sys- tem is grammar compilation. Because registers are so much faster than thread-local memory, it is critical to keep as many variables in registers as possible. One way to accomplish this is to un- roll loops at compilation time. Therefore, they in- lined the iteration over the grammar directly into the GPU kernels (i.e. the code itself), which al- lows the compiler to more effectively use all of its registers.</p><p>However, register space is limited on GPUs. Because the Berkeley grammar is so large, the compiler is not able to efficiently schedule all of the operations in the grammar, resulting in regis- ter spills. <ref type="bibr" target="#b0">Canny et al. (2013)</ref> found they had to partition the grammar into multiple different ker- nels. We discuss this partitioning in more detail in Section 7. However, in short, the entire grammar G is broken into multiple clusters G i where each rule belongs to exactly one cluster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(0, 1, 3) (0, 2, 3) (1, 2, 4) (1, 3, 4) (2, 3, 5) (2, 4, 5)</head><p>Grammar Queue (i, k, j)  <ref type="table">Table 1</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Pruning on a GPU</head><p>Now we turn to the algorithmic and architectural changes in our approach. First, consider trying to directly apply the coarse-to-fine method sketched in Section 3 to the dense baseline described above. The natural implementation would be for each thread to check if each rule is licensed before applying it. However, we would only avoid the work of applying the rule if all threads in the warp agreed to skip it. Since each thread in the warp is processing a different span (perhaps even from a different sentence), consensus from all 32 threads on any skip would be unlikely.</p><p>Another approach would be to skip enqueu- ing any parse item (s, i, k, j) where the pruning mask for any of (i, j), (i, <ref type="table">Table 1</ref> on the row labeled 'Reimpl' with 'Empty, Coarse' pruning.</p><note type="other">k), or (k, j) is entirely empty (i.e. all symbols are pruned in this cell by the coarse grammar). However, our experiments showed that only 40% of parse items are pruned in this manner. Because of the overhead associated with creating pruning masks and the further over- head of GPU communication, we found that this method did not actually produce any time savings at all. The result is a parsing speed of 185.5 sen- tences per second, as shown in</note><p>Instead, we take advantage of the partitioned structure of the grammar and organize our com- putation around the coarse symbol set. Recall that the baseline already partitions the grammar G into rule clusters G i to improve register sharing. (See Section 7 for more on the baseline clustering.) We create a separate work queue for each partition. We call each such queue a labeled work queue, and each one only queues items to which some rule in the corresponding partition applies. We call the set of coarse symbols for a partition (and therefore the corresponding labeled work queue) a signature.</p><p>During parsing, we only enqueue items (s, i, k, j) to a labeled queue if two conditions are met. First, the span (i, j)'s pruning mask must have a non-empty intersection with the signature of the queue. Second, the pruning mask for the children (i, k) and (k, j) must be non-empty.</p><p>Once on the GPU, parse items are processed us- ing the same style of compiled kernel as in <ref type="bibr" target="#b0">Canny et al. (2013)</ref>. Because the entire partition (though not necessarily the entire grammar) is applied to each item in the queue, we still do not need to worry about warp divergence.</p><p>At the top level, our system first computes prun- ing masks with a coarse grammar. Then it pro- cesses the same sentences with the fine gram- mar. However, to the extent that the signatures are small, items can be selectively queued only to certain queues. This approach is diagrammed in <ref type="figure">Figure 3</ref>.</p><p>We tested our new pruning approach using an X-bar grammar as the coarse pass. The result- ing speed is 187.5 sentences per second, labeled in <ref type="table">Table 1</ref> as row labeled 'Reimpl' with 'Labeled, Coarse' pruning. Unfortunately, this approach again does not produce a speedup relative to our reimplemented baseline. To improve upon this re- sult, we need to consider how the grammar clus- tering interacts with the coarse pruning phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Grammar Clustering</head><p>Recall that the rules in the grammar are partitioned into a set of clusters, and that these clusters are further divided into subclusters. How can we best cluster and subcluster the grammar so as to maxi- mize performance? A good clustering will group rules together that use the same symbols, since this means fewer memory accesses to read and write scores for symbols. Moreover, we would like the time spent processing each of the subclus- ters within a cluster to be about the same. We can- not move on to the next cluster until all threads from a cluster are finished, which means that the time a cluster takes is the amount of time taken by the longest-running subcluster. Finally, when pruning, it is best if symbols that have the same coarse projection are clustered together. That way, we are more likely to be able to skip a subcluster, since fewer distinct symbols need to be "off" for a parse item to be skipped in a given subcluster. <ref type="bibr" target="#b0">Canny et al. (2013)</ref> clustered symbols of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols. Then the rules of the grammar were laid out in a (sparse) three-dimensional tensor, with one di- mension representing the parent of the rule, one representing the left child, and one representing the right child. They then split the cube into 6x2x2 contiguous "major cubes," giving a partition of the rules into 24 clusters. They then further subdi- vided these cubes into 2x2x2 minor cubes, giv- ing 8 subclusters that executed in parallel. Note that the clusters induced by these major and minor cubes need not be of similar sizes; indeed, they of- ten are not. Clustering using this method is labeled 'Reimplementation' in <ref type="table">Table 1</ref>.</p><p>The addition of pruning introduces further con- siderations. First, we have a coarse grammar, with many fewer rules and symbols. Second, we are able to skip a parse item for an entire cluster if that item's pruning mask does not intersect the clus- ter's signature. Spreading symbols across clusters may be inefficient: if a parse item licenses a given symbol, we will have to enqueue that item to any queue that has the symbol in its signature, no mat- ter how many other symbols are in that cluster.</p><p>Thus, it makes sense to choose a clustering al- gorithm that exploits the structure introduced by the pruning masks. We use a very simple method: we cluster the rules in the grammar by coarse par- ent symbol. When coarse symbols are extremely unlikely (and therefore have few corresponding rules), we merge their clusters to avoid the over- head of beginning work on clusters where little work has to be done. <ref type="bibr">4</ref> In order to subcluster, we divide up rules among subclusters so that each subcluster has the same number of active parent symbols. We found this approach to subclustering worked well in practice.</p><p>Clustering using this method is labeled 'Parent' in <ref type="table">Table 1</ref>. Now, when we use a coarse pruning pass, we are able to parse nearly 280 sentences per second, a 70% increase in parsing performance relative to <ref type="bibr" target="#b0">Canny et al. (2013)</ref>'s system, and nearly 50% over our reimplemented baseline.</p><p>It turns out that this simple clustering algorithm produces relatively efficient kernels even in the un- pruned case. The unpruned Viterbi computations in a fine grammar using the clustering method of <ref type="bibr" target="#b0">Canny et al. (2013)</ref> yields a speed of 193 sen- tences per second, whereas the same computation using coarse parent clustering has a speed of 159 sentences per second. (See <ref type="table">Table 1</ref>.) This is not as efficient as <ref type="bibr" target="#b0">Canny et al. (2013)</ref>'s highly tuned method, but it is still fairly fast, and much simpler to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Pruning with Finer Grammars</head><p>The coarse to fine pruning approach of <ref type="bibr" target="#b8">Petrov and Klein (2007)</ref> employs an X-bar grammar as its first pruning phase, but there is no reason why we cannot begin with a more complex grammar for our initial pass. As <ref type="bibr" target="#b8">Petrov and Klein (2007)</ref> have shown, intermediate-sized Berkeley gram- mars prune many more symbols than the X-bar system. However, they are slower to parse with in a CPU context, and so they begin with an X-bar grammar.</p><p>Because of the overhead associated with trans- ferring work items to GPU, using a very small grammar may not be an efficient use of the GPU's computational resources. To that end, we tried computing pruning masks with one-split and two- split Berkeley grammars. The X-bar grammar can compute pruning masks at just over 1000 sen- tences per second, the 1-split grammar parses 858 sentences per second, and the 2-split grammar parses 526 sentences per second.</p><p>Because parsing with these grammars is still quite fast, we tried using them as the coarse pass instead. As shown in <ref type="table">Table 1</ref>, using a 1-split gram- mar as a coarse pass allows us to produce over 400 sentences per second, a full 2x improvement over our original system. Conducting a coarse pass with a 2-split grammar is somewhat slower, at a "mere" 343 sentences per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Minimum Bayes risk parsing</head><p>The Viterbi algorithm is a reasonably effective method for parsing. However, many authors have noted that parsers benefit substantially from minimum Bayes risk decoding <ref type="bibr" target="#b2">(Goodman, 1996;</ref><ref type="bibr" target="#b9">Simaan, 2003;</ref><ref type="bibr" target="#b6">Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b10">Titov and Henderson, 2006;</ref><ref type="bibr" target="#b8">Petrov and Klein, 2007)</ref>. MBR algorithms for parsing do not compute the best derivation, as in Viterbi parsing, but instead the parse tree that maximizes the expected count of some figure of merit. For instance, one might want to maximize the expected number of correct con- stituents <ref type="bibr" target="#b2">(Goodman, 1996)</ref>, or the expected rule counts <ref type="bibr" target="#b9">(Simaan, 2003;</ref><ref type="bibr" target="#b8">Petrov and Klein, 2007)</ref>. MBR parsing has proven especially useful in la- tent variable grammars. <ref type="bibr" target="#b8">Petrov and Klein (2007)</ref> showed that MBR trees substantially improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1.</p><p>Here, we implement the Max Recall algorithm of <ref type="bibr" target="#b2">Goodman (1996)</ref>. This algorithm maximizes the expected number of correct coarse symbols (A, i, j) with respect to the posterior distribution over parses for a sentence.</p><p>This particular MBR algorithm has the advan- tage that it is relatively straightforward to imple- ment. In essence, we must compute the marginal probability of each fine-labeled span µ(A x , i, j), and then marginalize to obtain µ(A, i, j). Then, for each span (i, j), we find the best possible split point k that maximizes C(i, j) = µ(A, i, j) + max k (C(i, k) + C(k, j)). Parse extraction is then just a matter of following back pointers from the root, as in the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Computing marginal probabilities</head><p>The easiest way to compute marginal probabilities is to use the log space semiring rather than the Viterbi semiring, and then to run the inside and outside algorithms as before. We should expect this algorithm to be at least a factor of two slower: the outside pass performs at least as much work as the inside pass. Moreover, it typically has worse memory access patterns, leading to slower perfor- mance.</p><p>Without pruning, our approach does not han- dle these log domain computations well at all: we are only able to compute marginals for 32.1 sentences/second, more than a factor of 5 slower than our coarse pass. To begin, log space addition requires significantly more operations than max, which is a primitive operation on GPUs. Beyond the obvious consequence that executing more op- erations means more time taken, the sheer number of operations becomes too much for the compiler to handle. Because the grammars are compiled into code, the additional operations are all inlined into the kernels, producing much larger kernels. Indeed, in practice the compiler will often hang if we use the same size grammar clusters as we did for Viterbi. In practice, we found there is an effec- tive maximum of 2000 rules per kernel using log sums, while we can use more than 10,000 rules rules in a single kernel with Viterbi.</p><p>With coarse pruning, however, we can avoid much of the increased cost associated with log domain computations. Because so many labeled spans are pruned, we are able to skip many of the grammar clusters and thus avoid many of the ex- pensive operations. Using coarse pruning and log domain calculations, our system produces MBR trees at a rate of 130.4 sentences per second, a four-fold increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Scaling with the Coarse Pass</head><p>One way to avoid the expense of log domain com- putations is to use scaled probabilities rather than log probabilities. Scaling is one of the folk tech- niques that are commonly used in the NLP com- munity, but not generally written about. Recall that floating point numbers are composed of a mantissa m and an exponent e, giving a number  <ref type="table">Table 2</ref>: Performance numbers for computing max constituent <ref type="bibr" target="#b2">(Goodman, 1996</ref>) trees on 20,000 sen- tences of length 40 or less from the Penn Tree- bank. For convenience, we have copied our pruned Viterbi system's result. f = m · 2 e . When a float underflows, the ex- ponent becomes too low to represent the available number of bits. In scaling, floating point numbers are paired with an additional number that extends the exponent. That is, the number is represented as f = f · exp(s). Whenever f becomes either too big or too small, the number is rescaled back to a less "dangerous" range by shifting mass from the exponent e to the scaling factor s.</p><p>In practice, one scale s is used for an entire span (i, j), and all scores for that span are rescaled in concert. In our GPU system, multiple scores in any given span are being updated at the same time, which makes this dynamic rescaling tricky and ex- pensive, especially since inter-warp communica- tion is fairly limited.</p><p>We propose a much simpler static solution that exploits the coarse pass. In the coarse pass, we compute Viterbi inside and outside scores for ev- ery span. Because the grammar used in the coarse pass is a projection of the grammar used in the fine pass, these coarse scores correlate reasonably closely with the probabilities computed in the fine pass: If a span has a very high or very low score in the coarse pass, it typically has a similar score in the fine pass. Thus, we can use the coarse pass's inside and outside scores as the scaling val- ues for the fine pass's scores. That is, in addition to computing a pruning mask, in the coarse pass we store the maximum inside and outside score in each span, giving two arrays of scores s I i,j and s O i,j . Then, when applying rules in the fine pass, each fine inside score over a split span (i, k, j) is scaled to the appropriate s I i,j by multiplying the score by exp</p><formula xml:id="formula_0">s I i,k + s I k,j − s I i,j</formula><p>, where s I i,k , s I k,j , s I i,j are the scaling factors for the left child, right child, and parent, respectively. The outside scores are scaled analogously.</p><p>By itself, this approach works on nearly ev- ery sentence. However, scores for approximately 0.5% of sentences overflow (sic). Because we are summing instead of maxing scores in the fine pass, the scaling factors computed using max scores are not quite large enough, and so the rescaled inside probabilities grow too large when multiplied to- gether. Most of this difference arises at the leaves, where the lexicon typically has more uncertainty than higher up in the tree. Therefore, in the fine pass, we normalize the inside scores at the leaves to sum to 1.0. 5 Using this slight modification, no sentences from the Treebank under-or overflow.</p><p>We know of no reason why this same trick can- not be employed in more traditional parsers, but it is especially useful here: with this static scal- ing, we can avoid the costly log sums without in- troducing any additional inter-thread communica- tion, making the kernels much smaller and much faster. Using scaling, we are able to push our parser to 190.6 sentences/second for MBR extrac- tion, just under half the speed of the Viterbi sys- tem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Parsing Accuracies</head><p>It is of course important verify the correctness of our system; one easy way to do so is to exam- ine parsing accuracy, as compared to the original Berkeley parser. We measured parsing accuracy on sentences of length ≤ 40 from section 22 of the Penn Treebank. Our Viterbi parser achieves 89.7 F1, while our MBR parser scores 91.0. These re- sults are nearly identical to the Berkeley parsers most comparable numbers: 89.8 for Viterbi, and 90.9 for their "Max-Rule-Sum" MBR algorithm. These slight differences arise from the usual mi- nor variation in implementation details. In partic- ular, we use one coarse pass instead of several, and a different MBR algorithm. In addition, there are some differences in unary processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Analyzing System Performance</head><p>In this section we attempt to break down how ex- actly our system is spending its time. We do this in an effort to give a sense of how time is spent dur- ing computation on GPUs. These timing numbers are computed using the built-in profiling capabil- ities of the programming environment. As usual, profiles exhibit an observer effect, where the act of measuring the system changes the execution. Nev-  <ref type="table">Table 3</ref>: Time spent in the passes of our differ- ent systems, in seconds per 1000 sentences. Prun- ing refers to using a 1-split grammar for the coarse pass.</p><p>ertheless, the general trends should more or less be preserved as compared to the unprofiled code.</p><p>To begin, we can compute the number of sec- onds needed to parse 1000 sentences. (We use sec- onds per sentence rather than sentences per second because the former measure is additive.) The re- sults are in <ref type="table">Table 3</ref>. In the case of pruned Viterbi, pruning reduces the amount of time spent in the fine pass by more than 4x, though half of those gains are lost to computing the pruning masks.</p><p>In <ref type="table">Table 4</ref>, we break down the time taken by our system into individual components. As ex- pected, binary rules account for the vast majority of the time in the unpruned Viterbi case, but much less time in the pruned case, with the total time taken for binary rules in the coarse and fine passes taking about 1/5 of the time taken by binaries in the unpruned version. Queueing, which involves copying memory around within the GPU to pro- cess the individual parse items, takes a fairly con- sistent amount of time in all systems. Overhead, which includes transport time between the CPU and GPU and other processing on the CPU, is rela- tively small for most system configurations. There is greater overhead in the scaling system, because scaling factors are copied to the CPU between the coarse and fine passes.</p><p>A final question is: how many sentences per second do we need to process to saturate the GPU's processing power? We computed Viterbi parses of successive powers of 10, from 1 to 100,000 sentences. 6 In <ref type="figure" target="#fig_4">Figure 4</ref>, we then plotted the throughput, in terms of number of sentences per second. Throughput increases through parsing 10,000 sentences, and then levels off by the time it reaches 100,000 sentences.   <ref type="table">Table 4</ref>: Breakdown of time spent in our different systems, in seconds per 1000 sentences. Binary and Unary refer to spent processing binary rules. Queueing refers to the amount of time used to move memory around within the GPU for processing. Overhead includes all other time, which includes communication between the GPU and the CPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Related Work</head><p>Apart from the model of <ref type="bibr" target="#b0">Canny et al. (2013)</ref>, there have been a few attempts at using GPUs in NLP contexts before. Johnson (2011) and Yi et al. (2011) both had early attempts at porting pars- ing algorithms to the GPU. However, they did not demonstrate significantly increased speed over a CPU implementation. In machine translation, <ref type="bibr" target="#b3">He et al. (2013)</ref> adapted algorithms designed for GPUs in the computational biology literature to speed up on-demand phrase table extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Conclusion</head><p>GPUs represent a challenging opportunity for nat- ural language processing. By carefully design- ing within the constraints imposed by the architec- ture, we have created a parser that can exploit the same kinds of sparsity that have been developed for more traditional architectures. One of the key remaining challenges going forward is confronting the kind of lexicalized sparsity common in other NLP models. The Berkeley parser's grammars-by virtue of being unlexicalized-can be applied uniformly to all parse items. The bilexical features needed by dependency models and lexicalized constituency models are not directly amenable to acceleration using the techniques we described here. Deter- mining how to efficiently implement these kinds of models is a promising area for new research.</p><p>Our system is available as open-source at https://www.github.com/dlwh/puck.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the architecture of our system, which is an extension of Canny et al. (2013)'s system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the fine pass. The original system of Canny et al. (2013) only used the fine pass, with no pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Schematic representation of the work queue used in Canny et al. (2013). The Viterbi inside loop for the grammar is inlined into a kernel. The kernel is applied to all items in the queue in a blockwise manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>System</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of speeds (sentences / second) for various sizes of input corpora. The full power of the GPU parser is only reached when run on large numbers of sentences.</figDesc><graphic url="image-1.png" coords="9,107.82,198.62,167.00,103.20" type="bitmap" /></figure>

			<note place="foot" n="2"> Older hardware (600 series or older) has 8 SMs. Newer hardware has more.</note>

			<note place="foot" n="3"> A thread can use more registers than this, but the full complement of 48 warps cannot execute if too many are used.</note>

			<note place="foot" n="4"> Specifically, after clustering based on the coarse parent symbol, we merge all clusters with less than 300 rules in them into one large cluster.</note>

			<note place="foot" n="6"> We replicated the Treebank for the 100,000 sentences pass.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by BBN un-der DARPA contract HR0011-12-C-0014, by a Google PhD fellowship to the first author, and an NSF fellowship to the second. We further gratefully acknowledge a hardware donation by NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multi-teraflop constituency parser using GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1898" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilevel coarse-to-fine pcfg parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Austerweil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Haxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Shrivaths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pozar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="168" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Massively parallel suffix array queries and on-demand phrase extraction for statistical machine translation using gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parsing in parallel on multiple cores and gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Programming guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuda</forename><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On maximizing metrics for syntactic disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalil Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Loss minimization in parse reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="560" to="567" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient parallel cky parsing on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yue</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Parsing Technologies</title>
		<meeting>the 2011 Conference on Parsing Technologies<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
