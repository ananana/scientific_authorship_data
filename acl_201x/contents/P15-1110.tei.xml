<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Optimization for Constituent Parsing via Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<email>zhigwang@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson</orgName>
								<address>
									<addrLine>1101 Kitchawan Yorktown Heights</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Watson</orgName>
								<address>
									<addrLine>1101 Kitchawan Yorktown Heights</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
							<email>xuen@brandeis.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Brandeis University</orgName>
								<address>
									<addrLine>415 South St Waltham</addrLine>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Optimization for Constituent Parsing via Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1138" to="1147"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The performance of discriminative constituent parsing relies crucially on feature engineering, and effective features usually have to be carefully selected through a painful manual process. In this paper, we propose to automatically learn a set of effective features via neural networks. Specifically, we build a feedforward neu-ral network model, which takes as input a few primitive units (words, POS tags and certain contextual tokens) from the local context, induces the feature representation in the hidden layer and makes parsing predictions in the output layer. The network simultaneously learns the feature representation and the prediction model parameters using a back propagation algorithm. By pre-training the model on a large amount of automatically parsed data, and then fine-tuning on the manually annotated Treebank data, our parser achieves the highest F 1 score at 86.6% on Chi-nese Treebank 5.1, and a competitive F 1 score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we significantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Constituent parsing seeks to uncover the phrase structure representation of sentences that can be used in a variety of natural language applications such as machine translation, information extrac- tion and question answering <ref type="bibr" target="#b14">(Jurafsky and Martin, 2008)</ref>. One of the major challenges for this task is that constituent parsers require an inference algo- rithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discrimina- tive models. So, for a long time, the task was mainly solved with generative models <ref type="bibr" target="#b5">(Collins, 1999;</ref><ref type="bibr" target="#b3">Charniak, 2000;</ref><ref type="bibr" target="#b20">Petrov et al., 2006</ref>). In the last few years, however, with the use of ef- fective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the gen- erative models <ref type="bibr" target="#b1">(Carreras et al., 2008;</ref><ref type="bibr" target="#b33">Zhu et al., 2013;</ref><ref type="bibr" target="#b26">Wang and Xue, 2014)</ref>.</p><p>Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many fea- tures may result in an overfitting problem. Usu- ally, an effective set of features have to be de- signed manually and selected through repeated ex- periments ( <ref type="bibr" target="#b23">Sagae and Lavie, 2005;</ref><ref type="bibr" target="#b28">Wang et al., 2006</ref>; <ref type="bibr" target="#b32">Zhang and Clark, 2009)</ref>. Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even af- ter this painstaking process, it is still hard to say whether the selected feature set is complete or op- timal to obtain the best possible results. A more desirable alternative is to learn features automat- ically with machine learning algorithms. <ref type="bibr" target="#b15">Lei et al. (2014)</ref> proposed to learn features by represent- ing the cross-products of some primitive units with low-rank tensors for dependency parsing. How- ever, to achieve competitive performance, they had to combine the learned features with the tradi- tional hand-crafted features. For constituent pars- ing, <ref type="bibr" target="#b9">Henderson (2003)</ref> employed a recurrent neu- ral network to induce features from an unbounded parsing history. However, the final performance was below the state of the art.</p><p>In this work, we design a much simpler neu- ral network to automatically induce features from just the local context for constituent parsing. Con-cretely, we choose the shift-reduce parsing strat- egy to build the constituent structure of a sentence, and train a feedforward neural network model to jointly learn feature representations and make parsing predictions. The input layer of the net- work takes as input a few primitive units (words, POS tags and certain contextual tokens) from the local context, the hidden layer aims to induce a distributed feature representation by combining all the primitive units with different weights, and the output layer attempts to make parsing predic- tions based on the feature representation. Dur- ing the training process, the model simultaneously learns the feature representation and prediction model parameters using a backpropagation algo- rithm. Theoretically, the learned feature represen- tation is optimal (or at least locally optimal) for the parsing predictions. In practice, however, our model does not work well if it is only trained on the manually annotated Treebank data sets. How- ever, when pre-trained on a large amount of auto- matically parsed data and then fine-tuned on the Treebank data sets, our model achieves a fairly large improvement in performance. We evaluated our model on both Chinese and English. On stan- dard data sets, our model reaches F 1 = 86.6% for Chinese and outperforms all the state-of-the- art systems, and for English our final performance is F 1 = 90.7% and this result surpasses that of all the previous neural network based models and is comparable to the state-of-the-art systems. On cross-domain data sets, our model outperforms the Berkeley Parser 1 by 3.4 percentage points for Chi- nese and 2.5 percentage points for English.</p><p>The remainder of this paper is organized as fol- lows: Section 2 introduces the shift-reduce con- stituent parsing approach. Section 3 describes our feature optimization model and some parameter estimation techniques. We discuss and analyze our experimental results in Section 4. Section 5 discusses related work. Finally, we conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shift-Reduce Constituent Parsing</head><p>Shift-reduce constituent parsing utilizes a series of shift-reduce decisions to construct syntactic trees. Formally, the shift-reduce system is a quadruple C = (S, T, s 0 , S t ), where S is a set of parser states (sometimes called configurations), T is a fi- nite set of actions, s 0 is an initialization function to map each input sentence into a unique initial state, and S t ∈ S is a set of terminal states. Each action t ∈ T is a transition function that maps a state into a new state. A parser state s ∈ S is defined as a tuple s = (σ, β), where σ is a stack which is maintained to hold partial subtrees that are already constructed, and β is a queue which is used for storing remaining unprocessed words. In particular, the initial state has an empty stack σ and a queue β containing the entire input sentence, and the terminal states have an empty queue β and a stack σ containing only one complete parse tree. The task of parsing is to scan the input sentence from left to right and perform a sequence of shift- reduce actions to transform the initial state into a terminal state.</p><p>In order to jointly assign POS tags and construct a constituent structure for an input sentence, we define the following actions for the action set T , following <ref type="bibr" target="#b26">Wang and Xue (2014)</ref>:</p><p>• SHIFT-X (sh-x): remove the first word from β, assign a POS tag X to the word and push it onto the top of σ;</p><p>• REDUCE-UNARY-X (ru-x): pop the top subtree from σ, construct a new unary node labeled with X for the subtree, then push the new subtree back onto σ. The head of the new subtree is inherited from its child;</p><p>• REDUCE-BINARY-{L/R}-X (rl/rr-x): pop the top two subtrees from σ, combine them into a new tree with a node labeled with X, then push the new subtree back onto σ. The left (L) and right (R) versions of the action indicate whether the head of the new subtree is inherited from its left or right child.</p><p>With these actions, our parser can process trees with unary and binary branches easily. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, for the sentence "the assets are sold", our parser can construct the parse tree by performing the action sequence {sh-DT, sh-NNS, rr-NP, sh-VBP, sh-VBN, ru-VP, rr-VP, rr-S}. To pro- cess multi-branch trees, we employ binarization and debinarization processes described in <ref type="bibr" target="#b32">Zhang and Clark (2009)</ref> to transform multi-branch trees into binary trees and restore the generated binary trees back to their original forms. For inference, we employ the beam search decoding algorithm <ref type="bibr" target="#b32">(Zhang and Clark, 2009)</ref> to balance the tradeoff between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Optimization Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>To determine which action t ∈ T should be per- formed at a given state s ∈ S, we need a model to score each possible s, t combination. In pre- vious approaches <ref type="bibr" target="#b23">(Sagae and Lavie, 2005;</ref><ref type="bibr" target="#b28">Wang et al., 2006;</ref><ref type="bibr" target="#b32">Zhang and Clark, 2009)</ref>, the model is usually defined as a linear model Score(s, t) = − → w · Φ(s, t), where Φ(s, t) is a vector of hand- crafted features for each state-action pair and − → w is the weight vector for these features. The hand- crafted features are usually constructed by com- pounding primitive units according to some fea- ture templates. For example, almost all the pre- vious work employed the list of primitive units in <ref type="table">Table 1</ref>(a), and constructed hand-crafted features by concatenating these primitive units according to the feature templates in <ref type="table">Table 1</ref>(b). Obviously, these feature templates are only a small subset of the cross products of all the primitive units. This feature set is the result of a large number of exper- iments through trial and error from previous work. Still we cannot say for sure that this is the optimal subset of features for the parsing task.</p><p>To cope with this problem, we propose to si- multaneously optimize feature representation and parsing accuracy via a neural network model. <ref type="figure" target="#fig_2">Fig- ure 2</ref> illustrates the architecture of our model. Our model consists of input, projection, hidden and output layers. First, in the input layer, all primi- tive units (shown in <ref type="table">Table 1</ref>(a)) are imported to the network. We also import the suffixes and prefixes of the first word in the queue, because these units have been shown to be very effective for predict- ing POS tags <ref type="bibr" target="#b21">(Ratnaparkhi, 1996)</ref>. Then, in the projection layer, each primitive unit is projected into a vector. Specifically, word-type units are represented as word embeddings, and other units are transformed into one-hot representations. The</p><formula xml:id="formula_0">(1) p 0 w, p 0 t,p 0 c, p 1 w, p 1 t,p 1 c, p 2 w, p 2 t,p 2 c, p 3 w, p 3 t,p 3 c</formula><p>(2) p 0l w, p 0l c, p 0r w, p 0r c,p 0u w, p 0u c, p 1l w, p 1l c, p 1r w, p 1r c,p 1u w, p 1u c (3) q 0 w, q 1 w, q 2 w, q 3 w (a) Primitive Units unigrams p 0 tc, p 0 wc, p 1 tc, p 1 wc, p 2 tc p 2 wc, p 3 tc, p 3 wc, q 0 wt, q 1 wt q 2 wt, q 3 wt, p 0l wc, p 0r wc p 0u wc, p 1l wc, p 1r wc, p 1u wc <ref type="table">Table 1</ref>: Primitive units (a) and feature templates (b) for shift-reduce constituent parsing, where p i represents the i th subtree in the stack and q i de- notes the i th word in the queue. w refers to the head word, t refers to the head POS, and c refers to the constituent label. p il and p ir refer to the left and right child for a binary subtree p i , and p iu refers to the child of a unary subtree p i .</p><formula xml:id="formula_1">bigrams p 0 wp 1 w, p 0 wp 1 c, p 0 cp 1 w, p 0 cp 1 c p 0 wq 0 w, p 0 wq 0 t, p 0 cq 0 w, p 0 cq 0 t q 0 wq 1 w, q 0 wq 1 t, q 0 tq 1 w, q 0 tq 1 t p 1 wq 0 w, p 1 wq 0 t, p 1 cq 0 w, p 1 cq 0 t trigrams p 0 cp 1 cp 2 c, p 0 wp 1 cp 2 c, p 0 cp 1 wq 0 t p 0 cp 1 cp 2 w, p 0 cp 1 cq 0 t, p 0 wp 1 cq 0 t p 0 cp 1 wq 0 t, p 0 cp 1 cq 0 w (b) Feature Templates</formula><p>vectors of all primitive units are concatenated to form a holistic vector for the projection layer. The hidden layer corresponds to the feature representa- tion we want to learn. Each dimension in the hid- den layer can be seen as an abstract factor of all primitive units, and it calculates a weighted sum of all nodes from the projection layer and applies a non-linear activation function to yield its acti- vation. We choose the logistic sigmoid function for the hidden layer. The output layer is used for making parsing predictions. Each node in the out- put layer corresponds to a shift-reduce action. We want to interpret the activation of the output layer as a probability distribution over all possible shift- reduce actions, therefore we normalize the out- put activations (weighted summations of all nodes from the hidden layer) with the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Estimation</head><p>Our model consists of three groups of parameters:</p><p>(1) the word embedding for each word type unit,</p><formula xml:id="formula_2">w 0 … w m … t 0 … t n … c 0 … c n … … … … … … … ... ... p(t|s )</formula><p>sigmoid softmax word embedding one-hot representation dense real-valued low-dimensional vector ...  (2) the connections between the projection layer and the hidden layer which are used for learning an optimal feature representation and (3) the con- nections between the hidden layer and the output layer which are used for making accurate pars- ing predictions. We decided to learn word em- beddings separately, so that we can take advantage of a large amount of unlabeled data. The remain- ing two groups of parameters can be trained si- multaneously by the back propagation algorithm <ref type="bibr" target="#b22">(Rumelhart et al., 1988</ref>) to maximize the likeli- hood over the training data.</p><p>We also employ three crucial techniques to seek more effective parameters. First, we utilize mini- batched AdaGrad <ref type="bibr" target="#b7">(Duchi et al., 2011)</ref>, in which the learning rate is adapted differently for differ- ent parameters at different training steps. With this technique, we can start with a very large learning rate which decreases during training, and can thus perform a far more thorough search within the pa- rameter space. In our experiments, we got a much faster convergence rate with slightly better accu- racy by using the learning rate α = 1 instead of the commonly-used α = 0.01. Second, we initial- ize the model parameters by pre-training. Unsu- pervised pre-training has demonstrated its effec- tiveness as a way of initializing neural network models ( <ref type="bibr" target="#b8">Erhan et al., 2010</ref>). Since our model re- quires many run-time primitive units (POS tags and constituent labels), we employ an in-house shift-reduce parser to parse a large amount of unla- beled sentences, and pre-train the model with the automatically parsed data. Third, we utilize the Dropout strategy to address the overfitting prob- lem. However, different from <ref type="bibr" target="#b11">Hinton et al. (2012)</ref>, we only use Dropout during testing, because we found that using Dropout during training did not improve the parsing performance (on the dev set) while greatly slowing down the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 ( <ref type="bibr" target="#b30">Xue et al., 2005)</ref> </p><note type="other">and the Wall Street Journal (WSJ) portion of Penn En- glish Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 301-325 as the development set, and Articles 271- 300 as the testing set. For English, we use sec- tions 2-21 for training, section 22 for developing and section 23 for testing.</note><p>We also utilized some unlabeled corpora and used the word2vec 2 toolkit to train word em- beddings. For Chinese, we used the unlabeled Chinese Gigaword (LDC2003T09) and performed Chinese word segmentation using our in-house segmenter. For English, we randomly selected 9 million sentences from our in-house newswire cor- pus, which has no overlap with our training, test- ing and development sets. We use Evalb 3 toolkit to evaluate parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Characteristics of Our Model</head><p>There are several hyper-parameters in our model, e.g., the word embedding dimension (wordDim), the hidden layer node size (hiddenSize), the Dropout ratio (dropRatio) and the beam size for inference (beamSize). The choice of these hyper- parameters may affect the final performance. In this subsection, we present some experiments to demonstrate the characteristics of our model, and select a group of proper hyper-parameters that we use to evaluate our final model. All the experi- ments in this subsection were performed on Chi- nese data and the evaluation is performed on Chi- nese development set.</p><p>First, we evaluated the effectiveness of vari- ous primitive units. We set wordDim = 300, hiddenSize = 300, beamSize = 8, and did not apply Dropout (dropRatio = 0).  Second, we uncovered the effect of the dimen- sion of word embedding. We set hiddenSize = 300, beamSize = 8, dropRatio = 0 and var- ied wordDim among {50, 100, 300, 500, 1000}. <ref type="figure" target="#fig_3">Figure 3(a)</ref> draws the parsing performance curve. When increasing wordDim from 50 to 300, pars- ing performance improves more than 1.5 percent- age points. After that, the curve flattens out, and parsing performance only gets marginal improve- ment. Therefore, in the following experiments, we fixed wordDim = 300.</p><p>Third, we tested the effect of hidden layer node size. We varied hiddenSize among {50, 100, 300, 500, 1000}. <ref type="figure" target="#fig_3">Figure 3(b)</ref> draws the pars- ing performance curve. We found increasing hiddenSize is helpful for parsing performance. However, higher hiddenSize would greatly in- crease the amount of computation. To keep the efficiency of our model, we fixed hiddenSize = 300 in the following experiments.</p><p>Fourth, we applied Dropout and tuned the Dropout ratio through experiments. <ref type="figure" target="#fig_3">Figure 3(c)</ref> shows the results. We found that the peak performance occurred at dropRatio = 0.5, which brought about an improvement of more than 1 percentage point over the model without Dropout (dropRatio = 0  </p><formula xml:id="formula_3">dropRatio = 0.5.</formula><p>Finally, we investigated the effect of beam size. <ref type="figure" target="#fig_3">Figure 3(d)</ref> shows the curve. We found increasing beamSize greatly improves the performance ini- tially, but no further improvement is observed after beamSize is greater than 8. Therefore, we fixed beamSize = 8 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semi-supervised Training</head><p>In this subsection, we investigated whether we can train more effective models using automati- cally parsed data. We randomly selected 200K sentences from our unlabeled data sets for both Chinese and English. Then, we used an in-house shift-reduce parser <ref type="bibr">4</ref> to parse these selected sen- tences. The size of the automatically parsed data set may have an impact on the final model. So we trained many models with varying amounts of automatically parsed data. We also designed two strategies to exploit the automatically parsed data. The first strategy (Mix-Train) is to directly add the automatically parsed data to the hand-annotated training set and train models with the mixed data set. The second strategy (Pre-Train) is to first pre- train models with the automatically parsed data, and then fine-tune models with the hand-annotated training set.   strategy, when we only use 50K automatically parsed sentences, the performance drops in com- parison with the model trained without using any automatically parsed data. When we increase the automatically parsed data to 100K sentences, the parsing performance improves about 1 percent but the POS tagging accuracy drops slightly. When we further increase the automatically parsed data to 200K sentences, both the parsing performance and POS tagging accuracy improve. For the Pre- Train strategy, the performance of all three config- urations improves performance against the model that does not use any automatically parsed data. The Pre-Train strategy consistently outperforms the Mix-Train strategy when the same amount of automatically parsed data is used. Therefore, for Chinese, the Pre-Train strategy is much more help- ful, and the more automatically parsed data we use the better performance we get. <ref type="table" target="#tab_5">Table 4</ref> presents results of different experimen- tal configurations for English. The performance trend for the Mix-Train strategy is different from that of Chinese. Here, no matter how much auto- matically parsed data we use, there is a consistent degradation in performance against the model that does not use any automatically parsed data at all. And the more automatically parsed data we use, the larger the drop in accuracy. For the Pre-Train strategy, the trend is similar to Chinese. The pars- ing performance of the Pre-Train setting consis- tently improves as the size of automatically parsed data increases.  <ref type="table">Table 5</ref>: Comparison with the state-of-the-art sys- tems on Chinese test set. * marks neural network based systems. ‡ marks shift-reduce parsing sys- tems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparing With State-of-the-art Systems</head><p>In this subsection, we present the performance of our models on the testing sets. We trained two systems. The first system ("Supervised") is trained only with the hand-annotated training set, and the second system ("Pretrain-Finetune") is trained with the Pre-Train strategy described in subsection 4.3 using additional automatically parsed data. The best parameters for the two sys- tems are set based on their performance on the de- velopment set. To further illustrate the effective- ness of our systems, we also compare them with some state-of-the-art systems. We group parsing systems into three categories: supervised single systems (SI), semi-supervised single systems (SE) and reranking systems (RE). Both of our two mod- els belong to semi-supervised single systems, be- cause our "Supervised" system utilized word em- beddings in its input layer. <ref type="table">Table 5</ref> lists the performance of our systems as well as the state-of-the-art systems on Chinese test set. Comparing the performance of our two sys- tems, we see that our "Pretrain-Finetune" system shows a fairly large gain over the "Supervised" system. One explanation is that our neural net- work model is a non-linear model, so the back propagation algorithm can only reach a local op- timum. In our "Supervised" system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In com- parison, our "Pretrain-Finetune" system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained  parameters. So it finds a much better local opti- mum than the "Supervised" system. Comparing our "Pretrain-Finetune" system with all the state- of-the-art systems, we see our system surpass all the other systems. Although our system only uti- lizes some basic primitive units (in <ref type="table">Table 1</ref>(a)), it still outperforms <ref type="bibr" target="#b26">Wang and Xue (2014)</ref>'s shift- reduce parsing system which uses more complex structural features and semi-supervised word clus- ter features. Therefore, our model can simultane- ously learn an effective feature representation and make accurate parsing predictions for Chinese. <ref type="table" target="#tab_8">Table 6</ref> presents the performance of our systems as well as the state-of-the-art systems on the En- glish test set. Our "Pretrain-Finetune" system still achieves much better performance than the "Su- pervised" system, although the gap is smaller than that of Chinese. Our "Pretrain-Finetune" system also outperforms all other neural network based systems (systems marked with *). Although our system does not outperform all the state-of-the-art systems, the performance is comparable to most of them. So our model is also effective for English parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Cross Domain Evaluation</head><p>In this subsection, we examined the robustness of our model by evaluating it on data sets from var- ious domains. We use the Berkeley Parser as our baseline parser, and trained it on our training set.</p><p>For Chinese, we performed our experiments on the cross domain data sets from Chinese Treebank 8.0 ( . It consists of six domains: newswire (nw), magazine articles (mz), broadcast news (bn), broadcast conversation (bc), weblogs (wb) and discussion forums (df). Since all of the mz domain data is already included in our train- ing set, we only selected sample sentences from the other five domains as the test sets 5 , and made sure these test sets had no overlap with our tree- bank training, development and test sets. Note that we did not use any data from these five do- mains for training or development. The models are still the ones described in the previous sub- section. The results are presented in <ref type="table" target="#tab_10">Table 7</ref>. Al- though our "Supervised" model got slightly worse performance than the Berkeley Parser ( <ref type="bibr" target="#b19">Petrov and Klein, 2007</ref>), as shown in <ref type="table">Table 5</ref>, it outper- formed the Berkeley Parser on the cross-domain data sets. This suggests that the learned fea- tures can better adapt to cross-domain situations. Compared with the Berkeley Parser, on average our "Pretrain-Finetune" model is 3.4 percentage points better in terms of parsing accuracy, and 3.2 percentage points better in terms of POS tag- ging accuracy. We also presented the performance of our pre-trained model ("Only-Pretrain"). We found the "Only-Pretrain" model performs poorly on this cross-domain data sets. But even pre- training based on this less than competitive model, our "Pretrain-Finetune" model achieves signifi- cant improvement over the "Supervised" model. So the Pre-Train strategy is crucial to our model. For English, we performed our experiments on the cross-domain data sets from OntoNote 5.0 ( <ref type="bibr" target="#b29">Weischedel et al., 2013)</ref>, which consists of nw, mz, bn, bc, wb, df and telephone conversations (tc). We also performed experiments on the SMS domain, using data annotated by the LDC for the DARPA BOLT Program. We randomly se- lected 300 sentences for each domain as the test sets <ref type="bibr">5</ref> . <ref type="table" target="#tab_11">Table 8</ref> presents our experimental results. To save space, we only presented the results of our "Pretrain-Finetune" model and the Berkeley    <ref type="bibr" target="#b19">and Klein, 2007</ref>) when evaluated on the standard Penn TreeBank test set <ref type="table" target="#tab_8">(Table 6</ref>), our parser is 2.5 percentage points bet- ter on average on the cross domain data sets. So our parser is also very robust for English on cross- domain data sets.</p><formula xml:id="formula_4">Only-Pretrain Supervised Pretrain-Finetune BerkeleyParser domain F 1 POS F 1 POS F 1 POS F 1 POS bc</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There has been some work on feature optimization in dependency parsing, but most prior work in this area is limited to selecting an optimal subset of features from a set of candidate features <ref type="bibr" target="#b18">(Nilsson and Nugues, 2010;</ref><ref type="bibr" target="#b0">Ballesteros and Bohnet, 2014</ref>). <ref type="bibr" target="#b15">Lei et al. (2014)</ref> proposed to learn features for de- pendency parsing automatically. They first repre- sented all possible features with a multi-way ten- sor, and then transformed it into a low-rank tensor as the final features that are actually used by their system. However, to obtain competitive perfor- mance, they had to combine the learned features with traditional hand-crafted features. <ref type="bibr" target="#b4">Chen and Manning (2014)</ref> proposed to learn a dense fea- ture vector for transition-based dependency pars- ing via neural networks. Their model had to learn POS tag embeddings and dependency label em- beddings first, and then induced the dense feature vector based on these embeddings. Comparing with their method, our model is much simpler. Our model learned features directly based on the orig- inal form of primitive units. There have also been some attempts to use neural networks for constituent parsing. <ref type="bibr" target="#b9">Henderson (2003)</ref> presented the first neural network for broad coverage parsing. Later, he also proposed to rerank k-best parse trees with a neural net- work model which achieved state-of-the-art per- formance <ref type="bibr" target="#b10">(Henderson, 2004</ref>). Collobert (2011) designed a recurrent neural network model to con- struct parse tree by stacks of sequences labeling, but its final performance is significantly lower than the state-of-the-art performance. <ref type="bibr" target="#b24">Socher et al. (2013)</ref> built a recursive neural network for con- stituent parsing. However, rather than performing full inference, their model can only score parse candidates generated from another parser. Our model also requires a parser to generate training samples for pre-training. However, our system is different in that, during testing, our model per- forms full inference with no need of other parsers. <ref type="bibr" target="#b25">Vinyals et al. (2014)</ref> employed a Long Short-Term Memory (LSTM) neural network for parsing. By training on a much larger hand-annotated data set, their performance reached 91.6% for English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed to learn features via a neural network model. By taking as input the primitive units, our neural network model learns feature representations in the hidden layer and made parsing predictions based on the learned fea- tures in the output layer. By employing the back- propagation algorithm, our model simultaneously induced features and learned prediction model pa- rameters. We show that our model achieved signif- icant improvement from pretraining on a substan- tial amount of pre-parsed data. Evaluated on stan- dard data sets, our model outperformed all state- of-the-art parsers on Chinese and all neural net- work based models on English. We also show that our model is particularly effective on cross- domain tasks for both Chinese and English.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of constituent tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Neural network architecture for constituent parsing, where w i denotes word type unit, t i denotes POS tag unit, c i denotes constituent label unit, suf f ix i and pref ix i (1 ≤ i ≤ 4) denotes i-character word suffix or prefix for the first word in the queue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Influence of hyper-parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 presents</head><label>2</label><figDesc>the results. By comparing numbers in other rows</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) . Therefore, we fixed</head><label>.</label><figDesc></figDesc><table>Primitive Units 
F 1 POS 

All Units 
86.7 96.7 
w/o Prefix &amp; Suffix 85.7 95.4 
w/o POS 
86.0 96.7 
w/o NT 
86.2 96.6 
Only Word 
82.7 95.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Influence of primitive units.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 shows results of different experimen- tal configurations for Chinese. For the Mix-Train</head><label>3</label><figDesc></figDesc><table>Mix-Train 

Pre-Train 
# Auto Sent F 1 POS F 1 POS 

0 
87.8 97.0 
-
-
50K 
87.2 96.8 88.4 97.1 
100K 
88.7 96.9 89.5 97.1 
200K 
89.2 97.2 89.5 97.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Semi-supervised training for Chinese. 

Mix-Train 
Pre-Train 
# Auto Sent F 1 POS F 1 POS 

0 
89.7 96.6 
-
-
50K 
89.4 96.1 90.2 96.4 
100K 
89.5 96.0 90.4 96.5 
200K 
89.2 95.8 90.8 96.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Semi-supervised training for English.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparing with the state-of-the-art sys-
tems on English test set. * marks neural network 
based systems.  ‡ marks shift-reduce parsing sys-
tems. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Cross-domain performance for Chinese. The "Only-Pretrain" model cannot successfully parse 
some sentences in bn domain, so we didn't give the numbers. 

Pretrain-Finetune BerkeleyParser 
Domain F 1 
POS 
F 1 
POS 

bc 
77.7 
92.2 
76.0 
91.1 
bn 
88.1 
95.4 
88.2 
95.0 
df 
82.5 
93.3 
79.4 
92.4 
nw 
89.6 
95.3 
86.2 
94.6 
wb 
83.3 
93.1 
82.0 
91.2 
sms 
79.2 
85.8 
74.6 
85.3 
tc 
74.2 
88.0 
71.1 
87.6 

average 82.1 
91.9 
79.6 
91.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Cross-domain performance for English. 

Parser. Except for the slightly worse performance 
on the bn domain, our model outperformed the 
Berkeley Parser on all the other domains. While 
our model is only 0.6 percentage point better than 
the Berkeley Parser (Petrov </table></figure>

			<note place="foot" n="2"> https://code.google.com/p/word2vec/ 3 http://nlp.cs.nyu.edu/evalb/</note>

			<note place="foot" n="4"> Its performance is F1 =83.9 on Chinese and F1 =90.8% on English.</note>

			<note place="foot" n="5"> The selected sentences can be downloaded from http://www.cs.brandeis.edu/ xuen/publications.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for comments. Haitao Mi is supported by DARPA HR0011-12-C-0015 (BOLT) and Nianwen Xue is supported by DAPRA HR0011-11-C-0145 (BOLT). The views and findings in this paper are those of the authors and are not endorsed by the DARPA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automatic feature selection for agenda-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A maximum-entropyinspired parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference</title>
		<meeting>the 1st North American chapter of the Association for Computational Linguistics conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">HEAD-DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno>number EPFL- CONF-192374</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural network probability estimation for broad coverage parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics</title>
		<meeting>the tenth conference on European chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-training with products of latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Speech and language processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic discovery of feature sets for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A maximum entropy model for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A classifier-based parser with linear run-time complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology</title>
		<meeting>the Ninth International Workshop on Parsing Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint pos tagging and transition-based constituent parsing in chinese with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parse reranking based on higher-order lexical dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1251" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A fast, accurate deterministic parser for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Elbachouti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.upenn.edu/LDC2013T21" />
		<title level="m">Chinese treebank 8.0. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transition-based parsing of the chinese treebank using a global discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies</title>
		<meeting>the 11th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
