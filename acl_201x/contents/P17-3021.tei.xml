<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study on End-to-End Sentence Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">Junshean</forename><surname>Espinosa</surname></persName>
							<email>kurtjunshean.espinosa@postgrad.manchester.ac.uk kpespinosa@up.edu.ph</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">National Centre for Text Mining</orgName>
								<orgName type="institution" key="instit2">University of Manchester</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of the Philippines Cebu</orgName>
								<address>
									<country key="PH">Philippines</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study on End-to-End Sentence Modelling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
						<meeting>ACL 2017, Student Research Workshop <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="128" to="135"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-3021</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Accurately representing the meaning of a piece of text, otherwise known as sentence modelling, is an important component in many natural language inference tasks. We survey the spectrum of these methods, which lie along two dimensions: input representation granularity and composition model complexity. Using this framework, we reveal in our quantitative and qualitative experiments the limitations of the current state-of-the-art model in the context of sentence similarity tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurately representing the meaning of a piece of text remains an open problem. To illustrate why it is difficult, consider the pair of sentences A and B below in the context of a sentence similarity task. If we use a very na¨ıvena¨ıve model such as bag- of-words to represent a sentence and use dis- crete counting of common words between the two sentences to determine their similarity, the score would be very low although they are highly sim- ilar. How then do we represent the meaning of sentences?</p><p>Firstly, we must be able to represent them in ways that computers can understand. Based on the Principle of Compositionality (Frege, 1892), we define the meaning of a sentence as a func- tion of the meaning of its constituents (i.e., words, phrases, morphemes). Generally, there are two main approaches to representing constituents: lo- calist and distributed representations. With the localist representation 1 , we represent each con- stituent with a unique representation usually taken from its position in a vocabulary V. However, this kind of representation suffers from the curse of dimensionality and does not consider the syn- tactic relationship of a constituent with other constituents. These two shortcomings are ad- dressed by the distributed representation <ref type="bibr" target="#b10">(Hinton, 1984)</ref> which encodes a constituent based on its co-occurrence with other constituents appearing within its context, into a dense n-dimensional vec- tor where n ⌧ |V |. Estimating the distributed rep- resentation has been an active research topic in itself. <ref type="bibr" target="#b20">Baroni et al. (2014)</ref> conducted a system- atic comparative evaluation of context-counting and context-predicting models for generating dis- tributed representations and concluded that the lat- ter outperforms the former, but <ref type="bibr" target="#b18">Levy et al. (2015)</ref> later have shown that simple pointwise mutual information (PMI) methods also perform simi- larly if they are properly tuned. To date, the most popular architectures to efficiently estimate these distributed representations are word2vec ( <ref type="bibr" target="#b21">Mikolov et al., 2013a</ref>) and <ref type="bibr">GloVe (Pennington et al., 2014</ref>). Subsequent developments estimate distributed representations at other levels of gran- ularity (see Section 2.1).</p><p>While much research has been directed into constructing representations for constituents, there has been far less consensus regarding the represen- tation of larger semantic structures such as phrases and sentences <ref type="bibr">(Blacoe and Lapata, 2012)</ref>. A sim- ple approach is based on looking up the vector rep- resentation of the constituents (i.e., embeddings) and taking their sum or average which yields a sin- gle vector of the same dimension. This strategy is effective in simple tasks but loses word order information and syntactic relations in the process ( <ref type="bibr" target="#b23">Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b33">Turney et al., 2010</ref>). Most modern neural network models have a sen- tence encoder that learns the representation of sen- tences more efficiently while preserving word or-der and compositionality (see Section 2.1).</p><p>In this work, we present a generalised frame- work for sentence modelling based on a survey of state-of-the-art methods. Using the framework as a guide, we conducted preliminary experiments by implementing an end-to-end version of the state- of-the-art model in which we reveal its limitations after evaluation on sentence similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The best way to evaluate sentence models is to assess how they perform on actual natural lan- guage inference (NLI) tasks. In this work, we examine three related tasks which are central to natural language understanding: paraphrase detec- tion ( <ref type="bibr">Dolan et al., 2004;</ref><ref type="bibr" target="#b37">Xu et al., 2015)</ref>, seman- tic similarity measurement <ref type="bibr" target="#b20">(Marelli et al., 2014;</ref><ref type="bibr" target="#b37">Xu et al., 2015;</ref><ref type="bibr">Agirre et al., 2016a</ref>) and inter- pretable semantic similarity measurement <ref type="bibr">(Agirre et al., 2016b)</ref>. (We refer the reader to the respec- tive papers for the task description and dataset de- tails).</p><p>Among the four broad types of methods we have identified in the literature (see Appendix C.1), we focus in this paper on deep learning (DL) methods because they support end-to-end learn- ing, i.e., they use few hand-crafted features-or none at all, making them easier to adapt to new domains. More importantly, these methods have obtained comparable performance relative to other top-ranking methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Modelling Framework</head><p>As a contribution of this work, we survey the spec- trum of DL methods, which lie on two dimensions: input representation granularity and composition model complexity, which are both central to sen- tence modelling (see Appendix <ref type="figure">Figure C</ref>.2 for a graphical illustration).</p><p>The first dimension (see horizontal axis of Ap- pendix <ref type="figure">Figure C.</ref>2) is the granularity of input rep- resentation. This dimension characterises a trade- off between syntactic dependencies captured in the representation and data sparsity. On the one hand, character-based methods ( <ref type="bibr" target="#b34">Vosoughi et al., 2016;</ref><ref type="bibr">dos Santos and Zadrozny, 2014;</ref> are not faced with the data sparsity prob- lem; however, it is not straightforward to deter- mine whether composing sentences based on in- dividual character representations would represent the originally intended semantics. On the other hand, while sentence embeddings ( <ref type="bibr" target="#b15">Kiros et al., 2015</ref>), which are learned by predicting the previ- ous and next sentences given the current sentence, could intuitively represent the actual semantics, it suffers from data sparsity.</p><p>The second dimension (see vertical axis of Ap- pendix <ref type="figure">Figure C.</ref>2) is the spectrum of composi- tion models ranging from bag-of-items-driven 2 ar- chitectures to compositionality-driven ones to ac- count for the morphological, lexical, syntactic, and compositional aspects of a sentence. Some of the popular methods are based on Bag-of-Item models, which represent a sentence by perform- ing algebraic operations (e.g., addition or aver- aging) over the vector representations of individ- ual constituents <ref type="bibr">(Blacoe and Lapata, 2012)</ref>. How- ever, these models have received criticism as they use linear bag-of-words context and thus do not take into account syntax. Spatial neural net- works, e.g., Convolutional Neural Networks or ConvNets ( <ref type="bibr" target="#b16">LeCun et al., 1998)</ref>, have been shown to capture morphological variations in short sub- sequences (dos <ref type="bibr">Santos and Zadrozny, 2014;</ref><ref type="bibr">Chiu and Nichols, 2016)</ref>. However, this architecture still does not capture the overall syntactic informa- tion. Thus <ref type="bibr" target="#b31">Sutskever et al. (2014)</ref> proposed the use of sequence-based neural networks, e.g., Recur- rent Neural Networks, Long Short Term Memory models <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref>, be- cause they can capture long-range temporal depen- dencies. <ref type="bibr" target="#b32">Tai et al. (2015)</ref> introduced Tree-LSTM, a generalisation of LSTMs to tree-structured net- work topologies, e.g., Recursive Neural Networks <ref type="bibr" target="#b29">(Socher et al., 2011</ref>). However, this type of net- work requires input from an external resource (i.e., dependency/constituency parser).</p><p>More complex models involved stacked archi- tectures of the three basic forms above <ref type="bibr" target="#b40">Yin et al., 2015;</ref><ref type="bibr">Cheng and Kartsaklis, 2015;</ref><ref type="bibr" target="#b43">Zhang et al., 2015;</ref><ref type="bibr" target="#b6">He et al., 2015</ref>) which capture the syntactic and semantic structure of a language. However, in addition to being com- putationally intensive, most of these architectures model sentences as vectors with a fixed size, they risk losing information especially when input sen- tence vectors are of varying lengths. Recently, <ref type="bibr">Bahdanau et al. (2014)</ref> introduced the concept of attention, originally in the context of machine translation, where the network learns to align parts of the source sentence that match the constituents of the target sentence, without having to explicitly form these parts as hard segments. This enables phrase-alignments between sentences as described by <ref type="bibr" target="#b39">Yin and Schütze (2016)</ref> in the context of a tex- tual entailment recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary Experiments</head><p>In this section, we describe the preliminary experi- ments we conducted in order to gain deeper under- standing on the limitations of the state-of-the-art model.</p><p>Firstly, we define sentence similarity as a supervised learning task where each train- ing example consists of a pair of sentences</p><formula xml:id="formula_0">(x a 1 , ..., x a Ta ), (x b 1 , ..., x b T b</formula><p>) of fixed-sized vectors (where x a i , x b j 2 R d input denoting constituent vec- tors from each sentence, respectively, which may be of different lengths T a 6 = T b ) along with a sin- gle real-valued label y for the pair. We evaluated the performance of the state-of-the-art model on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>Since we focus on end-to-end sentence modelling, we implement a simplified (see <ref type="table">Table 1</ref>) version of MaLSTM ( <ref type="bibr" target="#b24">Mueller and Thyagarajan, 2016)</ref>, i.e., the state-of-the-art model on this task (see Ap- pendix Figure C.1). The model uses a siamese ar- chitecture of Long-Short Term Memory (LSTM) to read word vectors representing each input sen- tence. Each LSTM cell has four components: in- put gate i t , forget gate f t , memory state c t , and output gate o t ; which decides the information to retain or forget in a sequence of inputs. Equa- tions 1-6 are the updates performed at each LSTM cell for a sequence of input (x 1 , ..., x T ) at each timestep t 2 {1, ..., T }, parameterised by weight </p><formula xml:id="formula_1">matrices W i , W f , W c , W o , U i , U f , U c , U o and bias vectors b i , b f , b c , b o . i t = (W i x t + U i h t1 + b i ) (1) f t = (W f x t + U f h t1 + b f ) (2) ˜ c t = tanh(W c x t + U c h t1 + b c ) (3) c t = i t ˜ c t + f t c t1 (4) o t = (W o x t + U o h t1 + b o ) (5) h t = o t tanh(c t )<label>(6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Details</head><p>We use the 300-dimensional pre-trained word2vec 3 (Mikolov et al., 2013b) word em- beddings and compare the performance with that of GloVe 4 ( <ref type="bibr" target="#b26">Pennington et al., 2014</ref>) embeddings. Out-of-embedding-vocabulary (OOEV) words are replaced with an &lt;unk&gt; token. We retain the word cases and keep the digits. For character representation, we fine-tune the 50-dimensional initial embeddings, modifying them during gra- dient updates of the neural network model by back-propagating gradients. The chosen size of the embeddings was found to perform best after initial experiments with different sizes. Our model uses 50-dimensional hidden repre- sentations h t and memory cells c t . Optimisation of the parameters is done using the SGD-based Adam method ( <ref type="bibr" target="#b14">Kingma and Ba, 2014</ref>) and we perform gradient clipping to prevent exploding gradients. We tune the hyper-parameters on the validation set by random search since it is infeasible to do a random search across the full hyper-parameter space due to time constraints. After conducting initial experiments, we found the optimal training parameters to be the following: batch size = 30, learning rate = 0.01, learning rate decay = 0.98, dropout = 0.5, number of LSTM layers = 1, maxi- mum epochs = 10, patience = 5 epochs. Patience is the early stopping condition based on performance on validation sets. We used the Tensorflow 5 li- brary to implement and train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset and Evaluation</head><p>We measure the model's performance on three benchmark datasets, i.e., <ref type="bibr">SICK 2014</ref><ref type="bibr" target="#b20">(Marelli et al., 2014</ref>   Furthermore, using the framework described in Section 2.1, we chose to compare the model per- formance at two levels of input representation (i.e., character-level vs word-level) and composi- tion models (i.e., LSTM vs vector sum) in order to eliminate the need for external tools such as parsers. <ref type="table" target="#tab_2">Table 2</ref> shows the performance across input repre- sentations and composition models. As expected, our simplified model performs relatively worse (Pearson correlation = 0.7355) when compared to what was reported in the original MaLSTM pa- per (Pearson correlation = 0.8822) on the SICK dataset (using word2vec). This performance dif- ference (around 15%) could be attributed to the additional features (see <ref type="table">Table 1</ref>) that the state-of- the-art model added to their system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>With respect to input representation, the word- based one yields better performance in all datasets over character-level representation for the obvi- ous reason that it carries more semantic infor- mation. Furthermore, the character-level repre- sentation using LSTM performs better than us- ing Vector Sum (VS) because it is able to retain sequential information. Regarding word embed- dings, GloVe resulted in higher performance com- pared to word2vec in all datasets and models ex- cept with VS on the SICK dataset where word2vec is slightly better. <ref type="table" target="#tab_3">Table 3</ref> shows the percentage of OOEV words in each dataset with respect to its vocabulary size. Upon closer inspection, we found out that word2vec does not have embed- dings for stopwords (e.g., a, to, of, and). With re- spect to token-based statistics, these OOEVs com- prised 95% (SICK), 67% (PIT) and 44% (STS) re- spectively in each dataset. Although further work is needed to ascertain the effect of this type of OO- EVs, we hypothesise that GloVe's superior perfor- mance could be attributed to it, if not to its word vector quality as claimed by <ref type="bibr" target="#b26">Pennington et al. (2014)</ref>.   With respect to the composition model, LSTM performs better than VS but only in the SICK dataset while VS dominates in both the PIT and STS datasets. Specifically, <ref type="figure" target="#fig_1">Figure 1</ref> shows the overall and the per-category performance of the model on the STS dataset. Overall, we can clearly see that VS outperforms LSTM by a considerable margin and also in each category except in Post- editing and Headlines. On the one hand, this sug- gests that simple compositional models can per- form competitively on clean and noisy datasets (e.g., less OOEVs). On the other hand, this shows the ability of LSTM models to capture long term dependencies especially on clean datasets (e.g., SICK dataset) because they contain sufficient se- mantic information while their performance de- creases dramatically on noisy data or on datasets with high proportion of OOEVs (e.g., PIT and STS datasets).</p><p>The worst performance was obtained on the PIT dataset in both the baseline <ref type="bibr">6</ref> and composi- tion models. Aside from PIT dataset's compara- tively higher percentage of OOEV words (see <ref type="table" target="#tab_3">Ta- ble 3</ref>), its diverse, short and noisy user-generated text ( <ref type="bibr" target="#b30">Strauss et al., 2016</ref>)-typical of social media text-make it a very challenging dataset.</p><p>To better understand the reason behind the per- formance drop of the model, we extracted the 100 most difficult sentence pairs in each dataset by ranking all of the pairs in the test set according to the absolute difference between the gold standard and predicted similarity scores.</p><p>We observed that around 60% of the difficult sentence pairs share many similar words (except for a word or two) or contain OOEV words that led to a complete change in meaning. Meanwhile the <ref type="bibr">6</ref> We represent each sentence with term-frequency vectors. rest are sentence pairs which are topically similar but completely mean different.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we show examples from each dataset and their corresponding scores (i.e., Pearson cor- relation) from the gold standard and the compo- sition models. The two sentences come from an actual pair in the dataset.</p><p>Example 1 (from SICK dataset) shows a pair of sentences which, although can be interpreted to come from the same domain food preparation, are semantically different in their verb, subject, and direct object, for which, presumably, they were labelled in the gold standard as highly dissimi- lar. However, both of the word-based models pre- dicted them to be highly similar (in varying de- grees). This limitation can be attributed to the re- latedness of their words (e.g., person vs woman, cutting vs scrubbing). Under the distributional hy- pothesis assumption <ref type="bibr" target="#b4">(Harris, 1940;</ref><ref type="bibr" target="#b2">Firth, 1957)</ref>, two words will have high similarity if they oc- cur in similar contexts even if they neither have the same nor similar meanings. Since word em- beddings are typically generated based on this as- sumption, the relatedness aspect is captured more than genuine similarity. Furthermore, the higher similarity obtained by the LSTM model over Vec- tor Sum can be attributed to its ability to capture syntactic structure in sequences such as sentences.</p><p>Examples 2 and 3 (from STS dataset) show sen- tence pairs which were labelled as completely dis- similar but were predicted with high similarity in both models. This shows the inability of the mod- els to put more weight on semantically rich words which change the overall meaning of a sentence when compared with another.</p><p>Example 4 (from PIT dataset) shows a sentence pair which was labelled as completely dissimi-lar, presumably because it lacks sufficient con- text for meaningful interpretation. However, they were predicted to some degree as similar possi- bly because some words are common to both sen- tences and some are likely related by virtue of co- occurrence in the same context (e.g., England, Eu- rope). See Appendix B for more examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>This work is intended to serve as an initial study on end-to-end sentence modelling to identify the limitations associated with it. The models and representations compared, while typical of current sentence modelling methods, are not an exhaustive set and some variations exist. A natural extension to this study is to explore other input granularity representations and composition models presented in the framework. For example, in this study we did not go beyond word representations; however, multi-word expressions are common occurrences in the English language. This could be addressed by modelling sentence constituents using recur- sive tree structures <ref type="bibr" target="#b32">(Tai et al., 2015)</ref> or by learning phrase representations ( <ref type="bibr" target="#b35">Wieting et al., 2015)</ref>.</p><p>The limitations of the current word embeddings as revealed in this paper has been studied in the context of word similarity tasks ( <ref type="bibr" target="#b17">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b9">Hill et al., 2016</ref>) but to our knowl- edge had never been investigated explicitly in the context of sentence similarity tasks. For exam- ple, <ref type="bibr" target="#b13">Kiela et al. (2015)</ref> have shown that specialis- ing semantic spaces to downstream tasks and ap- plications requiring similarity or relatedness can improve performance. Furthermore, some studies <ref type="bibr" target="#b1">(Faruqui et al., 2014;</ref><ref type="bibr" target="#b41">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b25">Ono et al., 2015;</ref><ref type="bibr">Ettinger et al., 2016</ref>) have proposed to learn word embeddings by going beyond the dis- tributional hypothesis assumption either through a retrofitting or joint-learning process with some us- ing semantic resources such as ontologies and en- tity relation databases. Thus, we will explore this direction as this will be particularly important in semantic processing since entities encode much of the semantic information in a language.</p><p>Furthermore, the inability of the state-of-the- art model to encode semantically rich words (e.g., socket, bug in Example 2) with higher weights rel- ative to other words, supports the assertion of Bla- coe and Lapata (2012) that distributive semantic representation and composition must be mutually learned. <ref type="bibr" target="#b35">Wieting et al. (2015)</ref> have showed that this kind of weighting for semantic importance can be learned automatically when training on a para- phrase database. Recent models ( <ref type="bibr" target="#b5">Hashimoto et al., 2016)</ref> proposed end-to-end joint modelling at dif- ferent linguistic levels of a sentence (i.e. morphol- ogy, syntax, semantics) on a hierarchy of tasks (i.e., POS tagging, dependency parsing, seman- tic role labelling)-often done separately-with the assumption that higher-level tasks benefit from lower-level ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>A</head><label></label><figDesc>:The shares of the company dropped. B:The organisation's stocks slumped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pearson correlation in STS 2016 evaluation sets (Key: L=LSTM, V=Vector Sum, C=char, W=word2vec, G=GloVe)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Pearson Correlation. Performance comparison across input representations and composition 
models. Baseline method uses cosine similarity measure to predict similarity between sentences. 

Dataset 
Vocab Size 
% OOEV 

word2vec GloVe 

SICK 2014 
2,423 
1.4 
1.1 
PIT 2015 
15,156 
16.5 
9.6 
STS 2016 
18,061 
11.1 
7.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Percentage of Out-of-Embedding-
Vocabulary (OOEV) words 

PIT 2015 (Xu et al., 2015), using Pearson correla-
tion. We assert that a robust model should perform 
consistently well in these three datasets. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Examples of difficult sentence pairs. Compositional models use GloVe embeddings.</figDesc><table></table></figure>

			<note place="foot" n="1"> The best example of this sparse representation is the &quot;one-hot&quot; representation (see Appendix A for details)</note>

			<note place="foot" n="2"> We use items instead of words to generalise amongst various representations.</note>

			<note place="foot" n="3"> code.google.com/p/word2vec 4 https://nlp.stanford.edu/projects/glove/ 5 https://www.tensorflow.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ASOBEK: Twitter paraphrase identification with simple overlap features and SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Eyecioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4166</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rupert Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Papers in linguistics</title>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gottlob Frege. 1892. Ubersicht und bedeutung</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Philosophy and Philosophical Criticism</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="25" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Review of Louis H. Gray, Foundations of Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="231" />
			<date type="published" when="1939" />
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01587</idno>
		<ptr target="https://arxiv.org/abs/1611.01587" />
		<title level="m">A Joint ManyTask Model: Growing a Neural Network for Multiple NLP Tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Umd-ttic-uw at semeval-2016 task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<title level="m">Distributed representations</title>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative Improvements to Distributional Sentence Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="891" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Specializing word embeddings for similarity or relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2044" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fbk-hlt-nlp at semeval-2016 task 2: A multitask, deep learning approach for interpretable semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Magnolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Feltracco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval pages</title>
		<meeting>SemEval pages</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="783" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word embedding-based antonym detection using thesauri and distributional information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nactem at semeval-2016 task 1: Inferring sentence-level semantic similarity from an ensemble of complementary lexical and sentence-level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Przybyła</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Nhung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Shardlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Kontonatsios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval pages</title>
		<meeting>SemEval pages</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="614" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Samsung poland nlp team at semeval-2016 task 1: Necessity for diversity; combining recursive autoencoders, wordnet and ensemble methods to measure semantic similarity. Proceedings of SemEval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katarzyna</forename><surname>Pakulska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystyna</forename><surname>Chodorowska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="602" to="608" />
		</imprint>
	</monogr>
	<note>Wojciech Walczak, and Piotr Andruszkiewicz</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Results of the wnut16 named entity recognition shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bethany</forename><forename type="middle">E</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Noisy User-generated Text</title>
		<meeting>the 2nd Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Vijayaraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<idno type="doi">10.1145/2911451.2914762</idno>
		<ptr target="https://doi.org/10.1145/2911451.2914762" />
		<title level="m">Tweet2vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08198</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02789</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit). Proceedings of SemEval</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Discriminative Phrase Embedding for Paraphrase Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00503</idno>
		<ptr target="http://arxiv.org/abs/1604.00503" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Why and how to pay different attention to phrase alignments of different intensities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06896</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MITRE: Seven systems for semantic similarity in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zarrella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Merkhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Strickhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Tian Tian Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SemEval pages</title>
		<meeting>the SemEval pages</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="271" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02806</idno>
		<ptr target="https://arxiv.org/abs/1610.02806" />
		<title level="m">Modelling Sentence Pairs with Tree-structured Attentive Encoder</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
