<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entropy Converges Between Dialogue Participants: Explanations from an Information-Theoretic Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
							<email>yang.xu@psu.edu,</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Sciences and Technology</orgName>
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<postCode>16802</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
							<email>reitter@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Sciences and Technology</orgName>
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<postCode>16802</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entropy Converges Between Dialogue Participants: Explanations from an Information-Theoretic Perspective</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="537" to="546"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The applicability of entropy rate constancy to dialogue is examined on two spoken dialogue corpora. The principle is found to hold; however, new entropy change patterns within the topic episodes of dialogue are described, which are different from written text. Speaker&apos;s dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy, respectively, which results in local convergence between these speakers in each topic episode. This implies that the sentence entropy in dialogue is conditioned on different contexts determined by the speaker&apos;s roles. Explanations from the perspectives of grounding theory and interactive alignment are discussed, resulting in a novel, unified information-theoretic approach of dialogue.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information in written text and speech is strate- gically distributed. It has been claimed to be ordered such that the rate of information is not only close to the channel capacity, but also ap- proximately constant <ref type="bibr">Charniak, 2002, 2003;</ref><ref type="bibr">Jaeger, 2010)</ref>; these results were devel- oped within the framework of Information Theory <ref type="bibr">(Shannon, 1948)</ref>. In these studies, the per-word cross-entropy of a sentence is used to model the amount of information transmitted. Language is treated as a series of random variables of words.</p><p>Most existing work examined written text as opposed to speech. Spoken dialogue is different from written text in many ways. For example, di- alogue contains more irregular or ungrammatical components, such as incomplete utterances, dis- fluencies etc. ( <ref type="bibr">Jurafsky and Martin, 2014, ch 12)</ref>, which are "theoretically uninterested complexities that are unwanted" ( <ref type="bibr">Pickering and Garrod, 2004)</ref>. Dialogue is also different from written text in high level discourse structure. The paragraphs in writ- ten text, which function as relatively standalone topic units, are constructed under the guidance of one consistent author. On the other hand, the con- stitution and transformation of topics in dialogue are more dynamic processes, which are the result of the joint activity from multiple speakers <ref type="bibr">(Linell, 1998)</ref>. In nature, written text is a monologue, while dialogue is a joint activity <ref type="bibr" target="#b5">(Clark, 1996)</ref>.</p><p>From the application perspective, investigating entropy in dialogue can help us better understand which speaker contributes the most information, and thus may potentially benefit tasks such as con- versational roles identification <ref type="bibr">(Traum, 2003)</ref> etc. From the theoretical perspective, we believe that such investigation will reveal some unique fea- tures of the formation of higher level discourse structure in dialogue that are different from writ- ten text, e.g., topic episode shifts, because pre- vious studies have found the correlation between entropy decrease and potential topic shift in writ- ten text ( <ref type="bibr">Qian and Jaeger, 2011</ref>). Finally, entropy is closely related to predictability and processing demands, which has implications for cognitive as- pects of communication.</p><p>The main purpose of this study is to character- ize how lexical entropy changes in spoken lan- guage. We will focus on spontaneous dialogue of two speakers and carry out two steps of in- vestigation. First, we examine the overall en- tropy patterns within dialogue as a whole context that does not differentiate speakers. Second, we zoom in to topic episodes within dialogue and ex- plore how each of the two speakers' entropy de- velops. The goal of the second step is to account the complexity of topic shifts within spoken dia- logues and to reach a more detailed understanding of human communication from an information- theoretic perspective. If topic shifts in dialogue do correlate with changes in entropy, how do they affect the two speakers, only one of whom typ- ically initiates the topic shift, while another fol- lows along? To answer this question, we use the transcribed text data from two well-developed cor- pora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The principle of entropy rate constancy</head><p>The constancy rate principle governing language generation in human communication was first pro- posed by <ref type="bibr" target="#b9">Genzel and Charniak (2002)</ref>. Inspired by ideas from Information Theory <ref type="bibr">(Shannon, 1948)</ref>, this principle asserts that people communicate (written or spoken) in a way that keeps the rate of information being transmitted approximately con- stant. <ref type="bibr" target="#b9">Genzel and Charniak (2002)</ref> provide evidence to support this principle by formulating the prob- lem into Equation 1. They treat text as a sequence of random variables X i , and X i corresponds to the i th word in the corpus. They focus on the en- tropy of a word conditioned on its context, i.e., X i |X 1 = w 1 , . . . , X i−1 = w i−1 , and decom- pose the context into two parts: the global con- text C i that refers to all the words from preced- ing sentences, and the local context L i that refers to all the preceding words within the same sen- tence as X i . Thus, the conditioned entropy of X i is also decomposed into two terms (see the right side of Equation 1): the local measure of entropy (first term), and the mutual information between the word and global context (second term).</p><formula xml:id="formula_0">H(X i |C i , L i ) = H(X i |L i ) − I(X i , C i |L i ) (1)</formula><p>The constancy rate principle predicts that the left side of Equation 1 should be constant as i increases. Because H(X i |C i , L i ) itself is diffi- cult to estimate (because it is hard to define C i mathematically), and that the mutual information turn I(X i , C i |L i ) is known to increase with i, the whole problem becomes examining whether the local measure of entropy H(X i |L i ) also increases with i. <ref type="bibr" target="#b9">Genzel and Charniak (2002)</ref> have con- firmed this prediction by showing that H(X i |L i ) does increase with i within multiple genres of written text of different languages.</p><p>The constancy rate principle also leads to an in- teresting prediction about the relationship between entropy change and topic shift in text. Generally, a sentence that initiate a shift in topic will have lower mutual information between its context, be- cause the previous context provides little informa- tion to the new topic. Thus, a topic shift corre- sponds to the drop of the mutual information term I(X i , C i |L i ). Then in order to keep constancy of the left term as predicted by the principle, the en- tropy term needs to decrease when a topic shift happens. <ref type="bibr" target="#b10">Genzel and Charniak (2003)</ref> verified this prediction by showing that paragraph-starting sentences have lower entropy than non-paragraph- starting ones, with the assumption that a new para- graph often indicates a topic shift in written text. More recently, latent topic modeling (Qian and Jaeger, 2011) showed that lower sentence entropy was associated with topic shifts.</p><p>Genzel and Charniak's work has been extended to integrate non-linguistic information into the principle. <ref type="bibr" target="#b7">Doyle and Frank (2015)</ref> leveraged Twit- ter data to find further support to the constancy rate principle: the entropy of message gradually increases as the context builds up, and it sharply goes down when there is a sudden change in the non-linguistic context (Baseball world series news reports, <ref type="bibr" target="#b7">Doyle and Frank, 2015)</ref>. Uniform Infor- mation Density (UID) (Jaeger and <ref type="bibr">Levy, 2006</ref>) ex- tends the principle in a framework that governs how people manage the amount of information in language production, from lexical levels to all levels of linguistic representations, e.g., syntactic levels. Its core idea is that people avoid salient changes in the density of information (i.e., amount of information per amount of linguistic signal) by making specific linguistic choices under certain contexts (Jaeger, 2010).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic shift in dialogues</head><p>As a conversation unfolds, topic changes natu- rally happen when a current topic is exhausted or a new one occurs, which is referred to as topic shift in the field of Conversation Analysis (CA) <ref type="bibr">(Ng and Bradac, 1993;</ref><ref type="bibr">Linell, 1998)</ref>. In CA, the basic unit of topical structure analysis in dialogue is episode, which refers to a sequence of speech events that are "about" something specific in the world <ref type="bibr">(Linell, 1998, ch 10, p 187)</ref>. Here, to be precise, we use the term topic episode.</p><p>According to related theories in CA, the for- mation of topic episode is a joint accomplishment from two speakers and a product of initiatives and responses <ref type="bibr">(Linell, 1990)</ref>. When establishing a new topic jointly, one speaker first produces an ini- tiatory contribution that introduce a "candidate" topic, and the other speaker makes a response that shares his perspective on that <ref type="bibr">(Linell, 1998)</ref>. From the information theoretic point of view, the initia- tor of a new topic plays a role of introducing nov- elty or surprisal into the context, while the other speaker, the responder, is more of a commenter or evaluator of information, who does not contribute as much in terms of novelty.</p><p>Since previous studies have shown that the de- crease of sentence entropy is correlated with topic shifts in written text <ref type="bibr" target="#b10">(Genzel and Charniak, 2003;</ref><ref type="bibr">Qian and Jaeger, 2011)</ref>, it is reasonable to expect the same effect to be present at the boundaries of topic episodes in dialogue. Furthermore, consid- ering the initiator vs. responder discrepancy in speaker roles, we expect their entropy change pat- terns also to be different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overall Trend of Entropy in Dialogue</head><p>In this section we examine whether the overall en- tropy increase trend is present in dialogue text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus data</head><p>The Switchboard corpus <ref type="bibr" target="#b11">(Godfrey et al., 1992</ref>) and the British National Corpus (BNC) <ref type="bibr" target="#b3">(BNC, 2007)</ref> are used in this study. Switchboard contains 1126 dialogues by telephone between two native North- American English speakers in each dialogue. We use only a subset of BNC (spoken part) that con- tain spoken conversations with exactly two partici- pants, so that the dialogue structures are consistent with Switchboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computing Entropy of One Sentence</head><p>We use language model to estimate the sentence entropy, which is similar to <ref type="bibr" target="#b10">Genzel and Charniak (2003)</ref>'s method. A sentence is considered as a sequence of words, W = {w 1 , w 2 , . . . , w n }, and its per-word entropy is estimated by:</p><formula xml:id="formula_1">H(w 1 . . . w n ) = − 1 n w i ∈W log P (w i |w 1 . . . w i−1 )</formula><p>where P (w i |w 1 . . . w i−1 ) is estimated using a trigram language model. The model is trained using Katz backoff <ref type="bibr">(Katz, 1987)</ref> and Lidstone smoothing <ref type="bibr" target="#b4">(Chen and Goodman, 1996)</ref>.</p><p>For the two corpora respectively, we extract the first 100 sentences from each conversation, and apply a 10-fold cross-validation, i.e., dividing all the data into 10 folds. Then we choose each fold as the testing set, and compute the entropy of each sentence in it, using the language model trained against the rest of the folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Eliminating sentence length effects</head><p>Intuitively, longer sentences tend to convey more information than short ones. Thus, the per-word entropy of a sentence should be correlated with the sentence length, i.e., the number of words. This correlation is confirmed in our data by calculat- ing the Pearson correlation between the per-word entropy and sentence length: For Switchboard, r = 0.258, p &lt; 0.001; for BNC, r = 0.088, p &lt; 0.001.</p><p>Sentence length is found to vary with its rela- tive position in text <ref type="bibr">(Keller, 2004</ref>). Thus, in order to truly examine the variation pattern of sentence entropy within dialogue, we need to eliminate the effect of sentence length from it. We calculate a normalized entropy that is independent of sen- tence length in the following way. (This method is used by <ref type="bibr" target="#b10">Genzel and Charniak (2003)</ref> to get the length-independent tree depth and branching fac- tor of sentence.) First, we compute ¯ e(n), the av- erage per-word entropy of sentences of the same length n, for all lengths (n = 1, 2, . . . ) that have occurred</p><formula xml:id="formula_2">¯ e(n) = 1 /|L(n)| s∈L(n) e(s)</formula><p>where e : S → R is the original per-word en- tropy of a sentence s, and L(n) = s|l(s) = n is the set of sentences of length n. Then we compute the sentence-length adjusted entropy measure that we want by</p><formula xml:id="formula_3">e (s) = e(s) ¯ e(n)</formula><p>This normalized entropy measure sums up to 1, and is not sensitive to sentence length. In later part of this paper, we demonstrate our results in both entropy and normalized entropy because the for- mer is the direct measure of information content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>We plot the per-word entropy and normalized en- tropy of sentence against its global position, which is the sentence position from the beginning of the dialogue <ref type="figure" target="#fig_1">(Figure 1</ref>). It can be seen that both mea- sures increase with global position. BNC shows larger slope than Switchboard, and the latter has a flatter curve but sharper increase at the early stage of conversations.</p><p>To test the reliability of the observed increas- ing trend, we fit linear mixed-effect models us- ing entropy and normalized entropy as response variables, and the global position of sentence as predictor (fixed effect), with a random intercept grouped by distinct dialogues. The lme4 pack- age in R is used ( <ref type="bibr" target="#b0">Bates et al., 2014</ref>). The re- sults show that the fixed effects of global position are significant for both measures in both corpora: Entropy in Switchboard, β = 4.2 × 10 −3 , p &lt; 0.001; normalized entropy in Switchboard, β = 5.9 × 10 −4 , p &lt; 0.001; entropy in BNC, β = 1.5 × 10 −2 , p &lt; 0.001; normalized entropy in BNC, β = 1.4 × 10 −3 , p &lt; 0.001).</p><p>In particular, since the curves of Switchboard seem flat after a boost in the early phase (be- tween 0 to 5 in global position), we fit extra models to examine whether the entropy increase for global positions larger than 10 is significant. The long-term changes are reliable, too: Entropy, β = 3.4 × 10 −3 , p &lt; 0.001; normalized entropy, β = 5.1 × 10 −4 , p &lt; 0.001.</p><p>In sum, we find increasing entropy over the course of the whole dialogue. These findings are consistent with previous findings on written text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topic Shift and Speaker Roles</head><p>Since the topic structure of dialogue differs from written text, it is our interest to investigate how this difference affects the sentence entropy patterns. First, we identify the boundaries of topic episodes, and examine the presence of entropy drop effect at the boundaries. Second, we differentiate the speakers' roles in initiating the topic episode, i.e., initiator vs. responder, and compare their entropy change patterns within the episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Topic segmentation</head><p>There are multiple computational frameworks for topic segmentation, such as the Bayesian model <ref type="bibr" target="#b8">(Eisenstein and Barzilay, 2008</ref>), Hidden Markov model <ref type="bibr" target="#b1">(Blei and Moreno, 2001</ref>), latent topic model ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> etc. Considering that perfor- mance is not the prior requirement in our task, and also to avoid being confounded by segmentation method that utilize entropy measure per se, we use a less sophisticated cohesion-based TextTiling al- gorithm <ref type="bibr" target="#b12">(Hearst, 1997)</ref> to carry out topic segmen- tation.</p><p>TextTiling algorithm inserts boundaries into di- alogue as a sequence of sentences. We treat the segments between those boundaries as topic episodes. For each episode within a dialogue, we assign it a unique episode index, indicating its rela- tive position in the dialogue (e.g., from 1 to N for a dialogue that contains N episodes). For each sen- tence, we assign it a within-episode position, indi- cating its relative position within the topic episode.</p><p>In <ref type="figure" target="#fig_4">Figure 2</ref> we plot the entropy (and normal- ized) of sentence against the within-episode posi- tions, grouped by episode index. Due to the space limit, we only present the first 6 topic episodes and the first 10 sentences in each episode. It can be seen that entropy drops at the beginning of topic episode, and then increases within the episode.</p><p>To examine the reliability of the entropy in- crease within topic episodes, we fit linear mixed effect models using entropy (and normalized) as response variables, and the within-episode po- sition of sentence as predictor (fixed effect), with a random intercept grouped by the unique episode index of each topic episode. We find a significant fixed effect of within-episode po- sition on both measures for both corpora: En- tropy in Switchboard, β = 5.9 × 10 −4 , p &lt; 0.001; normalized entropy in Switchboard, β = 4.5 × 10 −3 , p &lt; 0.001; entropy in BNC, β = 2.5 × 10 −2 , p &lt; 0.001; normalized entropy in BNC, β = 3.0 × 10 −3 , p &lt; 0.001.</p><p>Our results show that when we treat the sen- tences in dialogue indiscriminately, their entropy change patterns at topic boundaries are consistent with previous findings on written text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Identifying topic initiating utterances</head><p>Having dialogue segmented into topic episodes, our next step is to identify each speaker's role in initiating the topic. According to the theories 540  reviewed in Section 2.2, the key to identify the speaker roles is to identify who produces the initia- tory "candidate" topic. To be convenient, we use the term topic initiating utterance (TIU) to refer to the very first utterance produced by the initiator to bring up the new topic. Here, we give an empirical operational definition of TIU.</p><p>Since we treat dialogue as a series of sentences, and apply the TextTiling algorithm to insert topic boundaries indiscriminately (without differentiat- ing whether adjacent sentences are from the same speaker or not), it results in two types of topic boundaries: Within-turn boundaries, the ones lo- cated in the middle of a turn (i.e., from one speaker). Between-turn boundaries, the ones lo- cated at the gap between two different turns (i.e., from two speakers). Our survey shows that in Switchboard 27.2% of the topic boundaries are within turns, and 72.8% are between turns. For BNC the two proportions are 41.2% and 58.8% respectively.</p><p>Intuitively, a within-turn topic boundary sug- gests that the speaker of the current turn is initiat- ing the topic shift. On the other hand, a between- turn boundary suggests that the following speaker who first gives substantial contribution is more likely to be the initiator of the next topic. Follow- ing this intuition, for within-turn boundaries, we define TIU as the rest part of current turn after the boundary. For between-turn boundaries, we define TIU as the whole body of the next relatively long turn after the boundary, whose length is larger than N words. Note that the determination of threshold N is totally empirical, because our goal is to iden- tify the most probable TIU, based on the intuition that longer sentences tend to contain more infor- mation, and thus are more likely to initiate a new topic. For the results shown later in this paper, we use N = 5, and our experiments draw similar re- sults for N ≥ 5. The operational definition of TIU is demonstrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The effect of topic initiator vs. responder</head><p>Based on the operational definition of topic initiat- ing utterance (TIU), we distinguish the two speak- ers' roles in each topic segment: the author of TIU is the initiator of the current topic, while the other speaker is the responder.</p><p>Again, we plot the sentence entropy (and nor- malized) against the within-episode position re- spectively, this time, grouped by speaker roles (initiator vs. responder) in <ref type="figure" target="#fig_5">Figure 4</ref>. It can be seen that at the beginning of a topic, initiators have significantly higher entropy than responders. As the topic develops, the initiators' entropy de- creases <ref type="figure" target="#fig_5">(Figure 4a</ref>) or stays relatively steady <ref type="figure" target="#fig_5">(Fig- ure 4b)</ref>, and the responder's entropy increases. To- gether they form a convergence trend within topic episode.</p><p>We use standard linear mixed models to exam- ine the convergence trend observed, i.e., to test  <ref type="formula">4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8</ref>   <ref type="formula">1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8</ref>  whether the initiators' entropy reliably decreases and whether the responders' entropy reliably in- creases. Models are fitted for initiators and re- sponders respectively, using the entropy (and nor- malized) as response variables, and the within- episode position as predictor (fixed effect), with a random intercept grouped by the unique episode index. Our models show that for the entropy measure, the fixed effect of within-episode po- sition is reliably negative for initiators (Switch- board, β = −3.6 × 10 −2 , p &lt; 0.001; BNC, β = −2.9 × 10 −2 , p &lt; 0.05) and reliably positive for responders (Switchboard, β = 3.3 × 10 −1 , p &lt; 0.001; BNC, β = 1.4 × 10 −1 , p &lt; 0.001). For the normalized entropy measure, the fixed effect of within-episode position is insignificant for ini- tiators, which means there is neither increase nor decrease, and is reliably positive for responders (Switchboard, β = 1.4 × 10 −2 , p &lt; 0.001; BNC, β = 1.2 × 10 −2 , p &lt; 0.001). Thus, the conver- gence trend is confirmed.</p><p>The entropy change patterns of topic initiators (decrease or remain constant within topic episode) are inconsistent with previous findings that assert an entropy increase in written text (Genzel and Charniak, 2002, 2003), which will be discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Summary</head><p>Our main contribution is that we find new en- tropy change patterns in dialogues that are differ- ent from those in written text. Specifically, when distinguishing the speakers' roles by topic initia- tor vs. responder, we see that the initiator's en- tropy decreases (or remain steady) whilst the re- sponder's increases within a topic episode, and to- gether they form a convergence pattern. The par- tial trend of entropy decrease in topic initiators seems to be contrary to the principle of entropy rate constancy, but as we will discuss next, it is actually an effect of the unique topic shift mech- anism of dialogues that is different from written text, which does not violate the principle. From an information theoretic perspective, we view dialogue as a process of information ex- change, in which the interlocutors play the roles of information provider and receiver, interactively within each topic episode.</p><p>Beyond differences in speaker roles, we do observe that sentence entropy increases with its global position in the dialogue, which is consis- tent with written text data ( <ref type="bibr">Charniak, 2002, 2003;</ref><ref type="bibr">Qian and Jaeger, 2011;</ref><ref type="bibr">Keller, 2004</ref>).</p><p>Thus, overall speaking, spoken dialogue do follow the general principle of entropy rate constancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dialogue as a process of information exchange</head><p>By combining topic segmentation techniques and fine-grained discourse analysis, we provide a new angle to view the big picture of human communi- cation: the perspective of how information is dis- tributed between different speakers. One critical difference between written text and spoken text in conversation is that there is only one direct input source of information in the former, i.e., the author of the text, but for the latter, there are multiple direct input sources, i.e., the multiple speakers. That means, when language production is treated as a process of choosing proper words (or other representations) within a context, the def- inition of "context" is different between the two categories of text. In written language (see Equa- tion 1 in Section 2), C i , the global context of a word X i , is assumed to be all the words in pre- ceding sentences. This is a reasonable assump- tion, because when one author is writing a com- plete piece of text, he may organize information smoothly to keep the entropy rate constant. Within a dialogue, for any upcoming utterance, all pre- ceding utterances together can be viewed as the shared context for the two speakers. To help us un-derstand the nature of this shared context, we pro- pose the following mental experiment. Suppose we, as researchers and "super-readers", observe the transcript of a dialogue between interlocutors A and B. To us, all utterances are based upon the context of previous ones, which is why we can ob- serve consistent entropy increase within the whole dialogue <ref type="figure" target="#fig_1">(Figure 1 in Section 3)</ref>. Also, to us, a new topic episode in dialogue is just like a new para- graph in written text, within which we can observe steady entropy increase without differentiating the utterances from the two speakers. By contrast, let's look at the context used by the two speak- ers. They will not necessarily leverage the preced- ing utterances as a coherent context. A topic ini- tiator introduces new information from a context outside of the dialogue. Therefore the mutual in- formation between the initiator's current sentence and the previous context is reduced, which causes the sentence entropy to start high before decreas- ing. On the other side, a topic responder relies much on the previous shared context (because he is not an active topic influencer). The responder is dynamically updating the context as the initiator pours new information into the mix. This causes the mutual information with the previous context to be high, and thus the sentence entropy start low before increasing again.</p><p>We think that the respective cognitive load in the topic responder imposed by following the other speaker in a new topic direction may be complemented by reduced information at the lan- guage level. This is, again, compatible with a cog- nitive communication framework that imposes a tendency to limit or keep constant overall infor- mation levels. It is also an example of extralin- guistic information that causes complementary en- tropy changes in a speaker's language (cf., <ref type="bibr" target="#b7">Doyle and Frank, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dialogue as a process of building up common ground</head><p>Our findings can also be explained by a theory of grounding ( <ref type="bibr" target="#b6">Clark and Brennan, 1991;</ref><ref type="bibr" target="#b5">Clark, 1996)</ref> of communication. Dialogue can be seen as a joint activity during which multiple speakers contribute alternatively to build common ground <ref type="bibr" target="#b6">(Clark and Brennan, 1991)</ref>. Common ground can be under- stood as the mutual knowledge shared between in- terlocutors. <ref type="bibr" target="#b5">Clark (1996)</ref> proposes that joint activities have a number of characteristics: First, participants play different roles in the activity. Second, a major ac- tivity is usually comprised of sequences of sub- activities, and the participants' role may differ from sub-activity to next. Third, to achieve the goal of the activity, it requires coordination be- tween participants of different roles.</p><p>In our design, the local roles of topic initiator vs. topic responder correspond to roles suggested by the joint-activity theory. The initiator sets up the dominant goal of the sub-activity, i.e., devel- oping a new topic episode, and the responder joins him or her in order to achieve the goal. The con- verging sentence entropy indicates that the mutual knowledge between them is accumulating, i.e., the common ground is being gradually built up. Once the goal is achieved, i.e., the current topic is fully developed, a new goal will emerge, and a new common ground needs to be built again, which is sometimes accompanied by a change in partici- pants roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Convergence of linguistic behaviors</head><p>One mechanism that may lead to the conver- gence of sentence entropy may be the interactive alignment of linguistic features between speak- ers ( <ref type="bibr">Pickering and Garrod, 2004)</ref>; repeating words and syntactic structure leads to increased simi- larity. The entropy-converging pattern also re- flects the convergence of higher-level dialogical behavior, say, speakership occupancy; the dis- crepancy between the two speakers' roles gradu- ally becomes smaller, i.e., the "speaker" becomes more of a "listener", and vice versa. A psycholo- gist might treat the fragmented topic episodes in dialogues as the locus where interlocutors build temporarily shared understanding <ref type="bibr">(Linell, 1998)</ref>, through the process of "synchronization of two streams of consciousness" <ref type="bibr">(Schutz, 1967)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, we validate the principle of entropy rate constancy in spoken dialogue, using two com- mon corpora. Besides the results that are consis- tent with previous findings on written text, we find new entropy change patterns unique to dialogue. Speakers that actively initiate a new topic tend to use language with higher entropy compared to the language of those who passively respond to the topic shift. These two speaker's respective entropy levels converge as the topic develops. A model of this phenomenon may provide explanations from the perspectives of information exchange, com- mon ground building, and the convergence of lin- guistic behaviors in general.</p><p>With this, we put forward what we think is a new perspective to analyzing dialogue. As much dialogue happens for the purpose of information exchange, loosely defined, it makes sense to apply information-theoretic models to the semantics as well as the form of speaker's messages. The quan- titative approach taken here augments rather than supplants speech acts <ref type="bibr">(Searle, 1976)</ref>, identifying who leads the dialogic process by introducing top- ics and shifting them.</p><p>Furthermore, our approach actually provides a unified perspective of dialogue that combines Grounding theory <ref type="bibr" target="#b6">(Clark and Brennan, 1991)</ref> and Interactive Alignment ( <ref type="bibr">Pickering and Garrod, 2004</ref>). These two models are often described as opposite; by applying each theory to the dialogic structure between and within topic episodes, we find both of them can explain our findings. The entropy measure of information content quantifies interlocutors' contributions to common ground and also allows us to show convergence patterns.</p><p>This unified information-theoretic perspective may eventually allow us to identify further sys- tematic patterns of information exchange between dialogue participants. There is, of course, no rea- son to think that multi-party dialogue should work differently; we leave the empirical examination as an open task.</p><p>T Florian Jaeger. 2010. Redundancy and reduc- tion: Speakers manage syntactic information density. Cognitive Psychology 61(1):23-62.</p><p>T Florian Jaeger and Roger P <ref type="bibr">Levy. 2006</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Entropy (a) and normalized entropy (b) against global position of sentences (from 1 to 100). Shadow area indicates 95% bootstrapped Confidence Interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Entropy (a) and normalized entropy (b) against within-episode position grouped by episode index. The x-axis in each block indicates the within-episode position of sentence. The number 1 to 6 on top of the blocks are episode indexes. Shadow area indicates 95% bootstrapped Confidence Interval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Entropy (a) and normalized entropy (b) against within-episode position grouped by speaker roles (topic initiator vs. topic responder)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Basic statistics of corpora</head><label>1</label><figDesc></figDesc><table>Statistics 
Switchboard BNC 

# of dialogues. 
1126 
1346 
Avg # of turns in dialogue 
109 
52 
Avg # of sentences in dialogue 
141 
70 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been funded by the National Sci-ence Foundation under CRII IIS grant 1459300.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fitting linear mixedeffects models using lme4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bolker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5823</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Topic segmentation with an aspect hidden markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">J</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="343" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The British National Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bnc</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>version 3 (BNC XML Edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 34th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Using language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grounding in communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on socially shared cognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="127" to="149" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shared common ground influences information density in microblog texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian unsupervised topic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entropy rate constancy in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Genzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variation of entropy and parse trees of sentences as a function of the sentence number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Genzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
	<note>ICASSP-92., 1992 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Texttiling: Segmenting text into multi-paragraph subtopic passages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="64" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
