<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
						</author>
						<title level="a" type="main">Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1789" to="1798"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1164</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms. In specific, we systematically investigate the proposed model under the supervision of different attention strategies. Experimental results show that our approach advances state-of-the-arts and achieves the best F 1 score on ACE 2005 dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the ACE (Automatic Context Extraction) event extraction program, an event is repre- sented as a structure comprising an event trigger and a set of arguments. This work tackles event detection (ED) task, which is a crucial part of event extraction (EE) and fo- cuses on identifying event triggers and cate- gorizing them. For instance, in the sentence "He died in the hospital", an ED system is expected to detect a Die event along with the trigger word "died ". Besides, the task of EE also includes event argument extraction (AE), which involves event argument identi- fication and role classification. In the above sentence, the arguments of the event include "He"(Role = P erson) and "hospital"(Role = P lace). However, this paper does not focus on AE and only tackles the former task.</p><p>According to the above definitions, event ar- guments seem to be not essentially necessary to ED. However, we argue that they are capa- ble of providing significant clues for identifying and categorizing events. They are especially useful for ambiguous trigger words. For exam- ple, consider a sentence in ACE 2005 dataset:</p><p>Mohamad fired Anwar, his for- mer protege, in 1998.</p><p>In this sentence, "fired " is the trigger word and the other bold words are event arguments. The correct type of the event triggered by "fired " in this case is End-Position. How- ever, it might be easily misidentified as At- tack because "fired " is a multivocal word. In this case, if we consider the phrase "for- mer protege", which serves as an argumen- t (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event.</p><p>Unfortunately, most existing methods per- formed event detection individually, where the annotated arguments in training set are totally ignored <ref type="bibr" target="#b9">(Ji and Grishman, 2008;</ref><ref type="bibr" target="#b5">Gupta and Ji, 2009;</ref><ref type="bibr" target="#b8">Hong et al., 2011;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2015;</ref><ref type="bibr">Liu et al., 2016a,b;</ref>. Although some joint learning based methods have been pro- posed, which tackled event detection and argu- ment extraction simultaneously ( <ref type="bibr" target="#b24">Riedel et al., 2009;</ref><ref type="bibr" target="#b13">Li et al., 2013;</ref><ref type="bibr" target="#b25">Venugopal et al., 2014;</ref>), these approaches usu- ally only make remarkable improvements to AE, but insignificant to ED. <ref type="table">Table 1</ref> illustrates our observations. <ref type="bibr" target="#b13">Li et al. (2013)</ref> and  are state-of-the-art joint mod- els in symbolic and embedding methods for event extraction, respectively. Compared with state-of-the-art pipeline systems, both join-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ED AE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbolic</head><p>Hong's pipeline <ref type="formula" target="#formula_4">(2011)</ref>   <ref type="table">Table 1</ref>: Performances of pipeline and joint approaches on ACE 2005 dataset. The pipeline method in each group was the state-of-the-art system when the corresponding joint method was proposed.</p><p>t methods achieved remarkable improvements on AE (over 1.9 points), whereas achieved in- significant improvements on ED (less than 0.2 points). The symbolic joint method even per- formed worse (67.5 vs. 68.3) than pipeline sys- tem on ED.</p><p>We believe that this phenomenon may be caused by the following two reasons. On the one hand, since joint methods simultaneous- ly solve ED and AE, methods following this paradigm usually combine the loss functions of these two tasks and are jointly trained un- der the supervision of annotated triggers and arguments. However, training corpus contains much more annotated arguments than trigger- s (about 9800 arguments and 5300 triggers in ACE 2005 dataset) because each trigger may be along with multiple event arguments. Thus, the unbalanced data may cause joint models to favor AE task. On the other hand, in im- plementation, joint models usually pre-predict several potential triggers and arguments first and then make global inference to select cor- rect items. When pre-predicting potential trig- gers, almost all existing approaches do not leverage any argument information. In this way, ED does hardly benefit from the anno- tated arguments. By contrast, the component for pre-prediction of arguments always exploit- s the extracted trigger information. Thus, we argue that annotated arguments are actually used for AE, not for ED in existing joint meth- ods, which is also the reason we call it an in- direct way to use arguments for ED.</p><p>Contrast to joint methods, this paper pro- poses to exploit argument information explic- itly for ED. We have analyzed that arguments are capable of providing significant clues to ED, which gives us an enlightenment that ar- guments should be focused on when perform- ing this task. Therefore, we propose a neural network based approach to detect events in texts. And in the proposed approach, we adop- t a supervised attention mechanism to achieve this goal, where argument words are expect- ed to acquire more attention than other word- s. The attention value of each word in a giv- en sentence is calculated by an operation be- tween the current word and the target trigger candidate. Specifically, in training procedure, we first construct gold attentions for each trig- ger candidate based on annotated arguments. Then, treating gold attentions as the super- vision to train the attention mechanism, we learn attention and event detector jointly both in supervised manner. In testing procedure, we use the ED model with learned attention mechanisms to detect events.</p><p>In the experiment section, we systemati- cally conduct comparisons on a widely used benchmark dataset ACE2005 1 . In order to fur- ther demonstrate the effectiveness of our ap- proach, we also use events from FrameNet (FN) (F. <ref type="bibr" target="#b4">Baker et al., 1998)</ref> as extra training data, as the same as <ref type="bibr" target="#b15">Liu et al. (2016a)</ref> to al- leviate the data-sparseness problem for ED to augment the performance of the proposed ap- proach. The experimental results demonstrate that the proposed approach is effective for ED task, and it outperforms state-of-the-art ap- proaches with remarkable gains.</p><p>To sum up, our main contributions are: (1) we analyze the problem of joint models on the task of ED, and propose to use the annotated argument information explicitly for this task. (2) to achieve this goal, we introduce a su- pervised attention based ED model. Further- more, we systematically investigate different attention strategies for the proposed model. (3) we improve the performance of ED and achieve the best performance on the widely used benchmark dataset ACE 2005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The ED task is a subtask of ACE event eval- uations where an event is defined as a specif- ic occurrence involving one or more partici- pants. Event extraction task requires certain specified types of events, which are mentioned in the source language data, be detected. We firstly introduce some ACE terminologies to facilitate the understanding of this task:</p><p>Entity: an object or a set of objects in one of the semantic categories of interests.</p><p>Entity mention: a reference to an entity (typically, a noun phrase).</p><p>Event trigger: the main word that most clearly expresses an event occurrence.</p><p>Event arguments: the mentions that are involved in an event (participants).</p><p>Event mention: a phrase or sentence with- in which an event is described, including the trigger and arguments.</p><p>The goal of ED is to identify event triggers and categorize their event types. For instance, in the sentence "He died in the hospital", an ED system is expected to detect a Die event along with the trigger word "died ". The detec- tion of event arguments "He"(Role = P erson) and "hospital"(Role = P lace) is not involved in the ED task. The 2005 ACE evaluation in- cluded 8 super types of events, with 33 sub- types. Following previous work, we treat these simply as 33 separate event types and ignore the hierarchical structure among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head><p>Similar to existing work, we model ED as a multi-class classification task. In detail, given a sentence, we treat every token in that sen- tence as a trigger candidate, and our goal is to classify each of these candidates into one of 34 classes (33 event types plus an NA class).</p><p>In our approach, every word along with its context, which includes the contextual words and entities, constitute an event trigger candi- date. <ref type="figure" target="#fig_0">Figure 1</ref> describes the architecture of the proposed approach, which involves two com- ponents: (i) Context Representation Learn- ing (CRL), which reveals the representation of both contextual words and entities via at- tention mechanisms; (ii) Event Detector (ED), which assigns an event type (including the NA type) to each candidate based on the learned contextual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Representation Learning</head><p>In order to prepare for Context Representa- tion Learning (CRL), we limit the contex- t to a fixed length by trimming longer sen- tences and padding shorter sentences with a special token when necessary. Let n be the fixed length and w 0 be the current candidate trigger word, then its contextual words</p><formula xml:id="formula_0">C w is [w − n 2 , w − n 2 +1 , ..., w −1 , w 1 , ..., w n 2 −1 , w n 2</formula><p>] 2 , and its contextual entities, which is the corre- sponding entity types (including an NA type)</p><formula xml:id="formula_1">of C w , is [e − n 2 , e − n 2 +1 , ..., e −1 , e 1 , ..., e n 2 −1 , e n 2 ].</formula><p>For convenience, we use w to denote the cur- rent word, [w 1 , w 2 , ..., w n ] to denote the con- textual words C w and [e 1 , e 2 , ..., e n ] to denote the contextual entities C e in figure 1. Note that, both w, C w and C e mentioned above are originally in symbolic representation. Be- fore entering CRL component, we transform them into real-valued vector by looking up word embedding table and entity type embed- ding table. Then we calculate attention vec- tors for both contextual words and entities by performing operations between the curren- t word w and its contexts. Finally, the con- textual words representation c w and contex- tual entities representation c e are formed by the weighted sum of the corresponding embed- dings of each word and entity in C w and C e , respectively. We will give the details in the fol-lowing subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Word Embedding Table</head><p>Word embeddings learned from a large amoun- t of unlabeled data have been shown to be able to capture the meaningful semantic reg- ularities of words ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b3">Erhan et al., 2010)</ref>. This paper uses the learned word embeddings as the source of basic fea- tures. Specifically, we use the Skip-gram mod- el ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) to learn word embed- dings on the NYT corpus 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Entity Type Embedding Table</head><p>The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following existing work ( <ref type="bibr" target="#b13">Li et al., 2013;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2015)</ref>, we exploit the annotated entity information in our ED system. We randomly initialize embedding vector for each entity type (including the NA type) and update it in training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Representation Learning</head><p>In this subsection, we illustrate our proposed approach to learn representations of both con- textual words and entities, which serve as in- puts to the following event detector compo- nent. Recall that, we use the matrix C w and C e to denote contextual words and contextual entities, respectively.</p><p>As illustrated in <ref type="figure" target="#fig_0">figure 1</ref>, the CRL compo- nent needs three inputs: the current candidate trigger word w, the contextual words C w and the contextual entities C e . Then, two atten- tion vectors, which reflect different aspects of the context, are calculated in the next step. The contextual word attention vector α w is computed based on the current word w and its contextual words C w . We firstly transform each word w k (including w and every word in C w ) into a hidden representation w k by the following equation:</p><formula xml:id="formula_2">w k = f (w k W w ) (1)</formula><p>where f (·) is a non-linear function such as the hyperbolic tangent, and W w is the transforma- tion matrix. Then, we use the hidden represen- tations to compute the attention value for each</p><formula xml:id="formula_3">3 https://catalog.ldc.upenn.edu/LDC2008T19</formula><p>word in C w :</p><formula xml:id="formula_4">α k w = exp(w w T k ) ∑ i exp(w w T i )<label>(2)</label></formula><p>The contextual entity attention vector α e is calculated with a similar method to α w .</p><formula xml:id="formula_5">α k e = exp(w e e T k ) ∑ i exp(w e e T i )<label>(3)</label></formula><p>Note that, we do not use the entity informa- tion of the current candidate token to compute the attention vector. The reason is that only a small percentage of true event triggers are entities <ref type="bibr">4</ref> . Therefore, the entity type of a can- didate trigger is meaningless for ED. Instead, we use w e , which is calculated by transform- ing w from the word space into the entity type space, as the attention source. We combine α w and α e to obtain the final attention vector, α = α w +α e . Finally, the con- textual words representation c w and the con- textual entities representation c e are formed by weighted sum of C w and C e , respectively:</p><formula xml:id="formula_6">c w = C w α T<label>(4)</label></formula><formula xml:id="formula_7">c e = C e α T<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Event Detector</head><p>As illustrated in <ref type="figure" target="#fig_0">figure 1</ref>, we employ a three- layer (an input layer, a hidden layer and a soft- max output layer) Artificial Neural Networks (ANNs) ( <ref type="bibr" target="#b6">Hagan et al., 1996</ref>) to model the ED task, which has been demonstrated very effec- tive for event detection by <ref type="bibr" target="#b15">Liu et al. (2016a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Basic ED Model</head><p>Given a sentence, as illustrated in <ref type="figure" target="#fig_0">figure 1</ref>, we concatenate the embedding vectors of the con- text (including contextual words and entities) and the current candidate trigger to serve as the input to ED model. Then, for a given in- put sample x, ANN with parameter θ outputs a vector O, where the i-th value o i of O is the confident score for classifying x to the i-th event type. To obtain the conditional proba- bility p(i|x, θ), we apply a softmax operation over all event types:</p><formula xml:id="formula_8">p(i|x, θ) = e o i ∑ m k=1 e o k<label>(6)</label></formula><p>4 Only 10% of triggers in ACE 2005 are entities.</p><p>Given all of our (suppose T) training instances (x (i) ; y (i) ), we can then define the negative log- likelihood loss function:</p><formula xml:id="formula_9">J(θ) = − T ∑ i=1 log p(y (i) |x (i) , θ)<label>(7)</label></formula><p>We train the model by using a simple opti- mization technique called stochastic gradient descent (SGD) over shuffled mini-batches with the Adadelta rule <ref type="bibr" target="#b26">(Zeiler, 2012)</ref>. Regulariza- tion is implemented by a dropout <ref type="bibr" target="#b10">(Kim, 2014;</ref><ref type="bibr" target="#b7">Hinton et al., 2012</ref>) and L 2 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Supervised Attention</head><p>In this subsection, we introduce supervised at- tention to explicitly use annotated argument information to improve ED. Our basic idea is simple: argument words should acquire more attention than other words. To achieve this goal, we first construct vectors using annotat- ed arguments as the gold attentions. Then, we employ them as supervision to train the atten- tion mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constructing Gold Attention Vectors</head><p>Our goal is to encourage argument words to obtain more attention than other words. To achieve this goal, we propose two strategies to construct gold attention vectors: S1: only pay attention to argument words. That is, all argument words in the giv- en context obtain the same attention, whereas other words get no attention. For candidates without any annotated arguments in context (such as negative samples), we force all entities to average the whole attention. <ref type="figure" target="#fig_1">Figure 2</ref> illus- trates the details, where α * is the final gold attention vector. S2: pay attention to both arguments and the words around them. The assump- tion is that, not only arguments are important to ED, the words around them are also help- ful. And the nearer a word is to arguments, the more attention it should obtain. Inspired by <ref type="bibr" target="#b17">Mi et al. (2016)</ref>, we use a gaussian distri- bution g(·) to model the attention distribution of words around arguments. In detail, given an instance, we first obtain the raw attention vec- tor α in the same manner as S1 (see <ref type="figure" target="#fig_1">figure 2)</ref>. Then, we create a new vector α ′ with all points initialized with zero, and for each α i = 1, we update α ′ by the following algorithm:</p><formula xml:id="formula_10">Algorithm 1: Updating α ′ for k ∈ {−w, ..., 0, ..., w} do α ′ i+k = α ′ i+k + g(|k|, µ, σ) end</formula><p>where w is the window size of the attention mechanism and µ, σ are hyper-parameters of the gaussian distribution. Finally, we normal- ize α ′ to obtain the target attention vector α * .</p><p>Similar with S1, we treat all entities in the context as arguments if the current candidate does not has any annotated arguments (such as netative samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jointly Training ED and Attention</head><p>Given the gold attention α * (see subsection 3.2.2) and the machine attention α produced by our model (see subsection 3.1.3), we em- ploy the square error as the loss function of attentions:</p><formula xml:id="formula_11">D(θ) = T ∑ i=1 n ∑ j=1 (α * i j − α i j ) 2<label>(8)</label></formula><p>Combining equation 7 and equation 8, we de- fine the joint loss function of our proposed model as follows:</p><formula xml:id="formula_12">J ′ (θ) = J(θ) + λD(θ)<label>(9)</label></formula><p>where λ is a hyper-parameter for trade-off be- tween J and D. Similar to basic ED model, we minimize the loss function J ′ (θ) by using SGD over shuffled mini-batches with the Adadelta update rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We conducted experiments on ACE 2005 dataset. For the purpose of comparison, we fol-lowed the evaluation of ( <ref type="bibr" target="#b13">Li et al., 2013;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b16">Liu et al., 2016b</ref>): randomly se- lected 30 articles from different genres as the development set, and subsequently conducted a blind test on a separate set of 40 ACE 2005 newswire documents. We used the remaining 529 articles as our training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter Setting</head><p>Hyper-parameters are tuned on the develop- ment dataset. We set the dimension of word embeddings to 200, the dimension of entity type embeddings to 50, the size of hidden lay- er to 300, the output size of word transfor- mation matrix W w in equation 1 to 200, the batch size to 100, the hyper-parameter for the L 2 norm to 10 −6 and the dropout rate to 0.6. In addition, we use the standard normal dis- tribution to model attention distributions of words around arguments, which means that µ = 0.0, σ = 1.0, and the window size is set to 3 (see Subsection 3.2.2). The hyper-parameter λ in equation 9 is various for different atten- tion strategies, we will give its setting in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Correctness of Our Assumption</head><p>In this section, we conduct experiments on ACE 2005 corpus to demonstrate the correct- ness of our assumption that argument infor- mation is crucial to ED. To achieve this goal, we design a series of systems for comparison. ANN is the basic event detection model, in which the hyper-parameter λ is set to 0. This system does not employ argument information and computes attentions without supervision (see Subsection 3.1.3).</p><p>ANN-ENT assigns λ with 0, too. The dif- ference is that it constructs the attention vec- tor α by forcing all entities in the context to average the attention instead of computing it in the manner introduced in Subsection 3.1.3. Since all arguments are entities, this system is designed to investigate the effects of entities.</p><p>ANN-Gold1 uses the gold attentions con- structed by strategy S1 in both training and testing procedure.</p><p>ANN-Gold2 is akin to ANN-Gold1, but uses the second strategy to construct its gold attentions.</p><p>Note that, in order to avoid the interference of attention mechanisms, the last two systems are designed to use argument information (via gold attentions) in both training and testing procedure. Thus both ANN-Gold1 and ANN- Gold2 assign λ with 0.   <ref type="table" target="#tab_2">Table 2</ref> compares these systems on ACE 2005 corpus. From the table, we observe that systems with argument information (the last two systems) significantly outperform system- s without argument information (the first t- wo systems), which demonstrates that argu- ment information is very useful for this task. Moreover, since all arguments are entities, for preciseness we also investigate that whether ANN-Gold1/2 on earth benefits from entities or arguments. Compared with ANN-ENT (re- vising that this system only uses entity infor- mation), ANN-Gold1/2 performs much bet- ter, which illustrates that entity information is not enough and further demonstrates that argument information is necessary for ED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on ACE 2005 Corpus</head><p>In this section, we conduct experiments on ACE 2005 corpus to demonstrate the effective- ness of the proposed approach. Firstly, we in- troduce systems implemented in this work.</p><p>ANN-S1 uses gold attentions constructed by strategy S1 as supervision to learn atten- tion. In our experiments, λ is set to 1.0.</p><p>ANN-S2 is akin to ANN-S1, but use strat- egy S2 to construct gold attentions and the hyper-parameter λ is set to 5.0.</p><p>These two systems both employ supervised attention mechanisms. For comparison, we use an unsupervised-attention system ANN as our baseline, which is introduced in Subsection 4.2. In addition, we select the following state-of- the-art methods for comparison. 1). Li's joint model ( <ref type="bibr" target="#b13">Li et al., 2013</ref>) extracts events based on structure prediction. It is the best structure-based system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P R F 1 Li's joint model (2013) 73.7 62.3 67.5</head><p>Liu's PSL <ref type="bibr">(2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="75.3">64.4 69.4</head><p>Liu's FN-Based <ref type="bibr">(2016)</ref> 77.6 65.2 70.7</p><p>Ngyuen's joint <ref type="bibr">(2016)</ref> 66.0 73.0 69.     introduces the non-consecutive convo- lution to capture non-consecutive k-grams for event detection. It is the best reported representation-based approach on this task. <ref type="table" target="#tab_3">Table 3</ref> presents the experimental results on ACE 2005 corpus. From the table, we make the following observations: 1). ANN performs unexpectedly poorly, which indicates that unsupervised-attention mechanisms do not work well for ED. We be- lieve the reason is that the training data of ACE 2005 corpus is insufficient to train a pre- cise attention in an unsupervised manner, con- sidering that data sparseness is an important issue of ED ( <ref type="bibr" target="#b27">Zhu et al., 2014;</ref><ref type="bibr" target="#b15">Liu et al., 2016a</ref>).</p><p>2). With argument information employed via supervised attention mechanisms, both ANN-S1 and ANN-S2 outperform ANN with remarkable gains, which illustrates the effec- tiveness of the proposed approach.</p><p>3). ANN-S2 outperforms ANN-S1, but the latter achieves higher precision. It is not d- ifficult to understand. On the one hand, s- trategy S1 only focuses on argument words, which provides accurate information to iden- tify event type, thus ANN-S1 could achieve higher precision. On the other hand, S2 focus- es on both arguments and words around them, which provides more general but noised clues. Thus, ANN-S2 achieves higher recall with a little loss of precision.</p><p>4). Compared with state-of-the-art ap- proaches, our method ANN-S2 achieves the best performance. We also perform a t-test (p 0.05), which indicates that our method significantly outperforms all of the compared methods. Furthermore, another noticeable ad- vantage of our approach is that it achieves much higher precision than state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Augmentation with FrameNet</head><p>Recently, <ref type="bibr" target="#b15">Liu et al. (2016a)</ref> used events auto- matically detected from FN as extra training data to alleviate the data-sparseness problem for event detection. To further demonstrate the effectiveness of the proposed approach, we also use the events from FN to augment the performance of our approach.</p><p>In this work, we use the events published by <ref type="bibr" target="#b15">Liu et al. (2016a)</ref>  <ref type="bibr">5</ref> as extra training data. However, their data can not be used in the proposed approach without further processing, because it lacks of both argument and entity information. <ref type="figure" target="#fig_4">Figure 3</ref> shows several examples of this data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Processing of Events from FN</head><p>Liu et al. (2016a) detected events from FrameNet based on the observation that frames in FN are analogous to events in ACE (lexical unit of a frame ↔ trigger of an even- t, frame elements of a frame ↔ arguments of an event). All events they published are also frames in FN. Thus, we treat frame elements annotated in FN corpus as event arguments. Since frames generally contain more frame el- ements than events, we only use core 6 elements in this work. Moreover, to obtain entity infor- mation, we use RPI Joint Information Extrac- tion System 7 ( <ref type="bibr" target="#b13">Li et al., 2013</ref>) to label ACE entity mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>We use the events from FN as extra train- ing data and keep the development and test datasets unchanged.  From the results, we observe that: 1). With extra training data, ANN achieves significant improvements on F 1 measure (66.7 vs. 65.0). This result, to some extent, demon- strates the correctness of our assumption that the data sparseness problem is the reason that causes unsupervised attention mechanisms to be ineffective to ED.</p><p>2). Augmented with external data, both ANN-S1 and ANN-S2 achieve higher recall with a little loss of precision. This is to be ex- pected. On the one hand, more positive train- ing samples consequently make higher recal- l. On the other hand, the extra event sam- ples are automatically extracted from FN, thus false-positive samples are inevitable to be in- volved, which may result in hurting the preci- sion. Anyhow, with events from FN, our ap- proach achieves higher F 1 score. <ref type="bibr">6</ref> FrameNet classifies frame elements into three groups: core, peripheral and extra-thematic.</p><p>7 http://nlp.cs.rpi.edu/software/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Event detection is an increasingly hot and challenging research topic in NLP. Generally, existing approaches could roughly be divided into two groups. The first kind of approach tackled this task under the supervision of annotated trig- gers and entities, but totally ignored anno- tated arguments. The majority of existing work followed this paradigm, which includes feature-based methods and representation- based methods. Feature-based methods ex- ploited a diverse set of strategies to convert classification clues (i.e., POS tags, dependen- cy relations) into feature vectors <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b9">Ji and Grishman, 2008;</ref><ref type="bibr" target="#b22">Patwardhan and Riloff, 2009;</ref><ref type="bibr" target="#b5">Gupta and Ji, 2009;</ref><ref type="bibr" target="#b14">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b8">Hong et al., 2011;</ref><ref type="bibr" target="#b16">Liu et al., 2016b</ref>). Representation-based methods typi- cally represent candidate event mentions by embeddings and feed them into neural net- works ( <ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2015;</ref><ref type="bibr" target="#b15">Liu et al., 2016a;</ref>.</p><p>The second kind of approach, on the con- trast, tackled event detection and argument extraction simultaneously, which is called joint approach ( <ref type="bibr" target="#b24">Riedel et al., 2009;</ref><ref type="bibr" target="#b23">Poon and Vanderwende, 2010;</ref><ref type="bibr" target="#b13">Li et al., 2013</ref><ref type="bibr" target="#b25">Venugopal et al., 2014;</ref>. Join- t approach is proposed to capture internal and external dependencies of events, includ- ing trigger-trigger, argument-argument and trigger-argument dependencies. Theoretically, both ED and AE are expected to benefit from joint methods because triggers and arguments are jointly considered. However, in practice, existing joint methods usually only make re- markable improvements to AE, but insignif- icant to ED. Different from them, this work investigates the exploitation of argument in- formation to improve the performance of ED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we propose a novel approach to model argument information explicitly for ED via supervised attention mechanisms. Besides, we also investigate two strategies to construc- t gold attentions using the annotated argu- ments. To demonstrate the effectiveness of the proposed method, we systematically conduc-t a series of experiments on the widely used benchmark dataset ACE 2005. Moreover, we also use events from FN to augment the per- formance of the proposed approach. Experi- mental results show that our approach outper- forms state-of-the-art methods, which demon- strates that the proposed approach is effective for event detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of the proposed approach for event detection. In this figure, w is the candidate word, [w 1 , ..., w n ] is the contextual words of w, and [e 1 , ..., e n ] is the corresponding entity types of [w 1 , ... , w n ].</figDesc><graphic url="image-1.png" coords="3,307.27,63.75,222.34,209.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of S1 to construct gold attention vector. The word fired is the trigger candidate, and underline words are arguments of fired annotated in the corpus.</figDesc><graphic url="image-2.png" coords="5,72.00,588.76,222.24,55.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Liu's PSL (Liu et al., 2016b) employs both latent local and global information for event detection. It is the best-reported feature- based system. 3). Liu's FN-Based approach (Liu et al., 2016a) leverages the annotated corpus of FrameNet to alleviate data sparseness problem of ED based on the observation that frames in FN are analogous to events in ACE. 4). Ngyen's joint model (Nguyen et al., 2016) employs a bi-directional RNN to jointly ex- tract event triggers and arguments. It is the best-reported representation-based joint ap- proach proposed on this task. 5). Skip-CNN (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of events detected from FrameNet (published by Liu et al. (2016a)).</figDesc><graphic url="image-3.png" coords="7,307.27,569.06,222.24,50.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results on ACE 2005 
corpus.  † designates the systems that employ 
argument information. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results on ACE 2005. 
The first group illustrates the performances of 
state-of-the-art approaches. The second group 
illustrates the performances of the proposed 
approach.  † designates the systems that em-
ploy arguments information. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 presents</head><label>4</label><figDesc></figDesc><table>the ex-
perimental results. 

Methods 
P 
R 
F 1 

ANN 

69.9 60.8 65.0 

ANN-S1 

81.4 62.4 70.8 

ANN-S2 

78.0 66.3 71.7 

ANN +FrameNet 

72.5 61.7 66.7 
ANN-S1 +FrameNet 80.1 63.6 70.9 
ANN-S2 +FrameNet 76.8 67.5 71.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results on ACE 2005 
corpus. "+FrameNet" designates the systems 
that are augmented by events from FrameNet. 

</table></figure>

			<note place="foot" n="1"> https://catalog.ldc.upenn.edu/LDC2006T06</note>

			<note place="foot" n="2"> The current candidate trigger word w0 is not included in the context.</note>

			<note place="foot" n="5"> https://github.com/subacl/acl16</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Natural Sci-ence Foundation of China (No. 61533018) and the National Basic Research Program of China ( <ref type="bibr">No. 2014CB340503)</ref>. And this research work was also supported by Google through focused research awards program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
		<title level="m">Proceedings of the workshop on annotating and reasoning about time and events</title>
		<meeting>the workshop on annotating and reasoning about time and events</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1017</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C98-1013" />
	</analytic>
	<monogr>
		<title level="m">The 17th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting unknown time arguments based on crossevent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P09-2093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACLIJCNLP 2009 Conference Short Papers. Association for Computational Linguistics</title>
		<meeting>the ACLIJCNLP 2009 Conference Short Papers. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="369" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><forename type="middle">B</forename><surname>Martin T Hagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beale</surname></persName>
		</author>
		<title level="m">Neural network design. Pws Pub</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<ptr target="http-s://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P11-1113" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1127" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P08-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT. Association for Computational Linguistics</title>
		<meeting>ACL-08: HLT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://www.anthology.aclweb.org/D14-1181" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-1038</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-1038" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constructing information networks using one single model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1198</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1198" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1846" to="1851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P13-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using document level cross-event inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P10-1081" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leveraging framenet to improve automatic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1201</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1201" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2134" to="2143" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A probabilistic soft logic based approach to exploiting latent and global information in event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11990/12052" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth AAAI Conference on Artificail Intelligence</title>
		<meeting>the thirtieth AAAI Conference on Artificail Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2993" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00112</idno>
		<ptr target="https://arxiv.org/abs/1608.00112" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<ptr target="http-s://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Huu Thien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1034</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1034" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Huu Thien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-2060</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-2060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling skip-grams for event detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Huu Thien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D16-1085" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified model of phrasal and sentential evidence for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D09-1016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint inference for knowledge extraction from biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N10-1123" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="813" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Proceedings of the bionlp 2009 workshop companion volume for shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Woo</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihisa</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W09-1406" />
		<imprint>
			<date type="published" when="2009" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1090</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="831" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno>arX- iv:1212.5701</idno>
		<ptr target="https://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingual event extraction: a case study on trigger type determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-2136</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-2136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="842" to="847" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
