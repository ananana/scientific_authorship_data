<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Particle Filter Rejuvenation and Latent Dirichlet Allocation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandler</forename><surname>May</surname></persName>
							<email>cjmay@jhu.edu, clemmer.alexander@gmail.com, vandurme@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University ‡ Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Clemmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University ‡ Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University ‡ Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Particle Filter Rejuvenation and Latent Dirichlet Allocation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="446" to="451"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous research has established several methods of online learning for latent Dirichlet allocation (LDA). However , streaming learning for LDA-allowing only one pass over the data and constant storage complexity-is not as well explored. We use reservoir sampling to reduce the storage complexity of a previously-studied online algorithm, namely the particle filter, to constant. We then show that a simpler particle filter implementation performs just as well, and that the quality of the initialization dominates other factors of performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We extend a popular model, latent Dirichlet al- location (LDA), to unbounded streams of docu- ments. In order for inference to be practical in this setting it must use constant space asymptoti- cally and run in pseudo-linear time, perhaps O(n) or O(n log n). <ref type="bibr" target="#b6">Canini et al. (2009)</ref> presented a method for LDA inference based on particle filters, where a sam- ple set of models is updated online with each new token observed from a stream. In general, these models should be regularly resampled and rejuve- nated using Markov Chain Monte Carlo (MCMC) steps over the history in order to improve the ef- ficiency of the particle filter ( <ref type="bibr" target="#b9">Gilks and Berzuini, 2001</ref>). The particle filter of <ref type="bibr" target="#b6">Canini et al. (2009)</ref> re- juvenates over independent draws from the history by storing all past observations and states. This al- gorithm thus has linear storage complexity and is not an online learning algorithm in a strict sense <ref type="bibr" target="#b4">(Börschinger and Johnson, 2012</ref>).</p><p>In the current work we propose using reservoir sampling in the rejuvenation step to reduce the storage complexity of the particle filter to O(1). This improvement is practically useful in the large-data setting and is also scientifically interest- ing in that it recovers some of the cognitive plau- sibility which originally motivated <ref type="bibr" target="#b4">Börschinger and Johnson (2012)</ref>. However, in experiments on the dataset studied by <ref type="bibr" target="#b6">Canini et al. (2009)</ref>, we show that rejuvenation does not benefit the par- ticle filter's performance. Rather, performance is dominated by the effects of random initializa- tion (a problem for which we provide a correction while abiding by the same constraints as <ref type="bibr" target="#b6">Canini et al. (2009)</ref>). This result re-opens the question of whether rejuvenation is of practical importance in online learning for static Bayesian models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Latent Dirichlet Allocation</head><p>For a sequence of N words collected into doc- uments of varying length, we denote the j-th word as w j , and the document it occurs in as d i . LDA ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> "explains" the occurrence of each word by postulating that a document was generated by repeatedly: (1) sampling a topic z from θ (d) , the document-specific mixture of T top- ics, and (2) sampling a word w from φ (z) , the probability distribution the z-th topic defines over the vocabulary.</p><p>The goal is to infer θ and φ, under the model:</p><formula xml:id="formula_0">w i | z i , φ (z i ) ∼ Categorical(φ (z i ) ) φ (z) ∼ Dirichlet(β) z i | θ (d i ) ∼ Categorical(θ (d i ) ) θ (d) ∼ Dirichlet(α) initialize weights ω (p) 0 = 1/P for p = 1, . . . , P for i = 1, . . . , N do for p = 1, . . . , P do set ω (p) i = ω (p) i−1 P(wi | z (p) i−1 , wi−1) sample z (p) i w.p. P(z (p) i | z (p) i−1 , wi). if ω −2 2 ≤ ESS then for j ∈ R(i) do for p = 1, . . . , P do sample z (p) j w.p. P(z (p) j | z (p) i\j , wi) set ω (p) i</formula><p>= 1/P for each particle Algorithm 1: Particle filtering for LDA.</p><p>Computing φ and θ exactly is generally in- tractable, motivating methods for approximate in- ference such as variational Bayesian inference ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>, expectation propagation ( <ref type="bibr" target="#b16">Minka and Lafferty, 2002</ref>), and collapsed Gibbs sampling ( <ref type="bibr" target="#b10">Griffiths and Steyvers, 2004)</ref>.</p><p>A limitation of these techniques is they require multiple passes over the data to obtain good sam- ples of φ and θ. This requirement makes them im- practical when the corpus is too large to fit directly into memory and in particular when the corpus grows without bound. This motivates online learn- ing techniques, including sampling-based meth- ods ( <ref type="bibr" target="#b1">Banerjee and Basu, 2007;</ref><ref type="bibr" target="#b6">Canini et al., 2009)</ref> and stochastic variational inference <ref type="bibr" target="#b11">(Hoffman et al., 2010;</ref><ref type="bibr" target="#b15">Mimno et al., 2012;</ref><ref type="bibr" target="#b12">Hoffman et al., 2013)</ref>. However, where these approaches gener- ally assume the ability to draw independent sam- ples from the full dataset, we consider the case when it is infeasible to access arbitrary elements from the history. The one existing algorithm that can be directly applied under this constraint, to our knowledge, is the streaming variational Bayes framework ( <ref type="bibr" target="#b5">Broderick et al., 2013</ref>) in which the posterior is recursively updated as new data arrives using a variational approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Online LDA Using Particle Filters</head><p>Particle filters are a family of sequential Monte Carlo (SMC) sampling algorithms designed to es- timate the posterior distribution of a system with dynamic state ( <ref type="bibr">Doucet et al., 2001)</ref>. A particle fil- ter approximates the posterior by a weighted sam- ple of points, or particles, from the state space. The particle cloud is updated recursively for each new observation using importance sampling (an approach called sequential importance sampling). <ref type="bibr" target="#b6">Canini et al. (2009)</ref> apply this approach to LDA after analytically integrating out φ and θ, obtain- ing a Rao-Blackwellized particle filter ( <ref type="bibr" target="#b7">Doucet et al., 2000</ref>) that estimates the collapsed posterior P(z | w). In this setting, the P particles are sam- ples of the topic assignment vector z (p) , and they are propagated forward in state space one token at a time. In general, the larger P is, the more ac- curately we approximate the posterior; for small P , the approximation of the tails of the poste- rior will be particularly poor <ref type="bibr" target="#b18">(Pitt and Shephard, 1999</ref>). However, a larger value of P increases the runtime and storage requirements of the algorithm.</p><p>We now describe the Rao-Blackwellized parti- cle filter for LDA in detail (pseudocode is given in Algorithm 1). At the moment token i is observed, the particles form a discrete approximation of the posterior up to the (i − 1)-th word:</p><formula xml:id="formula_1">P(z i−1 | w i−1 ) ≈ p ω (p) i−1 I z i−1 (z (p) i−1 )</formula><p>where I z (z ) is the indicator function, evaluating to 1 if z = z and 0 otherwise. Now each par- ticle p is propagated forward by drawing a topic z </p><formula xml:id="formula_2">P(z i | w i ) ≈ p ω (p) i I z i (z (p) i ).</formula><p>Dropping the superscript (p) for notational conve- nience, the conditional posterior used in the prop- agation step is given by</p><formula xml:id="formula_3">P(z i |z i−1 , w i ) ∝ P(z i , w i | z i−1 , w i−1 ) = n (w i ) z i ,i\i + β n (·) z i ,i\i + W β n (d i ) z i ,i\i + α n (d i ) ·,i\i + T α</formula><p>where n (w i ) z i ,i\i is the number of times word w i has been assigned topic z i so far, n (·) z i ,i\i is the num- ber of times any word has been assigned topic z i , n</p><formula xml:id="formula_4">(d i )</formula><p>z i ,i\i is the number of times topic z i has been as- signed to any word in document d i , and n</p><formula xml:id="formula_5">(d i )</formula><p>·,i\i is the number of words observed in document d i . The particle weights are scaled as</p><formula xml:id="formula_6">ω (p) i ω (p) i−1 ∝ P(w i | z (p) i , w i )P(z (p) i | z (p) i−1 ) Q(z (p) i | z (p) i−1 , w i ) = P(w i | z (p) i−1 , w i−1 )</formula><p>where Q is the proposal distribution for the parti- cle state transition; in our case,</p><formula xml:id="formula_7">Q(z (p) i | z (p) i−1 , w i ) = P(z (p) i | z (p) i−1 , w i ),</formula><p>minimizing the variance of the importance weights conditioned on w i and z i−1 ( <ref type="bibr" target="#b7">Doucet et al., 2000</ref>).</p><p>Over time the particle weights tend to diverge. To combat this inefficiency, after every state tran- sition we estimate the effective sample size (ESS) of the particle weights as ω i −2 2 (Liu and Chen, 1998) and resample the particles when that esti- mate drops below a prespecified threshold. Sev- eral resampling strategies have been proposed ( <ref type="bibr" target="#b7">Doucet et al., 2000</ref>); we perform multinomial resampling as in Pitt and Shephard (1999) and <ref type="bibr" target="#b0">Ahmed et al. (2011)</ref>, treating the weights as un- normalized probability masses on the particles.</p><p>After resampling we are likely to have several copies of the same particle, yielding a degenerate approximation to the posterior. To reintroduce di- versity to the particle cloud we take MCMC steps over a sequence of states from the history ( <ref type="bibr" target="#b7">Doucet et al., 2000;</ref><ref type="bibr" target="#b9">Gilks and Berzuini, 2001</ref>). We call the indices of these states the rejuvenation sequence, denoted R(i) <ref type="figure" target="#fig_3">(Canini et al., 2009)</ref>. The transition probability for a state j ∈ R(i) is given by</p><formula xml:id="formula_8">P(z j | z N \j , w N ) ∝ n (w j ) z j ,N \j + β n (·) z j ,N \j + W β n (d j ) z j ,N \j + α n (d j )</formula><p>·,N \j + T α where subscript N \j denotes counts up to token N , excluding those for token j.</p><p>The rejuvenation sequence can be chosen by the practitioner. Choosing a long sequence (large |R(i)|) may result in a more accurate posterior ap- proximation but also increases runtime and stor- age requirements. The tokens in R(i) may be cho- sen uniformly at random from the history or under a biased scheme that favors recent observations. The particle filter studied empirically by <ref type="bibr" target="#b6">Canini et al. (2009)</ref> stored the entire history, incurring lin- ear storage complexity in the size of the stream. Ahmed et al. (2011) instead sampled ten docu- ments from the most recent 1000, achieving con- stant storage complexity at the cost of a recency bias. If we want to fit a model to a long non- i.i.d. stream, we require an unbiased rejuvenation sequence as well as sub-linear storage complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reservoir Sampling</head><p>Reservoir sampling is a widely-used family of al- gorithms for choosing an array ("reservoir") of k items. The most common <ref type="bibr">example, presented in Vitter (1985)</ref> as Algorithm R, chooses k elements of a stream such that each possible subset of k el- ements is equiprobable. This effects sampling k items uniformly without replacement, using run- time O(n) (constant per update) and storage O(k).</p><p>Initialize k-element array R ; Stream S ;</p><formula xml:id="formula_9">for i = 1, . . . , k do R[i] ← S[i] ; for i = k + 1, . . . , length(S) do j ← random(1, i); if j ≤ k then R[j] ← S[i] ;</formula><p>Algorithm 2: Algorithm R for reservoir sampling</p><p>To ensure constant space over an unbounded stream, we draw the rejuvenation sequence R(i) uniformly from a reservoir. As each token of the training data is ingested by the particle filter, we decide to insert that token into the reservoir, or not, independent of the other tokens in the current doc- ument. Thus, at the end of step i of the particle fil- ter, each of the i tokens seen so far in the training sequence has an equal probability of being in the reservoir, hence being selected for rejuvenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our particle filter on three datasets studied in <ref type="bibr" target="#b6">Canini et al. (2009)</ref>: diff3, rel3, and sim3. Each of these datasets is a collection of posts under three categories from the 20 News- groups dataset. <ref type="bibr">1</ref> We use a 60% training/40% test- ing split of this data that is available online. <ref type="bibr">2</ref> We preprocess the data by splitting each line on non-alphabet characters, converting the result- ing tokens to lower-case, and filtering out any to- kens that appear in a list of common English stop words. In addition, we remove the header of ev- ery file and filter every line that does not contain a non-trailing space (which removes embedded ASCII-encoded attachments). Finally, we shuffle the order of the documents. After these steps, we compute the vocabulary for each dataset as the set of all non-singleton types in the training data aug- mented with a special out-of-vocabulary symbol. During training we report the out-of-sample NMI, calculated by holding the word proportions φ fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document. Two Gibbs sweeps have been shown to yield good performance in practice ( <ref type="bibr" target="#b22">Yao et al., 2009)</ref>; we increase the num- ber of sweeps to five after inspecting the stability on our dataset. The variance of the particle filter is often large, so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction.</p><p>Fixed Initialization. Our first set of experi- ments has a similar parameterization 3 to the exper- iments of <ref type="bibr" target="#b6">Canini et al. (2009)</ref> except we draw the rejuvenation sequence from a reservoir. We initial- ize the particle filter with 200 Gibbs sweeps on the first 10% of each dataset. Then, for each dataset, for rejuvenation disabled, rejuvenation based on a reservoir of size 1000, and rejuvenation based on the entire history (in turn), we perform 30 runs of the particle filter from that fixed initial model. Our results <ref type="figure" target="#fig_1">(Figure 1</ref>) resemble those of <ref type="bibr" target="#b6">Canini et al. (2009)</ref>; we believe the discrepancies are mostly attributable to differences in preprocessing.</p><p>In these experiments, the initial model was not chosen arbitrarily. Rather, an initial model that yielded out-of-sample NMI close to the initial out- of-sample NMI scores reported in the previous  study was chosen from a set of 100 candidates.</p><p>Variable Initialization. We now investigate the significance of the initial model selection step used in the previous experiments. We run a new set of experiments in which the reservoir size is held fixed at 1000 and the size of the initialization sam- ple is varied. Specifically, we vary the size of the initialization sample, in documents, between zero (corresponding to no Gibbs initialization), 30, 100, and 300, and also perform a run of batch Gibbs sampling (with no particle filter). In each case, 2000 Gibbs sweeps are performed. In these experiments, the initial models are not held fixed; for each of the 30 runs for each dataset, the initial model was generated by a different Gibbs chain. The results for these experiments, depicted in <ref type="figure" target="#fig_3">Fig- ure 2</ref>, indicate that the size of the initialization sample improves mean NMI and reduces variance, and that the variance of the particle filter itself is dominated by the variance of the initial model. Tuned Initialization. We observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm, as measured by NMI. With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model. Thus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set, setting the particle filter's initial model to the model out of these 20 with the highest in-sample NMI. This procedure is per- formed independently for each run of the particle filter. We may not always have labeled data for initialization, so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80% of the initialization sample, held- out perplexity (per word) is estimated on the re- maining 20%, using a first-moment particle learn- ing approximation <ref type="bibr" target="#b19">(Scott and Baldridge, 2013)</ref>, and the particle filter is started from the model out of these 20 with the lowest held-out perplex- ity. The results, shown in <ref type="figure" target="#fig_4">Figure 3</ref>, show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Motivated by a desire for cognitive plausibility, Börschinger and Johnson (2011) used a particle filter to learn Bayesian word segmentation mod- els, following the work of <ref type="bibr" target="#b6">Canini et al. (2009)</ref>. They later showed that rejuvenation improved per- formance <ref type="bibr" target="#b4">(Börschinger and Johnson, 2012)</ref>, but this impaired cognitive plausibility by necessitat- ing storage of all previous states and observations. We attempted to correct this by drawing the re- juvenation sequence from a reservoir, but our re- sults indicate that the particle filter for LDA on our dataset is highly sensitive to initialization and not influenced by rejuvenation.</p><p>In the experiments of Börschinger and Johnson (2012), the particle cloud appears to be resampled once per utterance with a large rejuvenation se- quence; 4 each particle takes many more rejuvena- tion MCMC steps than new state transitions and thus resembles a batch MCMC sampler. In our ex- periments resampling is done on the order of once per document, leading to less than one rejuvena- tion step per transition. Future work should care- fully note this ratio: sampling history much more often than new states improves performance but contradicts the intuition behind particle filters.</p><p>We have also shown that tuning the initial model using in-sample NMI or held-out perplexity can improve mean NMI and reduce variance. Perplex- ity (or likelihood) is often used to estimate model performance in LDA ( <ref type="bibr" target="#b2">Blei et al., 2003;</ref><ref type="bibr" target="#b10">Griffiths and Steyvers, 2004;</ref><ref type="bibr" target="#b21">Wallach et al., 2009;</ref><ref type="bibr" target="#b11">Hoffman et al., 2010)</ref>, and does not compare the in- ferred model against gold-standard labels, yet it appears to be a good proxy for NMI in our experi- ment. Thus, if initialization continues to be crucial to performance, at least we may have the flexibil- ity of initializing without gold-standard labels.</p><p>We have focused on NMI as our evaluation met- ric for comparison with <ref type="bibr" target="#b6">Canini et al. (2009)</ref>. How- ever, evaluation of topic models is a subject of con- siderable debate ( <ref type="bibr" target="#b21">Wallach et al., 2009;</ref><ref type="bibr" target="#b22">Yao et al., 2009;</ref><ref type="bibr" target="#b17">Newman et al., 2010;</ref><ref type="bibr" target="#b14">Mimno et al., 2011</ref>) and it may be informative to investigate the effects of initialization and rejuvenation using other met- rics such as perplexity or semantic coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed reservoir sampling for reduc- ing the storage complexity of a particle filter from linear to constant. This work was motivated as an expected improvement on the model of <ref type="bibr" target="#b6">Canini et al. (2009)</ref>. However, in the process of estab- lishing an empirical baseline we discovered that rejuvenation does not play a significant role in the experiments of <ref type="bibr" target="#b6">Canini et al. (2009)</ref>. More- over, we found that performance of the particle filter was strongly affected by the random initial- ization of the model, and suggested a simple ap- proach to reduce the variability therein without using additional data. In conclusion, it is now an open question whether-and if so, under what assumptions-rejuvenation benefits particle filters for LDA and similar static Bayesian models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, w i ) and scaling the particle weight by P(w i | z (p) i−1 , w i−1 ). The particle cloud now approximates the posterior up to the i-th word:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fixed initialization with different reservoir sizes.</figDesc><graphic url="image-1.png" coords="4,72.00,62.81,226.76,226.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3</head><label></label><figDesc>T = 3, α = β = 0.1, P = 100, ess = 20, |R(i)| = 30</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Variable initialization with different initialization sample sizes.</figDesc><graphic url="image-2.png" coords="4,307.28,62.81,226.76,226.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Variable initialization with tuning.</figDesc><graphic url="image-3.png" coords="5,72.00,62.81,226.76,226.76" type="bitmap" /></figure>

			<note place="foot" n="1"> diff3: {rec.sport.baseball, sci.space, alt.atheism}; rel3: talk.politics.{misc, guns, mideast}; and sim3: comp.{graphics, os.ms-windows.misc, windows.x}. 2 http://qwone.com/ ˜ jason/20Newsgroups/ 20news-bydate.tar.gz</note>

			<note place="foot" n="4"> The ESS threshold is P ; the rejuvenation sequence is 100 or 1600 utterances, almost one sixth of the training data.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified analysis of streaming news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choon Hui</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International World Wide Web Conference (WWW)</title>
		<meeting>the 20th International World Wide Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Topic models over text streams: A study of batch and online unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 7th SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="431" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A particle filter algorithm for Bayesian wordsegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop (ALTA)</title>
		<meeting>the Australasian Language Technology Association Workshop (ALTA)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using rejuvenation to improve particle filtering for Bayesian word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Streaming variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Wibisono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashia</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online inference of topics with latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">R</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rao-Blackwellised particle filtering for dynamic Bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the 16th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="176" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo Methods in Practice</title>
		<editor>Arnaud Doucet, Nando de Freitas, and Neil Gordon</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Following a moving target-Monte Carlo inference for dynamic Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berzuini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="146" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online learning for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23 (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequential Monte Carlo methods for dynamic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">443</biblScope>
			<biblScope unit="page" from="1032" to="1044" />
			<date type="published" when="1998-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse stochastic inference for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Expectationpropagation for the generative aspect model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the 18th Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="352" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Filtering via simulation: Auxiliary particle filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shephard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">446</biblScope>
			<biblScope unit="page" from="590" to="599" />
			<date type="published" when="1999-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A recursive estimate for the predictive likelihood in a topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 16th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random sampling with a reservoir</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1985-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
