<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linguistically Regularized LSTM for Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
						</author>
						<title level="a" type="main">Linguistically Regularized LSTM for Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1679" to="1689"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1154</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation , but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification aims to classify text to sentiment classes such as positive or negative, or more fine-grained classes such as very positive, positive, neutral, etc. There has been a variety of approaches for this purpose such as lexicon-based classification <ref type="bibr" target="#b34">(Turney, 2002;</ref><ref type="bibr" target="#b31">Taboada et al., 2011</ref>), and early machine learning based methods ( <ref type="bibr" target="#b23">Pang et al., 2002;</ref><ref type="bibr" target="#b21">Pang and Lee, 2005)</ref>, and recently neural network models such as convolutional neu- ral network (CNN) <ref type="bibr" target="#b14">(Kim, 2014;</ref><ref type="bibr" target="#b12">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b16">Lei et al., 2015)</ref>, recursive autoen- coders <ref type="bibr" target="#b29">(Socher et al., 2011</ref><ref type="bibr" target="#b30">(Socher et al., , 2013</ref>, Long Short- Term Memory (LSTM) <ref type="bibr" target="#b19">(Mikolov, 2012;</ref><ref type="bibr" target="#b3">Chung et al., 2014;</ref><ref type="bibr" target="#b32">Tai et al., 2015;</ref>, and many more. * Corresponding Author: Minlie Huang</p><p>In spite of the great success of these neural mod- els, there are some defects in previous studies. First, tree-structured models such as recursive au- toencoders and Tree-LSTM ( <ref type="bibr" target="#b32">Tai et al., 2015;</ref>, depend on parsing tree structures and expensive phrase-level annotation, whose per- formance drops substantially when only trained with sentence-level annotation. Second, linguis- tic knowledge such as sentiment lexicon, negation words or negators (e.g., not, never), and intensity words or intensifiers (e.g., very, absolutely), has not been fully employed in neural models.</p><p>The goal of this research is to developing sim- ple sequence models but also attempts to fully em- ploying linguistic resources to benefit sentiment classification. Firstly, we attempts to develop sim- ple models that do not depend on parsing trees and do not require phrase-level annotation which is too expensive in real-world applications. Secondly, in order to obtain competitive performance, sim- ple models can benefit from linguistic resources. Three types of resources will be addressed in this paper: sentiment lexicon, negation words, and in- tensity words. Sentiment lexicon offers the prior polarity of a word which can be useful in deter- mining the sentiment polarity of longer texts such as phrases and sentences. Negators are typical sen- timent shifters ( <ref type="bibr" target="#b41">Zhu et al., 2014</ref>), which constantly change the polarity of sentiment expression. In- tensifiers change the valence degree of the modi- fied text, which is important for fine-grained sen- timent classification.</p><p>In order to model the linguistic role of senti- ment, negation, and intensity words, our central idea is to regularize the difference between the predicted sentiment distribution of the current po- sition 1 , and that of the previous or next positions, in a sequence model. For instance, if the cur-rent position is a negator not, the negator should change the sentiment distribution of the next posi- tion accordingly. To summarize, our contributions lie in two folds:</p><p>• We discover that modeling the linguistic role of sentiment, negation, and intensity words can enhance sentence-level sentiment classi- fication. We address the issue by imposing linguistic-inspired regularizers on sequence LSTM models.</p><p>• Unlike previous models that depend on pars- ing structures and expensive phrase-level an- notation, our models are simple and efficient, but the performance is on a par with the state- of-the-art.</p><p>The rest of the paper is organized as follows: In the following section, we survey related work. In Section 3, we briefly introduce the background of LSTM and bidirectional LSTM, and then de- scribe in detail the lingistic regularizers for senti- ment/negation/intensity words in Section 4. Ex- periments are presented in Section 5, and Conclu- sion follows in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Networks for Sentiment Classification</head><p>There are many neural networks proposed for sen- timent classification. The most noticeable models may be the recursive autoencoder neural network which builds the representation of a sentence from subphrases recursively <ref type="bibr" target="#b29">(Socher et al., 2011</ref><ref type="bibr" target="#b30">(Socher et al., , 2013</ref><ref type="bibr" target="#b5">Dong et al., 2014;</ref><ref type="bibr" target="#b26">Qian et al., 2015)</ref>. Such recur- sive models usually depend on a tree structure of input text, and in order to obtain competitive re- sults, usually require annotation of all subphrases. Sequence models, for instance, convolutional neu- ral network (CNN), do not require tree-structured data, which are widely adopted for sentiment clas- sification <ref type="bibr" target="#b14">(Kim, 2014;</ref><ref type="bibr" target="#b12">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b16">Lei et al., 2015)</ref>. Long short-term memory models are also common for learning sentence-level rep- resentation due to its capability of modeling the prefix or suffix context <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>). LSTM can be commonly applied to sequential data but also tree-structured data ( <ref type="bibr" target="#b32">Tai et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Applying Linguistic Knowledge for Sentiment Classification</head><p>Linguistic knowledge and sentiment resources, such as sentiment lexicons, negation words (not, never, neither, etc.) or negators, and intensity words (very, extremely, etc.) or intensifiers, are useful for sentiment analysis in general. Sentiment lexicon ( <ref type="bibr" target="#b9">Hu and Liu, 2004;</ref><ref type="bibr" target="#b39">Wilson et al., 2005</ref>) usually defines prior polarity of a lex- ical entry, and is valuable for lexicon-based mod- els <ref type="bibr" target="#b34">(Turney, 2002;</ref><ref type="bibr" target="#b31">Taboada et al., 2011)</ref>, and ma- chine learning approaches <ref type="bibr" target="#b22">(Pang and Lee, 2008)</ref>. There are recent works for automatic construction of sentiment lexicons from social data  and for multiple languages <ref type="bibr" target="#b1">(Chen and Skiena, 2014)</ref>. A noticeable work that ultilizes sentiment lexicons can be seen in ( <ref type="bibr" target="#b33">Teng et al., 2016)</ref> which treats the sentiment score of a sen- tence as a weighted sum of prior sentiment scores of negation words and sentiment words, where the weights are learned by a neural network.</p><p>Negation words play a critical role in modify- ing sentiment of textual expressions. Some early negation models adopt the reversing assumption that a negator reverses the sign of the sentiment value of the modified text ( <ref type="bibr" target="#b25">Polanyi and Zaenen, 2006</ref>; <ref type="bibr" target="#b13">Kennedy and Inkpen, 2006</ref>). The shifting hyothesis assumes that negators change the senti- ment values by a constant amount <ref type="bibr" target="#b31">(Taboada et al., 2011;</ref><ref type="bibr" target="#b17">Liu and Seneff, 2009)</ref>. Since each negator can affect the modified text in different ways, the constant amount can be extended to be negator- specific ( <ref type="bibr" target="#b41">Zhu et al., 2014)</ref>, and further, the ef- fect of negators could also depend on the syntax and semantics of the modified text ( <ref type="bibr" target="#b41">Zhu et al., 2014</ref>). Other approaches to negation modeling can be seen in ( <ref type="bibr" target="#b11">Jia et al., 2009;</ref><ref type="bibr" target="#b38">Wiegand et al., 2010;</ref><ref type="bibr" target="#b0">Benamara et al., 2012;</ref><ref type="bibr" target="#b15">Lapponi et al., 2012)</ref>.</p><p>Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensity words can change the valence degree (i.e., sentiment intensity) of the modified text. In <ref type="bibr" target="#b37">(Wei et al., 2011</ref>) the authors propose a lin- ear regression model to predict the valence value for content words. In ( <ref type="bibr" target="#b18">Malandrakis et al., 2013</ref>), a kernel-based model is proposed to combine se- mantic information for predicting sentiment score. In the SemEval-2016 task 7 subtask A, a learning- to-rank model with a pair-wise strategy is pro- posed to predict sentiment intensity scores ( <ref type="bibr" target="#b36">Wang et al., 2016)</ref>. Linguistic intensity is not limited to sentiment or intensity words, and there are works that assign low/medium/high intensity scales to adjectives such as okay, good, great <ref type="bibr" target="#b27">(Sharma et al., 2015</ref>) or to gradable terms (e.g. large, huge, gi- gantic) <ref type="bibr" target="#b28">(Shivade et al., 2015</ref>).</p><p>In ( <ref type="bibr" target="#b4">Dong et al., 2015)</ref>, a sentiment parser is proposed, and the authors studied how sentiment changes when a phrase is modified by negators or intensifiers.</p><p>Applying linguistic regularization to text clas- sification can be seen in <ref type="bibr" target="#b40">(Yogatama and Smith, 2014</ref>) which introduces three linguistically moti- vated structured regularizers based on parse trees, topics, and hierarchical word clusters for text cat- egorization. Our work differs in that <ref type="bibr" target="#b40">(Yogatama and Smith, 2014</ref>) applies group lasso regularizers to logistic regression on model parameters while our regularizers are applied on intermediate out- puts with KL divergence.</p><p>3 Long Short-term Memory Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Long Short-Term Memory (LSTM)</head><p>Long Short-Term Memory has been widely adopted for text processing. Briefly speaking, in LSTM, the hidden states h t and memory cell c t is a function of their previous c t−1 and h t−1 and input vector x t , or formally as follows:</p><formula xml:id="formula_0">c t , h t = g (LST M ) (c t−1 , h t−1 , x t )<label>(1)</label></formula><p>The hidden state h t ∈ R d denotes the represen- tation of position t while also encoding the pre- ceding contexts of the position. For more details about LSTM, we refer readers to <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional LSTM</head><p>In LSTM, the hidden state of each position (h t ) only encodes the prefix context in a forward di- rection while the backward context is not consid- ered. Bidirectional LSTM ( <ref type="bibr" target="#b7">Graves et al., 2013</ref>) exploited two parallel passes (forward and back- ward) and concatenated hidden states of the two LSTMs as the representation of each position. The forward and backward LSTMs are respectively formulated as follows:</p><formula xml:id="formula_1">− → c t , − → h t = g (LST M ) ( − → c t−1 , − → h t−1 , x t ) (2) ← − c t , ← − h t = g (LST M ) ( ← − c t+1 , ← − h t+1 , x t ) (3)</formula><p>where g (LST M ) is the same as that in Eq (1). Particularly, parameters in the two LSTMs are shared. The representation of the entire sentence is</p><formula xml:id="formula_2">[ − → h n , ← − h 1 ],</formula><p>where n is the length of the sen- tence. At each position t, the new representa- tion is</p><formula xml:id="formula_3">h t = [ − → h t , ← − h t ]</formula><p>, which is the concatenation of hidden states of the forward LSTM and back- ward LSTM. In this way, the forward and back- ward contexts can be considered simultaneously. The central idea of the paper is to model the linguistic role of sentiment, negation, and inten- sity words in sentence-level sentiment classifica- tion by regularizing the outputs at adjacent posi- tions of a sentence. For example in <ref type="figure" target="#fig_0">Fig 1,</ref> in sen- tence "It's not an interesting movie", the predicted sentiment distributions at "*an interesting movie 2 " and "*interesting movie" should be close to each other, while the predicted sentiment distribution at "*interesting movie" should be quite different from the preceding positions (in the backward di- rection) ("*movie") since a sentiment word ("in- teresting") is seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Linguistically Regularized LSTM</head><p>We propose a generic regularizer and three spe- cial regularizers based on the following linguistic observations:</p><p>• Non-Sentiment Regularizer: if the two ad- jacent positions are all non-opinion words, the sentiment distributions of the two posi- tions should be close to each other. Though this is not always true (e.g., soap movie), this assumption holds at most cases.</p><p>• Sentiment Regularizer: if the word is a sen- timent word found in a lexicon, the sentiment distribution of the current position should be significantly different from that of the next or previous positions. We approach this phe- nomenon with a sentiment class specific shift- ing distribution.</p><p>• Negation Regularizer: Negation words such as "not" and "never" are critical sentiment shifter or converter: in general they shift sen- timent polarity from the positive end to the negative end, but sometimes depend on the negation word and the words they modify.</p><p>The negation regularizer models this linguis- tic phenomena with a negator-specific trans- formation matrix.</p><p>• Intensity Regularizer: Intensity words such as "very" and "extremely" change the va- lence degree of a sentiment expression: for instance, from positive to very positive. Mod- eling this effect is quite important for fine- grained sentiment classification, and the in- tensity regularizer is designed to formulate this effect by a word-specific transformation matrix.</p><p>More formally, the predicted sentiment distri- bution (p t , based on h t , see Eq. 5) at position t should be linguistically regularized with respect to that of the preceding (t − 1) or following (t + 1) positions. In order to enforce the model to produce coherent predictions, we plug a new loss term into the original cross entropy loss:</p><formula xml:id="formula_4">L(θ) = − ∑ i ˆ y i log y i + α ∑ i ∑ t L t,i + β||θ|| 2<label>(4)</label></formula><p>wherê y i is the gold distribution for sentence i, y i is the predicted distribution, L t,i is one of the above regularizers or combination of these regu- larizers on sentence i, α is the weight for the reg- ularization term, and t is the word position in a sentence.</p><p>Note that we do not consider the modification span of negation and intensity words to preserve the simplicity of the proposed models. Nega- tion scope resolution is another complex problem which has been extensively studied ( <ref type="bibr" target="#b43">Zou et al., 2013;</ref><ref type="bibr" target="#b20">Packard et al., 2014;</ref><ref type="bibr" target="#b6">Fancellu et al., 2016)</ref>, which is beyond the scope of this work. Instead, we resort to sequence LSTMs for encoding sur- rounding contexts at a given position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Non-Sentiment Regularizer (NSR)</head><p>This regularizer constrains that the sentiment dis- tributions of adjacent positions should not vary much if the additional input word x t is not a senti- ment word, formally as follows:</p><formula xml:id="formula_5">L (N SR) t = max(0, D KL (p t ||p t−1 ) − M ) (5)</formula><p>where M is a hyperparameter for margin, p t is the predicted distribution at state of position t, (i.e., h t ), and D KL (p||q) is a symmetric KL divergence defined as follows:</p><formula xml:id="formula_6">D KL (p||q) = 1 2 C ∑ l=1 p(l) log q(l) + q(l) log p(l)<label>(6)</label></formula><p>where p, q are distributions over sentiment labels l and C is the number of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentiment Regularizer (SR)</head><p>The sentiment regularizer constrains that the sen- timent distributions of adjacent positions should drift accordingly if the input word is a sentiment word. Let's revisit the example "It's not an inter- esting movie" again. At position t = 2 (in the backward direction) we see a positive word "in- teresting" so the predicted distribution would be more positive than that at position t = 1 (movie). This is the issue of sentiment drift.</p><p>In order to address the sentiment drift issue, we propose a polarity shifting distribution s c ∈ R C for each sentiment class defined in a lexicon. For instance, a sentiment lexicon may have class labels like strong positive, weakly positive, weakly nega- tive, and strong negative, and for each class, there is a shifting distribution which will be learned by the model. The sentiment regularizer states that if the current word is a sentiment word, the senti- ment distribution drift should be observed in com- parison to the previous position, in more details:</p><formula xml:id="formula_7">p (SR) t−1 = p t−1 + s c(xt)<label>(7)</label></formula><formula xml:id="formula_8">L (SR) t = max(0, D KL (p t ||p (SR) t−1 ) − M )<label>(8)</label></formula><p>where p</p><p>t−1 is the drifted sentiment distribution after considering the shifting sentiment distribu- tion corresponding to the state at position t, c(x t ) is the prior sentiment class of word x t , and s c ∈ θ is a parameter to be optimized but could also be set fixed with prior knowledge. Note that in this way all words of the same sentiment class share the same drifting distribution, but in a refined setting, we can learn a shifting distribution for each senti- ment word if large-scale datasets are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Negation Regularizer (NR)</head><p>The negation regularizer approaches how negation words shift the sentiment distribution of the modi- fied text. When the input x t is a negation word, the sentiment distribution should be shifted/reversed accordingly. However, the negation role is more complex than that by sentiment words, for exam- ple, the word "not" in "not good" and "not bad" have different roles in polarity change. The former changes the polarity to negative, while the latter changes to neutral instead of positive.</p><p>To respect such complex negation effects, we propose a transformation matrix T m ∈ R C×C for each negation word m, and the matrix will be learned by the model. The regularizer assumes that if the current position is a negation word, the sentiment distribution of the current position should be close to that of the next or previous po- sition with the transformation.</p><formula xml:id="formula_10">p (N R) t−1 = sof tmax(T x j × p t−1 )<label>(9)</label></formula><formula xml:id="formula_11">p (N R) t+1 = sof tmax(T x j × p t+1 )<label>(10)</label></formula><formula xml:id="formula_12">L (N R) t = min { max(0, D KL (p t ||p (N R) t−1 ) − M ) max(0, D KL (p t ||p (N R) t+1 ) − M )<label>(11)</label></formula><p>where p (N R) t−1 and p (N R) t+1 is the sentiment distuibu- tion after transformation, T x j ∈ θ is the transfor- mation matrix for a negation word x j , a parameter to be learned during training. In total, we train m transformation matrixs for m negation words. Such negator-specific transformation is in accor- dance with the finding that each negator has its in- dividual negation effect ( <ref type="bibr" target="#b41">Zhu et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Intensity Regularizer (IR)</head><p>Sentiment intensity of a phrase indicates the strength of associated sentiment, which is quite important for fine-grained sentiment classification or rating. Intensifier can change the valence de- gree of the content word. The intensity regularizer models how intensity words influence the senti- ment valence of a phrase or a sentence.</p><p>The formulation of the intensity effect is quite the same as that in the negation regularizer, but with different parameters of course. For each in- tensity word, there is a transform matrix to favor the different roles of various intensifiers on sen- timent drift. For brevity, we will not repeat the formulas here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Applying Linguistic Regularizers to Bidirectional LSTM</head><p>To preserve the simplicity of our proposals, we do not consider the modification span of negation and intensity words, which is a quite challenging problem in the NLP community ( <ref type="bibr" target="#b43">Zou et al., 2013;</ref><ref type="bibr" target="#b20">Packard et al., 2014;</ref><ref type="bibr" target="#b6">Fancellu et al., 2016)</ref>. How- ever, we can alleviate the problem by leveraging bidirectional LSTM. For a single LSTM, we employ a backward LSTM from the end to the beginning of a sentence. This is because, at most times, the modified words of negation and intensity words are usually at the right side of the modified text. But sometimes, the modified words are at the left side of negation and intensity words. To better address this issue, we employ bidirectional LSTM and let the model de- termine which side should be chosen.</p><p>More formally, in Bi-LSTM, we compute a transformed sentiment distribution on − → p t−1 of the forward LSTM and also that on ← − p t+1 of the back- ward LSTM, and compute the minimum distance of the distribution of the current position to the two distributions. This could be formulated as follows: Note that R ∈ {N R, IR} indicating the formu- lation works for both negation and intensity regu- larizers.</p><formula xml:id="formula_13">− → p (R) t−1 = sof tmax(T x j × − → p t−1 )<label>(12)</label></formula><formula xml:id="formula_14">← − p (R) t+1 = sof tmax(T x j × ← − p t+1 )<label>(13)</label></formula><formula xml:id="formula_15">L (R) t = min { max(0, D KL ( − → p t || − → p (R) t−1 ) − M ) max(0, D KL ( ← − p t || ← − p (R) t+1 ) − M )<label>(14)</label></formula><p>Due to the same consideration, we redefine L (N SR) t and L (SR) t with bidirectional LSTM simi- larly. The formulation is the same and omitted for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>Our models address these linguistic factors with mathematical operations, parameterized with shifting distribution vectors or transformation ma- trices. In the sentiment regularizer, the senti- ment shifting effect is parameterized with a class- specific distribution (but could also be word- specific if with more data). In the negation and intensity regularizers, the effect is parameterized with word-specific transformation matrices. This is to respect the fact that the mechanism of how negation and intensity words shift sentiment ex- pression is quite complex and highly dependent on individual words. Negation/Intensity effect also depends on the syntax and semantics of the mod- ified text, however, for simplicity we resort to se- quence LSTM for encoding surrounding contexts in this paper. We partially address the modification scope issue by applying the minimization operator in Eq. 11 and Eq. 14, and the bidirectional LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Sentiment Lexicon</head><p>Two datasets are used for evaluating the proposed models: Movie Review (MR) <ref type="bibr" target="#b21">(Pang and Lee, 2005</ref>) where each sentence is annotated with two classes as negative, positive and Stanford Senti- ment Treebank (SST) <ref type="bibr" target="#b30">(Socher et al., 2013</ref>) with five classes { very negative, negative, neutral, pos- itive, very positive}. Note that SST has provided phrase-level annotation on all inner nodes, but we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level anno- tation.</p><p>The sentiment lexicon contains two parts. The first part comes from MPQA ( <ref type="bibr" target="#b39">Wilson et al., 2005</ref>), which contains 5, 153 sentiment words, each with polarity rating. The second part consists of the leaf nodes of the SST dataset (i.e., all sentiment words) and there are 6, 886 polar words except neural ones. We combine the two parts and ignore those words that have conflicting sentiment labels, and produce a lexicon of 9, 750 words with 4 senti- ment labels. For negation and intensity words, we collect them manually since the number is small, some of which can be seen in <ref type="table" target="#tab_1">Table 2.   Dataset  MR  SST  # sentences in total  10,662 11,885  #sen containing sentiment word 10,446 11,211  #sen containing negation word  1,644  1,832  #sen containing intensity word  2,687  2,472   Table 1</ref>: The data statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Details of Experiment Setting</head><p>In order to let others reproduce our results, we present all the details of our models. We adopt Glove vectors ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>) as the ini- tial setting of word embeddings V . The shifting vector for each sentiment class (s c ), and the trans- formation matrices for negation and intensity (T m ) are initialized with a prior value. The other pa- rameters for hidden layers (W ( * ) , U ( * ) , S) are ini- tialized with U nif orm(0, 1/sqrt(d)), where d is the dimension of hidden representation, and we set d=300. We adopt adaGrad to train the models, and the learning rate is 0.1. It's worth noting that, we adopt stochastic gradient descent to update the word embeddings (V ), with a learning rate of 0.2 but without momentum. The optimal setting for α and β is 0.5 and 0.0001 respectively. During training, we adopt the dropout operation before the softmax layer, with a probability of 0.5. Mini-batch is taken to train the models, each batch containing 25 samples. After training with 3,000 mini-batch (about 9 epochs on MR and 10 epochs on SST), we choose the results of the model that performs best on the validation dataset as the final performance.</p><p>Negation word no, nothing, never, neither, not, seldom, scarcely, etc.</p><p>Intensity word terribly, greatly, absolutely, too, very, completely, etc. <ref type="table">Table 2</ref>: Examples of negation and intensity words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overall Comparison</head><p>We include several baselines, as listed below: RNN/RNTN: Recursive Neural Network over parsing trees, proposed by <ref type="bibr" target="#b29">(Socher et al., 2011)</ref> and Recursive Tensor Neural Network ( <ref type="bibr" target="#b30">Socher et al., 2013</ref>) employs tensors to model correlations between different dimensions of child nodes' vec- tors.</p><p>LSTM/Bi-LSTM: Long Short-Term Memory ( ) and the bidirectional variant as introduced previously. Tree-LSTM: Tree-Structured Long Short-Term Memory ( <ref type="bibr" target="#b32">Tai et al., 2015</ref>) introduces memory cells and gates into tree-structured neural network.</p><p>CNN: Convolutional Neural Network <ref type="bibr" target="#b12">(Kalchbrenner et al., 2014</ref>) generates sentence represen- tation by convolution and pooling operations.</p><p>CNN-Tensor: In <ref type="figure" target="#fig_0">(Lei et al., 2015)</ref>, the convo- lution operation is replaced by tensor product and a dynamic programming is applied to enumerate all skippable trigrams in a sentence. Very strong results are reported.</p><p>DAN: Deep Average Network (DAN) <ref type="bibr" target="#b10">(Iyyer et al., 2015</ref>) averages all word vectors in a sen- tence and connects an MLP layer to the output layer.</p><p>Neural Context-Sensitive Lexicon: NCSL ( <ref type="bibr" target="#b33">Teng et al., 2016</ref>) treats the sentiment score of a sentence as a weighted sum of prior scores of words in the sentence where the weights are learned by a neural network.  <ref type="table">Table 3</ref>: The accuracy on MR and SST. Phrase- level means the models use phrase-level annota- tion for training. And Sent.-level means the mod- els only use sentence-level annotation. Results marked with * are re-printed from the references, while those with # are obtained either by our own implementation or with the same codes shared by the original authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Firstly, we evaluate our model on the MR dataset and the results are shown in <ref type="table">Table 3</ref>. We have the following observations:</p><p>First, both LR-LSTM and LR-Bi-LSTM out- performs their counterparts (81.5% vs. 77.4% and 82.1% vs. 79.3%, resp.), demonstrating the ef- fectiveness of the linguistic regularizers. Second, LR-LSTM and LR-Bi-LSTM perform slightly bet- ter than Tree-LSTM but Tree-LSTM leverages a constituency tree structure while our model is a simple sequence model. As future work, we will apply such regularizers to tree-structured models.</p><p>Last, on the MR dataset, our model is compa- rable to or slightly better than CNN.</p><p>For fine-grained sentiment classification, we evaluate our model on the SST dataset which has five sentiment classes { very negative, negative, neutral, positive, very positive} so that we can evaluate the sentiment shifting effect of intensity words. The results are shown in <ref type="table">Table 3</ref>. We have the following observations:</p><p>First, linguistically regularized LSTM and Bi- LSTM are better than their counterparts. It's worth noting that LR-Bi-LSTM (trained with just sentence-level annotation) is even comparable to Bi-LSTM trained with phrase-level annotation. That means, LR-Bi-LSTM can avoid the heavy phrase-level annotation but still obtain compara- ble results.</p><p>Second, our models are comparable to Tree- LSTM but our models are not dependent on a parsing tree and more simple, and hence more efficient. Further, for Tree-LSTM, the model is heavily dependent on phrase-level annotation, oth- erwise the performance drops substantially (from 51% to 48.1%).</p><p>Last, on the SST dataset, our model is better than CNN, DAN, and NCSL. We conjecture that the strong performance of CNN-Tensor may be due to the tensor product operation, the enumer- ation of all skippable trigrams, and the concate- nated representations of all pooling layers for final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The Effect of Different Regularizers</head><p>In order to reveal the effect of each individual reg- ularizer, we conduct ablation experiments. Each time, we remove a regularizer and observe how the performance varies. First of all, we conduct this experiment on the entire datasets, and then we experiment on sub-datasets that only contain nega- tion words or intensity words.</p><p>The experiment results are shown in <ref type="table" target="#tab_1">Table 4</ref> where we can see that the non-sentiment regular- izer (NSR) and sentiment regularizer (SR) play a key role 3 , and the negation regularizer and in-Method MR SST LR-Bi-LSTM 82.1 48.6 LR-Bi-LSTM (-NSR) 80.8 46.9 LR-Bi-LSTM <ref type="table">(-SR)</ref> 80.6 46.9 LR-Bi-LSTM <ref type="table">(-NR)</ref> 81.2 47.6 LR-Bi-LSTM <ref type="table">(-IR)</ref> 81.7 47.9 LR-LSTM 81.5 48.2 LR-LSTM <ref type="table">(-NSR)</ref> 80.2 46.4 LR-LSTM <ref type="table">(-SR)</ref> 80.2 46.6 LR-LSTM <ref type="table">(-NR)</ref> 80.8 47.4 LR-LSTM <ref type="table">(-IR)</ref> 81.2 47.4 tensity regularizer are effective but less important than NSR and SR. This may be due to the fact that only 14% of sentences contains negation words in the test datasets, and 23% contains intensity words, and thus we further evaluate the models on two subsets, as shown in <ref type="table" target="#tab_3">Table 5</ref>.</p><p>The experiments on the subsets show that: 1) With linguistic regularizers, LR-Bi-LSTM outper- forms Bi-LSTM remarkably on these subsets; 2) When the negation regularizer is removed from the model, the performance drops significantly on both MR and SST subsets; 3) Similar observations can be found regarding the intensity regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Neg  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">The Effect of the Negation Regularizer</head><p>To further reveal the linguistic role of negation words, we compare the predicted sentiment distri- butions of a phrase pair with and without a nega- tion word. The experimental results performed on MR are shown in <ref type="figure">Fig. 2</ref>. Each dot denotes a phrase words, see Tab. 1.</p><p>pair (for example, &lt;interesting, not interesting&gt;), where the x-axis denotes the positive score 4 of a phrase without negators (e.g., interesting), and the y-axis indicates the positive score for the phrase with negators (e.g., not interesting). The curves in the figures show this function:</p><formula xml:id="formula_16">[1 − y, y] = sof tmax(T nw * [1 − x, x]) where [1 − x, x] is a sentiment distribution on [negative, positive], x</formula><p>is the positive score of the phrase without negators (x-axis) and y that of the phrase with negators (y- axis), and T nw is the transformation matrix for the negation word nw (see Eq. 9). By looking into the <ref type="figure">Figure 2</ref>: The sentiment shifts with negators. Each dot &lt; x, y &gt; indicates that x is the sentiment score of a phrase without negator and y is that of the phrase with a negator.</p><p>detailed results of our model, we have the follow- ing statements: First, there is no dot at the up-right and bottom- left blocks, indicating that negators generally shift/convert very positive or very negative phrases to other polarities. Typical phrases include not very good, not too bad.</p><p>Second, the dots at the up-left and bottom-right respectively indicates the negation effects: chang- ing negative to positive and positive to negative. Typical phrases include never seems hopelessly (up-left), no good scenes (bottom-right), not in- teresting (bottom-right), etc. There are also some positive/negative phrases shifting to neutral senti- ment such as not so good, and not too bad.</p><p>Last, the dots located at the center indicate that neutral phrases maintain neutral sentiment with negators. Typical phrases include not at home, not here, where negators typically modify non- sentiment words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">The Effect of the Intensity Regularizer</head><p>To further reveal the linguistic role of inten- sity words, we perform experiments on the SST dataset, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. We show the matrix that indicates how the sentiment shifts af- ter being modified by intensifiers. Each number in a cell (m ij ) indicates how many phrases are predicted with a sentiment label i but the predic- tion of the phrases with intensifiers changes to la- bel j. For instance, the number 20 (m 21 ) in the second matrix , means that there are 20 phrases predicted with a class of negative (-) but the pre- diction changes to very negative (--) after being modified by intensifier "very". Results in the first matrix show that, for intensifier "most", there are 21/21/13/12 phrases whose sentiment is shifted af- ter being modified by intensifiers, from negative to very negative (eg. most irresponsible picture), positive to very positive (eg. most famous author), neutral to negative (eg. most plain), and neutral to positive (eg. most closely), respectively.</p><p>There are also many phrases retaining the senti- ment after being modified with intensifiers. Not surprisingly, for very positive/negative phrases, phrases modified by intensifiers still maintain the strong sentiment. For the left phrases, they fall into three categories: first, words modified by in- tensifiers are non-sentiment words, such as most of us, most part; second, intensifiers are not strong enough to shift sentiment, such as most complex (from neg. to neg.), most traditional (from pos. to pos.); third, our models fail to shift sentiment with intensifiers such as most vital, most resonant film.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We present linguistically regularized LSTMs for sentence-level sentiment classification. The pro- posed models address the sentient shifting effect of sentiment, negation, and intensity words. Fur- thermore, our models are sequence LSTMs which do not depend on a parsing tree-structure and do not require expensive phrase-level annotation. Re- sults show that our models are able to address the linguistic role of sentiment, negation, and intensity words.</p><p>To preserve the simplicity of the proposed mod- els, we do not consider the modification scope of negation and intensity words, though we partially address this issue by applying a minimization op- erartor (see <ref type="bibr">Eq. 11, Eq. 14)</ref> and bi-directional LSTM. As future work, we plan to apply the lin- guistic regularizers to tree-LSTM to address the scope issue since the parsing tree is easier to indi- cate the modification scope explicitly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of Linguistically Regularized LSTM. Note that we apply a backward LSTM (from right to left) to encode sentence since most negators and intensifiers are modifying their following words.</figDesc><graphic url="image-1.png" coords="3,307.27,235.02,226.77,131.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t+1 are the sentiment distribu- tions transformed from the previous distribution − → p t−1 and next distribution ← − p t+1 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The sentiment shifting with intensifiers. The number in cell(m ij ) indicates how many phrases are predicted with sentiment label i but the prediction of phrases with intensifiers changes to label j.</figDesc><graphic url="image-3.png" coords="9,101.76,211.25,158.71,86.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The accuracy for LR-Bi-LSTM and LR-
LSTM with regularizer ablation. NSR, SR, NR and 
IR denotes Non-sentiment Regularizer, Sentiment 
Regularizer, Negation Regularizer, and Intensity 
Regularizer respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The accuracy on the negation sub-dataset 
(Neg. Sub.) that only contains negators, and in-
tensity sub-dataset (Int. Sub.) that only contains 
intensifiers. 

</table></figure>

			<note place="foot" n="1"> Note that in sequence models, the hidden state of the current position also encodes forward or backward contexts.</note>

			<note place="foot" n="2"> The asterisk denotes the current position.</note>

			<note place="foot" n="3"> Kindly note that almost all sentences contain sentiment</note>

			<note place="foot" n="4"> The score is obtained from the predicted distribution, where 1 means positive and 0 means negative.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly supported by the Na-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How do negation and modality impact on opinions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Chardon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics</title>
		<meeting>the Workshop on ExtraPropositional Aspects of Meaning in Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building sentiment lexicons for all major languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="383" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A statistical parsing framework for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<editor>AAAI. AAAI</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural networks for negation scope detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Fancellu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="495" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The effect of negation on sentiment analysis and retrieval effectiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1827" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment classification of movie reviews using contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representing and resolving negation for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Lapponi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilja</forename><surname>Øvrelid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 12th International Conference on Data Mining Workshops</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Review sentiment scoring via a parse-and-paraphrase paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributional semantic models for affective text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Iosif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2379" to="2392" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks. Presentation at Google, Mountain View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-04-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple negation scope resolution through deep parsing: A semantic solution to a semantic problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Emily</forename><surname>Woodley Packard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextual valence shifters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livia</forename><surname>Polanyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing attitude and affect in text: Theory and applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning tag embeddings and tag-specific composition functions in recursive neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1365" to="1374" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adjective intensity and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raksha</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Astha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Corpus-based discovery of semantic intensity scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Folser-Lussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HTL</title>
		<meeting>NAACL-HTL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lexicon-based methods for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Tofiloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Voll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context-sensitive lexicon features for neural sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duy-Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1629" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dont count, predict! an automatic approach to learning sentiment lexicons for short text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ecnu at semeval-2016 task 7: An enhanced supervised learning method for lexicon sentiment intensity ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feixiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval pages</title>
		<meeting>SemEval pages</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="491" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A regression approach to affective rating of chinese words from anew</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Hsien</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Chun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="121" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on the role of negation in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Montoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on negation and speculation in natural language processing. Association for Computational Linguistics</title>
		<meeting>the workshop on negation and speculation in natural language processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Linguistic structured sparsity in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="786" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An empirical study on the effect of negation words on sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tree kernel-based negation and speculation scope detection with structured syntactic parse features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="968" to="976" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
