<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Retries of Voice Search Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rivka</forename><surname>Levitan</surname></persName>
							<email>rlevitan@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Elson</surname></persName>
							<email>elson@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Retries of Voice Search Queries</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="230" to="235"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When a system fails to correctly recognize a voice search query, the user will frequently retry the query, either by repeating it exactly or rephrasing it in an attempt to adapt to the system&apos;s failure. It is desirable to be able to identify queries as retries both offline, as a valuable quality signal, and online, as contextual information that can aid recognition. We present a method than can identify retries offline with 81% accuracy using similarity measures between two subsequent queries as well as system and user signals of recognition accuracy. The retry rate predicted by this method correlates significantly with a gold standard measure of accuracy, suggesting that it may be useful as an offline predictor of accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With ever more capable smartphones connecting users to cloud-based computing, voice has been a rapidly growing modality for searching for infor- mation online. Our voice search application con- nects a speech recognition service with a search engine, providing users with structured answers to questions, Web results, voice actions such as set- ting an alarm, and more. In the multimodal smart- phone interface, users can press a button to ac- tivate the microphone, and then speak the query when prompted by a beep; after receiving results, the microphone button is available if they wish to follow up with a subsequent voice query.</p><p>Traditionally, the evaluation of speech recogni- tion systems has been carried by preparing a test set of annotated utterances and comparing the ac- curacy of a system's transcripts of those utterances against the annotations. In particular, we seek to measure and minimize the word error rate (WER) of a system, with a WER of zero indicating perfect transcription. For voice search interfaces such as the present one, though, query-level metrics like WER only tell part of the story. When a user is- sues two queries in a row, she might be seeking the same information for a second time due to a sys- tem failure the first time. When this happens, from an evaluation standpoint it is helpful to break down why the first query was unsuccessful: it might be a speech recognition issue (in particular, a mis- taken transcription), a search quality issue (where a correct transcript is interpreted incorrectly by the semantic understanding systems), a user interface issue, or another factor. As a second voice query may also be a new query or a follow-up query, as opposed to a retry of the first query, the detection of voice search retry pairs in the query steam is non-trivial.</p><p>Correctly identifying a retry situation in the query stream has two main benefits. The first involves offline evaluation and monitoring. We would like to know the rate at which users were forced to retry their voice queries, as a measure of quality. The second has a more immediate ben- efit for individual users: if we can detect in real time that a new voice search is really a retry of a previous voice search, we can take immediate cor- rective action, such as reranking transcription hy- potheses to avoid making the same mistake twice, or presenting alternative searches in the user inter- face to indicate that the system acknowledges it is having difficulty.</p><p>In this paper, we describe a method for the clas- sification of subsequent voice searches as either retry pairs of a certain type, or non-retry pairs. We identify four salient types of retry pairs, describe a test set and identify the features we extracted to build an automatic classifier. We then describe the models we used to build the classifier and their rel-ative performance on the task, and leave the issue of real-time corrective action to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous work in voice-enabled information re- trieval has investigated the problem of identifying voice retries, and some has taken the additional step of taking corrective action in instances where the user is thought to be retrying an earlier utter- ance. Zweig (2009) describes a system switching approach in which the second utterance is recog- nized by a separate model, one trained differently than the primary model. The "backup" system is found to be quite effective at recognizing those utterances missed by the primary system. Retry cases are identified with joint language modeling across multiple transcripts, with the intuition that retry pairs tend to be closely related or exact dupli- cates. They also propose a joint acoustic model in which portions of both utterances are averaged for feature extraction. <ref type="bibr">Zweig et al. (2008)</ref> similarly create a joint decoding model under the assump- tion that a discrete sent of entities (names of busi- nesses with directory information) underlies both queries. While we follow this work in our usage of joint language modeling, our application encom- passes open domain voice searches and voice ac- tions (such as placing calls), so we cannot use sim- plifying domain assumptions.</p><p>Other approaches include Cevik, <ref type="bibr" target="#b1">Weng and Lee (2008)</ref>, who use dynamic time warping to de- fine pattern boundaries using spectral features, and then consider the best matching patterns to be re- peated. <ref type="bibr">Williams (2008)</ref> measures the overlap be- tween the two utterances' n-best lists (alternate hy- potheses) and upweights hypotheses that are com- mon to both attempts; similarly, Orlandi, Culy and Franco (2003) remove hypotheses that are seman- tically equivalent to a previously rejected hypoth- esis. Unlike these approaches, we do not assume a strong notion of dialog state to maintain per-state models.</p><p>Another consequence of the open-domain na- ture of our service is that users are conditioned to interact with the system as they would with a search engine, e.g., if the results of a search do not satisfy their information need, they rephrase queries in order to refine their results. This can happen even if the first transcript was correct and the rephrased query can be easily confused for a retry of a utterance where the recognition failed. For purposes of latently monitoring the accuracy of the recognizer from usage logs, this is a signifi- cant complicating factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data and Annotation</head><p>Our data consists of pairs of queries sampled from anonymized session logs. We consider a pair of voice searches (spoken queries) to be a potential retry pair if they are consecutive; we assume that a voice search cannot be a retry of another voice search if a typed search occurs between them. We also exclude pairs for which either member has no recognition result. For the purpose of our analy- sis, we further restricted our data to query pairs whose second member had been previously ran- domly selected for transcription. A set of 8,254 query pairs met these requirements and are consid- ered potential retry pairs. 1,000 randomly selected pairs from this set were separated out and anno- tated by the authors, leaving a test set of 7,254 po- tential retry pairs. Among the annotated develop- ment set, 18 inaudible or unintelligible pairs were discarded, for a final development set of 982 pairs.</p><p>The problem as we have formulated it requires a labeling system that identifies repetitions and rephrases as retries, while excluding query pairs that are superficially similar but have different search intents. Our system includes five labels. <ref type="figure" target="#fig_0">Figure 1</ref> shows the guidelines for annotation that define each category.</p><p>The first distinction is between query pairs with the same search intent ("Is the user looking for the same information?") and those with different search intents. We define search intent as the re- sponse the user wants and expects from the sys- tem. If the second query's search intent is differ- ent, it is by definition no retry.</p><p>The second distinction we make is between cases where the first query was recognized cor-rectly and those where it was not. Although a query that was recognized correctly may be retried-for example, the user may want to be reminded of information she already received (other)-we are only interested in cases where the system is in error.</p><p>If the search intent is the same for both queries, and the system incorrectly recognized the first, we consider the second query a retry. We dis- tinguish between cases where the user repeated the query exactly, repetition, and where the user rephrased the query in an attempt to adapt to the system's failure, rephrase. This category includes many kinds of rephrasings, such as adding or drop- ping terms, or replacing them with synonyms. The rephrased query may be significantly differ- ent from the original, as in the following example: The rephrased query dropped a term ("Navigate to") and added another ("Little Italy Baltimore").</p><p>This example illustrates another difficulty of the data: the unreliability of the automatic speech recognition (ASR) means that terms that are in fact identical ("Chiapparelli's") may be recog- nized very differently ("chaparral ease" or "chip- per rally's"). In the next example, the recognition hypotheses of two identical queries have only a single word in common:</p><p>Q1. I get in the house Google. ("I did it Google") Q2. I did it crash cool. ("I did it Google") Conversely, recognition hypotheses that are nearly identical are not necessarily retries. Often, these are "serial queries," a series of queries the user is making of the same form or on the same topic, often to test the system. These complementary problems mean that we cannot use na¨ıvena¨ıve text similarity features to identify retries. Instead, we combine features that model the first query's likely accuracy to broader similar- ity features to form a more nuanced picture of a likely retry.</p><p>The five granular retry labels were collapsed into binary categories: search retry, other, and no retry were mapped to NO RETRY; and repetition and rephrase were mapped to RETRY. The label distribution of the final dataset is shown in <ref type="figure" target="#fig_3">Figure  2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Features</head><p>The features we consider can be divided into three main categories. The first group of features, sim- ilarity, is intended to measure the similarity be- tween the two queries, as similar queries are (with the above caveats) more likely to be retries. We calculate the edit distance between the two tran- scripts at the character and word level, as well as the two most similar phonetic rewrites. We include both raw and normalized values as features. We also count the number of unigrams the two tran- scripts have in common and the length, absolute and relative, of the longest unigram overlap.</p><p>As we have shown in the previous section, sim- ilarity features alone cannot identify a retry, since ASR errors and user rephrases can result in recog- nition hypotheses that are significantly different from the original query, while a nearly identical pair of queries can have different search intents. Our second group of features, correctness, goes up a level in our labeling decision tree ( <ref type="figure" target="#fig_0">Figure 1</ref>) and attempts to instead answer the question: "Was the first query transcribed incorrectly?" We use the confidence score assigned by the recognizer to the first recognition hypothesis as a measure of the system's opinion of its own performance. Since this score, while informative, may be inaccurate, we also consider signals from the user that might indicate the accuracy of the hypothesis. A boolean feature indicates whether the user interacted with any of the results (structured or unstructured) that were presented by the system in response to the first query, which should constitute an implicit ac- ceptance of the system's recognition hypothesis. The length of the interval between the two queries is another feature, since a query that occurs imme- diately after another is likely to be a retry. We also include the difference and ratio of the two queries' speaking rate, roughly calculated as the number of vowels divided by the audio duration in sec-onds, since a speaker is likely to hyperarticulate (speak more loudly and slowly) after being misun- derstood (( <ref type="bibr">Wade et al., 1992;</ref><ref type="bibr">Oviatt et al., 1996;</ref><ref type="bibr">Levow, 1998;</ref><ref type="bibr" target="#b0">Bell and Gustafson, 1999;</ref><ref type="bibr">Soltau and Waibel, 1998)</ref>).</p><p>The third feature group, recognizability, at- tempts to model the characteristics of a query that is likely to be misrecognized (for the first query of the pair) or is likely to be a retry of a previ- ous query (for the second query). We look at the language model (LM) score and the number of al- ternate pronunciations of the first query, predicting that a misrecognized query will have a lower LM score and more alternate pronunciations. In ad- dition, we look at the number of characters and unigrams and the audio duration of each query, with the intuition that the length of a query may be correlated with its likelihood of being retried (or a retry). This feature group also includes two heuristic features intended to flag the "serial queries" mentioned before: the number of capital- ized words in each query, and whether each one begins with a question word (who, what, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Prediction task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Results</head><p>A logistic regression model was trained on these features to predict the collapsed binary categories of NO RETRY (search retry, other, no retry) vs. RETRY (rephrase, repetition). The results of run- ning this model with each combination of the fea- ture groups are shown in <ref type="table">Table 1</ref> Individually, each feature group peformed sig- nificantly better than the baseline strategy of al- ways predicting NO RETRY (62.4%). Each pair of feature groups performed better than any indi- vidual group, and the final combination of all three feature groups had the highest precision, recall, and accuracy, suggesting that each aspect of the retry conceptualization provides valuable informa- tion to the model. Of the similarity features, the ones that con- tributed significantly in the final model were char- acter edit distance (normalized) and phoneme edit distance (raw and normalized); as expected, re- tries are associated with more similar query pairs. Of the correctness features, high recognizer con- fidence, the presence of a positive reaction from the user such as a link click, and a long inter- val between queries were all negatively associated with retries. The significant recognizability fea- tures included length of the first query in charac- ters (longer queries were less likely to be retried) and the number of capital letters in each query (as our LM is case-sensitive): queries transcribed with more capital letters were more likely to be retried, but less likely to themselves be retries. In addition, the language model likelihood for the first query was, as expected, significantly lower for retries. Interestingly, the score of the second query was lower for retries as well. This accords with our finding that retries of misrecognized queries are themselves misrecognized 60%-70% of the time, which highlights the potential value of corrective action informed by the retry context.</p><p>Several features, though not significant in the model, are significantly different between the RETRY and NO RETRY categories, which affords us further insight into the characteristics of a retry. T -tests between the two categories showed that all edit distance features-character, word, reduced, and phonetic; raw and normalized-are signifi- cantly more similar between retry query pairs. 1 Similarly, the number of unigrams the two queries have in common is significantly higher for retries. The duration of each member of the query pair, in seconds and word count, is significantly more similar between retry pairs, and each member of a retry pair tends to be shorter than members of a no retry pair. Finally, members of NO RETRY query pairs were significantly more similar in speaking rate, and the relative speaking rate of the second query was significantly slower for RETRY pairs, possibly due to hyperarticulation. <ref type="figure" target="#fig_4">Figure 3</ref> shows a breakdown of the true granular labels versus the predicted binary labels. The pri- mary source of error is the REPHRASE category, which is identified as a retry with only 16.5% ac- curacy. This result reflects the fact that although rephrases conceptually belong in the retry cate- gory, their characteristics are materially different. Most notably, all edit distance features are signif- icantly greater for rephrases. Differences in du- ration between the two queries in a pair, in sec- onds and words, are significantly greater as well. Rephrases also are significantly longer, in seconds and words, than strict retries. The model includ- ing only correctness and recognizability features does significantly better on rephrases than the full model, identifying them as retries with 25.6% ac- curacy, confirming that the similarity features are the primary culprit. Future work may address this issue by including features crafted to examine the similarity between substrings of the two queries, rather than the query as a whole, and by expand- ing the similarity definition to include synonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis</head><p>To test the model's performance with a larger, unseen dataset, we looked at how many retries it detected in the test set of potential retry pairs (n=7,254). We do not have retry annotations for this larger set, but we have transcriptions for the first member of each query pair, enabling us to cal- culate the word error rate (WER) of each query's recognition hypothesis, and thus obtain ground truth for half of our retry definition. A perfect model should never predict RETRY when the first query is transcribed correctly (WER==0). As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, our model assigns a RETRY label to approximately 14% of the queries follow- ing an incorrectly recognized search, and only 2% of queries following a correctly recognized search. While this provides us with only a lower bound on our model's error, this significant correlation with an orthogonal accuracy metric shows that we have modeled at least this aspect of retries correctly, and suggests a correlation between retry rate and tradi- tional WER-based evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a method for characterizing re- tries in an unrestricted voice interface to a search system. One particular challenge is the lack of simplifying assumptions based on domain and state (as users may consider the system to be stateless when issuing subsequent queries). We introduce a labeling scheme for retries that en- compasses rephrases-cases in which the user re- worded her query to adapt to the system's error- as well as repetitions.</p><p>Our model identifies retries with 81% accuracy, significantly above baseline. Our error analysis confirms that user rephrasings complicate the bi- nary class separation; an approach that models typical typed rephrasings may help overcome this difficulty. However, our model's performance to- day correlates strongly with an orthogonal accu- racy metric, word error rate, on unseen data. This suggests that "retry rate" is a reasonable offline quality metric, to be considered in context among other metrics and traditional evaluation based on word error rate. alogue sessions. In Proceedings of the 9th Annual Conference of the International Speech Communi- cation Association <ref type="bibr">(INTERSPEECH 2008</ref> In Proceed- ings of the 9th Annual Conference of the Interna- tional Speech Communication Association <ref type="bibr">(INTERSPEECH 2008)</ref>, pages 1157-1160, Brisbane, Aus- tralia.</p><p>Geoffrey Zweig.</p><p>2009.</p><p>New methods for the analysis of repeated utterances.</p><p>In </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Retry annotation decision tree.</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,226.78,136.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Q1. Navigate to chaparral ease. ("Navigate to Chiappar- elli's.") Q2. Chipper rally's Little Italy Baltimore. ("Chiappar- elli's Little Italy Baltimore.")</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Q1. How tall is George Clooney? Q2. How old is George Clooney? Q1. Weather in New York. Q2. Weather in Los Angeles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Retry label distribution.</figDesc><graphic url="image-2.png" coords="3,308.46,63.19,106.59,68.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance on each of the granular categories.</figDesc><graphic url="image-4.png" coords="5,72.00,62.81,226.76,130.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance on unseen data. A perfect model would have a predicted retry rate of 0 when WER==0.</figDesc><graphic url="image-5.png" coords="5,307.28,62.80,181.41,168.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Proceed- ings of the 10th Annual Conference of the Inter- national Speech Communication Association (IN- TERSPEECH 2009), pages 2791-2794, Brighton, United Kingdom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Features 
Precision Recall F1 
Accuracy 
Similarity 
0.54 
0.65 
0.59 0.72 
Correctness 
0.53 
0.67 
0.59 0.73 
Recognizability 0.49 
0.63 
0.55 0.70 
Sim. &amp; Corr. 
0.67 
0.71 
0.69 0.77 
Sim. &amp; Rec. 
0.62 
0.70 
0.66 0.76 
Corr. &amp; Rec. 
0.65 
0.71 
0.68 0.77 
All Features 
0.70 
0.76 
0.73 0.81 

Table 1: Results of the binary prediction task. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>), pages 471-474, Brisbane, Australia.</head><label></label><figDesc></figDesc><table>Gina-Anne Levow. 1998. Characterizing and recog-
nizing spoken corrections in human-computer di-
alogue. In Proceedings of the 17th international 
conference on Computational linguistics-Volume 1, 
pages 736-742. Association for Computational Lin-
guistics. 

Marco Orlandi, Christopher Culy, and Horacio Franco. 
2003. Using dialog corrections to improve speech 
recognition. In Error Handling in Spoken Language 
Dialogue Systems. International Speech Communi-
cation Association. 

Sharon Oviatt, G-A Levow, Margaret MacEachern, 
and Karen Kuhn. 1996. Modeling hyperarticu-
late speech during human-computer error resolu-
tion. In Spoken Language, 1996. ICSLP 96. Pro-
ceedings., Fourth International Conference on, vol-
ume 2, pages 801-804. IEEE. 

Hagen Soltau and Alex Waibel. 1998. On the influ-
ence of hyperarticulated speech on recognition per-
formance. In ICSLP. Citeseer. 

Elizabeth Wade, Elizabeth Shriberg, and Patti Price. 
1992. User behaviors affecting speech recognition. 
In ICSLP. 

Jason D. Williams. 2008. Exploiting the asr n-
best by tracking multiple dialog state hypotheses. 
In Proceedings of the 9th Annual Conference of 
the International Speech Communication Associa-
tion (INTERSPEECH 2008), pages 191-194, Bris-
bane, Australia. 

Geoffrey Zweig, Dan Bohus, Xiao Li, and Patrick 
Nguyen. 
2008. 
Structured models for joint 
decoding of repeated utterances. 
</table></figure>

			<note place="foot">* This work was done while the first author was an intern at Google Inc.</note>

			<note place="foot" n="1"> T-tests reported here use a conservative significance threshold of p &lt; 0.00125 to control for family-wise type I error (&quot;data dredging&quot; effects).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Daisy Stanton and Maryam Kamvar for their helpful comments on this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Repetition and its phonetic realizations: Investigating a swedish database of spontaneous computer-directed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Gustafson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPhS</title>
		<meeting>ICPhS</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1221" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detection of repetitions in spontaneous speech in di</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Cevik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
