<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Models for Image Captioning: The Quirks and What Works</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">♠ University of Washington ♣ University of California at Berkeley</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Models for Image Captioning: The Quirks and What Works</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="100" to="105"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
					<note>Corresponding authors:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Two recent approaches have achieved state-of-the-art results in image caption-ing. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper , we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition , and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent progress in automatic image captioning has shown that an image-conditioned language model can be very effective at generating captions. Two leading approaches have been explored for this task. The first decomposes the problem into an initial step that uses a convolutional neural net- work to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a mini- mum number of the detected words <ref type="bibr" target="#b7">(Fang et al., 2015)</ref>. The second approach uses the activations from final hidden layer of an object detection CNN as the input to a recurrent neural network lan- guage model <ref type="bibr">(RNN LM)</ref>. This is referred to as a Multimodal Recurrent Neural Network (MRNN) <ref type="bibr" target="#b13">Mao et al., 2015;</ref><ref type="bibr" target="#b2">Chen and Zitnick, 2015)</ref>. Similar in spirit is the the log-bilinear (LBL) LM of <ref type="bibr" target="#b11">Kiros et al. (2014)</ref>.</p><p>In this paper, we study the relative merits of these approaches. By using an identical state-of- the-art CNN as the input to RNN-based and ME- based models, we are able to empirically com- pare the strengths and weaknesses of the lan- guage modeling components. We find that the approach of directly generating the text with an MRNN 1 outperforms the ME LM when measured by BLEU on the COCO dataset ( <ref type="bibr" target="#b12">Lin et al., 2014</ref>), 2 but this recurrent model tends to reproduce cap- tions in the training set. In fact, a simple k-nearest neighbor approach, which is common in earlier re- lated work <ref type="bibr" target="#b8">(Farhadi et al., 2010;</ref><ref type="bibr" target="#b14">Mason and Charniak, 2014</ref>), performs similarly to the MRNN. In contrast, the ME LM generates the most novel captions, and does the best at captioning images for which there is no close match in the training data. With a Deep Multimodal Similarity Model (DMSM) incorporated, 3 the ME LM significantly outperforms other methods according to human judgments. In sum, the contributions of this pa- per are as follows:</p><p>1. We compare the use of discrete detections and continuous valued CNN activations as the conditioning information for language models trained to generate image captions.</p><p>2. We show that a simple k-nearest neighbor re- trieval method performs at near state-of-the- art for this task and dataset.</p><p>3. We demonstrate that a state-of-the-art MRNN-based approach tends to reconstruct previously seen captions; in contrast, the two stage ME LM approach achieves similar or better performance while generating relatively novel captions.</p><p>4. We advance the state-of-the-art BLEU scores on the COCO dataset.</p><p>5. We present human evaluation results on the systems with the best performance as mea- sured by automatic metrics.</p><p>6. We explore several issues with the statistical models and the underlying COCO dataset, in- cluding linguistic irregularities, caption repe- tition, and data set overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>All language models compared here are trained using output from the same state-of-the-art CNN.</p><p>The CNN used is the 16-layer variant of VGGNet ( <ref type="bibr" target="#b20">Simonyan and Zisserman, 2014</ref>) which was ini- tially trained for the ILSVRC2014 classification task ( <ref type="bibr" target="#b19">Russakovsky et al., 2015)</ref>, and then fine- tuned on the Microsoft COCO data set <ref type="bibr" target="#b7">(Fang et al., 2015;</ref><ref type="bibr" target="#b12">Lin et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Detector Conditioned Models</head><p>We study the effect of leveraging an explicit de- tection step to find key objects/attributes in images before generation, examining both an ME LM ap- proach as reported in previous work <ref type="bibr" target="#b7">(Fang et al., 2015)</ref>, and a novel LSTM approach introduced here. Both use a CNN trained to output a bag of words indicating the words that are likely to ap- pear in a caption, and both use a beam search to find a top-scoring sentence that contains a subset of the words. This set of words is dynamically ad- justed to remove words as they are mentioned. We refer the reader to <ref type="bibr" target="#b7">Fang et al. (2015)</ref> for a full description of their ME LM approach, whose 500-best outputs we analyze here. <ref type="bibr">4</ref> We also in- clude the output from their ME LM that leverages scores from a Deep Multimodal Similarity Model (DMSM) during n-best re-ranking. Briefly, the DMSM is a non-generative neural network model which projects both the image pixels and caption text into a comparable vector space, and scores their similarity.</p><p>In the LSTM approach, similar to the ME LM approach, we maintain a set of likely words D that <ref type="bibr">4</ref> We will refer to this system as D-ME.</p><p>have not yet been mentioned in the caption un- der construction. This set is initialized to all the words predicted by the CNN above some thresh- old α. <ref type="bibr">5</ref> The words already mentioned in the sentence history h are then removed to produce a set of conditioning words D \ {h}. We in- corporate this information within the LSTM by adding an additional input encoded to represent the remaining visual attributes D \ {h} as a con- tinuous valued auxiliary feature vector <ref type="bibr" target="#b15">(Mikolov and Zweig, 2012)</ref>. This is encoded as f</p><formula xml:id="formula_0">(s h −1 + v∈D\{h} g v + Uq h,D )</formula><p>, where s h −1 and g v are respectively the continuous-space representations for last word h −1 and detector v ∈ D \ {h}, U is learned matrix for recurrent histories, and f (·) is the sigmoid transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Recurrent Neural Network</head><p>In this section, we explore a model directly con- ditioned on the CNN activations rather than a set of word detections. Our implementation is very similar to captioning models described in  In this model, we feed each image into our CNN and retrieve the 4096-dimensional final hid- den layer, denoted as fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500- dimensional representation that serves as the ini- tial hidden state to a gated recurrent neural net- work (GRNN) ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>). The GRNN is trained jointly with H to produce the caption one word at a time, conditioned on the previous word and the previous recurrent state. For decod- ing, we perform a beam search of size 10 to emit tokens until an END token is produced. We use a 500-dimensional GRNN hidden layer and 200- dimensional word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">k-Nearest Neighbor Model</head><p>Both <ref type="bibr" target="#b6">Donahue et al. (2015)</ref> and <ref type="bibr">Karpathy and FeiFei (2015)</ref> present a 1-nearest neighbor baseline. As a first step, we replicated these results using the cosine similarity of the fc7 layer between each test set image t and training image r. We randomly emit one caption from t's most similar training im- age as the caption of t. As reported in previous results, performance is quite poor, with a BLEU <ref type="figure">Figure 1</ref>: Example of the set of candidate captions for an image, the highest scoring m captions (green) and the con- sensus caption (orange). This is a real example visualized in two dimensions. score of 11.2%.</p><p>However, we explore the idea that we may be able to find an optimal k-nearest neighbor consen- sus caption. We first select the k = 90 nearest training images of a test image t as above. We de- note the union of training captions in this set as C = c 1 , ..., c 5k . 6 For each caption c i , we com- pute the n-gram overlap F-score between c i and each other caption in C. We define the consen- sus caption c * to be caption with the highest mean n-gram overlap with the other captions in C. We have found it is better to only compute this average among c i 's m = 125 most similar captions, rather than all of C. The hyperparameters k and m were obtained by a grid search on the validation set.</p><p>A visual example of the consensus caption is given in <ref type="figure">Figure 1</ref>. Intuitively, we are choosing a single caption that may describe many different images that are similar to t, rather than a caption that describes the single image that is most similar to t. We believe that this is a reasonable approach to take for a retrieval-based method for captioning, as it helps ensure incorrect information is not men- tioned. Further details on retrieval-based methods are available in, e.g., <ref type="bibr" target="#b17">(Ordonez et al., 2011;</ref><ref type="bibr" target="#b9">Hodosh et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Microsoft COCO Dataset</head><p>We work with the Microsoft COCO dataset ( <ref type="bibr" target="#b12">Lin et al., 2014</ref>), with 82,783 training images, and the validation set split into 20,243 validation im- ages and 20,244 testval images. Most images con- tain multiple objects and significant contextual in- formation, and each image comes with 5 human- <ref type="bibr">6</ref> Each training image has 5 captions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metrics</head><p>The quality of generated captions is measured au- tomatically using BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>) and METEOR <ref type="bibr" target="#b4">(Denkowski and Lavie, 2014</ref>). BLEU roughly measures the fraction of N -grams (up to 4 grams) that are in common between a hy- pothesis and one or more references, and penalizes short hypotheses by a brevity penalty term. 7 ME- TEOR ( <ref type="bibr" target="#b4">Denkowski and Lavie, 2014</ref>) measures un- igram precision and recall, extending exact word matches to include similar words based on Word- Net synonyms and stemmed tokens. We also re- port the perplexity (PPLX) of studied detection- conditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Comparison</head><p>In <ref type="table">Table 1</ref>, we summarize the generation perfor- mance of our different models. The discrete de- tection based models are prefixed with "D". Some example generated results are show in <ref type="table" target="#tab_1">Table 2</ref>. We see that the detection-conditioned LSTM LM produces much lower PPLX than the detection-conditioned ME LM, but its BLEU score is no better. The MRNN has the lowest PPLX, and highest BLEU among all LMs stud-  Perhaps most surprisingly, the k-nearest neigh- bor algorithm achieves a higher BLEU score than all other models. However, as we will demonstrate in Section 3.5, the generated captions perform sig- nificantly better than the nearest neighbor captions in terms of human quality judgements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">n-best Re-Ranking</head><p>In addition to comparing the ME-based and RNN- based LMs independently, we explore whether combining these models results in an additive im- provement. To this end, we use the 500-best list from the D-ME and add a score for each hypoth- esis from the MRNN. <ref type="bibr">8</ref> We then re-rank the hy- potheses using MERT <ref type="bibr" target="#b16">(Och, 2003)</ref>. As in previous work <ref type="bibr" target="#b7">(Fang et al., 2015)</ref>, model weights were opti- mized to maximize BLEU score on the validation set. We further extend this combination approach to the D-ME model with DMSM scores included during re-ranking ( <ref type="bibr" target="#b7">Fang et al., 2015)</ref>.</p><p>Results are show in <ref type="table" target="#tab_3">Table 3</ref>. We find that com- bining the D-ME, DMSM, and MRNN achieves a 1.6 BLEU improvement over the D-ME+DMSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Human Evaluation</head><p>Because automatic metrics do not always corre- late with human judgments <ref type="bibr" target="#b1">(Callison-Burch et al., 2006;</ref><ref type="bibr" target="#b9">Hodosh et al., 2013)</ref>, we also performed hu- man evaluations using the same procedure as in <ref type="bibr" target="#b7">Fang et al. (2015)</ref>. Here, human judges were pre- sented with an image, a system generated caption, and a human generated caption, and were asked which caption was "better". <ref type="bibr">9</ref> For each condition, 5 judgments were obtained for 1000 images from the testval set. <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results are shown in</head><p>The D- ME+DMSM outperforms the MRNN by 5 per- centage points for the "Better Or Equal to Hu- man" judgment, despite both systems achieving the same BLEU score. The k-Nearest Neighbor system performs 1.4 percentage points worse than the MRNN, despite achieving a slightly higher BLEU score. Finally, the combined model does not outperform the D-ME+DMSM in terms of hu- man judgments despite a 1.6 BLEU improvement.</p><p>Although we cannot pinpoint the exact reason for this mismatch between automated scores and human evaluation, a more detailed analysis of the difference between systems is performed in Sec- tions 4 and 5.  <ref type="table">Table 4</ref>: Results when comparing produced captions to those written by humans, as judged by humans. These are the per- cent of captions judged to be "better than" or "better than or equal to" a caption written by a human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Language Analysis</head><p>Examples of common mistakes we observe on the testval set are shown in <ref type="table" target="#tab_5">Table 5</ref>. The D-ME system has difficulty with anaphora, particularly within the phrase "on top of it", as shown in examples (1), (2), and (3). This is likely due to the fact that is maintains a local context window. In contrast, the MRNN approach tends to generate such anaphoric relationships correctly. However, the D-ME LM maintains an explicit coverage state vector tracking which attributes have already been emitted. The MRNN implicitly maintains the full state using its recurrent layer, which sometimes results in multiple emission mis- takes, where the same attribute is emitted more than once. This is particularly evident when coor- dination ("and") is present (examples (4) and <ref type="formula">(5)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Repeated Captions</head><p>All of our models produce a large number of cap- tions seen in the training and repeated for differ- ent images in the test set, as shown in <ref type="table">Table 6</ref> (also observed by <ref type="bibr" target="#b21">Vinyals et al. (2014)</ref> for their LSTM-based model). There are at least two po- tential causes for this repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D-ME+DMSM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRNN</head><p>1 a slice of pizza sitting on top of it a bed with a red blanket on top of it 2 a black and white bird perched on top of it a birthday cake with candles on top of it 3 a little boy that is brushing his teeth with a toothbrush in her mouth a little girl brushing her teeth with a toothbrush 4 a large bed sitting in a bedroom a bedroom with a bed and a bed 5 a man wearing a bow tie a man wearing a tie and a tie First, the systems often produce generic cap- tions such as "a close up of a plate of food", which may be applied to many publicly available im- ages. This may suggest a deeper issue in the train- ing and evaluation of our models, which warrants more discussion in future work. Second, although the COCO dataset and evaluation server 10 has en- couraged rapid progress in image captioning, there may be a lack of diversity in the data. We also note that although caption duplication is an issue in all systems, it is a greater issue in the MRNN than the D-ME+DMSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Image Diversity</head><p>The strong performance of the k-nearest neighbor algorithm and the large number of repeated cap- tions produced by the systems here suggest a lack of diversity in the training and test data. <ref type="bibr">11</ref> We believe that one reason to work on image captioning is to be able to caption compositionally novel images, where the individual components of the image may be seen in the training, but the en- tire composition is often not.</p><p>In order to evaluate results for only compo- sitionally novel images, we bin the test images based on visual overlap with the training data. For each test image, we compute the fc7 cosine similarity with each training image, and the mean value of the 50 closest images. We then compute BLEU on the 20% least overlapping and 20% most 10 http://mscoco.org/dataset/ 11 This is partially an artifact of the manner in which the Microsoft COCO data set was constructed, since each image was chosen to be in one of 80 pre-defined object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition</head><p>Train  overlapping subsets. Results are shown in <ref type="table" target="#tab_7">Table 7</ref>.</p><p>The D- ME+DMSM outperforms the k-nearest neighbor approach by 2.5 BLEU on the "20% Least" set, even though performance on the whole set is com- parable. Additionally, the D-ME+DMSM out- performs the MRNN by 2.1 BLEU on the "20% Least" set, but performs 2.1 BLEU worse on the "20% Most" set. This is evidence that D- ME+DMSM generalizes better on novel images than the MRNN; this is further supported by the relatively low percentage of captions it gener- ates seen in the training data <ref type="table">(Table 6</ref>) while still achieving reasonable captioning performance. We hypothesize that these are the main reasons for the strong human evaluation results of the D- ME+DMSM shown in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that a gated RNN conditioned di- rectly on CNN activations (an MRNN) achieves better BLEU performance than an ME LM or LSTM conditioned on a set of discrete activations; and a similar BLEU performance to an ME LM combined with a DMSM. However, the ME LM + DMSM method significantly outperforms the MRNN in terms of human quality judgments. We hypothesize that this is partially due to the lack of novelty in the captions produced by the MRNN. In fact, a k-nearest neighbor retrieval algorithm introduced in this paper performs similarly to the MRNN in terms of both automatic metrics and hu- man judgements.</p><p>When we use the MRNN system alongside the DMSM to provide additional scores in MERT re- ranking of the n-best produced by the image- conditioned ME LM, we advance by 1.6 BLEU points on the best previously published results on the COCO dataset. Unfortunately, this improve- ment in BLEU does not translate to improved hu- man quality judgments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Mul- timodal Recurrent Neural Network (MRNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example generated captions. 

annotated captions. The images create a challeng-
ing testbed for image captioning and are widely 
used in recent automatic image captioning work. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Model performance on testval after re-ranking. 
 †: previously reported and reconfirmed BLEU scores from 
(Fang et al., 2015). +DMSM had resulted in the highest score 
yet reported. 

ied in our experiments. It significantly improves 
BLEU by 2.1 absolutely over the D-ME LM base-
line. METEOR is similar across all three LM-
based methods. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example errors in the two basic approaches. 

System 
Unique 
Seen In 
Captions Training 
Human 
99.4% 
4.8% 
D-ME+DMSM 
47.0% 
30.0% 
MRNN 
33.1% 
60.3% 
D-ME+DMSM+MRNN 
28.5% 
61.3% 
k-Nearest Neighbor 
36.6% 
100% 
Table 6: Percentage unique (Unique Captions) and novel 
(Seen In Training) captions for testval images. For example, 
28.5% unique means 5,776 unique strings were generated for 
all 20,244 images. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Performance for different portions of testval, based 
on visual overlap with the training. 

</table></figure>

			<note place="foot" n="1"> In our case, a gated recurrent neural network (GRNN) is used (Cho et al., 2014), similar to an LSTM. 2 This is the largest image captioning dataset to date. 3 As described by Fang et al. (2015).</note>

			<note place="foot" n="5"> In all experiments in this paper, α=0.5.</note>

			<note place="foot" n="7"> We use the length of the reference that is closest to the length of the hypothesis to compute the brevity penalty.</note>

			<note place="foot" n="8"> The MRNN does not produce a diverse n-best list. 9 The captions were randomized and the users were not informed which was which.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Re-evaluation the role of bleu in machine translation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. Comput. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meteor universal: language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL 2014 Workshop Statistical Machine Translation</title>
		<meeting>EACL 2014 Workshop Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<title level="m">Long-term recurrent convolutional networks for visual recognition and description</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. Comput. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From captionons to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Dollá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. Comput. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Every picture tells a story: generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Comput. Vision (ECCV)</title>
		<meeting>European Conf. Comput. Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: data models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intell. Research</title>
		<imprint>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. Comput. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-RNN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Domainspecific image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, ACL &apos;03</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Im2Text: Describing images using 1 million captioned photogrphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show and tell: a neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conf. Comput. Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
