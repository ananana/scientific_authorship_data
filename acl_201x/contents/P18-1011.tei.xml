<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Knowledge Graph Embedding Using Simple Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Information Security</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Knowledge Graph Embedding Using Simple Constraints</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="110" to="121"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>110</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate en-tailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalabil-ity. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/i ieir-km/ComplEx-NNE_AER.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past decade has witnessed great achievements in building web-scale knowledge graphs (KGs), e.g., <ref type="bibr">Freebase (Bollacker et al., 2008</ref>), DBpedia ( <ref type="bibr" target="#b11">Lehmann et al., 2015)</ref>, and Google's Knowledge * Corresponding author: Quan Wang.</p><p>Vault ( <ref type="bibr">Dong et al., 2014)</ref>. A typical KG is a multi- relational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity). Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks <ref type="bibr" target="#b32">(Wasserman-Pritsker et al., 2015;</ref><ref type="bibr" target="#b5">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b37">Yang and Mitchell, 2017)</ref>.</p><p>Recently, the concept of knowledge graph em- bedding has been presented and quickly become a hot research topic. The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent struc- ture of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples <ref type="bibr" target="#b3">(Bordes et al., 2011</ref><ref type="bibr" target="#b2">(Bordes et al., , 2013</ref><ref type="bibr" target="#b6">Jenatton et al., 2012;</ref><ref type="bibr" target="#b23">Nickel et al., 2011</ref>). Recent attempts focused on either designing more complicated triple scoring models <ref type="bibr" target="#b27">(Socher et al., 2013;</ref><ref type="bibr" target="#b1">Bordes et al., 2014;</ref><ref type="bibr" target="#b31">Wang et al., 2014;</ref><ref type="bibr" target="#b13">Lin et al., 2015b;</ref><ref type="bibr" target="#b33">Xiao et al., 2016;</ref><ref type="bibr" target="#b22">Nickel et al., 2016b;</ref><ref type="bibr" target="#b28">Trouillon et al., 2016;</ref><ref type="bibr" target="#b14">Liu et al., 2017)</ref>, or incorporating extra informa- tion beyond KG triples ( <ref type="bibr">Chang et al., 2014;</ref><ref type="bibr" target="#b39">Zhong et al., 2015;</ref><ref type="bibr" target="#b12">Lin et al., 2015a;</ref><ref type="bibr" target="#b20">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b30">Guo et al., 2015;</ref><ref type="bibr" target="#b16">Luo et al., 2015b;</ref><ref type="bibr">Xie et al., 2016a,b;</ref><ref type="bibr" target="#b34">Xiao et al., 2017)</ref>. See ( <ref type="bibr" target="#b29">Wang et al., 2017</ref>) for a thorough review. This paper, by contrast, investigates the poten- tial of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate en- tailment constraints over relation representations. By using the former, we learn compact represen- tations for entities, which would naturally induce sparsity and interpretability <ref type="bibr" target="#b19">(Murphy et al., 2012)</ref>. By using the latter, we further encode regularities of logical entailment between relations into their distributed representations, which might be advan- tageous to downstream tasks like link prediction and relation extraction <ref type="bibr" target="#b24">(Rocktäschel et al., 2015;</ref><ref type="bibr">Guo et al., 2016</ref>). These constraints impose prior beliefs upon the structure of the embedding space, and will help us to learn more predictive embed- dings, without significantly increasing the space or time complexity.</p><p>Our work has some similarities to those which integrate logical background knowledge into KG embedding <ref type="bibr" target="#b24">(Rocktäschel et al., 2015;</ref><ref type="bibr">Guo et al., 2016</ref><ref type="bibr" target="#b4">Guo et al., , 2018</ref>). Most of such works, however, need grounding of first-order logic rules. The grounding process could be time and space in- efficient especially for complicated rules. To avoid grounding, <ref type="bibr">Demeester et al. (2016)</ref> tried to model rules using only relation representations. But their work creates vector representations for entity pairs rather than individual entities, and hence fails to handle unpaired entities. Moreover, it can only in- corporate strict, hard rules which usually require extensive manual effort to create. <ref type="bibr" target="#b18">Minervini et al. (2017b)</ref> proposed adversarial training which can integrate first-order logic rules without grounding. But their work, again, focuses on strict, hard rules. <ref type="bibr" target="#b17">Minervini et al. (2017a)</ref> tried to handle uncertainty of rules. But their work assigns to different rules a same confidence level, and considers only equiva- lence and inversion of relations, which might not always be available in a given KG.</p><p>Our approach differs from the aforementioned works in that: (i) it imposes constraints directly on entity and relation representations without ground- ing, and can easily scale up to large KGs; (ii) the constraints, i.e., non-negativity and approximate entailment derived automatically from statistical properties, are quite universal, requiring no man- ual effort and applicable to almost all KGs; (iii) it learns an individual representation for each enti- ty, and can successfully make predictions between unpaired entities. We evaluate our approach on publicly available KGs of WordNet, Freebase, and DBpedia as well.</p><p>Experimental results indicate that our approach is simple yet surprisingly effective, achieving signif- icant and consistent improvements over competi- tive baselines, but without negative impacts on ef- ficiency or scalability. The non-negativity and ap- proximate entailment constraints indeed improve model interpretability, resulting in a substantially increased structuring of the embedding space.</p><p>The remainder of this paper is organized as fol- lows. We first review related work in Section 2, and then detail our approach in Section 3. Exper- iments and results are reported in Section 4, fol- lowed by concluding remarks in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years have seen growing interest in learn- ing distributed representations for entities and re- lations in KGs, a.k.a. KG embedding. Early works on this topic devised very simple models to learn such distributed representations, solely on the ba- sis of triples observed in a given KG, e.g., TransE which takes relations as translating operations be- tween head and tail entities ( <ref type="bibr" target="#b2">Bordes et al., 2013)</ref>, and RESCAL which models triples through bilin- ear operations over entity and relation representa- tions <ref type="bibr" target="#b23">(Nickel et al., 2011</ref>). Later attempts roughly fell into two groups: (i) those which tried to design more complicated triple scoring models, e.g., the TransE extensions ( <ref type="bibr" target="#b31">Wang et al., 2014;</ref><ref type="bibr" target="#b13">Lin et al., 2015b;</ref><ref type="bibr" target="#b7">Ji et al., 2015)</ref>, the RESCAL extensions ( <ref type="bibr" target="#b38">Yang et al., 2015;</ref><ref type="bibr" target="#b22">Nickel et al., 2016b;</ref><ref type="bibr" target="#b28">Trouillon et al., 2016;</ref><ref type="bibr" target="#b14">Liu et al., 2017)</ref>, and the (deep) neural network models <ref type="bibr" target="#b27">(Socher et al., 2013;</ref><ref type="bibr" target="#b1">Bordes et al., 2014;</ref><ref type="bibr" target="#b26">Shi and Weninger, 2017;</ref><ref type="bibr" target="#b25">Schlichtkrull et al., 2017;</ref><ref type="bibr">Dettmers et al., 2018</ref>); (ii) those which tried to integrate extra information beyond triples, e.g., entity types ( <ref type="bibr" target="#b30">Guo et al., 2015;</ref><ref type="bibr" target="#b36">Xie et al., 2016b</ref>), relation paths ( <ref type="bibr" target="#b20">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b12">Lin et al., 2015a)</ref>, and textual descriptions ( <ref type="bibr" target="#b35">Xie et al., 2016a;</ref><ref type="bibr" target="#b34">Xiao et al., 2017)</ref>. Please refer to ( <ref type="bibr" target="#b21">Nickel et al., 2016a;</ref><ref type="bibr" target="#b29">Wang et al., 2017</ref>) for a thorough review of these techniques. In this paper, we show the po- tential of using very simple constraints (i.e., non- negativity constraints and approximate entailmen- t constraints) to improve KG embedding, without significantly increasing the model complexity.</p><p>A line of research related to ours is KG embed- ding with logical background knowledge incorpo- rated ( <ref type="bibr" target="#b24">Rocktäschel et al., 2015;</ref><ref type="bibr">Guo et al., 2016</ref><ref type="bibr" target="#b4">Guo et al., , 2018</ref>. But most of such works require grounding of first-order logic rules, which is time and space inefficient especially for compli- cated rules. To avoid grounding, <ref type="bibr">Demeester et al. (2016)</ref> proposed lifted rule injection, and Minervi- ni et al. (2017b) investigated adversarial training. Both works, however, can only handle strict, hard rules which usually require extensive effort to cre- ate. <ref type="bibr" target="#b17">Minervini et al. (2017a)</ref> tried to handle uncer- tainty of background knowledge. But their work considers only equivalence and inversion between relations, which might not always be available in a given KG. Our approach, in contrast, imposes con- straints directly on entity and relation representa- tions without grounding. And the constraints used are quite universal, requiring no manual effort and applicable to almost all KGs.</p><p>Non-negativity has long been a subject studied in various research fields. Previous studies reveal that non-negativity could naturally induce sparsity and, in most cases, better interpretability ( <ref type="bibr" target="#b10">Lee and Seung, 1999</ref>). In many NLP-related tasks, non- negativity constraints are introduced to learn more interpretable word representations, which capture the notion of semantic composition ( <ref type="bibr" target="#b19">Murphy et al., 2012;</ref><ref type="bibr" target="#b15">Luo et al., 2015a;</ref><ref type="bibr">Fyshe et al., 2015)</ref>. In this paper, we investigate the ability of non-negativity constraints to learn more accurate KG embeddings with good interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>This section presents our approach. We first intro- duce a basic embedding technique to model triples in a given KG ( § 3.1). Then we discuss the non- negativity constraints over entity representations ( § 3.2) and the approximate entailment constraints over relation representations <ref type="bibr">( § 3.3)</ref>. And finally we present the overall model ( § 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Basic Embedding Model</head><p>We choose <ref type="bibr">ComplEx (Trouillon et al., 2016)</ref> as our basic embedding model, since it is simple and effi- cient, achieving state-of-the-art predictive perfor- mance. Specifically, suppose we are given a KG containing a set of triples O = {(e i , r k , e j )}, with each triple composed of two entities e i , e j ∈ E and their relation r k ∈ R. Here E is the set of entities and R the set of relations. ComplEx then repre- sents each entity e ∈ E as a complex-valued vector e ∈ C d , and each relation r ∈ R a complex-valued vector r ∈ C d , where d is the dimensionality of the embedding space. Each x ∈ C d consists of a real vector component <ref type="bibr">Re(x)</ref> and an imaginary vector component Im(x), i.e., x = Re(x) + iIm(x). For any given triple (e i , r k , e j ) ∈ E × R × E, a multi- linear dot product is used to score that triple, i.e.,</p><formula xml:id="formula_0">φ(e i , r k , e j ) Re(e i , r k , ¯ e j )</formula><p>Re(</p><formula xml:id="formula_1">[e i ] [r k ] [¯ e j ] ),<label>(1)</label></formula><p>where e i , r k , e j ∈ C d are the vectorial representa- tions associated with e i , r k , e j , respectively; ¯ e j is the conjugate of e j ; <ref type="bibr">[·]</ref> is the -th entry of a vector; and Re(·) means taking the real part of a complex value. Triples with higher φ(·, ·, ·) scores are more likely to be true. Owing to the asymmetry of this scoring function, i.e., φ(e i , r k , e j ) = φ(e j , r k , e i ), ComplEx can effectively handle asymmetric rela- tions ( <ref type="bibr" target="#b28">Trouillon et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-negativity of Entity Representations</head><p>On top of the basic ComplEx model, we further re- quire entities to have non-negative (and bounded) vectorial representations. In fact, these distributed representations can be taken as feature vectors for entities, with latent semantics encoded in different dimensions. In ComplEx, as well as most (if not all) previous approaches, there is no limitation on the range of such feature values, which means that both positive and negative properties of an entity can be encoded in its representation. However, as pointed out by <ref type="bibr" target="#b19">Murphy et al. (2012)</ref>, it would be uneconomical to store all negative properties of an entity or a concept. For instance, to describe cats (a concept), people usually use positive properties such as cats are mammals, cats eat fishes, and cats have four legs, but hardly ever negative properties like cats are not vehicles, cats do not have wheels, or cats are not used for communication.</p><p>Based on such intuition, this paper proposes to impose non-negativity constraints on entity repre- sentations, by using which only positive properties will be stored in these representations. To better compare different entities on the same scale, we further require entity representations to stay within the hypercube of [0, 1] d , as approximately Boolean embeddings ( <ref type="bibr" target="#b9">Kruszewski et al., 2015)</ref>, i.e.,</p><formula xml:id="formula_2">0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E,<label>(2)</label></formula><p>where e ∈ C d is the representation for entity e ∈ E, with its real and imaginary components denoted by Re(e), Im(e) ∈ R d ; 0 and 1 are d-dimensional vectors with all their entries being 0 or 1; and ≥, ≤ , = denote the entry-wise comparisons throughout the paper whenever applicable. As shown by <ref type="bibr" target="#b10">Lee and Seung (1999)</ref>, non-negativity, in most cases, will further induce sparsity and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximate Entailment for Relations</head><p>Besides the non-negativity constraints over entity representations, we also study approximate entail- ment constraints over relation representations. By approximate entailment, we mean an ordered pair of relations that the former approximately entails the latter, e.g., BornInCountry and Nationality, stating that a person born in a country is very like- ly, but not necessarily, to have a nationality of that country. Each such relation pair is associated with a weight to indicate the confidence level of entail- ment. A larger weight stands for a higher level of confidence. We denote by r p λ − → r q the approxi- mate entailment between relations r p and r q , with confidence level λ. This kind of entailment can be derived automatically from a KG by modern rule mining systems ( <ref type="bibr">Galárraga et al., 2015)</ref>. Let T denote the set of all such approximate entailments derived beforehand.</p><p>Before diving into approximate entailment, we first explore the modeling of strict entailment, i.e., entailment with infinite confidence level λ = +∞. The strict entailment r p → r q states that if relation r p holds then relation r q must also hold. This en- tailment can be roughly modelled by requiring</p><formula xml:id="formula_3">φ(e i , r p , e j ) ≤ φ(e i , r q , e j ), ∀e i , e j ∈ E, (3)</formula><p>where φ(·, ·, ·) is the score for a triple predicted by the embedding model, defined by Eq. (1). Eq. <ref type="formula">(3)</ref> can be interpreted as follows: for any two entities e i and e j , if (e i , r p , e j ) is a true fact with a high score φ(e i , r p , e j ), then the triple (e i , r q , e j ) with an even higher score should also be predicted as a true fact by the embedding model. Note that given the non-negativity constraints defined by Eq. (2), a sufficient condition for Eq. (3) to hold, is to further impose Re(r p ) ≤ Re(r q ), Im(r p ) = Im(r q ), <ref type="formula">(4)</ref> where r p and r q are the complex-valued represen- tations for r p and r q respectively, with the real and imaginary components denoted by Re(·), Im(·) ∈ R d . That means, when the constraints of Eq. <ref type="formula">(4)</ref> (along with those of Eq. <ref type="formula" target="#formula_2">(2)</ref>) are satisfied, the re- quirement of Eq. (3) (or in other words r p → r q ) will always hold. We provide a proof of sufficien- cy as supplementary material.</p><p>Next we examine the modeling of approximate entailment. To this end, we further introduce the confidence level λ and allow slackness in Eq. (4), which yields</p><formula xml:id="formula_4">λ Re(r p ) − Re(r q ) ≤ α,<label>(5)</label></formula><formula xml:id="formula_5">λ Im(r p ) − Im(r q ) 2 ≤ β.<label>(6)</label></formula><p>Here α, β ≥ 0 are slack variables, and (·) 2 means an entry-wise operation. Entailments with higher confidence levels show less tolerance for violating the constraints. When λ = +∞, Eqs. <ref type="formula" target="#formula_4">(5)</ref> - <ref type="formula" target="#formula_5">(6)</ref> degenerate to Eq. (4). The above analysis indicates that our approach can model entailment simply by imposing constraints over relation representations, without traversing all possible (e i , e j ) entity pairs (i.e., grounding). In addition, different confidence levels are encoded in the constraints, making our approach moderately tolerant of uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Overall Model</head><p>Finally, we combine together the basic embedding model of ComplEx, the non-negativity constraints on entity representations, and the approximate en- tailment constraints over relation representations.</p><p>The overall model is presented as follows:</p><formula xml:id="formula_6">min Θ,{α,β}</formula><formula xml:id="formula_7">D + ∪D − log 1 + exp(−y ijk φ(e i , r k , e j )) + µ T 1 (α + β) + ηΘ 2 2 , s.t. λ Re(r p ) − Re(r q ) ≤ α, λ Im(r p ) − Im(r q ) 2 ≤ β, α, β ≥ 0, ∀r p λ − → r q ∈ T , 0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E.<label>(7)</label></formula><p>Here, Θ {e : e ∈ E} ∪ {r : r ∈ R} is the set of all entity and relation representations; D + and D − are the sets of positive and negative training triples respectively; a positive triple is directly observed in the KG, i.e., (e i , r k , e j ) ∈ O; a negative triple can be generated by randomly corrupting the head or the tail entity of a positive triple, i.e., (e i , r k , e j ) or (e i , r k , e j ); y ijk = ±1 is the label (positive or negative) of triple (e i , r k , e j ). In this optimization, the first term of the objective function is a typical logistic loss, which enforces triples to have scores close to their labels. The second term is the sum of slack variables in the approximate entailment con- straints, with a penalty coefficient µ ≥ 0. The mo- tivation is, although we allow slackness in those constraints we hope the total slackness to be smal- l, so that the constraints can be better satisfied. The last term is L 2 regularization to avoid over-fitting, and η ≥ 0 is the regularization coefficient.</p><p>To solve this optimization problem, the approx- imate entailment constraints (as well as the corre- sponding slack variables) are converted into penal- ty terms and added to the objective function, while the non-negativity constraints remain as they are. As such, the optimization problem of Eq. <ref type="formula" target="#formula_7">(7)</ref> can be rewritten as:</p><formula xml:id="formula_8">min Θ D + ∪D − log 1 + exp(−y ijk φ(e i , r k , e j )) + µ T λ1 Re(r p )−Re(r q ) + + µ T λ1 Im(r p )−Im(r q ) 2 + ηΘ 2 2 , s.t. 0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E,<label>(8)</label></formula><p>where [x] + = max(0, x) with max(·, ·) being an entry-wise operation. The equivalence between E- q. <ref type="formula" target="#formula_7">(7)</ref> and Eq. <ref type="formula" target="#formula_8">(8)</ref>  While favouring a better structuring of the em- bedding space, imposing the additional constraints will not substantially increase model complexity. Our approach has a space complexity of O(nd + md), which is the same as that of ComplEx. Here, n is the number of entities, m the number of re- lations, and O(nd + md) to store a d-dimensional complex-valued vector for each entity and each re- lation. The time complexity (per iteration) of our approach is O(sd+td+¯ nd), where s is the average number of triples in a mini-batch, ¯ n the average number of entities in a mini-batch, and t the total number of approximate entailments in T . O(sd) is to handle triples in a mini-batch, O(td) penalty terms introduced by the approximate entailments, and O(¯ nd) further the non-negativity constraints on entity representations. Usually there are much fewer entailments than triples, i.e., t s, and also ¯ n ≤ 2s. 1 So the time complexity of our approach is on a par with O(sd), i.e., the time complexity of ComplEx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>This section presents our experiments and results. We first introduce the datasets used in our exper- iments ( § 4.1). Then we empirically evaluate our approach in the link prediction task ( § 4.2). After that, we conduct extensive analysis on both entity representations ( § 4.3) and relation representation- s ( § 4.4) to show the interpretability of our model. <ref type="bibr">1</ref> There will be at most 2s entities contained in s triples.</p><p>Code and data used in the experiments are avail- able at https://github.com/iieir-km/ ComplEx-NNE_AER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The first two datasets we used are WN18 and F- B15K, released by <ref type="bibr" target="#b2">Bordes et al. (2013)</ref>. <ref type="bibr">2</ref> WN18 is a subset of WordNet containing 18 relations and 40,943 entities, and FB15K a subset of Freebase containing 1,345 relations and 14,951 entities. We create our third dataset from the mapping-based objects of core DBpedia. <ref type="bibr">3</ref> We eliminate relations not included within the DBpedia ontology such as HomePage and Logo, and discard entities appearing less than 20 times. The final dataset, referred to as DB100K, is composed of 470 relations and 99,604 entities. Triples on each datasets are further divid- ed into training, validation, and test sets, used for model training, hyperparameter tuning, and evalu- ation respectively. We follow the original split for WN18 and FB15K, and draw a split of 597,572/ 50,000/50,000 triples for DB100K.</p><p>We further use AMIE+ (Galárraga et al., 2015) 4 to extract approximate entailments automatically from the training set of each dataset. As suggested by <ref type="bibr" target="#b4">Guo et al. (2018)</ref>, we consider entailments with PCA confidence higher than 0.8. <ref type="bibr">5</ref> As such, we ex- tract 17 approximate entailments from WN18, 535 from FB15K, and 56 from DB100K. <ref type="table" target="#tab_0">Table 1</ref> gives some examples of these approximate entailments, along with their confidence levels. <ref type="table" target="#tab_1">Table 2</ref> further summarizes the statistics of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Link Prediction</head><p>We first evaluate our approach in the link predic- tion task, which aims to predict a triple (e i , r k , e j ) with e i or e j missing, i.e., predict e i given (r k , e j ) or predict e j given (e i , r k ).</p><p>Evaluation Protocol: We follow the protocol introduced by <ref type="bibr" target="#b2">Bordes et al. (2013)</ref>. For each test triple (e i , r k , e j ), we replace its head entity e i with every entity e i ∈ E, and calculate a score for the corrupted triple (e i , r k , e j ), e.g., φ(e i , r k , e j ) de- fined by Eq. (1). Then we sort these scores in de-hypernym −1 1. <ref type="bibr">00</ref> −−→ hyponym synset domain topic of −1 0.99 −−→ member of domain topic   scending order, and get the rank of the correct enti- ty e i . During ranking, we remove corrupted triples that already exist in either the training, validation, or test set, i.e., the filtered setting as described in <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>. This whole procedure is re- peated while replacing the tail entity e j . We report on the test set the mean reciprocal rank (MRR) and the proportion of correct entities ranked in the top n (HITS@N), with n = 1, 3, 10.</p><p>Comparison Settings: We compare the perfor- mance of our approach against a variety of KG em- bedding models developed in recent years. These models can be categorized into three groups:</p><p>• • Other extensions of ComplEx that integrate logical background knowledge in addition to triples, including RUGE ( <ref type="bibr" target="#b4">Guo et al., 2018)</ref> and ComplEx R ( <ref type="bibr" target="#b17">Minervini et al., 2017a</ref>). The former requires grounding of first-order logic rules. The latter is restricted to relation equiv- alence and inversion, and assigns an identical confidence level to all different rules.</p><p>• Latest developments or implementations that achieve current state-of-the-art performance reported on the benchmarks of WN18 and F- B15K, including R-GCN ( <ref type="bibr" target="#b25">Schlichtkrull et al., 2017)</ref>, <ref type="bibr">ConvE (Dettmers et al., 2018)</ref>, and S- ingle DistMult ( <ref type="bibr" target="#b8">Kadlec et al., 2017)</ref>. <ref type="bibr">6</ref> The first two are built based on neural network ar- chitectures, which are, by nature, more com- plicated than the simple models. The last one is a re-implementation of DistMult, generat- ing 1000 to 2000 negative training examples per positive one, which leads to better perfor- mance but requires significantly longer train- ing time.</p><p>We further evaluate our approach in two differ- ent settings: (i) ComplEx-NNE that imposes only the Non-Negativity constraints on Entity represen- tations, i.e., optimization Eq. <ref type="formula" target="#formula_8">(8)</ref>  Implementation Details: We compare our ap- proach against all the three groups of baselines on the benchmarks of WN18 and FB15K. We direct- ly report their original results on these two datasets to avoid re-implementation bias. On DB100K, the newly created dataset, we take the first two groups of baselines, i.e., those simple embedding models and ComplEx extensions with logical background knowledge incorporated. We do not use the third group of baselines due to efficiency and complex- ity issues. We use the code provided by <ref type="bibr" target="#b28">Trouillon et al. (2016)</ref>  <ref type="bibr">7</ref> for TransE, DistMult, and ComplEx, and the code released by their authors for ANAL- OGY 8 and RUGE <ref type="bibr">9</ref> . We re-implement HolE and ComplEx R so that all the baselines (as well as our approach) share the same optimization mode, i.e., SGD with AdaGrad and gradient normalization, to facilitate a fair comparison. <ref type="bibr">10</ref> We follow <ref type="bibr" target="#b28">Trouillon et al. (2016)</ref> to adopt a ranking loss for TransE and a logistic loss for all the other methods.    <ref type="table">Table 4</ref>: Link prediction results on the test set of DB100K, with best scores highlighted in bold, sta- tistically significant improvements marked by " * ".</p><p>Among those baselines, RUGE and ComplEx R require additional logical background knowledge. RUGE makes use of soft rules, which are extracted by AMIE+ from the training sets. As suggested by <ref type="bibr" target="#b4">Guo et al. (2018)</ref>, length-1 and length-2 rules with PCA confidence higher than 0.8 are utilized. Note that our approach also makes use of AMIE+ rules with PCA confidence higher than 0.8. But it only considers entailments between a pair of relations, i.e., length-1 rules. ComplEx R takes into account equivalence and inversion between relations. We derive such axioms directly from our approximate entailments. If r p λ 1 − → r q and r q λ 2 − → r p with λ 1 , λ 2 &gt; 0.8, we think relations r p and r q are equivalent. For all the methods, we create 100 mini-batches on each dataset, and conduct a grid search to find hyperparameters that maximize MRR on the val- idation set, with at most 1000 iterations over the training set. Specifically, we tune the embedding size d ∈ {100, 150, 200}, the L 2 regularization coefficient η ∈ {0.001, 0.003, 0.01, 0.03, 0.1}, the ratio of negative over positive training examples α ∈ {2, 10}, and the initial learning rate γ ∈ {0.01, 0.05, 0.1, 0.5, 1.0}. For TransE, we tune the mar- gin of the ranking loss δ ∈ {0.1, 0.2, 0.5, 1, 2, 5, 10}. Other hyperparameters of ANALOGY and RUGE are set or tuned according to the default set- tings suggested by their authors ( <ref type="bibr" target="#b14">Liu et al., 2017;</ref><ref type="bibr" target="#b4">Guo et al., 2018)</ref>. After getting the best ComplEx model, we tune the relation constraint penalty of our approach ComplEx-NNE+AER (µ in Eq. <ref type="formula" target="#formula_8">(8)</ref> Experimental Results: <ref type="table" target="#tab_3">Table 3</ref> presents the re- sults on the test sets of WN18 and FB15K, where the results for the baselines are taken directly from previous literature. <ref type="table">Table 4</ref> further provides the re- sults on the test set of DB100K, with all the meth- ods tuned and tested in (almost) the same setting. On all the datasets, we test statistical significance of the improvements achieved by ComplEx-NNE/ ComplEx-NNE+AER over ComplEx, by using a paired t-test. The reciprocal rank or HITS@N val- ue with n = 1, 3, 10 for each test triple is used as paired data. The symbol " * " indicates a signifi- cance level of p &lt; 0.05.</p><p>The results demonstrate that imposing the non- negativity and approximate entailment constraints indeed improves KG embedding. ComplEx-NNE and ComplEx-NNE+AER perform better than (or at least equally well as) ComplEx in almost all the metrics on all the three datasets, and most of the improvements are statistically significant (except those on WN18). More interestingly, just by intro- ducing these simple constraints, ComplEx-NNE+ AER can beat very strong baselines, including the best performing basic models like ANALOGY, those previous extensions of ComplEx like RUGE or ComplEx R , and even the complicated develop- ments or implementations like ConvE or Single DistMult. This demonstrates the superiority of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis on Entity Representations</head><p>This section inspects how the structure of the enti- ty embedding space changes when the constraints are imposed. We first provide the visualization of entity representations on DB100K. On this dataset each entity is associated with a single type label. <ref type="bibr">11</ref> We pick 4 types reptile, wine region, species, and programming language, and randomly select 30 entities from each type. <ref type="figure" target="#fig_7">Figure 1</ref> visualizes the representations of these entities learned by Com- plEx and ComplEx-NNE+AER (real components only), with the optimal configurations determined by link prediction (see § 4.2 for details, applicable to all analysis hereafter). During the visualization, we normalize the real component of each entity by</p><formula xml:id="formula_9">[˜ x] = [x] −min(x)</formula><p>max(x)−min(x) , where min(x) or max(x) is the minimum or maximum entry of x respectively. We observe that after imposing the non-negativity constraints, ComplEx-NNE+AER indeed obtains compact and interpretable representations for enti- ties. Each entity is represented by only a relatively small number of "active" dimensions. And entities   with the same type tend to activate the same set of dimensions, while entities with different types of- ten get clearly different dimensions activated. Then we investigate the semantic purity of these dimensions. Specifically, we collect the represen- tations of all the entities on DB100K (real compo- nents only). For each dimension of these represen- tations, top K percent of entities with the highest activation values on this dimension are picked. We can calculate the entropy of the type distribution of the entities selected. This entropy reflects diversity of entity types, or in other words, semantic purity. If all the K percent of entities have the same type, we will get the lowest entropy of zero (the high- est semantic purity). On the contrary, if each of them has a distinct type, we will get the highest en- tropy (the lowest semantic purity). <ref type="figure" target="#fig_8">Figure 2</ref> shows the average entropy over all dimensions of entity representations (real components only) learned by <ref type="bibr">ComplEx, ComplEx-NNE, and ComplEx-NNE+</ref>  -0.57 -0.08 -0.52 -0.81 -0.05 -0.10 -0.00 0.01 -0.06 -0.00 -0.57 -0.08 -0.52 -0.81 -0.05 -0.09 -0.00 0.02 -0.06 -0.00 AER, as K varies. We can see that after impos- ing the non-negativity constraints, ComplEx-NNE and ComplEx-NNE+AER can learn entity repre- sentations with latent dimensions of consistently higher semantic purity. We have conducted the same analyses on imaginary components of entity representations, and observed similar phenomena.</p><p>The results are given as supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis on Relation Representations</head><p>This section further provides a visual inspection of the relation embedding space when the constraints are imposed. To this end, we group relation pairs involved in the DB100K entailment constraints in- to 3 classes: equivalence, inversion, and others. <ref type="bibr">12</ref> We choose 2 pairs of relations from each class, and visualize these relation representations learned by ComplEx-NNE+AER in <ref type="figure" target="#fig_10">Figure 3</ref>, where for each relation we randomly pick 5 dimensions from both its real and imaginary components. By imposing the approximate entailment constraints, these rela- tion representations can encode logical regularities quite well. Pairs of relations from the first class (e- quivalence) tend to have identical representations r p ≈ r q , those from the second class (inversion) complex conjugate representations r p ≈ ¯ r q ; and the others representations that Re(r p ) ≤ Re(r q ) and Im(r p ) ≈ Im(r q ).</p><p>12 Equivalence and inversion are detected using heuristics introduced in § 4.2 (implementation details). See the supple- mentary material for detailed properties of these three classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper investigates the potential of using very simple constraints to improve KG embedding. T- wo types of constraints have been studied: (i) the non-negativity constraints to learn compact, inter- pretable entity representations, and (ii) the approx- imate entailment constraints to further encode log- ical regularities into relation representations. Such constraints impose prior beliefs upon the structure of the embedding space, and will not significantly increase the space or time complexity. Experimen- tal results on benchmark KGs demonstrate that our method is simple yet surprisingly effective, show- ing significant and consistent improvements over strong baselines. The constraints indeed improve model interpretability, yielding a substantially in- creased structuring of the embedding space. <ref type="bibr">Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christopher Meek. 2014</ref>. Typed tensor decompo- sition of knowledge bases for relation extraction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is shown in the supplementary material. We use SGD in mini-batch mode as our optimizer, with AdaGrad (Duchi et al., 2011) to tune the learning rate. After each gradient descen- t step, we project (by truncation) real and imagi- nary components of entity representations into the hypercube of [0, 1] d , to satisfy the non-negativity constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Simple embedding models that utilize triples alone without integrating extra information, including TransE (Bordes et al., 2013), Dist- Mult (Yang et al., 2015), HolE (Nickel et al., 2016b), ComplEx (Trouillon et al., 2016), and ANALOGY (Liu et al., 2017). Our ap- proach is developed on the basis of ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>with µ = 0; and (ii) ComplEx-NNE+AER that further imposes the Approximate Entailment constraints over Relation representations besides those non-negativity ones, i.e., optimization Eq. (8) with µ &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>WN18</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>And</head><label></label><figDesc>similarly, if r −1 p λ 1 − → r q and r −1 q λ 2 − → r p with λ 1 , λ 2 &gt; 0.8, we consider r p as an inverse of r q .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) in the range of {10 −5 , 10 −4 , · · · , 10 4 , 10 5 }, with all its other hyperparameters fixed to their optimal configurations. We then directly set µ = 0 to get the optimal ComplEx-NNE model. The weight of soft constraints in ComplEx R is tuned in the same range as µ. The optimal configurations for our ap- proach are: d = 200, η = 0.03, α = 10, γ = 1.0, µ = 10 on WN18; d = 200, η = 0.01, α = 10, γ = 0.5, µ = 10 −3 on FB15K; and d = 150, η = 0.03, α = 10, γ = 0.1, µ = 10 −5 on DB100K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of real components of entity representations (rows) learned by ComplExNNE+AER (left) and ComplEx (right). From top to bottom, entities belong to type reptile, wine region, species, and programming language in turn. Values range from 0 (white) via 0.5 (orange) to 1 (black). Best viewed in color.</figDesc><graphic url="image-1.png" coords="8,310.25,74.33,210.75,82.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average entropy over all dimensions of real components of entity representations learned by ComplEx (circles), ComplEx-NNE (squares), and ComplEx-NNE+AER (triangles) as K varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of relation representations learned by ComplEx-NNE+AER, with the top 4 relations from the equivalence class, the middle 4 the inversion class, and the bottom 4 others.</figDesc><graphic url="image-2.png" coords="9,123.83,71.46,157.79,189.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Approximate entailments extracted from 
WN18 (top), FB15K (middle), and DB100K (bot-
tom), where r −1 means the inverse of relation r. 

Dataset 
# Ent # Rel 
# Train/Valid/Test 
# Cons 

WN18 
40,943 
18 141,442 5,000 5,000 
17 
FB15K 14,951 1,345 483,142 50,000 59,071 
535 
DB100K 99,604 470 597,572 50,000 50,000 
56 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of datasets, where the columns 
respectively indicate the number of entities, rela-
tions, training/validation/test triples, and approxi-
mate entailments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Link prediction results on the test sets of WN18 and FB15K. Results for TransE and DistMult 
are taken from (Trouillon et al., 2016). Results for the other baselines are taken from the original papers. 
Missing scores not reported in the literature are indicated by "-". Best scores are highlighted in bold, 
and " * " indicates statistically significant improvements over ComplEx. 

HITS@N 

MRR 
1 
3 
10 

TransE 
0.111 
0.016 
0.164 
0.270 
DistMult 
0.233 
0.115 
0.301 
0.448 
HolE 
0.260 
0.182 
0.309 
0.411 
ComplEx 
0.242 
0.126 
0.312 
0.440 
ANALOGY 
0.252 
0.143 
0.323 
0.427 

RUGE 
0.246 
0.129 
0.325 
0.433 
ComplEx R 
0.253 
0.167 
0.294 
0.420 

ComplEx-NNE 
0.298  *  0.229  *  0.330  *  0.426 
ComplEx-NNE+AER 0.306  *  0.244  *  0.334  *  0.418 

</table></figure>

			<note place="foot" n="2"> https://everest.hds.utc.fr/doku.php? id=en:smemlj12 3 http://downloads.dbpedia.org/2016-10/ core/ 4 https://www.mpi-inf.mpg.de/departmen ts/databases-and-information-systems/res earch/yago-naga/amie/ 5 PCA confidence is the confidence under the partial completeness assumption. See (Galárraga et al., 2015) for details.</note>

			<note place="foot" n="6"> We do not consider Ensemble DistMult (Dettmers et al., 2018) which combines several different models together, to facilitate a fair comparison. 7 https://github.com/ttrouill/complex 8 https://github.com/quark0/ANALOGY 9 https://github.com/iieir-km/RUGE 10 An exception here is that ANALOGY uses asynchronous SGD with AdaGrad (Liu et al., 2017).</note>

			<note place="foot" n="11"> http://downloads.dbpedia.org/2016-10/ core-i18n/en/instance_types_wkd_uris_en. ttl.bz2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank all the anonymous review-ers for their insightful and valuable suggestions, which help to improve the quality of this paper. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="259" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcíadurán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with iterative guidance from soft rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4816" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deriving Boolean structures from distributional vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DBpedia: A largescale, multilingual knowledge base extracted from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online learning of interpretable word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1687" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context-dependent knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1656" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularizing knowledge graph embeddings via equivalence and inversion axioms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emir</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vít</forename><surname>Nováček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Vandenbussche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial sets for regularising neural link predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 33rd Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning effective and interpretable semantic models using non-negative sparse embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1933" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="156" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<title level="m">Modeling relational data with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ProjE: Embedding projection for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1859" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to identify the best contexts for knowledge-based WSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Wasserman-Pritsker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1662" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TransG: A generative model for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SSP: Semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3104" to="3110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in LSTMs for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1436" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aligning knowledge and text embeddings by entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
