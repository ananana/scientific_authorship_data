<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntactically Guided Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SDL Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SDL Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Syntactically Guided Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="299" to="305"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We report on investigations motivated by the idea that the structured search spaces defined by syn- tactic machine translation approaches such as Hi- ero <ref type="bibr" target="#b5">(Chiang, 2007)</ref> can be used to guide Neural Machine Translation (NMT) <ref type="bibr" target="#b16">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr">Sutskever et al., 2014;</ref><ref type="bibr" target="#b7">Cho et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>. NMT and Hiero have complementary strengths and weaknesses and differ markedly in how they define probabil- ity distributions over translations and what search procedures they use.</p><p>The NMT encoder-decoder formalism provides a probability distribution over translations y = y T 1 of a source sentence x as ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref> P (y T 1 |x) = g(y t−1 , s t , c t )</p><p>(1) where s t = f (s t−1 , y t−1 , c t ) is a decoder state variable and c t is a context vector depending on the source sentence and the attention mechanism.</p><p>This posterior distribution is potentially very powerful, however it does not easily lend itself to sophisticated search procedures. Decoding is done by 'beam search to find a translation that ap- proximately maximizes the conditional probabil- ity' ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>. Search looks only one word ahead and no deeper than the beam.</p><p>Hiero defines a synchronous context-free gram- mar (SCFG) with rules: X → α, γ, where α and γ are strings of terminals and non-terminals in the source and target languages. A target language sentence y can be a translation of a source lan- guage sentence x if there is a derivation D in the grammar which yields both y and x: y = y(D), x = x(D). This defines a regular language Y over strings in the target language via a projection of the sentence to be translated: <ref type="bibr" target="#b13">Iglesias et al., 2011;</ref><ref type="bibr" target="#b1">Allauzen et al., 2014</ref>). Scores are defined over derivations via a log-linear model with features {φ i } and weights λ. The decoder searches for the translation y(D) in Y with the highest derivation score S(D) <ref type="bibr">(Chiang, 2007, Eq. 24)</ref> :</p><formula xml:id="formula_0">Y = {y(D) : x(D) = x} (</formula><formula xml:id="formula_1">ˆ y = y    argmax D:x(D)=x P G (D)P LM (y(D)) λ LM S(D)    (2)</formula><p>where P LM is an n-gram language model and</p><formula xml:id="formula_2">P G (D) ∝ (X→→γ,α)∈D i φ i (X → γ, α) λ i .</formula><p>Hiero decoders attempt to avoid search er- rors when combining the translation and lan- guage model for the translation hypotheses <ref type="bibr" target="#b5">(Chiang, 2007;</ref><ref type="bibr" target="#b12">Iglesias et al., 2009</ref>). These procedures search over a vast space of translations, much larger than is considered by the NMT beam search. However the Hiero context-free grammars that make efficient search possible are weak models of translation. The basic Hiero formalism can be ex- tended through 'soft syntactic constraints' ( <ref type="bibr">Venugopal et al., 2009;</ref><ref type="bibr" target="#b20">Marton and Resnik, 2008)</ref> or by adding very high dimensional features <ref type="bibr" target="#b4">(Chiang et al., 2009)</ref>, however the translation score assigned by the grammar is still only the product of prob- abilities of individual rules. From the modelling perspective, this is an overly strong conditional in- dependence assumption. NMT clearly has the po- tential advantage in incorporating long-term con- text into translation scores.</p><p>NMT and Hiero differ in how they 'consume' source words. Hiero applies the translation rules to the source sentence via the CYK algorithm, with each derivation yielding a complete and unam- biguous translation of the source words. The NMT beam decoder does not have an explicit mecha- nism for tracking source coverage, and there is ev- idence that may lead to both 'over-translation' and 'under-translation' ( <ref type="bibr">Tu et al., 2016</ref>).</p><p>NMT and Hiero also differ in their internal rep- resentations. The NMT continuous representa- tion captures morphological, syntactic and seman- tic similarity <ref type="bibr" target="#b8">(Collobert and Weston, 2008</ref>) across words and phrases. However, extending these rep- resentations to the large vocabularies needed for open-domain MT is an open area of research ( <ref type="bibr" target="#b14">Jean et al., 2015a;</ref><ref type="bibr" target="#b18">Luong et al., 2015;</ref><ref type="bibr" target="#b25">Sennrich et al., 2015;</ref><ref type="bibr" target="#b6">Chitnis and DeNero, 2015)</ref>. By contrast, Hiero (and other symbolic systems) can easily use translation grammars and language models with very large vocabularies ( <ref type="bibr" target="#b11">Heafield et al., 2013;</ref><ref type="bibr" target="#b17">Lin and Dyer, 2010)</ref>. Moreover, words and phrases can be easily added to a fully-trained symbolic MT system. This is an important consideration for commercial MT, as customers often wish to customise and personalise SMT systems for their own application domain. Adding new words and phrases to an NMT system is not as straightfor- ward, and it is not clear that the advantages of the continuous representation can be extended to the new additions to the vocabularies.</p><p>NMT has the advantage of including long-range context in modelling individual translation hy- potheses. Hiero considers a much bigger search space, and can incorporate n-gram language mod- els, but a much weaker translation model. In this paper we try to exploit the strengths of each ap- proach. We propose to guide NMT decoding using Hiero. We show that restricting the search space of the NMT decoder to a subset of Y spanned by Hi- ero effectively counteracts NMT modelling errors. This can be implemented by generating translation lattices with Hiero, which are then rescored by the NMT decoder. Our approach addresses the lim- ited vocabulary issue in NMT as we replace NMT OOVs with lattice words from the much larger Hi- ero vocabulary. We also find good gains from neu- ral and Kneser-Ney n-gram language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Syntactically Guided NMT (SGNMT) 2.1 Hiero Predictive Posteriors</head><p>The Hiero decoder generates translation hypothe- ses as weighted finite state acceptors (WFSAs), or lattices, with weights in the tropical semiring. For a translation hypothesis y(D) arising from the Hi- ero derivation D, the path weight in the WFSA is − log S(D), after Eq. 2. While this representa- tion is correct with respect to the Hiero translation grammar and language model scores, having Hi- ero scores at the path level is not convenient for working with the NMT system. What we need are predictive probabilities in the form of Eq. 1.</p><p>The Hiero WFSAs are determinised and min- imised with epsilon removal under the tropical semiring, and weights are pushed towards the ini- tial state under the log semiring <ref type="bibr" target="#b21">(Mohri and Riley, 2001</ref>). The resulting transducer is stochastic in the log semiring, i.e. the log sum of the arc log prob- abilities leaving a state is 0 (= log 1). In addi- tion, because the WFSA is deterministic, there is a unique path leading to every state, which corre- sponds to a unique Hiero translation prefix. Sup- pose a path to a state accepts the translation prefix y t−1</p><p>1 . An outgoing arc from that state with symbol y has a weight that corresponds to the (negative log of the) conditional probability</p><formula xml:id="formula_3">P Hiero (y t = y|y t−1 1 , x).<label>(3)</label></formula><p>This conditional probability is such that for a Hi- ero translation y T 1 = y(D) accepted by the WFSA</p><formula xml:id="formula_4">P Hiero (y T 1 ) = T t=1 P Hiero (y t |y t−1 1 , x) ∝ S(D).<label>(4)</label></formula><p>The Hiero WFSAs have been transformed so that their arc weights have the negative log of the con- ditional probabilities defined in Eq. 3. All the probability mass of this distribution is concen- trated on the Hiero translation hypotheses. The complete translation and language model scores computed over the entire Hiero translations are pushed as far forward in the WFSAs as possible. This is commonly done for left-to-right decoding in speech recognition ( <ref type="bibr" target="#b22">Mohri et al., 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NMT-Hiero Decoding</head><p>As above, suppose a path to a state in the WFSA accepts a Hiero translation prefix y t−1 1 , and let y t be a symbol on an outgoing arc from that state. We define the joint NMT+Hiero score as log P (y t |y t−1 1 , x) = λ Hiero log P Hiero (y t |y t−1</p><formula xml:id="formula_5">1 , x) + λ N M T log P N M T (y t |y t−1 1 , x) y t ∈ Σ N M T log P N M T (unk|y t−1 1 , x) y t ∈ Σ N M T<label>(5)</label></formula><p>Note that the NMT-HIERO decoder only con- siders hypotheses in the Hiero lattice. As dis- cussed earlier, the Hiero vocabulary can be much larger than the NMT output vocabulary Σ N M T . If a Hiero translation contains a word not in the NMT vocabulary, the NMT model provides a score and updates its decoder state as for an unknown word.</p><p>Our decoding algorithm is a natural extension of beam search decoding for NMT. Due to the form of Eq. 5 we can build up hypotheses from left-to- right on the target side. Thus, we can represent a partial hypothesis h = (y t 1 , h s ) by a transla- tion prefix y t 1 and an accumulated score h s . At each iteration we extend the current hypotheses by one target token, until the best scoring hypothesis reaches a final state of the Hiero lattice. We re- fer to this step as node expansion, and in Sec. 3.1 we report the number of node expansions per sen- tence, as an indication of computational cost.</p><p>We can think of the decoding algorithm as breath-first search through the translation lattices with a limited number of active hypotheses (a beam). Rescoring is done on-the-fly: as the de- coder traverses an edge in the WFSA, we update its weight by Eq. 5. The output-synchronous char- acteristic of beam search enables us to compute the NMT posteriors only once for each history based on previous calculations. Alternatively, we can think of the algorithm as NMT decoding with revised posterior probabil- ities: instead of selecting the most likely sym- bol y t according the NMT model, we adjust the NMT posterior with the Hiero posterior scores and delete NMT entries that are not allowed by the lat- tice. This may result in NMT choosing a different symbol, which is then fed back to the neural net- work for the next decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>We evaluate SGNMT on the WMT news-test2014 test sets (the filtered version) for English-German (En-De) and English-French (En-Fr). We also re- port results on WMT news-test2015 En-De.</p><p>The En-De training set includes Europarl v7, Common Crawl, and News Commentary v10. Sen- tence pairs with sentences longer than 80 words or length ratios exceeding 2.4:1 were deleted, as were Common Crawl sentences from other lan- guages <ref type="bibr" target="#b26">(Shuyo, 2010)</ref>. The En-Fr NMT system was trained on preprocessed data <ref type="bibr" target="#b24">(Schwenk, 2014)</ref> used by previous work <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b14">Jean et al., 2015a</ref>), but with truecasing like our Hiero baseline. Follow- ing (Jean et al., 2015a), we use news-test2012 and news-test2013 as a development set. The NMT vo- cabulary size is 50k for En-De and 30k for En-Fr, taken as the most frequent words in training ( <ref type="bibr" target="#b14">Jean et al., 2015a</ref>). Tab. 1 provides statistics and shows the severity of the OOV problem for NMT.</p><p>The BASIC NMT system is built using the Blocks framework <ref type="bibr">(van Merriënboer et al., 2015)</ref> based on the Theano library ( <ref type="bibr" target="#b3">Bastien et al., 2012</ref>) with standard hyper-parameters ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>: the encoder and decoder networks consist of 1000 gated recurrent units ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>). The decoder uses a single maxout ( <ref type="bibr" target="#b10">Goodfellow et al., 2013</ref>) output layer with the feed-forward at- tention model ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>.</p><p>The En-De Hiero system uses rules which en- courage verb movement (de <ref type="bibr" target="#b9">Gispert et al., 2010)</ref>. The rules for En-Fr were extracted from the full data set available at the WMT'15 website using a shallow-1 grammar (de <ref type="bibr" target="#b9">Gispert et al., 2010)</ref>. 5- gram Kneser-Ney language models (KN-LM) for the Hiero systems were trained on WMT'15 par- allel and monolingual data <ref type="bibr" target="#b11">(Heafield et al., 2013)</ref>.    <ref type="table">Table 3</ref>: BLEU English-German news-test2015 scores calculated with mteval-v13a.pl.</p><p>Our SGNMT system 1 is built with the Pyfst inter- face 2 to OpenFst (Allauzen et al., 2007).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SGNMT Performance</head><p>Tab. 2 compares our combined NMT+Hiero de- coding with NMT results in the literature. We use a beam size of 12. In En-De and in En-Fr, we find that our BASIC NMT system performs similarly (within 0.5 BLEU) to previously published results <ref type="bibr">(16.31 vs. 16.46 and 30.42 vs. 29.97</ref>).</p><p>In NMT-HIERO, decoding is as described in Sec. 2.2, but with λ Hiero = 0. The decoder searches through the Hiero lattice, ignoring the Hiero scores, but using Hiero word hypotheses in place of any UNKs that might have been produced by NMT. The results show that NMT-HIERO is much more effective in fixing NMT OOVs than the 'UNK Replace' technique ( <ref type="bibr" target="#b18">Luong et al., 2015)</ref>; this holds in both En-De and En-Fr.</p><p>For the NMT-HIERO+TUNING systems, lattice MERT ( <ref type="bibr" target="#b19">Macherey et al., 2008</ref>) is used to optimise λ Hiero and λ N M T on the tuning sets. This yields further gains in both En-Fr and En-De, suggesting that in addition to fixing UNKs, the Hiero predic- tive posteriors can be used to improve the NMT translation model scores.</p><p>Tab. 3 reports results of our En-De system with reshuffling and tuning on news-test2015. BLEU scores are directly comparable to WMT'15 re- sults <ref type="bibr">3</ref> . By comparing row 3 to row 10, we see that constraining NMT to the search space defined by the Hiero lattices yields an improvement of +0.8 BLEU for single NMT. If we allow Hiero to fix NMT UNKs, we see a further +2.7 BLEU gain (row 11). The majority of gains come from fix- ing UNKs, but there is still improvement from the constrained search space for single NMT.</p><p>We next investigate the contribution of the Hi- ero system scores. We see that, once lattices are generated, the KN-LM contributes more to rescoring than the Hiero grammar scores (rows <ref type="bibr">[12]</ref><ref type="bibr">[13]</ref><ref type="bibr">[14]</ref>. Further gains can be achieved by adding a feed-forward neural language model with NPLM ( <ref type="bibr">Vaswani et al., 2013</ref>) (row 15). We observe that n-best list rescoring with NMT ( <ref type="bibr" target="#b23">Neubig et al., 2015</ref>) also outperforms both the Hiero and NMT  baselines, although lattice rescoring gives the best results (row 9 vs. row 15). Lattice rescoring with SGNMT also uses far fewer node expansions per sentence. We report n-best rescoring speeds for rescoring each hypothesis separately, and a depth- first (DFS) scheme that efficiently traverses the n- best lists. Both these techniques are very slow compared to lattice rescoring. <ref type="figure" target="#fig_2">Fig. 1</ref> shows that we can reduce the beam size from 12 to 5 with only a minor drop in BLEU. This is nearly 100 times faster than DFS over the 1000-best list.</p><p>Cost of Lattice Preprocessing As described in Sec. 2.1, we applied determinisation, minimisa- tion, and weight pushing to the Hiero lattices in order to work with probabilities. Tab. 4 shows that those operations are generally fast 4 .</p><p>Lattice Size For previous experiments we set the Hiero pruning parameters such that lattices had 8,510 nodes on average. <ref type="figure" target="#fig_3">Fig. 2</ref> plots the BLEU score over the lattice size. We find that SGNMT works well on lattices of moderate or large size, but pruning lattices too heavily has a negative ef- fect as they are then too similar to Hiero first best hypotheses. We note that lattice rescoring involves nearly as many node expansions as unconstrained NMT decoding. This confirms that the lattices at 8,510 nodes are already large enough for SGNMT. Local Softmax In SGNMT decoding we have the option of normalising the NMT translation probabilities over the words on outgoing words from each state rather than over the full 50,000 words translation vocabulary. There are ∼4.5 arcs per state in our En-De'14 lattices, and so avoiding the full softmax could cause significant computa- tional savings. We find this leads to only a modest 0.5 BLEU degradation: 21.45 BLEU in En-De'14, compared to 21.87 BLEU using NMT probabili- ties computed over the full vocabulary.</p><p>Modelling Errors vs. Search Errors In our En- De'14 experiments with λ Hiero = 0 we find that constraining the NMT decoder to the Hiero lattices yields translation hypotheses with much lower NMT probabilities than unconstrained BA- SIC NMT decoding: under the NMT model, NMT hypotheses are 8,300 times more likely (median) than NMT-HIERO hypotheses. We conclude (ten- tatively) that BASIC NMT is not suffering only from search errors, but rather that NMT-HIERO discards some hypotheses ranked highly by the NMT model but lower in the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have demonstrated a viable approach to Syn- tactically Guided Neural Machine Translation for- mulated to exploit the rich, structured search space generated by Hiero and the long-context transla- tion scores of NMT. SGNMT does not suffer from the severe limitation in vocabulary size of basic NMT and avoids any difficulty of extending dis- tributed word representations to new vocabulary items not seen in training data. <ref type="bibr">Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014</ref>. Sequence to sequence learning with neural net- works. In Advances in Neural Information Process- ing Systems, pages 3104-3112. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance with NPLM over beam size on English-German news-test2015. A beam of 12 corresponds to row 15 in Tab. 3. Determini-MinimiWeight Sentences sation sation pushing per second 2.51 1.57 1.47</figDesc><graphic url="image-1.png" coords="5,72.00,62.81,218.27,122.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SGNMT performance over lattice size on English-German news-test2015. 8,510 nodes per lattice corresponds to row 14 in Tab. 3.</figDesc><graphic url="image-2.png" coords="5,333.47,62.80,165.88,82.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Jean et al., 2015a, Tab. 2)</head><label></label><figDesc></figDesc><table>SGNMT 
Setup 
BLEU Setup 
BLEU 
BASIC NMT 
16.46 BASIC NMT 
16.31 
NMT-LV 
16.95 HIERO 
19.44 
+ UNK Replace 
18.89 NMT-HIERO 
20.69 
-
-+ Tuning 
21.43 
+ Reshuffle 
19.40 + Reshuffle 
21.87 
+ Ensemble 
21.59 

(a) English-German 

(Jean et al., 2015a, Tab. 2) 
SGNMT 
Setup 
BLEU Setup 
BLEU 
BASIC NMT 
29.97 BASIC NMT 
30.42 
NMT-LV 
33.36 HIERO 
32.86 
+ UNK Replace 
34.11 NMT-HIERO 
35.37 
-
-+ Tuning 
36.29 
+ Reshuffle 
34.60 + Reshuffle 
36.61 
+ Ensemble 
37.19 

(b) English-French 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores on news-test2014 calculated with multi-bleu.perl. NMT-LV refers to the 
RNNSEARCH-LV model from (Jean et al., 2015a) for large output vocabularies. 

Search 
Vocab. NMT Grammar KN-LM NPLM 
# of node exp-
BLEU 
BLEU 
space 
scores 
scores 
scores 
scores 
ansions per sen. 
(single) (ensemble) 

1 

Lattice 
Hiero 


-
21.1 (Hiero) 

2 

Lattice 
Hiero 



-
21.7 (Hiero) 
3 Unrestricted 
NMT 

254.8 
19.5 
21.8 

4 

100-best 
Hiero 

2,233.6 
(DFS: 832.1) 

22.8 
23.3 

5 

100-best 
Hiero 



22.9 
23.4 

6 

100-best 
Hiero 




22.9 
23.3 

7 

1000-best 
Hiero 

21,686.2 
(DFS: 6,221.8) 

23.3 
23.8 

8 

1000-best 
Hiero 



23.4 
23.9 

9 

1000-best 
Hiero 




23.5 
24.0 

10 

Lattice 
NMT 

243.3 
20.3 
21.4 

11 

Lattice 
Hiero 

243.3 
23.0 
24.2 

12 

Lattice 
Hiero 


243.3 
23.0 
24.2 

13 

Lattice 
Hiero 


240.5 
23.4 
24.5 

14 

Lattice 
Hiero 



243.9 
23.4 
24.4 

15 

Lattice 
Hiero 




244.3 
24.0 
24.4 
16 Neural MT -UMontreal-MILA (Jean et al., 2015b) 
22.8 
25.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Time for lattice preprocessing operations 
on English-German news-test2015. 

</table></figure>

			<note place="foot" n="1"> http://ucam-smt.github.io/sgnmt/html/ 2 https://pyfst.github.io/</note>

			<note place="foot" n="3"> http://matrix.statmt.org/matrix/systems list/1774</note>

			<note place="foot" n="4"> Testing environment: Ubuntu 14.04, Linux 3.13.0, single Intel R Xeon R X5650 CPU at 2.67 GHz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the U.K. En-gineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
	<note>Wojciech Skut, and Mehryar Mohri</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pushdown automata in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="687" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: new features and speed improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">11,001 new features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="218" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variablelength word encodings for neural translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chitnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2088" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Adrì A De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">R</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="505" to="533" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical phrasebased translation with weighted finite state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">R</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="433" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1373" to="1383" />
		</imprint>
	</monogr>
	<note>Adrì a de Gispert, and Michael Riley</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for WMT15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">413</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Data-intensive text processing with MapReduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan &amp;Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lattice-based minimum error rate training for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="725" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Soft syntactic constraints for hierarchical phrased-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A weight pushing algorithm for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1603" to="1606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural reranking improves subjective quality of machine translation: NAIST at WAT2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Asian Translation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Universit du Maine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language detection library for Java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakatani</forename><surname>Shuyo</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/language-detection/" />
		<imprint>
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
