<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
						</author>
						<title level="a" type="main">An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="221" to="231"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1021</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition , we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the amount of the knowledge bases (KBs) grows, people are paying more attention to seeking effective methods for accessing these precious in- tellectual resources. There are several tailor-made languages designed for querying KBs, such as SPARQL ( <ref type="bibr" target="#b14">Prudhommeaux and Seaborne, 2008)</ref>. However, to handle such query languages, users are required to not only be familiar with the partic- ular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) <ref type="bibr" target="#b19">(Unger et al., 2014)</ref>, which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years.</p><p>Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research direc- tions for this task: semantic parsing-based (SP- based) <ref type="bibr">Collins, 2009, 2012;</ref><ref type="bibr" target="#b13">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b7">Cai and Yates, 2013;</ref><ref type="bibr" target="#b1">Berant et al., 2013;</ref><ref type="bibr" target="#b26">Yih et al., 2015</ref><ref type="bibr" target="#b28">Yih et al., , 2016</ref><ref type="bibr" target="#b15">Reddy et al., 2016</ref>) and information retrieval-based (IR-based) <ref type="bibr" target="#b24">(Yao and Van Durme, 2014;</ref><ref type="bibr" target="#b3">Bordes et al., 2014a</ref><ref type="bibr">Bordes et al., ,b, 2015</ref><ref type="bibr" target="#b8">Dong et al., 2015;</ref><ref type="bibr" target="#b22">Xu et al., 2016a</ref>,b) methods. SP-based methods usually fo- cus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct se- lections from candidate answers.</p><p>Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task ( <ref type="bibr" target="#b6">Bordes et al., 2014b</ref>). Different from previous methods, NN- based methods represent both of the questions and the answers as semantic vectors. Then the com- plex process of KB-QA could be converted into a similarity matching process between an input question and its candidate answers in a semantic space. The candidates with the highest similarity score will be selected as the final answers. Be- cause they are more adaptive, NN-based methods have attracted more and more attention, and this paper also focuses on using end-to-end neural net- works to answer questions over knowledge base.</p><p>In NN-based methods, the crucial step is to compute the similarity score between a question and a candidate answer, where the key is to learn their representations. Previous methods put more emphasis on learning representation of the answer end. For example, <ref type="bibr" target="#b3">Bordes et al. (2014a)</ref> consid- er the importance of the subgraph of the candidate answer. <ref type="bibr" target="#b8">Dong et al. (2015)</ref> make use of the context and the type of the answer. However, the repre- sentation of the question end is oligotrophic. Ex- isting approaches often represent a question into a single vector using simple bag-of-words (BOW) model <ref type="bibr">(Bordes et al., 2014a,b)</ref>, whereas the relat- edness to the answer end is neglected. We argue that a question should be represented differently according to the different focuses of various an- swer aspects <ref type="bibr">1</ref> .</p><p>Take the question "Who is the president of France?" and one of its candidate answers "Fran- cois Hollande" as an example. When dealing with the answer entity Francois Holland, "president" and "France" in the question is more focused, and the question representation should bias towards the two words.</p><p>While facing the answer type /business/board member, "Who" should be the most prominent word. Meanwhile, some questions may value answer type more than other answer aspects. While in some other questions, answer relation may be the most important information we should consider, which is dynamic and flexible corresponding to d- ifferent questions and answers. Obviously, this is an attention mechanism, which reveals the mutual influences between the representation of questions and the corresponding answer aspects.</p><p>We believe that such kind of representation is more expressive. <ref type="bibr" target="#b8">Dong et al. (2015)</ref> represents questions using three CNNs with different param- eters when dealing with different answer aspect- s including answer path, answer context and an- swer type. The method is very enlightening and achieves the best performance on WebQeustion- s at that time among the end-to-end approach- es. However, we argue that simply selecting three independent CNNs is mechanical and inflexible. Thus, we go one step further, and propose a cross- attention based neural network to perform KB-QA. The cross-attention model, which stands for the mutual attention between the question and the answer aspects, contains two parts: the answer- towards-question attention part and the question- towards-answer attention part. The former help learn flexible and adequate question representa- tion, and the latter help adjust the question-answer weight, getting the final score. We illustrate in sec- tion 3.2 for more details. In this way, we formulate the cross-attention mechanism to model the ques- tion answering procedure. Note that our proposed model is an entire end-to-end approach which only depends on training data. Some integrated systems which use extra patterns and resources are not di- rectly comparable to ours. Our target is to explore a better solution following the end-to-end KB-QA technical path.</p><p>Moreover, we notice that the representations of the KB resources (entities and relations) are also limited in previous work. specifically, they are of- ten learned barely on the QA training data, which results in two limitations. 1) The global infor- mation of the KB is deficient. For example, if question-answer pair (q, a) appears in the train- ing data, and the global KB information implies us that a is similar to a 2 , denoted by (a ∼ a ), then (q, a ) is more probable to be right. However, cur- rent QA training mechanism cannot guarantee (a ∼ a ) could be learned.</p><p>2) The problem of out-of- vocabulary (OOV) stands out. Due to the limited coverage of the training data, the OOV problem is common while testing, and many answer enti- ties in testing candidate set have never been seen before. The attention of these resources become the same because they shared the same OOV em- bedding, and this will do harm to the proposed at- tention model. To tackle these two problems, we additionally incorporates KB itself as training data for training embeddings besides original question- answer pairs. In this way, the global structure of the whole knowledge could be captured, and the OOV problem could be alleviated naturally.</p><p>In summary, the contributions are as follows. 1) We present a novel cross-attention based NN model tailored to KB-QA task, which considers the mutual influence between the representation of questions and the corresponding answer aspects. 2) We leverage the global KB information, aiming at represent the answers more precisely. It also al-leviates the OOV problem, which is very helpful to the cross-attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) The experimental results on the open dataset</head><p>WebQuestions demonstrate the effectiveness of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>The goal of KB-QA task could be formulated as follows. Given a natural language question q, the system returns an entity set A as answers. The architecture of our proposed KB-QA system is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which illustrates the basic flow of our approach. First, we identify the topic enti- ty of the question, and generate candidate answer- s from Freebase. Then, a cross-attention based neural network is employed to represent the ques- tion under the influence of the candidate answer aspects. Finally, the similarity score between the question and each corresponding candidate answer is calculated, and the candidates with highest score will be selected as the final answers 3 . We utilize Freebase ( <ref type="bibr" target="#b2">Bollacker et al., 2008</ref>) as our knowledge base. It has more than 3 billion facts, and is used as the supporting KB for many QA tasks. In Freebase, the facts are represented by subject-predicate-object triples (s, p, o). For clarity, we call each basic el- ement a resource, which could be either an entity or a relation. For example, (/m/0f8l9c, location.country.capital,/m/05qtj) 4 describes the fact that the capital of France is Paris, where /m/0f8l9c and /m/05qtj are entities denoting France and Paris respective-ly, and location.country.capital is a relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Generation</head><p>All the entities in Freebase should be candidate an- swers ideally, but in practice, this is time consum- ing and not really necessary. For each question q, we use Freebase API ( <ref type="bibr" target="#b2">Bollacker et al., 2008)</ref> to identify a topic entity, which could be simply un- derstood as the main entity of the question. For ex- ample, France is the topic entity of question "Who is the president of France?". Freebase API method is able to resolve as many as 86% questions if we use the top1 result <ref type="bibr" target="#b24">(Yao and Van Durme, 2014</ref>). After getting the topic entity, we collect all the en- tities directly connected to it and the ones connect- ed with 2-hop 5 . These entities constitute a candi- date set C q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Neural Cross-Attention Model</head><p>We present a cross-attention based neural network, which represents the question dynamically accord- ing to different answer aspects, also considering their connections. Concretely, each aspect of the answer focuses on different words of the question and thus decides how the question is represented. Then the question pays different attention to each answer aspect to decide their weights. <ref type="figure" target="#fig_1">Figure 2</ref> is the architecture of our model. We will illustrate how the system works as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Question Representation</head><p>First of all, we have to obtain the representation of each word in the question. These representa- tions retain all the information of the question, and could serve the following steps. Suppose question q is expressed as q = (x 1 , x 2 , ..., x n ), where x i de- notes the ith word. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we first look up a word embedding matrix E w ∈ R d×vw to get the word embeddings, which is randomly ini- tialized, and updated during the training process. Here, d means the dimension of the embeddings and v w denotes the vocabulary size of natural lan- guage words.</p><p>Then, the embeddings are fed into a long short- term memory (LSTM) (Hochreiter and Schmidhu- ber, 1997) networks. LSTM has been proven to  be effective in many natural language processing (NLP) tasks such as machine translation <ref type="bibr" target="#b18">(Sutskever et al., 2014</ref>) and dependency parsing <ref type="bibr" target="#b9">(Dyer et al., 2015)</ref>, and it is adept in harnessing long sentences. Note that if we use unidirectional L- STM, the outcome of a specific word contains on- ly the information of the words before it, whereas the words after it are not taken into account. To avoid this, we employ bidirectional LSTM as Bah- danau (2015) does, which consists of both forward and backward networks. The forward LSTM han- dles the question from left to right, and the back- ward LSTM processes in the reverse order. Thus, we could acquire two hidden state sequences, one from the forward one (</p><formula xml:id="formula_0">− → h 1 , − → h 2 , .</formula><p>.., − → h n ) and the other from the backward one (</p><formula xml:id="formula_1">← − h 1 , ← − h 2 , ..., ← − h n ).</formula><p>We con- catenate the forward hidden state and the back- ward hidden state of each word, resulting in</p><formula xml:id="formula_2">h j = [ − → h j ; ← − h j ].</formula><p>The hidden unit of forward and backward LSTM is d 2 , so the concatenated vector is of di- mension d. In this way, we obtain the representa- tion of each word in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Answer aspect representation</head><p>We directly use the embedding for each answer aspect through the KB embedding matrix E k ∈ R d×v k . Here, v k means the vocabulary size of the KB resources. The embedding matrix is ran- domly initialized and learned during training, and could be further enhanced with the help of glob- al information as described in Section 3.3. Con- cretely, we employ four kinds of answer aspect- s: answer entity a e , answer relation a r , answer type a t and answer context a c 6 . Their embeddings are denoted as e e , e r , e t and e c , respectively. It is worth noting that the answer context consist- s of multiple KB resources, and we denote it as (c 1 , c 2 , ..., c m ). We first acquire their KB embed- dings (e c 1 , e c 2 , ..., e cm ) through E k , then calculate an average embedding by e c = 1</p><formula xml:id="formula_3">m m i=1 e c i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Cross-Attention model</head><p>The most crucial part of the proposed approach is the cross-attention mechanism. The cross- attention mechanism is composed of two parts: the answer-towards-question attention part and the question-towards-answer attention part.</p><p>The proposed cross-attention model could also be intuitively interpreted as a re-reading mecha- nism ( <ref type="bibr" target="#b11">Hermann et al., 2015)</ref>. Our aim is to select correct answers from a candidate set. When we judge a candidate answer, suppose we first look at its type, and we will reread the question to find out which part of the question should be more fo- cused (handling attention). Then we go to next aspect and reread the question again, until the all the aspects are utilized. After we read all the an- swer aspects and get all the scores, the final sim- ilarity score between question and answer should be a weighted sum of all the scores. We believe that this mechanism is beneficial for the system to better understand the question with the help of the answer aspects, and it may lead to a performance promotion.</p><p>• Answer-towards-question(A-Q) attention Based on our assumption, each answer aspect should focus on different words of the same ques- tion. The extent of attention can be measured by the relatedness between each word representa- tion h j and an answer aspect embedding e i . We propose the following formulas to calculate the weights.</p><formula xml:id="formula_4">α ij = exp(ω ij ) n k=1 exp(ω ik )<label>(1)</label></formula><formula xml:id="formula_5">ω ij = f (W T [h j ; e i ] + b)<label>(2)</label></formula><p>Here, α ij denotes the weight of attention from an- swer aspect e i to the jth word in the question, where e i ∈ {e e , e r , e t , e c }. f (·) is a non-linear ac- tivation function, such as hyperbolic tangent trans- formation here. Let n be the length of the ques- tion. W ∈ R 2d×d is an intermediate matrix and b is the offset. Both of them are randomly initialized and updated during training. Subsequently, ac- cording to the specific answer aspect e i , the atten- tion weights are employed to calculate a weighted sum of the hidden representations, resulting in a semantic vector that represent the question.</p><formula xml:id="formula_6">q i = n j=1 α ij h j<label>(3)</label></formula><p>The similarity score of the question q and this particular candidate answer aspect e i (e i ∈ {e e , e r , e t , e c }) could be defined as follows.</p><formula xml:id="formula_7">S (q, e i ) = h(q i , e i )<label>(4)</label></formula><p>The scoring function h(·) is computed as the in- ner product between the sentence representation q i , which has already carried the attention from the answer aspect part, and the corresponding answer aspect e i , and is parametrized into the network and updated during the training process.</p><p>• Question-towards-answer(Q-A) attention Intuitively, different question should value the four answer aspect differently. Since we have al- ready calculated the scores of (q, e i ), we define the final similarity score of the question q and each candidate answer a as follows.</p><p>S (q, a) = ei∈{ee,er,et,ec}</p><formula xml:id="formula_8">β ei S (q, e i )<label>(5)</label></formula><formula xml:id="formula_9">β ei = exp (ω ei ) e k ∈{ee,er,et,ec} exp (ω e k )<label>(6)</label></formula><formula xml:id="formula_10">ω ei = f W T [q; e i ] + b<label>(7)</label></formula><formula xml:id="formula_11">q = 1 n n j h j<label>(8)</label></formula><p>Here β e i denotes the attention of question toward- s answer aspects, indicating which answer as- pect should be more focused in one (q, a) pair. W ∈ R 2d×d is also a intermediate matrix as in the answer-towards-question attention part, and b is an offset value. 7 q is calculated by averagely pooling all the bi-directional LSTM hidden state sequences, resulting a vector which represents the question to determine which answer aspect should be more focused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training</head><p>We first construct the training data. Since we have (q, a) pairs as supervision data, candidate set C q can be divided into two subsets, namely, correc- t answer set P q and wrong answer set N q . For each correct answer a ∈ P q , we randomly select k wrong answers a ∈ N q as negative examples. For some topic entities, there may be not enough wrong answers to acquire k wrong answers. Un- der this circumstance, we extend N q from other randomly selected candidate set C q . With the gen- erated training data, we are able to make use of pairwise training. The training loss is given as fol- lows, which is a hinge loss.</p><formula xml:id="formula_12">L q,a,a = [γ + S (q, a ) − S (q, a)] + (9)</formula><p>where γ is a positive real number that ensures a margin between positive and negative examples. And [z] + means max(0, z). The intuition of this training strategy is to guarantee the score of posi- tive question-answer pairs to be higher than nega- tive ones with a margin. The objective function is as follows.</p><formula xml:id="formula_13">min q 1 |P q | a∈Pq a ∈Nq L q,a,a<label>(10)</label></formula><p>We adopt stochastic gradient descent (SGD) to minimize the learning process, shuffled mini- batches are utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Inference</head><p>In testing stage, given the candidate answer set C q , we have to calculate S(q, a) for each a ∈ C q , and find out the maximum value S max .</p><formula xml:id="formula_14">S max = arg max a∈Cq {S (q, a)}<label>(11)</label></formula><p>It is worth noting that many questions have more than one answer, so it is improper to set the can- didate answer which have the maximum value as the final answer. Instead, we take advantage of the margin γ. If the score of a candidate answer is within the margin compared with S max , we put it in the final answer set.</p><formula xml:id="formula_15">A = {â|S max − S (q, ˆ a) &lt; γ}<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Global Knowledge</head><p>In this section, we elaborate how the global in- formation of a KB could be leveraged. As stat- ed before, we try to take into account the com- plete knowledge information of the KB. To this end, we adopt TransE model ( <ref type="bibr" target="#b5">Bordes et al., 2013)</ref> and integrate its outcome into our training process. In TransE, relations are considered as translations in the embedding space. For consistency, we de- note each fact as (s, p, o). TransE utilizes pair- wise training strategy as well. Randomly sampled corrupted facts (s , p, o ) are the negative exam- ples. The distance measure d(s + p, o) is defined as s + p − o 2 2 . And the training loss is given as follows.</p><formula xml:id="formula_16">L k = (s,p,o)∈S (s ,p,o )∈S [γ k + d (s + p, o) − d s + p, o ] +<label>(13)</label></formula><p>Where S is the set of KB facts and S is the cor- rupted facts. In our QA task, we filter out the com- pletely unrelated facts to save time. Specifically, we first collect all the topic entities of all the ques- tions as initial set. Then, we expand the set by adding directly connected and 2-hop entities. Fi- nally, all the facts containing these entities form the positive set, and the negative facts are random- ly corrupted. This is a compromising solution due to the large scale of Freebase. To employ the glob- al information in our training process, we adop- t a multi-task training strategy. Specifically, we perform KB-QA training and TransE training in turn. The proposed training process ensures that the global KB information acts as additional su- pervision, and the interconnections among the re- sources are fully considered. In addition, as more KB resources are involved, the OOV problem is relieved. Since all the OOV resources have exact- ly the same attention towards a question, it will weaken the effectiveness of the attention model. So the alleviation of OOV is able to bring addi- tional benefits to the attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the proposed method, we conduct ex- periments on WebQuestions ( <ref type="bibr" target="#b1">Berant et al., 2013)</ref> dataset that includes 3,778 question-answer pairs for training and 2,032 for testing. The question- s are collected from Google Suggest API, and the answers are labeled manually by Amazon MTurk. All the answers are from Freebase. We use three- quarter of the training data as training set, and the left as validate set. We use F 1 score as evaluation matric, and the average result is computed by the script provided by <ref type="bibr" target="#b1">Berant et al. (2013)</ref>. Note that our proposed approach is an en- tire end-to-end method, which totally depends on training data. It is worth noting that <ref type="bibr" target="#b26">Yih et al. (2015;</ref> achieve much higher F 1 scores than other methods. Their staged system is able to address more questions with constraints and aggregations. However, their approach applies numbers of manually designed rules and features, which come from the observations on the training set questions. These particular manual efforts re- duce the adaptability of their approach. Moreover, there are some integrated systems such as <ref type="bibr" target="#b22">Xu et al. (2016a;</ref><ref type="bibr" target="#b21">2016b</ref>) achieve higher F 1 scores which leverage Wikipedia free text as external knowl- edge, so their systems are not directly comparable to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>For KB-QA training, we use mini-batch stochastic gradient descent to minimize the pairwise train- ing loss. The minibatch size is set to 100. The learning rate is set to 0.01. Both the word embed- ding matrix E w and KB embedding matrix E v are normalized after each epoch. The embedding size d = 512, then the hidden unit size is 256. Mar- gin γ is set to 0.6. Negative example number k = 2000. We set the embedding dimension to 512 in TransE training process, and the minibatch size is also 100. γ k is set to 1. All these hyperparameters of the proposed network is determined according to the performance on the validate set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The effectiveness of the proposed approach</head><p>To demonstrate the effectiveness of the pro- posed approach, we compare our method with state-of-the-art end-to-end NN-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Avg   <ref type="table" target="#tab_2">Table 1</ref> shows the results on WebQuestions dataset. <ref type="bibr" target="#b6">Bordes et al. (2014b)</ref> apply BOW method to obtain a single vector for both questions and answers. <ref type="bibr" target="#b3">Bordes et al. (2014a)</ref> further improve their work by proposing the concept of subgraph embeddings. Besides the answer path, the sub-graph contains all the entities and relations con- nected to the answer entity. The final vector is also obtained by bag-of-words strategy. <ref type="bibr" target="#b23">Yang et al. (2014)</ref> follow the SP-based manner, but uses embeddings to map entities and relations into K- B resources, then the question can be converted into logical forms. They jointly consider the t- wo mapping processes. <ref type="bibr" target="#b8">Dong et al. (2015)</ref> use three columns of Convolutional Neural Network- s (CNNs) to represent questions corresponding to three aspects of the answers, namely the answer context, the answer path and the answer type. <ref type="bibr" target="#b4">Bordes et al. (2015)</ref> put KB-QA into the memory net- works framework <ref type="bibr" target="#b17">(Sukhbaatar et al., 2015)</ref>, and achieves the state-of-the-art performance of end- to-end methods. Our approach employs bidirec- tional LSTM, cross-attention model and global K- B information.</p><p>From the results, we observe that our approach achieves the best performance of all the end-to-end methods on WebQuestions. <ref type="bibr" target="#b6">Bordes et al. (2014b;</ref><ref type="bibr" target="#b3">2014a;</ref> all utilize BOW model to represent the questions, while ours takes advantage of the at- tention of answer aspects to dynamically represent the questions. Also note that <ref type="bibr" target="#b4">Bordes et al. (2015)</ref> uses additional training data such as Reverb <ref type="bibr" target="#b10">(Fader et al., 2011</ref>) and their original dataset Simple- Questions. <ref type="bibr" target="#b8">Dong et al. (2015)</ref> employs three fixed CNNs to represent questions, while ours is able to express the focus of each unique answer aspec- t to the words in the question. Besides, our ap- proach employs the global KB information. So, we believe that the results faithfully show that the proposed approach is more effective than the other competitive methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Analysis</head><p>In this part, we further discuss the impacts of the components of our model. <ref type="table" target="#tab_4">Table 2</ref> indicates the effectiveness of different parts in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Avg  LSTM employs unidirectional LSTM, and us- es the last hidden state as the question repre- sentation. Bi LSTM adopts a bidirectional LST- M. A-Q-ATT denotes the answer-towards-question attention part, and C-ATT stands for our cross- attention. GKI means global knowledge informa- tion. Bi LSTMS+C-ATT+GKI is our full proposed approach. From the results, we could observe the following. 1) Bi LSTM+C-ATT dramatically improves the F 1 score by 2.7 points compared with Bi LSTM, 0.2 points higher than Bi LSTM+A-Q-ATT. Simi- larly, Bi LSTM+C-ATT+GKI significantly outper- forms Bi LSTM+GKI by 2.5 points, improving Bi LSTM+A-Q-ATT+GKI by 0.3 points. The re- sults prove that the proposed cross-attention mod- el is effective.</p><p>2) Bi LSTM+GKI performs better than Bi LSTM, and achieves an improvement of 1.3 points. Similarly, Bi LSTM+C-ATT+GKI im- proves Bi LSTM+C-ATT by 1.1 points, which indicates that the proposed training strategy successfully leverages the global information of the underlying KB.</p><p>3) Bi LSTM+C-ATT+GKI achieves the best performance as we expected, and improves the o- riginal Bi LSTM dramatically by 3.8 points. This directly shows the power of the attention model and the global KB information.</p><p>To illustrate the effectiveness of the atten- tion mechanism clearly, we present the attention weights of a question in the form of heat map as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. From this example we observe that our meth- ods is able to capture the attention properly. It is instructive to figure out the attention part of the question when dealing with different answer as- pects. The heat map will help us understand which parts are most useful for selecting correct answer- s. For instance, from <ref type="figure" target="#fig_2">Figure 3</ref>, we can see that location.country is paying great attention to "Where", indicating that "Where" is much more important than the other parts in the question when dealing with this type. In other words, the other parts are not that crucial since "Where" is strongly implying that the question is asking about a loca- tion. As for Q-A attention part, we see that answer type and answer relation are more important than other answer aspects in this example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Error Analysis</head><p>We randomly sample 100 imperfectly answered questions and categorize the errors into two main classes as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrong attention</head><p>In some occasions (18 in 100 questions, 18%), we find the generated attention weights unrea- sonable. For instance, for question "What are the songs that Justin Bieber wrote?", answer type /music/composition pays the most atten- tion on "What" rather than "songs". We think this is due to the bias of the training data, and we be- lieve these errors could be solved by introducing more instructive training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex questions and label errors</head><p>Another challenging problem is the complex questions (35%). For example, "When was the last time Knicks won the championship?" is actu- ally to ask the last championship, but the predicted answers give all the championships. This is due to that the model cannot learn what "last" mean in the training process. In addition, the label mis- takes also influence the evaluation (3%), such as, "What college did John Nash teach at?", where the labeled answer is Princeton University, but</p><p>Massachusetts Institute of Technology should also be an answer, and the proposed method is able to answer it correctly. Other errors include topic entity generation error and the multiple answers error (giving more answers than expected). We guess these errors are caused by the simple implementations of the related steps in our method, and we will not explain them in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The past years have seen a growing amount of re- search on KB-QA, shaping an interaction paradig- m that allows end users to profit from the expres- sive power of Semantic Web data while at the same time hiding their complexity behind an intuitive and easy-to-use interface. At the same time the growing amount of data has led to a heterogeneous data landscape where QA systems struggle to keep up with the volume, variety and veracity of the un- derlying knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Neural Network-based KB-QA</head><p>In recent years, deep neural networks have been applied to many NLP tasks, showing promising re- sults. <ref type="bibr" target="#b6">Bordes et al. (2014b)</ref> was the first to intro- duce NN-based method to solve KB-QA problem. The questions and KB triples were represented by vectors in a low dimensional space. Thus the co- sine similarity could be used to find the most pos- sible answer. BOW method was employed to ob- tain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. <ref type="bibr" target="#b3">Bordes et al. (2014a)</ref> further im- proved their work by proposing the concept of subgraph embeddings. The key idea was to in- volve as much information as possible in the an- swer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also ob- tained by bag-of-words strategy. <ref type="bibr" target="#b27">Yih et al. (2014)</ref> focused on single-relation questions. The KB-QA task was divided into t- wo steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relation- s. <ref type="bibr" target="#b23">Yang et al. (2014)</ref> tackled entity and relation mapping as joint procedures. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain inter- mediate mapping results.</p><p>The most similar work to ours is <ref type="bibr" target="#b8">Dong et al. (2015)</ref>. They considered the different aspects of answers, using three columns of CNNs to repre- sent questions respectively. The difference is that our approach uses cross-attention mechanism for each unique answer aspect, so the question repre- sentation is not fixed to only three types. More- over, we utilize the global KB information. <ref type="bibr" target="#b22">Xu et al. (2016a;</ref><ref type="bibr" target="#b21">2016b)</ref> proposed integrated systems to address KB-QA problems incorporat- ing Wikipedia free text, in which they used multi- channel CNNs to extract relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Attention-based Model</head><p>The attention mechanism has been widely used in different areas. <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> first applied attention model in NLP. They improved the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. <ref type="bibr" target="#b16">Rush et al. (2015)</ref> implemented sentence-level summa- rization task. They utilized local attention-based model that generated each word of the summa- ry conditioned on the input sentence. <ref type="bibr" target="#b20">Wang et al. (2016)</ref> proposed an inner attention mechanis- m that the attention was imposed directly to the input. And their experiment on answer selection showed the advantage of inner attention compared with traditional attention methods. <ref type="bibr" target="#b29">Yin et al. (2016)</ref> tackled simple question an- swering by an attentive convolutional neural net- work. They stacked an attentive max-pooling above convolution layer to model the relationship between predicates and question patterns. Our ap- proach differs from previous work in that we use attentions to help represent questions dynamical- ly, not generating current word from vocabulary as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we focus on KB-QA task. Firstly, we consider the impacts of the different answer aspects when representing the question, and pro- pose a novel cross-attention model for KB-QA. Specifically, we employ the focus of the answer aspects to each question word and the attention weights of the question towards the answer aspect- s. This kind of dynamic representation is more precise and flexible. Secondly, we leverage the global KB information, which could take full ad- vantage of the complete KB, and also alleviate the OOV problem for the attention model. The ex- tensive experiments demonstrate that the proposed approach could achieve better performance com- pared with state-of-the-art end-to-end methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of the proposed KB-QA system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of the proposed crossattention based neural network. Note that only one aspect(in orange color) is depicted for clarity. The other three aspects follow the same way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The visualized attention heat map. Answer entity: /m/06npd(Slovakia), answer relation: partially containedby, answer type: /location/country, answer context: (/m/04dq9kf, /m/01mp, ...)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : The evaluation results on WebQuestions.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : The ablation results of our models.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> An answer aspect could be the answer entity itself, the answer type, the answer context, etc.</note>

			<note place="foot" n="2"> The complete KB is able to offer this kind of information, e.g., a and a share massive context.</note>

			<note place="foot" n="3"> We also adopt a margin strategy to obtain multiple answers for a question and this will be explained in the next section. 4 Note that the Freebase prefixes are omitted for neatness.</note>

			<note place="foot" n="5"> For example, (/m/0f8l9c, governing officials, government.position held.office holder, /m/02qg4z) is a 2-top connection.</note>

			<note place="foot" n="6"> Answer context is the 1-hop entities and predicates which connect to the answer entity along the answer path.</note>

			<note place="foot" n="7"> Note that the W and b in the two attention part is different and independent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Natural Sci-ence Foundation of China (No.61533018) and the National Program of <ref type="bibr">China (973 program No. 2014CB340505)</ref>. And this research work was al-so supported by Google through focused research awards program. We would like to thank the anonymous reviewers for their useful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D13-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1067</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Largescale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P13-1042" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Question answering over freebase with multicolumn convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1026</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1033</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D11-1142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D13-1161" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sparql query language for rdf. w3c recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Prudhommeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Seaborne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transforming dependency structures to logical forms for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/Q16-1010" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="127" to="141" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Mark Steedman, and Mirella Lapata</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1044</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An introduction to question answering over linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reasoning Web International Summer School</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="100" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inner attention based recurrent neural networks for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1122</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1122" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hybrid question answering over knowledge base and free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1226" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Question answering on freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1220</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1220" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2326" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haechang</forename><surname>Rim</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1071</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1071" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<idno type="doi">10.3115/v1/P14-1090</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-1090" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1128</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1128" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-2105</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-2105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="643" to="648" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The value of 230 semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-2033</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-2033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple question answering by attentive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1164" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1746" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning context-dependent mappings from sentences to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P09-1110" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="976" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.1420</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
