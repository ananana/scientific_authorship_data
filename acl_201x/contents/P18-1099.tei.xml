<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptation with Adversarial Training and Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firoj</forename><surname>Alam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering †</orgName>
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<orgName type="institution" key="instit3">Nanyang Technological University</orgName>
								<address>
									<country>Singapore †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering †</orgName>
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<orgName type="institution" key="instit3">Nanyang Technological University</orgName>
								<address>
									<country>Singapore †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering †</orgName>
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<orgName type="institution" key="instit3">Nanyang Technological University</orgName>
								<address>
									<country>Singapore †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qatar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering †</orgName>
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
								<orgName type="institution" key="instit3">Nanyang Technological University</orgName>
								<address>
									<country>Singapore †</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptation with Adversarial Training and Graph Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1077" to="1087"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1077</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and un-labeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unla-beled data for the current event. We propose a novel model that performs adver-sarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters ( <ref type="bibr" target="#b8">Imran et al., 2015)</ref>. In such events, affected people post timely and useful in- formation of various types such as reports of in- jured or dead people, infrastructure damage, ur- gent needs (e.g., food, shelter, medical assistance) on these social networks. Humanitarian organiza- tions believe timely access to this important infor- mation from social networks can help significantly and reduce both human loss and economic dam- age ( <ref type="bibr" target="#b26">Varga et al., 2013;</ref><ref type="bibr" target="#b20">Power et al., 2013)</ref>.</p><p>In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the prede- fined classes of interest (e.g., relevant vs. non- relevant) in real-time. Recently, deep neural net- works (DNNs) have shown great performance in classification tasks in NLP and data mining. How- ever the success of DNNs on a task depends heav- ily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake). On the other hand, in most cases, we can have ac- cess to a good amount of labeled and abundant un- labeled data from past similar events (e.g., Floods) and possibly some unlabeled data for the current event. In such situations, we need methods that can leverage the labeled and unlabeled data in a past event (we refer to this as a source domain), and that can adapt to a new event (we refer to this as a target domain) without requiring any la- beled data in the new event. In other words, we need models that can do domain adaptation to deal with the distribution drift between the domains and semi-supervised learning to leverage the un- labeled data in both domains.</p><p>Most recent approaches to semi-supervised learning ( <ref type="bibr" target="#b28">Yang et al., 2016)</ref> and domain adapta- tion ( <ref type="bibr" target="#b4">Ganin et al., 2016</ref>) use the automatic fea- ture learning capability of DNN models. In this paper, we extend these methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework. In this framework, the basic task-solving network (a convolutional neu- ral network in our case) is put together with two other networks -one for semi-supervised learning and the other for domain adaptation. The semi- supervised component learns internal representa-tions (features) by predicting contextual nodes in a graph that encodes similarity between labeled and unlabeled training instances. The domain adap- tation is achieved by training the feature extractor (or encoder) in adversary with respect to a domain discriminator, a binary classifier that tries to dis- tinguish the domains. The overall idea is to learn high-level abstract representation that is discrim- inative for the main classification task, but is in- variant across the domains. We propose a stochas- tic gradient descent (SGD) algorithm to train the components of our model simultaneously.</p><p>The evaluation of our proposed model is con- ducted using two Twitter datasets on scenarios where there is only unlabeled data in the target do- main. Our results demonstrate the following.</p><p>1. When the network combines the semi- supervised component with the supervised component, depending on the amount of la- beled data used, it gives 5% to 26% absolute gains in F1 compared to when it uses only the supervised component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Domain adaptation with adversarial training</head><p>improves over the adaptation baseline (i.e., a transfer model) by 1.8% to 4.1% absolute F1. 3. When the network combines domain adver- sarial training with semi-supervised learning, we get further gains ranging from 5% to 7% absolute in F1 across events.</p><p>Our source code is available on Github 1 and the data is available on CrisisNLP 2 .</p><p>The rest of the paper is organized as follows. In Section 2, we present the proposed method, i.e., domain adaptation and semi-supervised graph em- bedding learning. In Section 3, we present the ex- perimental setup and baselines. The results and analysis are presented in Section 4. In Section 5, we present the works relevant to this study. Fi- nally, conclusions appear in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Model</head><p>We demonstrate our approach for domain adapta- tion with adversarial training and graph embed- ding on a tweet classification task to support cri-</p><formula xml:id="formula_0">sis response efforts. Let D l S = {t i , y i } Ls i=1 and D u S = {t i } Us i=1</formula><p>be the set of labeled and un- labeled tweets for a source crisis event S (e.g., Nepal earthquake), where y i ∈ {1, . . . , K} is the class label for tweet t i , L s and U s are the num- ber of labeled and unlabeled tweets for the source event, respectively. In addition, we have unla- beled tweets D u T = {t i } Ut i=1 for a target event T (e.g., Queensland flood) with U t being the num- ber of unlabeled tweets in the target domain. Our ultimate goal is to train a cross-domain model p(y|t, θ) with parameters θ that can classify any tweet in the target event T without having any in- formation about class labels in T . <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall architecture of our neural model. The input to the network is a tweet t = (w 1 , . . . , w n ) containing words that come from a finite vocabulary V defined from the train- ing set. The first layer of the network maps each of these words into a distributed representation R d by looking up a shared embedding matrix E ∈ R |V|×d . We initialize the embedding matrix E in our network with word embeddings that are pre- trained on a large crisis dataset (Subsection 2.5). However, embedding matrix E can also be initial- ize randomly. The output of the look-up layer is a matrix X ∈ R n×d , which is passed through a number of convolution and pooling layers to learn higher-level feature representations. A convolu- tion operation applies a filter u ∈ R k.d to a win- dow of k vectors to produce a new feature h t as</p><formula xml:id="formula_1">h t = f (u.X t:t+k−1 )<label>(1)</label></formula><p>where X t:t+k−1 is the concatenation of k look-up vectors, and f is a nonlinear activation; we use rectified linear units or ReLU. We apply this fil- ter to each possible k-length windows in X with stride size of 1 to generate a feature map h j as:</p><formula xml:id="formula_2">h j = [h 1 , . . . , h n+k−1 ]<label>(2)</label></formula><p>We repeat this process N times with N different filters to get N different feature maps. We use a wide convolution ( <ref type="bibr" target="#b10">Kalchbrenner et al., 2014</ref>), which ensures that the filters reach the entire tweet, including the boundary words. This is done by performing zero-padding, where out-of- range (i.e., t&lt;1 or t&gt;n) vectors are assumed to be zero. With wide convolution, o zero-padding size and 1 stride size, each feature map contains (n + 2o − k + 1) convoluted features. After the convolution, we apply a max-pooling operation to each of the feature maps,  where µ p (h j ) refers to the max operation applied to each window of p features with stride size of 1 in the feature map h i . Intuitively, the convolu- tion operation composes local features into higher- level representations in the feature maps, and max- pooling extracts the most important aspects of each feature map while reducing the output dimen- sionality. Since each convolution-pooling opera- tion is performed independently, the features ex- tracted become invariant in order (i.e., where they occur in the tweet). To incorporate order infor- mation between the pooled features, we include a fully-connected (dense) layer</p><formula xml:id="formula_3">m = [µ p (h 1 ), · · · , µ p (h N )]<label>(3</label></formula><formula xml:id="formula_4">! ! ! ! ! ! ! ! ! ! ! ! ! ! ! loss L G −λ d ∂L D ∂Λ Dense (z d ) ! ! ! Domain label</formula><formula xml:id="formula_5">z = f (V m)<label>(4)</label></formula><p>where V is the weight matrix. We choose a con- volutional architecture for feature composition be- cause it has shown impressive results on similar tasks in a supervised setting <ref type="bibr" target="#b18">(Nguyen et al., 2017)</ref>. The network at this point splits into three branches (shaded with three different colors in <ref type="figure" target="#fig_0">Figure 1</ref>) each of which serves a different purpose and contributes a separate loss to the overall loss of the model as defined below:</p><formula xml:id="formula_6">L(Λ, Φ, Ω, Ψ) = L C (Λ, Φ) + λg L G (Λ, Ω) + λ d L D (Λ, Ψ) (5)</formula><p>where Λ = {U, V } are the convolutional filters and dense layer weights that are shared across the three branches. The first component L C (Λ, Φ) is a supervised classification loss based on the labeled data in the source event. The second component L G (Λ, Ω) is a graph-based semi-supervised loss that utilizes both labeled and unlabeled data in the source and target events to induce structural simi- larity between training instances. The third com- ponent L D (Λ, Ω) is an adversary loss that again uses all available data in the source and target do- mains to induce domain invariance in the learned features. The tunable hyperparameters λ g and λ d control the relative strength of the components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Component</head><p>The supervised component induces label informa- tion (e.g., relevant vs. non-relevant) directly in the network through the classification loss L C (Λ, Φ), which is computed on the labeled instances in the source event, D l S . Specifically, this branch of the network, as shown at the top in <ref type="figure" target="#fig_0">Figure 1</ref>, takes the shared representations z as input and pass it through a task-specific dense layer</p><formula xml:id="formula_7">z c = f (V c z) (6)</formula><p>where V c is the corresponding weight matrix. The activations z c along with the activations from the semi-supervised branch z s are used for classifica- tion. More formally, the classification layer de- fines a Softmax</p><formula xml:id="formula_8">p(y = k|t, θ) = exp W T k [z c ; z s ] k exp W T k [z c ; z s ] (7)</formula><p>where <ref type="bibr">[.;</ref> .] denotes concatenation of two column vectors, W k are the class weights, and θ = {U, V, V c , W } defines the relevant parameters for this branch of the network with Λ = {U, V } being the shared parameters and Φ = {V c , W } being the parameters specific to this branch. Once learned, we use θ for prediction on test tweets. The classi-</p><formula xml:id="formula_9">fication loss L C (Λ, Φ) (or L C (θ)) is defined as LC(Λ, Φ) = − 1 Ls Ls i=1 I(yi = k) log p(yi = k|ti, Λ, Φ) (8)</formula><p>where I(.) is an indicator function that returns 1 when the argument is true, otherwise it returns 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised Component</head><p>The semi-supervised branch (shown at the mid- dle in <ref type="figure" target="#fig_0">Figure 1</ref>) induces structural similarity be- tween training instances (labeled or unlabeled) in the source and target events. We adopt the recently proposed graph-based semi-supervised deep learning framework ( <ref type="bibr" target="#b28">Yang et al., 2016)</ref>, which shows impressive gains over existing semi- supervised methods on multiple datasets. In this framework, a "similarity" graph G first encodes relations between training instances, which is then used by the network to learn internal representa- tions (i.e., embeddings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Learning Graph Embeddings</head><p>The semi-supervised branch takes the shared rep- resentation z as input and learns internal represen- tations by predicting a node in the graph context of the input tweet. Following ( <ref type="bibr" target="#b28">Yang et al., 2016)</ref>, we use negative sampling to compute the loss for pre- dicting the context node, and we sample two types of contextual nodes: (i) one is based on the graph G to encode structural information, and (ii) the second is based on the labels in D l S to incorpo- rate label information through this branch of the network. The ratio of positive and negative sam- ples is controlled by a random variable ρ 1 ∈ (0, 1), and the proportion of the two context types is con- trolled by another random variable ρ 2 ∈ (0, 1); see Algorithm 1 of (Yang et al., <ref type="bibr">2016</ref>) for details on the sampling procedure.</p><p>Let (j, γ) is a tuple sampled from the distribu- tion p(j, γ|i, D l S , G), where j is a context node of an input node i and γ ∈ {+1, −1} denotes whether it is a positive or a negative sample; γ = +1 if t i and t j are neighbors in the graph (for graph-based context) or they both have same labels (for label-based context), otherwise γ = −1. The negative log loss for context prediction L G (Λ, Ω) can be written as</p><formula xml:id="formula_10">L G (Λ, Ω) = − 1 Ls + Us Ls+Us i=1 E (j,γ) log σ γC T j zg(i)<label>(9)</label></formula><p>where z g (i) = f (V g z(i)) defines another dense layer (marked as Dense (z g ) in <ref type="figure" target="#fig_0">Figure 1</ref>) having weights V g , and C j is the weight vector associ- ated with the context node t j . Note that here Λ = {U, V } defines the shared parameters and Ω = {V g , C} defines the parameters specific to the semi-supervised branch of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Graph Construction</head><p>Typically graphs are constructed based on a re- lational knowledge source, e.g., citation links in ( <ref type="bibr" target="#b12">Lu and Getoor, 2003)</ref>, or distance between in- stances ( <ref type="bibr" target="#b30">Zhu, 2005</ref>). However, we do not have ac- cess to such a relational knowledge in our setting.</p><p>On the other hand, computing distance between n(n−1)/2 pairs of instances to construct the graph is also very expensive ( <ref type="bibr" target="#b17">Muja and Lowe, 2014</ref>). Therefore, we choose to use k-nearest neighbor- based approach as it has been successfully used in other study ( <ref type="bibr" target="#b25">Steinbach et al., 2000</ref>). The nearest neighbor graph consists of n ver- tices and for each vertex, there is an edge set con- sisting of a subset of n instances, i.e., tweets in our training set. The edge is defined by the dis- tance measure d(i, j) between tweets t i and t j , where the value of d represents how similar the two tweets are. We used k-d tree data structure <ref type="bibr" target="#b2">(Bentley, 1975)</ref> to efficiently find the nearest in- stances. To construct the graph, we first represent each tweet by averaging the word2vec vectors of its words, and then we measure d(i, j) by com- puting the Euclidean distance between the vectors. The number of nearest neighbor k was set to 10. The reason of averaging the word vectors is that it is computationally simpler and it captures the rel- evant semantic information for our task in hand. Likewise, we choose to use Euclidean distance in- stead of cosine for computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Adversarial Component</head><p>The network described so far can learn abstract features through convolutional and dense lay- ers that are discriminative for the classification task (relevant vs. non-relevant). The super- vised branch of the network uses labels in the source event to induce label information directly, whereas the semi-supervised branch induces sim- ilarity information between labeled and unlabeled instances. However, our goal is also to make these learned features invariant across domains or events (e.g., Nepal Earthquake vs. Queensland Flood). We achieve this by domain adversarial training of neural networks ( <ref type="bibr" target="#b4">Ganin et al., 2016)</ref>.</p><p>We put a domain discriminator, another branch in the network (shown at the bottom in <ref type="figure" target="#fig_0">Figure 1</ref>) that takes the shared internal representation z as input, and tries to discriminate between the do- mains of the input -in our case, whether the in- put tweet is from D S or from D T . The domain discriminator is defined by a sigmoid function:</p><formula xml:id="formula_11">ˆ δ = p(d = 1|t, Λ, Ψ) = sigm(w T d z d )<label>(10)</label></formula><p>where d ∈ {0, 1} denotes the domain of the input tweet t, w d are the final layer weights of the dis- criminator, and z d = f (V d z) defines the hidden layer of the discriminator with layer weights V d .</p><p>Here Λ = {U, V } defines the shared parameters, and Ψ = {V d , w d } defines the parameters specific to the domain discriminator. We use the negative log-probability as the discrimination loss:</p><formula xml:id="formula_12">J i (Λ, Ψ) = −d i logˆδlogˆ logˆδ − (1 − d i ) log 1 − ˆ δ<label>(11)</label></formula><p>We can write the overall domain adversary loss over the source and target domains as</p><formula xml:id="formula_13">L D (Λ, Ψ) = − 1 Ls + Us Ls+Us i=1 J i (Λ, Ψ) − 1 Ut U t i=1 J i (Λ, Ψ) (12)</formula><p>where L s + U s and U t are the number of training instances in the source and target domains, respec- tively. In adversarial training, we seek parameters (saddle point) such that</p><formula xml:id="formula_14">θ * = argmin Λ,Φ,Ω max Ψ L(Λ, Φ, Ω, Ψ)<label>(13)</label></formula><p>which involves a maximization with respect to Ψ and a minimization with respect to {Λ, Φ, Ω}. In other words, the updates of the shared parameters Λ = {U, V } for the discriminator work adversari- ally to the rest of the network, and vice versa. This is achieved by reversing the gradients of the dis- crimination loss L D (Λ, Ψ), when they are back- propagated to the shared layers (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Training</head><p>Algorithm 1 illustrates the training algorithm based on stochastic gradient descent (SGD). We first initialize the model parameters. The word embedding matrix E is initialized with pre-trained word2vec vectors (see Subsection 2.5) and is kept fixed during training. <ref type="bibr">3</ref> Other parameters are ini- tialized with small random numbers sampled from 3 Tuning E on our task by backpropagation increased the training time immensely (3 days compared to 5 hours on a Tesla GPU) without any significant performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Model Training with SGD</head><formula xml:id="formula_15">Input : data D l S , D u S , D u T ; graph G Output: learned parameters θ = {Λ, Φ} 1. Initialize model parameters {E, Λ, Φ, Ω, Ψ}; 2. repeat // Semi-supervised for each batch sampled from p(j, γ|i, D l S , G) do a</formula><note type="other">) Compute loss LG(Λ, Ω) b) Take a gradient step for LG(Λ, Ω); end // Supervised &amp; domain adversary for each batch sampled from D l S do a) Compute LC(Λ, Φ) and LD(Λ, Ψ) b) Take gradient steps for LC(Λ, Φ) and LD(Λ, Ψ); end // Domain adversary for each batch sampled from D u S and D u T do a) Compute LD(Λ, Ψ) b) Take a gradient step for LD(Λ, Ψ); end until convergence; a uniform distribution (Bengio and Glorot, 2010). We use AdaDelta (Zeiler, 2012) adaptive update to update the parameters.</note><p>In each iteration, we do three kinds of gradi- ent updates to account for the three different loss components. First, we do an epoch over all the training instances updating the parameters for the semi-supervised loss, then we do an epoch over the labeled instances in the source domain, each time updating the parameters for the supervised and the domain adversary losses. Finally, we do an epoch over the unlabeled instances in the two domains to account for the domain adversary loss.</p><p>The main challenge in adversarial training is to balance the competing components of the net- work. If one component becomes smarter than the other, its loss to the shared layer becomes useless, and the training fails to converge <ref type="bibr" target="#b0">(Arjovsky et al., 2017)</ref>. Equivalently, if one component becomes weaker, its loss overwhelms that of the other, caus- ing the training to fail. In our experiments, we observed the domain discriminator is weaker than the rest of the network. This could be due to the noisy nature of tweets, which makes the job for the domain discriminator harder. To balance the components, we would want the error signals from the discriminator to be fairly weak, also we would want the supervised loss to have more impact than the semi-supervised loss. In our experiments, the weight of the domain adversary loss λ d was fixed to 1e − 8, and the weight of the semi-supervised loss λ g was fixed to 1e − 2. Other sophisticated weighting schemes have been proposed recently ( <ref type="bibr" target="#b4">Ganin et al., 2016;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b14">Metz et al., 2016)</ref>. It would be interesting to see how our model performs using these advanced tuning methods, which we leave as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Crisis Word Embedding</head><p>As mentioned, we used word embeddings that are pre-trained on a crisis dataset. To train the word- embedding model, we first pre-processed tweets collected using the AIDR system ( ) during different events occurred between 2014 and 2016. In the preprocessing step, we lowercased the tweets and removed URLs, digit, time patterns, special characters, single character, username started with the @ symbol. After pre- processing, the resulting dataset contains about 364 million tweets and about 3 billion words.</p><p>There are several approaches to train word embeddings such as continuous bag-of-words (CBOW) and skip-gram models of wrod2vec ( <ref type="bibr" target="#b15">Mikolov et al., 2013)</ref>, and Glove ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>). For our work, we trained the CBOW model from word2vec. While training CBOW, we filtered out words with a frequency less than or equal to 5, and we used a context window size of 5 and k = 5 negative samples. The resulting embedding model contains about 2 million words with vector dimensions of 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings</head><p>In this section, we describe our experimental set- tings -datasets used, settings of our models, com- pared baselines, and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>To conduct the experiment and evaluate our sys- tem, we used two real-world Twitter datasets col- lected during the 2015 Nepal earthquake (NEQ) and the 2013 Queensland floods (QFL). These datasets are comprised of millions of tweets col- lected through the Twitter streaming API 4 using event-specific keywords/hashtags.</p><p>To obtain the labeled examples for our task we employed paid workers from the Crowdflower 5 - a crowdsourcing platform. The annotation con- sists of two classes relevant and non-relevant. For the annotation, we randomly sampled 11,670 and 10,033 tweets from the Nepal earthquake and the Queensland floods datasets, respectively. Given a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Relevant Non-relevant Train Dev Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NEQ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5,527 6,141 7,000 1,167 3,503</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QFL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5,414 4,619 6,019 1,003 3,011</head><p>Table 1: Distribution of labeled datasets for Nepal earthquake (NEQ) and Queensland flood (QFL).</p><p>tweet, we asked crowdsourcing workers to assign the "relevant" label if the tweet conveys/reports information useful for crisis response such as a re- port of injured or dead people, some kind of in- frastructure damage, urgent needs of affected peo- ple, donations requests or offers, otherwise assign the "non-relevant" label. We split the labeled data into 60% as training, 30% as test and 10% as de- velopment. <ref type="table">Table 1</ref> shows the resulting datasets with class-wise distributions. Data preprocessing was performed by following the same steps used to train the word2vec model (Subsection 2.5). In all the experiments, the classification task consists of two classes: relevant and non-relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Settings and Baselines</head><p>In order to demonstrate the effectiveness of our joint learning approach, we performed a series of experiments. To understand the contribution of different network components, we performed an ablation study showing how the model performs as a semi-supervised model alone and as a do- main adaptation model alone, and then we com- pare them with the combined model that incorpo- rates all the components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Settings for Semi-supervised Learning</head><p>As a baseline for the semi-supervised experi- ments, we used the self-training approach <ref type="bibr" target="#b23">(Scudder, 1965)</ref>. For this purpose, we first trained a su- pervised model using the CNN architecture (i.e., shared components followed by the supervised part in <ref type="figure" target="#fig_0">Figure 1</ref>). The trained model was then used to automatically label the unlabeled data. In- stances with a classifier confidence score ≥ 0.75 were then used to retrain a new model. Next, we run experiments using our graph- based semi-supervised approach (i.e., shared com- ponents followed by the supervised and semi- supervised parts in <ref type="figure" target="#fig_0">Figure 1</ref>), which exploits unla- beled data. For reducing the computational cost, we randomly selected 50K unlabeled instances from the same domain. For our semi-supervised setting, one of the main goals was to understand how much labeled data is sufficient to obtain a reasonable result. Therefore, we experimented our system by incrementally adding batches of in- stances, such as 100, 500, 2000, 5000, and all in- stances from the training set. Such an understand- ing can help us design the model at the onset of a crisis event with sufficient amount of labeled data. To demonstrate that the semi-supervised approach outperforms the supervised baseline, we run su- pervised experiments using the same number of la- beled instances. In the supervised setting, only z c activations in <ref type="figure" target="#fig_0">Figure 1</ref> are used for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Settings for Domain Adaptation</head><p>To set a baseline for the domain adaptation experi- ments, we train a CNN model (i.e., shared compo- nents followed by the supervised part in <ref type="figure" target="#fig_0">Figure 1</ref>) on one event (source) and test it on another event (target). We call this as transfer baseline.</p><p>To assess the performance of our domain adap- tation technique alone, we exclude the semi- supervised component from the network. We train and evaluate models with this network configura- tion using different source and target domains.</p><p>Finally, we integrate all the components of the network as shown in <ref type="figure" target="#fig_0">Figure 1</ref> and run domain adaptation experiments using different source and target domains. In all our domain adaptation ex- periments, we only use unlabeled instances from the target domain. In domain adaption literature, this is known as unsupervised adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training Settings</head><p>We use 100, 150, and 200 filters each having the window size of 2, 3, and 4, respectively, and pool- ing length of 2, 3, and 4, respectively. We do not tune these hyperparameters in any experimental setting since the goal was to have an end-to-end comparison with the same hyperparameter setting and understand whether our approach can outper- form the baselines or not. Furthermore, we do not filter out any vocabulary item in any settings.</p><p>As mentioned before in Subsection 2.4, we used AdaDelta <ref type="bibr" target="#b29">(Zeiler, 2012</ref>) to update the model pa- rameters in each SGD step. The learning rate was set to 0.1 when optimizing on the classification loss and to 0.001 when optimizing on the semi- supervised loss. The learning rate for domain ad- versarial training was set to 1.0. The maximum number of epochs was set to 200, and dropout rate of 0.02 was used to avoid overfitting ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>). We used validation-based early stop- ping using the F-measure with a patience of 25,  <ref type="table">Table 2</ref>: Results using supervised, self-training, and graph-based semi-supervised approaches in terms of Weighted average AUC, precision (P), re- call (R) and F-measure (F1).</p><p>i.e., we stop training if the score does not increase for 25 consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Evaluation Metrics</head><p>To measure the performance of the trained mod- els using different approaches described above, we use weighted average precision, recall, F-measure, and Area Under ROC-Curve (AUC), which are standard evaluation measures in the NLP and ma- chine learning communities. The rationale behind choosing the weighted metric is that it takes into account the class imbalance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>In this section, we present the experimental results and discuss our main findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semi-supervised Learning</head><p>In <ref type="table">Table 2</ref>, we present the results obtained from the supervised, self-training based semi-supervised, and our graph-based semi-supervised experiments for the both datasets. It can be clearly observed that the graph-based semi-supervised approach outperforms the two baselines -supervised and self-training based semi-supervised. Specifically, the graph-based approach shows 4% to 13% ab- solute improvements in terms of F1 scores for the Nepal and Queensland datasets, respectively. To determine how the semi-supervised ap- proach performs in the early hours of an event when only fewer labeled instances are available, we mimic a batch-wise (not to be confused with minibatch in SGD) learning setting. In <ref type="table">Table 3</ref>, we present the results using different batch sizes - 100, 500, 1,000, 2,000, and all labels.</p><p>From the results, we observe that models' per- formance improve as we include more labeled data  <ref type="table">Table 3</ref>: Weighted average F-measure for the graph-based semi-supervised settings using differ- ent batch sizes. L refers to labeled data, U refers to unlabeled data, All L refers to all labeled instances for that particular dataset.</p><p>-from 43.63 to 60.89 for NEQ and from 48.97 to 80.16 for QFL in the case of labeled only (L). When we compare supervised vs. semi-supervised (L vs. L+U), we observe significant improvements in F1 scores for the semi-supervised model for all batches over the two datasets. As we include un- labeled instances with labeled instances from the same event, performance significantly improves in each experimental setting giving 5% to 26% absolute improvements over the supervised mod- els. These improvements demonstrate the effec- tiveness of our approach. We also notice that our semi-supervised approach can perform above 90% depending on the event. Specifically, major im- provements are observed from batch size 100 to 1,000, however, after that the performance im- provements are comparatively minor. The results obtained using batch sizes 500 and 1,000 are rea- sonably in the acceptable range when labeled and unlabeled instances are combined (i.e., L+50kU for Nepal and L+∼21kU for Queensland), which is also a reasonable number of training examples to obtain at the onset of an event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Adaptation</head><p>In  The results with domain adversarial training show improvements across both events -from 1.8% to 4.1% absolute gains in F1. These re- sults attest that adversarial training is an effective approach to induce domain invariant features in the internal representation as shown previously by <ref type="bibr" target="#b4">Ganin et al. (2016)</ref>.</p><p>Finally, when we do both semi-supervised learning and unsupervised domain adaptation, we get further improvements in F1 scores ranging from 5% to 7% absolute gains. From these im- provements, we can conclude that domain adap- tation with adversarial training along with graph- based semi-supervised learning is an effective method to leverage unlabeled and labeled data from a different domain.</p><p>Note that for our domain adaptation methods, we only use unlabeled data from the target do- main. Hence, we foresee future improvements of this approach by utilizing a small amount of target domain labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Two lines of research are directly related to our work: (i) semi-supervised learning and (ii) do- main adaptation. Several models have been pro- posed for semi-supervised learning. The earli- est approach is self-training <ref type="bibr" target="#b23">(Scudder, 1965)</ref>, in which a trained model is first used to label un- labeled data instances followed by the model re- training with the most confident predicted labeled instances. The co-training <ref type="bibr" target="#b16">(Mitchell, 1999</ref>) ap- proach assumes that features can be split into two sets and each subset is then used to train a classi- fier with an assumption that the two sets are con- ditionally independent. Then each classifier clas- sifies the unlabeled data, and then most confident data instances are used to re-train the other classi- fier, this process repeats multiple times.</p><p>In the graph-based semi-supervised approach, nodes in a graph represent labeled and unlabeled instances and edge weights represent the similar- ity between them. The structural information en- coded in the graph is then used to regularize a model ( <ref type="bibr" target="#b30">Zhu, 2005)</ref>. There are two paradigms in semi-supervised learning: 1) inductive -learning a function with which predictions can be made on unobserved instances, 2) transductive -no explicit function is learned and predictions can only be made on observed instances. As mentioned be- fore, inductive semi-supervised learning is prefer- able over the transductive approach since it avoids building the graph each time it needs to infer the labels for the unlabeled instances.</p><p>In our work, we use a graph-based inductive deep learning approach proposed by <ref type="bibr" target="#b28">Yang et al. (2016)</ref> to learn features in a deep learning model by predicting contextual (i.e., neighboring) nodes in the graph. However, our approach is different from <ref type="bibr" target="#b28">Yang et al. (2016)</ref> in several ways. First, we construct the graph by computing the distance be- tween tweets based on word embeddings. Second, instead of using count-based features, we use a convolutional neural network (CNN) to compose high-level features from the distributed represen- tation of the words in a tweet. Finally, for context prediction, instead of performing a random walk, we select nodes based on their similarity in the graph. Similar similarity-based graph has shown impressive results in learning sentence representa- tions ( <ref type="bibr" target="#b22">Saha et al., 2017</ref>).</p><p>In the literature, the proposed approaches for domain adaptation include supervised, semi- supervised and unsupervised. It also varies from linear kernelized approach <ref type="bibr" target="#b3">(Blitzer et al., 2006</ref>) to non-linear deep neural network techniques <ref type="bibr" target="#b5">(Glorot et al., 2011;</ref><ref type="bibr" target="#b4">Ganin et al., 2016)</ref>. One direction of research is to focus on feature space distribu- tion matching by reweighting the samples from the source domain ( <ref type="bibr" target="#b6">Gong et al., 2013</ref>) to map source into target. The overall idea is to learn a good feature representation that is invariant across domains. In the deep learning paradigm, <ref type="bibr" target="#b5">Glorot et al. (Glorot et al., 2011</ref>) used Stacked De- noising Auto-Encoders (SDAs) for domain adap- tation. SDAs learn a robust feature representation, which is artificially corrupted with small Gaussian noise. Adversarial training of neural networks has shown big impact recently, especially in areas such as computer vision, where generative unsu- pervised models have proved capable of synthe- sizing new images ( <ref type="bibr" target="#b7">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b21">Radford et al., 2015;</ref><ref type="bibr" target="#b13">Makhzani et al., 2015)</ref>. <ref type="bibr" target="#b4">Ganin et al. (2016)</ref> proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target. We extend this work by combining with semi-supervised graph embedding for unsupervised domain adaptation.</p><p>In a recent work, <ref type="bibr" target="#b11">Kipf and Welling (2016)</ref> present CNN applied directly on graph-structured datasets -citation networks and on a knowledge graph dataset. Their study demonstrate that graph convolution network for semi-supervised classifi- cation performs better compared to other graph based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we presented a deep learning frame- work that performs domain adaptation with adver- sarial training and graph-based semi-supervised learning to leverage labeled and unlabeled data from related events. We use a convolutional neu- ral network to compose high-level representation from the input, which is then passed to three com- ponents that perform supervised training, semi- supervised learning and domain adversarial train- ing. For domain adaptation, we considered a sce- nario, where we have only unlabeled data in the target event. Our evaluation on two crisis-related tweet datasets demonstrates that by combining domain adversarial training with semi-supervised learning, our model gives significant improve- ments over their respective baselines. We have also presented results of batch-wise incremen- tal training of the graph-based semi-supervised approach and show approximation regarding the number of labeled examples required to get an ac- ceptable performance at the onset of an event.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The system architecture of the domain adversarial network with graph-based semi-supervised learning. The shared components part is shared by supervised, semi-supervised and domain classifier.</figDesc><graphic url="image-1.png" coords="3,143.58,71.32,164.78,173.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 ,</head><label>4</label><figDesc></figDesc><table>we present domain adaptation results. 
The first block shows event-specific (i.e., train and 
test on the same event) results for the supervised 
CNN model. These results set the upper bound 
for our domain adaptation methods. The trans-
fer baselines are shown in the next block, where 
we train a CNN model in one domain and test 
it on a different domain. Then, the third block 
shows the results for the domain adversarial ap-
proach without the semi-supervised loss. These 
results show the importance of domain adversarial 
component. After that, the fourth block presents 
the performance of the model trained with graph 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Domain adaptation experimental results. 
Weighted average AUC, precision (P), recall (R) 
and F-measure (F1). 

embedding without domain adaptation to show the 
importance of semi-supervised learning. The final 
block present the results for the complete model 
that includes all the loss components. 
</table></figure>

			<note place="foot" n="1"> https://github.com/firojalam/ domain-adaptation 2 http://crisisnlp.qcri.org</note>

			<note place="foot" n="4"> https://dev.twitter.com/streaming/overview 5 http://crowdflower.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno>abs/1701.07875</idno>
		<ptr target="http://arxiv.org/abs/1701.07875" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th Intl. Conference on Artificial Intelligence and Statistics</title>
		<meeting>of the 13th Intl. Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy, AISTATS &apos;10</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Louis</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP. ACL</title>
		<meeting>of EMNLP. ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of MLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Processing social media messages in mass emergency: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Vieweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AIDR: Artificial intelligence for disaster response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Vieweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on World Wide Web</title>
		<meeting>the 23rd International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="159" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>CoRR abs/1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>CoRR abs/1611.02163</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations. Available as arXiv preprint</title>
		<meeting>the International Conference on Learning Representations. Available as arXiv preprint</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The role of unlabeled data in supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth international colloquium on cognitive science. Citeseer</title>
		<meeting>the sixth international colloquium on cognitive science. Citeseer</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable nearest neighbor algorithms for high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David G Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2227" to="2240" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust classification of crisis-related data on social networks using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamela Ali Al</forename><surname>Mannai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International AAAI Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Hassan Sajjad, Muhammad Imran, and Prasenjit Mitra</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding fires with twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bella</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ratcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop<address><addrLine>ALTA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>CoRR abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularized and retrofitted models for learning sentence representation with context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanay</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naeemul</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparison of document clustering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD workshop on text mining</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="525" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aid is out there: Looking for help from tweets during a large scale disaster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyonori</forename><surname>Ohtake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takao</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stijn De</forename><surname>Saeger</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1159" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1619" to="1629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integrating social media communications into the rapid assessment of sudden onset disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Vieweg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="444" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
