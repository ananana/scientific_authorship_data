<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Methodical Evaluation of Arabic Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Elrazzaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Elbassuoni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Shaban</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chadi</forename><surname>Helwe</surname></persName>
						</author>
						<title level="a" type="main">Methodical Evaluation of Arabic Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="454" to="458"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2072</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many unsupervised learning techniques have been proposed to obtain meaningful representations of words from text. In this study, we evaluate these various techniques when used to generate Arabic word embeddings. We first build a benchmark for the Arabic language that can be utilized to perform intrinsic evaluation of different word embeddings. We then perform additional extrinsic evaluations of the embed-dings based on two NLP tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed word representations, commonly re- ferred to as word embeddings, represent words as vectors in a low-dimensional space. The goal of this deep representation of words is to capture syn- tactic and semantic relationships between words. These word embeddings have been proven to be very useful in various NLP applications, particu- larly those employing deep learning.</p><p>Word embeddings are typically learned using unsupervised learning techniques on large text corpora. Many techniques have been proposed to learn such embeddings ( <ref type="bibr" target="#b8">Pennington et al., 2014;</ref><ref type="bibr" target="#b6">Mikolov et al., 2013;</ref><ref type="bibr" target="#b7">Mnih and Kavukcuoglu, 2013)</ref>. While most of the work has focused on English word embeddings, few attempts have been carried out to learn word embeddings for other lan- guages, mostly using the above mentioned tech- niques.</p><p>In this paper, we focus on Arabic word embed- dings. Particularly, we provide a thorough evalu- ation of the quality of four Arabic word embed- dings that have been generated by previous work <ref type="bibr" target="#b10">(Zahran et al., 2015;</ref><ref type="bibr" target="#b0">Al-Rfou et al., 2013</ref>). We use both intrinsic and extrinsic evaluation methods to evaluate the different embeddings. For the intrin- sic evaluation, we build a benchmark consisting of over 115,000 word analogy questions for the Ara- bic language. Unlike previous attempts to evalu- ate Arabic embeddings, which relied on translat- ing existing English benchmarks, our benchmark is the first specifically built for the Arabic lan- guage and is publicly available for future work in this area <ref type="bibr">1</ref> . Translating an English benchmark is not the best strategy to evaluate Arabic embed- dings for the following reasons. First, the cur- rently available English benchmarks are specifi- cally designed for the English language and some of the questions there are not applicable to Arabic. Second, Arabic has more relations compared to English and these should be included in the bench- mark as well. Third, translating an English bench- mark is subject to errors since it is usually carried out in an automatic fashion.</p><p>In addition to the new benchmark, we also ex- tend the basic analogy reasoning task by taking into consideration more than two word pairs when evaluating a relation, and by considering the top-5 words rather than only the top-1 word when an- swering an analogy question. Finally, we perform an extrinsic evaluation of the different embeddings using two different NLP tasks, namely Document Classification and Named Entity Recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a wealth of research on evaluating unsu- pervised word embeddings, which can be can be broadly divided into intrinsic and extrinsic evalu-  <ref type="bibr" target="#b9">Schnabel et al., 2015)</ref>. However, all of these tasks and benchmarks are build for English and thus cannot be used to assess the quality of Arabic word embeddings, which is the main focus here.</p><formula xml:id="formula_0">Relation (a, b) (c, d)<label>#pairs #tuples</label></formula><p>To the best of our knowledge, only a hand- ful of recent studies attempted evaluating Ara- bic word embeddings. <ref type="bibr" target="#b10">Zahran et al. (Zahran et al., 2015</ref>) translated the English benchmark in ( <ref type="bibr" target="#b6">Mikolov et al., 2013)</ref> and used it to evalu- ate different embedding techniques when applied on a large Arabic corpus. However, as the au- thors themselves point out, translating an English benchmark is not the best strategy to evaluate Ara- bic embeddings. Zahran et al. also consider ex- trinsic evaluation on two NLP tasks, namely query expansion for IR and short answer grading. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmark</head><p>Our benchmark is the first specifically designed for the Arabic language. It consists of nine re- lations, each consisting of over 100 word pairs. An Arabic linguist who was properly introduced to the word-analogy task provided the list of rela- tions. Once the nine relations were defined, two different people collectively generated the word pairs. The two people are native Arabic speak- ers, and one of them is a co-author and the other is not. <ref type="table">Table 1</ref> displays the list of all relations in our benchmark as well as two example word pairs for each relation. The full benchmark and the evalua- tion tool can be obtained from the following link: http://oma-project.com/res_home.</p><p>Translating an English benchmark is not ade- quate for many reasons. First, the currently avail- able English benchmarks contain many questions that are not applicable to Arabic. For example, comparative and superlative relations are the same in Arabic, except that the superlatives are usually prefixed with the Arabic equivalent of "the". An- other example is the opposite relation, where some words in Arabic do not have antonyms, in which case the antonym is typically expressed by prefix- ing the word with "not". Second, Arabic has more relations compared to English. For instance, in Arabic there is the pair relation (see <ref type="table">Table 1</ref> for an example). Third, translating an English bench-mark is considerably difficult due to the high am- biguity of the Arabic language.</p><p>Given our benchmark, we generate a test bank consisting of over 100,000 tuples. Each tuple con- sists of two word pairs (a, b) and (c, d) from the same relation. For each of our nine relations, we generate a tuple by combining two different word pairs from the same relation. Once tuples have been generated, they can be used as word analogy questions to evaluate different word em- beddings as defined by <ref type="bibr" target="#b6">Mikolov et al. (Mikolov et al., 2013)</ref></p><note type="other">. A word analogy question for a tu- ple consisting of two word pairs (a, b) and (c, d) can be formulated as follows: "a to b is like c to ?". Each such question will then be answered by calculating a target vector t = b − a + c. We then calculate the cosine similarity between the target vector t and the vector representation of each word w in a given word embeddings V . Fi- nally, we retrieve the most similar word w to t, i.e., argmax w∈V &amp;w / ∈{a,b,c} w·t</note><formula xml:id="formula_1">||w||||t|| . If w = d (i.</formula><p>e., the same word) then we assume that the word embed- dings V has answered the question correctly.</p><p>We also use our benchmark to generate addi- tional analogy questions by using more than two word pairs per question. This provides a more ac- curate representation of a relation as mentioned in <ref type="bibr" target="#b6">(Mikolov et al., 2013)</ref>. For each relation, we gen- erate a question per word pair consisting of the word pair plus 10 random word pairs from the same relation. Thus, each question would con- sist of 11 word pairs (a i , b i ) where 1 ≤ i ≤ 11. We then use the average of the first 10 word pairs to generate the target vector t as follows: t = 1 10 10 i (b i − a i ) + a 11 . Finally we retrieve the closest word w to the target vector t using cosine similarity as in the previous case. The question is considered to be answered correctly if the answer word w is the same as b 11 .</p><p>Moreover, we also extend the traditional word analogy task by taking into consideration if the correct answer is among the top-5 closest words in the embedding space to the target vector t, which allows us to more leniently evaluate the embed- dings. This is particularly important in the case of Arabic since many forms of the same word exist, usually with additional prefixes or suffixes such as the equivalent of the article "the" or possessive determiners such as "her", "his", or "their". For example, consider one question which asks " to is like to ?", i.e., "man to woman is like king to ?", with the answer being " " or "queen". Now, if we rely only on the top-1 word and it happens to be " ", which means "his queen" in English, the question would be consid- ered to be answered wrongly. To relax this and ensure that different forms of the same word will not result in a mismatch, we use the top-5 words for evaluation rather than the top-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We  <ref type="bibr">et al., 2013)</ref>. To the best of our knowledge, these are the only available word embeddings that have been constructed for the Arabic language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intrinsic Evaluation</head><p>As we mentioned in the previous section, we use our word analogy benchmark to evaluate the em- beddings using four different criteria, namely us- ing top-1 and top-5 words when representing rela- tions using two versus 11 word pairs. Tables 2 dis- plays the accuracy of each embedding technique for the four evaluation criteria. Note that we con- sider a question to be answered wrongly if at least one of the words in the question are not present in the word embeddings. That is, we take into con- sideration the coverage of the embeddings as well ( <ref type="bibr" target="#b4">Gao et al., 2014</ref>). As can be seen in <ref type="table" target="#tab_3">Table 2</ref>, the CBOW model consistently outperforms all other compared mod- els for all four evaluation criteria. The perfor- mance of Polyglot is particularly low since the em- beddings were trained on a much smaller corpus (Arabic portion of Wikipedia), and thus both its coverage and the quality of the embeddings are much lower. As can also be seen from the table, the accuracies of all the methods are boosted when   <ref type="table">Table 3</ref>: F-measure for two NLP tasks representing a relation using 11 pairs rather just two pairs. This validates that it is indeed more ap- propriate to use more than two pairs to represent relations in word analogy tasks. When considering the top-5 matches, the accu- racies of the embeddings are boosted drastically, which indeed shows that relying on just the top-1 word to assess the quality of embeddings might be unduly harsh, particularly in the case of Arabic.</p><note type="other">Model CBOW Skip-gram GloVe Polyglot CBOW Skip-gram Glove Polyglot Relation top-1</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extrinsic Evaluation</head><p>We perform extrinsic evaluation of the four word embeddings using two NLP tasks, namely: Arabic Document Classification and Arabic Named En- tity Recognition (NER). In the Document Clas- sification task, the goal is to classify Arabic Wikipedia articles into four different classes (per- son (PER), organization (ORG), location (LOC), or miscellaneous (MISC)). To do this, we re- lied on a neural network with a Long Short-Term Memory (LSTM) layer <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>), which is fed from the word embed- dings. The LSTM layer is followed by two fully- connected layers, which in turn are followed by a softmax layer that predicts class-assignment prob- abilities. The model was trained for 150 epochs on 8,000 articles, validated on 1,000 articles, and tested on another 1,000 articles.</p><p>In the NER task, the goal is to label each word in a given sequence using one of the following la- bels: PER, LOC, ORG, and MISC, which repre- sent different Named Entity classes. The same ar- chitecture as in the Document Classification task was used for this task as well. The model was trained for 150 epochs on 3,852 sentences and tested on 963 sentence using Columbia's Uni- versity Arabic Named Entity Recognition Corpus <ref type="bibr">(Columbia University, 2016)</ref>. We used an LSTM neural network for both tasks since they flexibly make use of contextual data and thus are com-monly used in NLP tasks such as Document Clas- sification and NER.</p><p>As can be seen in <ref type="table">Table 3</ref>, the first three meth- ods CBOW, Skip-gram and GloVe seem to per- form relatively well for both the Document Clas- sification task as well as the NER task with very comparable performance in terms of F-measure. They also clearly outperform Polyglot when it comes to both tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Our experimental results indicate the superiority of CBOW and SKip-gram as word embeddings compared to Polyglot. This can be mainly at- tributed to the fact that the first two embeddings were trained using a much larger corpus and thus had both better coverage and higher accuracies when it comes to the word analogy task. This is also evident in the case of the extrinsic evaluation. Thus, when training word embeddings, it is cru- cial to use large training data to obtain meaningful embeddings.</p><p>Moreover, when performing the intrinsic eval- uation of the different embeddings, we observed that relying on just the top-1 word is unduly harsh for Arabic. This is mainly attributed to the fact that for Arabic, and unlike other languages such as En- glish, different forms of the same word exist and these must be taken into consideration when eval- uating the embeddings. Thus, it is advised to use the top-k matches to perform the evaluation, where k is 5 for instance. It is also advisable to represent a relation with multiple word pairs, rather than just two as is currently done in most similar studies, to guarantee that the relation is well represented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we described the first word anal- ogy benchmark specifically designed for the Ara- bic language. We used our benchmark to evaluate available Arabic word embeddings using the basic analogy reasoning task as well as extensions of it. In addition, we also evaluated the quality of the various embeddings using two NLP tasks, namely Document Classification and NER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Dahou et al. (Dahou et al., 2016) used the anal- ogy questions from (Zahran et al., 2015) after cor- recting some Arabic spelling mistakes resulting from the translation and after adding new analogy questions to make up for the inadequacy of the English questions for the Arabic language. They also performed an extrinsic evaluation using sen- timent analysis. Finally, Al-Rfou et al. (Al-Rfou et al., 2013) generated word embeddings for 100 different languages, including Arabic, and evalu- ated the embeddings using part-of-speech tagging, however the evaluation was done only for a hand- ful of European languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Intrinsic evaluation of the word embeddings using different criteria 

Model 
Document Classification NER 
CBOW 
0.948 
0.800 
Skip-gram 
0.954 
0.799 
GloVe 
0.946 
0.816 
Polyglot 
0.882 
0.649 

</table></figure>

			<note place="foot" n="1"> http://oma-project.com/res_home</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was made possible by NPRP 6-716-1-138 grant from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibilty of the au-thors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.1662</idno>
		<title level="m">Polyglot: Distributed word representations for multilingual nlp</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Arabic Named Entity Recognition Task</title>
		<ptr target="http://www1.ccls.columbia.edu/˜ybenajiba/downloads.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word embeddings and convolutional neural network for arabic sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelghani</forename><surname>Dahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2418" to="2427" />
		</imprint>
	</monogr>
	<note>Mohamed Houcine Haddoud, and Pengfei Duan</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.1640</idno>
		<title level="m">Wordrep: A benchmark for research on learning word representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Word representations in vector space and their applications for arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Mohamed A Zahran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magooda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Mahgoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Raafat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atyia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
