<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Latent Structure in Task-Oriented Dialogues</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
							<email>zhaike@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Redmond</orgName>
								<address>
									<postCode>20740, 98052</postCode>
									<region>MD, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
							<email>jason.williams@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Redmond</orgName>
								<address>
									<postCode>20740, 98052</postCode>
									<region>MD, WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Latent Structure in Task-Oriented Dialogues</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="36" to="46"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue, since it provides a basis for analysing, evaluating, and building conversational systems. We propose three new unsupervised models to discover latent structures in task-oriented dialogues. Our methods synthesize hidden Markov models (for underlying state) and topic models (to connect words to states). We apply them to two real, non-trivial datasets: human-computer spoken dialogues in bus query service, and human-human text-based chats from a live technical support service. We show that our models extract meaningful state representations and dialogue structures consistent with human annotations. Quantitatively, we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling human conversation is a fundamental scientific pursuit. In addition to yielding ba- sic insights into human communication, compu- tational models of conversation underpin a host of real-world applications, including interactive dialogue systems <ref type="bibr" target="#b39">(Young, 2006</ref>), dialogue sum- marization ( <ref type="bibr" target="#b25">Murray et al., 2005;</ref><ref type="bibr" target="#b9">Daum√© III and Marcu, 2006</ref>; <ref type="bibr" target="#b23">Liu et al., 2010)</ref>, and even medi- cal applications such as diagnosis of psychological conditions <ref type="bibr" target="#b10">(DeVault et al., 2013</ref>).</p><p>Computational models of conversation can be broadly divided into two genres: modeling and control. Control is concerned with choosing ac- tions in interactive settings-for example to maxi- mize task completion-using reinforcement learn- * Work done at Microsoft Research. ing ( <ref type="bibr" target="#b22">Levin et al., 2000</ref>), supervised learning <ref type="bibr" target="#b18">(Hurtado et al., 2010)</ref>, hand-crafted rules <ref type="bibr" target="#b21">(Larsson and Traum, 2000</ref>), or mixtures of these <ref type="bibr" target="#b15">(Henderson and Lemon, 2008)</ref>. By contrast, modeling-the genre of this paper-is concerned with inferring a phenomena in an existing corpus, such as di- alogue acts in two-party conversations ( <ref type="bibr" target="#b32">Stolcke et al., 2000</ref>) or topic shifts in multi-party dia- logues ( <ref type="bibr" target="#b12">Galley et al., 2003;</ref><ref type="bibr" target="#b28">Purver et al., 2006;</ref><ref type="bibr" target="#b17">Hsueh et al., 2006</ref>; <ref type="bibr" target="#b1">Banerjee and Rudnicky, 2006</ref>).</p><p>Many past works rely on supervised learning or human annotations, which usually requires man- ual labels and annotation guidelines ( <ref type="bibr" target="#b19">Jurafsky et al., 1997)</ref>. It constrains scaling the size of training examples, and application domains. By contrast, unsupervised methods operate only on the observ- able signal (e.g. words) and are estimated with- out labels or their attendant limitations <ref type="bibr" target="#b8">(Crook et al., 2009</ref>). They are particularly relevant because conversation is a temporal process where models are trained to infer a latent state which evolves as the dialogue progresses ( <ref type="bibr" target="#b2">Bangalore et al., 2006;</ref><ref type="bibr" target="#b33">Traum and Larsson, 2003)</ref>.</p><p>Our basic approach is to assume that each ut- terance in the conversation is in a latent state, which has a causal effect on the words the conver- sants produce. Inferring this model yields basic insights into the structure of conversation and also has broad practical benefits, for example, speech recognition ( <ref type="bibr" target="#b36">Williams and Balakrishnan, 2009)</ref>, natural language generation ( <ref type="bibr" target="#b29">Rieser and Lemon, 2010)</ref>, and new features for dialogue policy opti- mization ( <ref type="bibr" target="#b31">Singh et al., 2002;</ref><ref type="bibr" target="#b39">Young, 2006</ref>).</p><p>There has been limited past work on unsuper- vised methods for conversation modeling. Choti- mongkol (2008) studies task-oriented conversa- tion and proposed a model based on a hidden Markov model (HMM). <ref type="bibr" target="#b30">Ritter et al. (2010)</ref> ex- tends it by introducing additional word sources, and applies to non-task-oriented conversations- social interactions on Twitter, where the subjects discussed are very diffuse. The additional word sources capture the subjects, leaving the state- specific models to express common dialogue flows such as question/answer pairs.</p><p>In this paper, we retain the underlying HMM, but assume words are emitted using topic models (TM), exemplified by latent Dirichlet allocation ( <ref type="bibr">Blei et al., 2003, LDA)</ref>. LDA assumes each word in an utterance is drawn from one of a set of latent topics, where each topic is a multinomial distri- bution over the vocabulary. The key idea is that the set of topics is shared across all states, and each state corresponds to a mixture of topics. We propose three model variants that link topics and states in different ways.</p><p>Sharing topics across states is an attractive property in task-oriented dialogue, where a sin- gle concept can be discussed at many points in a dialogue, yet different topics often appear in pre- dictable sequences. Compared to past works, the decoupling of states and topics gives our mod- els more expressive power and the potential to be more data efficient. Empirically, we find that our models outperform past approaches on two real- world corpora of task-oriented dialogues.</p><p>This paper is organized as follows: Section 2 in- troduces two task-oriented domains and corpora; Section 3 details three new unsupervised genera- tive models which combine HMMs and LDA and efficient inference schemes; Section 4 evaluates our models qualitatively and quantitatively, and fi- nally conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>To test the generality of our models, we study two very different datasets: a set of human-computer spoken dialogues in quering bus timetable <ref type="bibr">(BusTime)</ref>, and a set of human-human text-based dia- logues in the technical support domain <ref type="bibr">(TechSupport)</ref>. In BusTime, the conversational structure is known because the computer followed a determin- istic program <ref type="bibr" target="#b37">(Williams, 2012)</ref>, making it possible to directly compare an inferred model to ground truth on this corpus. <ref type="bibr">1</ref> In TechSupport, there is no known flowchart, 2 making this a realistic applica- tion of unsupervised methods.</p><p>BusTime This corpus consists of logs of tele- phone calls between a spoken dialogue system and real bus users in Pittsburgh, USA <ref type="bibr" target="#b5">(Black et al., 2010</ref>). For the user side, the words logged are the words recognized by the automatic speech recog- nizer. The vocabulary of the recognizer was con- strained to the bus timetable task, so only words known to the recognizer in advance are output. Even so, the word error rate is approximately 30- 40%, due to the challenging audio conditions of usage-with traffic noise and extraneous speech. The system asked users sequentially for a bus route, origin and destination, and optionally date and time. The system confirmed low-confidence speech recognition results. Due to the speech recognition channel, system and user turns always alternate. An example dialogue is given below:</p><p>System: Say a route like bus-route, or say I'm not sure. User: bus-route. System: I thought you said bus-route, is that right? User: Yes. System: Say where're you leaving from, like location. User: location. System: Okay, location, where are you going to? ...</p><p>We discard dialogues with fewer than 20 ut- terances. We also map all named entities (e.g., "downtown" and "28X") to their semantic types (resp. location and bus-route) to reduce vo- cabulary size. The corpus we use consists of ap- proximately 850 dialogue sessions or 30, 000 ut- terances. It contains 370, 000 tokens (words or se- mantic types) with vocabulary size 250.</p><p>TechSupport This corpus consists of logs of real web-based human-human text "chat" con- versations between clients and technical support agents at a large corporation. Usually, clients and agents first exchange names and contact informa- tion; after that, dialogues are quite free-form, as agents ask questions and suggest fixes. Most dia- logues ultimately end when the client's issue has been resolved; some clients are provided with a reference number for future follow-up. An exam- ple dialogue is given below: (b) LM-HMMS <ref type="figure">Figure 1</ref>: Plate diagrams of baseline models, from existing work <ref type="bibr" target="#b7">(Chotimongkol, 2008;</ref><ref type="bibr" target="#b30">Ritter et al., 2010)</ref>. Variable definitions are given in the text.</p><p>... This data is less structured than BusTime; clients' issues span software, hardware, network- ing, and other topics. In addition, clients use com- mon internet short-hand (e.g., "thx", "gtg", "ppl", "hv", etc), with mis-spellings (e.g., "ofice", "off- fice", "erorr", etc). In addition, chats from the web interface are segmented into turns when a user hits "Enter" on a keyboard. Therefore, clients' input and agents' responses do not necessarily alternate consecutively, e.g., an agent's response may take multiple turns as in the above example. Also, it is unreasonable to group consecutive chats from the same party to form a "alternating" structure like BusTime dataset due to the asynchronism of different states. For instance, the second block of client inputs clearly comes from two different states which should not be merged together.</p><p>We discard dialogues with fewer than 30 utter- ances. We map named entities to their semantic types, apply stemming, and remove stop words. <ref type="bibr">3</ref> The corpus we use contains approximately 2, 000 dialogue sessions or 80, 000 conversation utter- ances. It consists of 770, 000 tokens, with a a vo- cabulary size of 6, 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Latent Structure in Dialogues</head><p>In this work, our goal is to infer latent structure presented in task-oriented conversation. We as- sume that the structure can be encoded in a prob- abilistic state transition diagram, where the dia- logue is in one state at each utterance, and states have a causal effect on the words observed. We as- sume the boundaries between utterances are given, which is trivial in many corpora.</p><p>The simplest formulation we consider is an HMM where each state contains a unigram lan- guage model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by <ref type="bibr" target="#b3">Barzilay and Lee (2004)</ref>. We call it LM-HMM as in <ref type="figure">Figure 1(a)</ref>. For a corpus of M dialogues, the m-th dialogue contains n utterances, each of which contains N n words (we omit index m from terms because it will be clear from context). At n-th utterance, we assume the dialogue is in some latent state s n . Words in n-th utterance w n,1 , . . . , w n,Nn are gen- erated (independently) according to the LM. When an utterance is complete, the next state is drawn according to HMM, i.e., P (s |s).</p><p>While LM-HMM captures the basic intuition of conversation structure, it assumes words are con- ditioned only on state. <ref type="bibr" target="#b30">Ritter et al. (2010)</ref> extends LM-HMM to allow words to be emitted from two additional sources: the topic of current dialogue œÜ, or a background LM œà shared across all dia- logues. A multinomial œÄ indicates the expected fraction of words from these three sources. For every word in an utterance, first draw a source in- dicator r from œÄ, and then generate the word from the corresponding source. We call it LM-HMMS <ref type="figure">(Figure 1(b)</ref>). <ref type="bibr" target="#b30">Ritter et al. (2010)</ref> finds these al- ternate sources are important in non-task-oriented domains, where events are diffuse and fleeting. For example, Twitter exchanges often focus on a particular event (labeled X), and follow patterns like "saw X last night?", "X was amazing". Here X appears throughout the dialogue but does not help to distinguish conversational states in social media. We also explore similar variants.</p><p>In this paper, these two models form our base- lines. For all models, we use Markov chain Monte Carlo (MCMC) inference <ref type="bibr" target="#b26">(Neal, 2000</ref>) to find la- tent variables that best fit observed data. We also assume symmetric Dirichlet priors on all multino- mial distributions and apply collapsed Gibbs sam- pling. In the rest of this section, we present our models and their inference algorithms in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TM-HMM</head><p>Our approach is to modify the emission probabil- ities of states to be distributions over topics rather than distributions over words. In other words, in- stead of generating words via a LM, we generate words from a topic model (TM), where each state maps to a mixture of topics. The key benefit of this additional layer of abstraction is to enable states to express higher-level concepts through pooling of topics across states. For example, topics might be inferred for content like "bus-route" or "lo-  cations"; and other topics for dialogue acts, like to "ask" or "confirm" information. States could then be combinations of these, e.g., a state might express "ask bus route" or "confirm location". This approach also decouples the number of top- ics from the number of states. Throughout this pa- per, we denote the number of topics as K and the number of states as T . We index words, turns and dialogues in the same ways as baseline models.</p><formula xml:id="formula_0">s 0 w 0,i N 0 s 1 ... s n M z 0,i w 1,i N 1 z 1,i w n,i N n z n,i T K Œ∏ t œÜ k s 0 w 0,i N 0 s 1 ... s n z 0,i w 1,i N 1 z 1,i w n,i</formula><formula xml:id="formula_1">g k M K h m u m g k M (b) TM-HMMS s 0 w 0,i N 0 s 1 ... s n M z 0,i w 1,i N 1 z 1,i w n,i N n z n,i K r 1,i r 0,i r n,i T Œ∏ m œÜ k œÑ t s</formula><p>We develop three generative models. In the first variant (TM-HMM, <ref type="figure" target="#fig_1">Figure 2</ref>(a)), we assume every state s in HMM is associated with a distribution over topics Œ∏, and topics generate words w at each utterance. The other two models allow words to be generated from different sources (in addition to states), akin to the LM-HMMS model. TM-HMM generates a dialogue as following:</p><p>1: For each utterance n in that dialogue, sample a state s n based on the previous state s n‚àí1 . 2: For each word in utterance n, first draw a topic z from the state-specified distribution over topics Œ∏ sn conditioned on s n , then gener- ate word w from the topic-specified distribu- tion over vocabulary œÜ z based on z. We assume Œ∏'s and œÜ's are drawn from corre- sponding Dirichlet priors, as in LDA.</p><p>The posterior distributions of state assignment s n and topic assignment z n,i are</p><formula xml:id="formula_2">p(s n |s ‚àín , z, Œ±, Œ≥) ‚àù p(s n |s ‚àín , Œ≥) ¬∑ p(z n |s, z ‚àín , Œ±),<label>(1)</label></formula><formula xml:id="formula_3">p(z n,i |s, w, z ‚àí(n,i) , Œ±, Œ≤) ‚àù p(z n,i |s, z ‚àí(n,i) , Œ±) ¬∑ p(w n,i |s n , w ‚àí(n,i) , z, Œ≤),</formula><p>where Œ±, Œ≤, Œ≥ are symmetric Dirichlet priors on state-wise topic distribution Œ∏ t 's, topic-wise word distribution œÜ t 's and state transition multinomials, respectively. All probabilities can be computed using collapsed Gibbs sampler for LDA ( <ref type="bibr" target="#b14">Griffiths and Steyvers, 2004</ref>) and HMM <ref type="bibr" target="#b13">(Goldwater and Griffiths, 2007)</ref>. We iteratively sample all param- eters until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TM-HMMS</head><p>TM-HMMS <ref type="figure" target="#fig_1">(Figure 2</ref>(b)) extends TM-HMM to al- low words to be generated either from state LM (as in LM-HMM), or a set of dialogue topics (akin to LM-HMMS). Because task-oriented dia- logues usually focus on a specific domain, a set of words appears repeatedly throughout a given dialogue. Therefore, the topic distribution is of- ten stable throughout the entire dialogue, and does not vary from turn to turn. For example, in the troubleshooting domain, dialogues about network connections, desktop productivity, and anti-virus software could each map to different session-wide topics. To express this, words in the TM-HMMS model are generated either from a dialogue-specific topic distribution, or from a state-specific language model. <ref type="bibr">4</ref> A distribution over sources is sampled once at the beginning of each dialogue and selects the expected fraction of words generated from different sources. The generative story for a dialogue session is:</p><p>1: At the beginning of each session, draw a dis- tribution over topics Œ∏ and a distribution over word sources œÑ . 2: For each utterance n in the conversation, draw a state s n based on previous state s n‚àí1 . 3: For each word in utterance n, first choose a word source r according to œÑ , and then de- pending on r, generate a word w either from the session-wide topic distribution Œ∏ or the language model specified by the state s n .</p><p>Again, we impose Dirichlet priors on distributions over topics Œ∏'s and distributions over words œÜ's as in LDA. We also assume the distributions over sources œÑ 's are governed by a Beta distribution. The session-wide topics is slightly different from that used in LM-HMMS: LM-HMMS was de- veloped for social chats on Twitter where topics are very diffuse and unlikely to repeat; hence of- ten unique to each dialogue. By contrast, our mod- els are designed for task-oriented dialogues which pertain to a given domain where topics are more tightly clustered; thus, in TM-HMMS session-wide topics are shared across the corpus.</p><p>The posterior distributions of state assignment s n , word source r n,i and topic assignment z n,i are p(s n |r, s ‚àín , w, Œ≥, œÄ) ‚àù p(s n |s ‚àín , Œ≥) ¬∑ p(w n |r, s, œÄ),</p><formula xml:id="formula_4">p(r n,i |r ‚àí(n,i) , s, w, œÄ) ‚àù p(r n,i |r ‚àí(n,i) , œÄ) ¬∑ p(w n,i |r, s, w ‚àí(n,i) , z, Œ≤),<label>(2)</label></formula><formula xml:id="formula_5">p(z n,i |r, w, z ‚àí(n,i) , Œ±, Œ≤) ‚àù p(z n,i |r, z ‚àí(n,i) , Œ±) ¬∑ p(w n,i |r, w ‚àí(n,i) , z, Œ≤),</formula><p>where œÄ is a symmetric Dirichlet prior on session- wise word source distribution œÑ m 's, and other symbols are defined above. All these probabilities are Dirichlet-multinomial distributions and there- fore can be computed efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TM-HMMSS</head><p>The TM-HMMSS <ref type="figure" target="#fig_1">(Figure 2</ref>(c)) model modifies TM-HMMS to re-sample the distribution over word sources œÑ at every utterance, instead of once at the beginning of each session. This modifica- tion allows the fraction of words drawn from the session-wide topics to vary over the course of the dialogue. This is attractive in task-oriented di- alogue, where some sections of the dialogue al- ways follow a similar script, regardless of session topic-for example, the opening, closing, or ask- ing the user if they will take a survey. To support these patterns, TM-HMMSS conditions the source generator distribution on the current state. The generative story of TM-HMMSS is very similar to TM-HMMS, except the distribution over word sources œÑ 's are sampled at every state. A dialogue is generated as following:</p><p>1: For each session, draw a topic distribution Œ∏. 2: For each utterance n in the conversation, draw a state s n based on previous state s n‚àí1 , and subsequently retrieve the state-specific distri- bution over word sources œÑ sn . 3: For each word in utterance n, first sample a word source r according to œÑ sn , and then de- pending on r, generate a word w either from the session-wide topic distribution Œ∏ or the language model specified by the state s n . As in TM-HMMS, we assume multinomial distri- butions Œ∏'s and œÜ's are drawn from Dirichlet pri- ors; and œÑ 's are governed by Beta distributions.</p><p>The inference for TM-HMMSS is exactly same as the inference for TM-HMMS, except the poste- rior distributions over word source r n,i is now</p><formula xml:id="formula_6">p(r n,i |r ‚àí(n,i) , s, w, œÄ) ‚àù p(r n,i |r ‚àí(n,i) , s n , œÄ) ¬∑ p(w n,i |r, s, w ‚àí(n,i) , z, Œ≤),<label>(3)</label></formula><p>where the first term is integrated over all sessions and conditioned on the state assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Supporting Multiple Parties</head><p>Since our primary focus is task-oriented dia- logues between two parties, we assume every word source is associated with two sets of LMs- one for system/agent and another for user/client. This configuration is similar to PolyLDA (Mimno et al., 2009) or LinkLDA ( <ref type="bibr" target="#b38">Yano et al., 2009</ref>), such that utterances from different parties are treated as different languages or blog-post and comments pairs. In this work, we implement all models un- der this setting, but omit details in plate diagrams for the sake of simplicity. In settings where the agent and client always al- ternate, each state emits both text before transi- tioning to the next state. This is the case in the BusTime dataset, where the spoken dialogue sys- tem enforces strict turn-taking. In settings where agents or client may produce more than one utter- ance in a row, each state emits either agent text or client text, then transitions to the next state. This is the case in the TechSupport corpus, where either conversant may send a message at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Likelihood Estimation</head><p>To evaluate performance across different models, we compute the likelihood on held-out test set. For TM-HMM model, there are no local depen- dencies, and we therefore compute the marginal likelihood using the forward algorithm. However, for TM-HMMS and TM-HMMSS models, the la- tent topic distribution Œ∏ creates local dependen- cies, rendering computation of marginal likeli-hoods intractable. Hence, we use a Chib-style estimator ( . Although it is computationally more expensive, it gives less bi- ased approximation of marginal likelihood, even for finite samples. This ensures likelihood mea- surements are comparable across models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we examine the effectiveness of our models. We first evaluate our models qualitatively by exploring the inferred state diagram. We then perform quantitative analysis with log likelihood measurements and an ordering task on a held-out test set. We train all models with 80% of the en- tire dataset and use the rest for testing. We run the Gibbs samplers for 1000 iterations and update all hyper-parameters using slice sampling <ref type="bibr" target="#b27">(Neal, 2003;</ref><ref type="bibr" target="#b35">Wallach, 2008</ref>) every 10 iterations. The training likelihood suggest all models converge within 500‚àí800 iterations. For all Chib-style esti- mators, we collect 100 samples along the Markov chain to approximate the marginal likelihood. <ref type="figure">Figure 3</ref> shows the state diagram for BusTime cor- pus inferred by TM-HMM without any supervi- sion. <ref type="bibr">5</ref> Every dialogue is opened by asking the user to say a bus route, or to say "I'm not sure." It then transits to a state about location, e.g., origin and destination. Both these two states may continue to a confirmation step immediately after. After verifying all the necessary information, the system asks if the user wants "the next few buses". <ref type="bibr">6</ref> Oth- erwise, the system follows up with the user on the particular date and time information. After system reads out bus times, the user has options to "re- peat" or ask for subsequent schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative Evaluation</head><p>In addition, we also include the human- annotated dialogue flow in <ref type="figure" target="#fig_4">Figure 4</ref> for refer- ence <ref type="bibr" target="#b37">(Williams, 2012)</ref>. It only illustrates the most common design of system actions, without show- ing edge cases. Comparing these two figures, the dialogue flow inferred by our model along the most probable path (highlighted in bold red in <ref type="figure">Fig- ure 3)</ref> is consistent with underlying design. Fur- thermore, our models are able to capture edge cases-omitted for space-through a more gen- eral and probabilistic fashion. In summary, our <ref type="bibr">5</ref> Recall in BusTime, state transitions occur after each pair of system/user utterances, so we display them synchronously. <ref type="bibr">6</ref> The system was designed this way because most users say "yes" to this question, obviating the date and time. models yield a very similar flowchart to the under- lying design in a completely unsupervised way. 7 <ref type="figure" target="#fig_6">Figure 5</ref> shows part of the flowchart for the TechSupport corpus, generated by the TM- HMMSS model. 8 A conversation usually starts with a welcome message from a customer support agent. Next, clients sometimes report a problem; otherwise, the agent gathers the client's identity. After these preliminaries, the agent usually checks the system version or platform settings. Then, in- formation about the problem is exchanged, and a cycle ensues where agents propose solutions, and clients attempt them, reporting results. Usually, a conversation loops among these states until ei- ther the problem is resolved (as the case shown in the <ref type="figure">figure)</ref> or the client is left with a reference number for future follow-up (not shown due to space limit). Although technical support is task- oriented, the scope of possible issues is vast and not prescribed. The table in <ref type="figure" target="#fig_6">Figure 5</ref> lists the top ranked words of selected topics-the categories clients often report problems in. It illustrates that, qualitatively, TM-HMMSS discovers both problem categories and conversation structures on our data.</p><p>As one of the baseline model, we also include a part of flowchart generated by LM-HMM model with similar settings of T = 20 states. Illus- trated by the highlighted states in 6, LM-HMM model conflates interactions that commonly occur at the beginning and end of a dialogue-i.e., "ac- knowledge agent" and "resolve problem", since their underlying language models are likely to pro- duce similar probability distributions over words. By incorporating topic information, our proposed models (e.g., TM-HMMSS in <ref type="figure" target="#fig_6">Figure 5</ref>) are able to enforce the state transitions towards more frequent flow patterns, which further helps to overcome the weakness of language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>In this section, we evaluate our models using log likelihood and an ordering task on a held-out test set. Both evaluation metrics measure the predic- tive power of a conversation model. state: ask for bus route <ref type="bibr">(route:0.14)</ref>, (say:0.13), (&lt;bus-route&gt;:0.12), (not:0.10), (sure:0.10), (im:0.09), (a:0.08), (bus:0.07), (like:0.06), ... e.g.: say a bus route like &lt;bus-route&gt; or say i am not sure (&lt;bus-route&gt;:0.7), (the:0.07), (im:0.06), (not: 0.05), (sure:0.04), (route:0.02), (any:0.01), ... e.g.: &lt;bus-route&gt;/im not sure <ref type="bibr">0.53</ref> state: confirm low-confidence speech recognition results (right:0.19), (is:0.19), (that:0.19), (&lt;location&gt;:0.12), (&lt;bus-route&gt;: 0.05), (i:0.04), (you:0.03), (said:0.03), (thought:0.03), (over:0.03), ... e.g.: i thought you said (&lt;bus-route&gt;/&lt;location&gt;) is that right (yes:0.45), (no:0.3), (yeah:0.12), (wrong:0.04), (correct:0.03), (back:0.02), (go:0.02), (nope:0.01), ... e.g.: yes/no/yeah/wrong/correct/go back/nope Downtown, is that correct?</p><p>Did you just say Norwood?</p><p>Say just the day you want.</p><p>Say just the &lt;me you want.</p><p>I'm sorry, I can't find any bus at all that run from Milton to Norwell. I checked route 61C and I also checked all the other bus routes I know too.</p><p>Repeat, next, previous  Log Likelihood The likelihood metric measures the probability of generating the test set under a specified model. As shown in <ref type="figure" target="#fig_9">Figure 7</ref>, our models yield as good or better likelihood than LM-HMM and LM-HMMS models on both datasets under all settings. For our proposed models, TM-HMMS and TM-HMMSS perform better than TM-HMM on TechSupport, but not necessarily on BusTime.</p><p>In addition, we notice that the marginal benefit of TM-HMMSS over TM-HMM is greater on Tech- Support dataset, where each dialogue focuses on one of many possible tasks. This coincides with our belief that topics are more conversation de- pendent and shared across the entire corpus in cus- tomer support data-i.e., different clients in differ- ent sessions might ask about similar issues.</p><p>Ordering Test <ref type="bibr" target="#b30">Ritter et al. (2010)</ref> proposes an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data. They use Kendall's œÑ as evaluation metric, which is based on the agreement between pairwise orderings of two sequences <ref type="bibr" target="#b20">(Kendall, 1938)</ref>. It ranges from ‚àí1 to +1, where +1 indicates an identical ordering and ‚àí1 indicates a reverse ordering. The idea is to generate all permutations of the utterances in a dialogue (including true ordering), and compute the log likelihood for each under the model. Then, Kendall's œÑ is computed between the most proba- ble permutation and true ordering. The result is the average of œÑ values for all dialogues in test corpus. <ref type="bibr" target="#b30">Ritter et al. (2010)</ref> limits their dataset by choos- ing Twitter dialogues containing 3 to 6 posts (ut- terances), making it tractable to enumerate all per- mutations. However, our datasets are much larger, and enumerating all possible permutations of dia- logues with more than 20 or 30 utterances is infea- sible. Instead, we incrementally build up the per- mutation set by adding one random permutation at a time, and taking the most probable permutation after each addition. If this process were continued (intractably!) until all permutations are enumer- ated, the true value of Kendall's œÑ test would be reached. In practice, the value appears to plateau after a few dozen measurements.</p><p>We present our results in <ref type="figure" target="#fig_10">Figure 8</ref>. Our mod- els consistently perform as good or better than Agent: conversation opening + identity check help, answer, desk, may, &lt;agent-name&gt;, welcom, name, number, phone, ... e.g.: welcome to answer desk, i'm &lt;agent- name&gt;, how can i help you, may i have your name?</p><p>Client: report problem tri, get, comput, cant, window, message, error, problem, instal, say, ... e.g.: get problem in windows, cant install on computer, it says error message Agent: conversation closure thank, answer, desk, &lt;client-name&gt;, contact, help, chat, day, welcom, ... e.g.: thank you for contacting answer desk, you are welcome, have a nice day Agent: acknowledge identity thank, minut, pleas, let, &lt;client-name&gt;, check, give, moment, ok, wait, ... e.g.: thank you, &lt;client-name&gt;, please</p><p>give me a moment, let me check </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head><p>Top Ranked Words p u rc h a s e microsoft, store, purchas, able, get, sir, order, site, mr, contact, mac, . . .      states. Cyan blocks are system actions and yellow blocks are user responses. In every block, the upper cell shows the top ranked words, and the lower cell shows example word sequences or string patterns of that state. Transition probability cut-off is 0.05. States are la- belled manually. A poorly-inferred state is highlighted, which seems to conflate the "acknowledge agent" and "resolve problem" states, and TM-HMMSS model has properly disentangled ( <ref type="figure" target="#fig_6">Figure 5)</ref>. the baseline models. For BusTime data, all models perform relatively well except LM-HMM which only indicates weak correlations. TM- HMM out-performs all other models under all set- tings. This is also true for TechSupport dataset. LM-HMMS, TM-HMMS and TM-HMMSS mod- els perform considerably well on BusTime, but not on TechSupport data. These three models al- low words to be generated from additional sources other than states. Although this improves log like- lihood, it is possible these models encode less in- formation about the state sequences, at least in the more diffuse TechSupport data. In summary, under both quantitative evaluation measures, our models advance state-of-the-art, however which of our models is best depends on the application. S S model negative log likelihood  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We have presented three new unsupervised mod- els to discover latent structures in task-oriented dialogues. We evaluated on two very different corpora-logs from spoken, human-computer dia- logues about bus time, and logs of textual, human- human dialogues about technical support. We have shown our models yield superior perfor- mance both qualitatively and quantitatively. One possible avenue for future work is scala- bility. <ref type="bibr">Parallelization (Asuncion et al., 2012</ref>) or online learning ( <ref type="bibr">Doucet et al., 2001</ref>) could signif- icantly speed up inference. In addition to MCMC, another class of inference method is variational Bayesian analysis ( <ref type="bibr" target="#b6">Blei et al., 2003;</ref><ref type="bibr" target="#b4">Beal, 2003)</ref>, which is inherently easier to distribute <ref type="bibr" target="#b40">(Zhai et al., 2012</ref>) and online update <ref type="bibr" target="#b16">(Hoffman et al., 2010</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Plate diagrams of proposed models. TM-HMM is an HMM with state-wise topic distributions. TM-HMMS adds session-wise topic distribution and a source generator. TM-HMMSS adds a state-wise source generator. Variable definitions are given in the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>state</head><label></label><figDesc>: ask if user is traveling now (say:0.8), (the:0.07), (you:0.07), (no:0.06), (yes:0.06), (do: 0.06), (want:0.06), (buses:0.05), (few:0.05), (next:0.04), ... e.g.: do you want the next few buses say yes or no (yes:0.5), (no:0.17), (yeah:0.16), (&lt;bus-route&gt;: 0.07), (back:0.04), (go:0.04), (nope:0.01), ... e.g.: yes/no/yeah 0.31 state: read out bus timetables (&lt;location&gt;:0.08), (at:0.05), (&lt;time&gt;:0.05), (next:0.05), (say:0.05), (from:0.04), (there: 0.04), (&lt;bus-route&gt;:0.04), (to:0.04), ... e.g.: there is a &lt;bus-route&gt; from &lt;location&gt; to &lt;location&gt; at &lt;time&gt; say next or repeat (next:0.4), (repeat:0.16), (over:0.11), (start:0.11), (previous:0.07), (go:0.06), (back:0.06), (goodbye:0.05), ... e.g.: next/repeat/start over/previous 0.12 0.42 state: ask for date and time (optional) (&lt;time&gt;:0.14), (&lt;date&gt;:0.1), (the:0.06), (or:0.05), (like:0.05), (say:0.05), (you:0.05), (want:0.05), (at:0.04), (depart:0.04), ... e.g.: say the time you want to depart like &lt;time&gt; (&lt;time&gt;:0.26), (&lt;date&gt;:0.14), (m:0.11), (depart:0.07), (a:0.07), (at:0.07), (by:0.03), ... e.g.: depart (at/by) &lt;time&gt; a m &lt;date&gt; 0.55 Start I heard 61C, is that right?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>At 11 :Figure 3 :</head><label>113</label><figDesc>Figure 3: (Upper) Part of the flowchart inferred on BusTime, by TM-HMM model with K = 10 topics and T = 10 states. The most probable path is highlighted, which is consistent with the underlying design (Figure 4). Cyan blocks are system actions and yellow blocks are user responses. In every block, the upper cell shows the top ranked words marginalized over all topics and the lower cell shows some examples of that state. Transition probability cut-off is 0.1. States are labelled manually. Figure 4: (Left) Hand-crafted reference flowchart for BusTime (Williams, 2012). Only the most common dialogue flows are displayed. System prompts shown are example paraphrases. Edge cases are not included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 3: (Upper) Part of the flowchart inferred on BusTime, by TM-HMM model with K = 10 topics and T = 10 states. The most probable path is highlighted, which is consistent with the underlying design (Figure 4). Cyan blocks are system actions and yellow blocks are user responses. In every block, the upper cell shows the top ranked words marginalized over all topics and the lower cell shows some examples of that state. Transition probability cut-off is 0.1. States are labelled manually. Figure 4: (Left) Hand-crafted reference flowchart for BusTime (Williams, 2012). Only the most common dialogue flows are displayed. System prompts shown are example paraphrases. Edge cases are not included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>b</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Part of flowchart (left) and topic table (right) on TechSupport dataset, generated by TM-HMMSS model under settings of K = 20 topics and T = 20 states. The topic table lists top ranked words in issues discussed in the chats. Cyan blocks are system actions and yellow blocks are user responses. In every block, the upper cell shows top ranked words, and the lower cell shows example string patterns of that state. Transition probability cut-off is 0.05. States and topics are labelled manually.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Part of flowchart on TechSupport dataset, generated by LM-HMM model with T = 20 states. Cyan blocks are system actions and yellow blocks are user responses. In every block, the upper cell shows the top ranked words, and the lower cell shows example word sequences or string patterns of that state. Transition probability cut-off is 0.05. States are labelled manually. A poorly-inferred state is highlighted, which seems to conflate the "acknowledge agent" and "resolve problem" states, and TM-HMMSS model has properly disentangled (Figure 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Negative log likelihood on BusTime (upper) and TechSupport (lower) datasets (smaller is better) under different settings of topics K and states T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Average Kendall's œÑ measure on BusTime (upper) and TechSupport (lower) datasets (larger is better) against number of random permutations, under various settings of topics K and states T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>table lists top</head><label>lists</label><figDesc></figDesc><table>ranked words in issues 
discussed in the chats. Cyan blocks are system actions and yellow blocks are user responses. In every 
block, the upper cell shows top ranked words, and the lower cell shows example string patterns of that 
state. Transition probability cut-off is 0.05. States and topics are labelled manually. 

Agent: conversation opening + identity check 

answer, desk, help, &lt;agent-name&gt;, 
welcom, today, may, name, number, ... 
e.g.: welcome to answer desk, i'm &lt;agent-
name&gt;, how can i help you, may i have 
your name, case/phone number, account? 

Client: acknowledge agent / resolved problem 

thank, ok, help, much, good, great, 
&lt;agent-name&gt;, day, appreci, bye, ... 
e.g.: ok, thanks, great, &lt;agent-name&gt; 
appreciate your help, good day, bye 

Agent: conversation closure 

answer, desk, thank, contact, day, chat, 
great, session, com, help, ... 
e.g.: thank you for contacting answer desk, 
you are welcome, have a nice day 

Agent: acknowledge problem 

issu, sorri, call, help, number, suport, 
concern, &lt;client-name&gt;, &lt;phone&gt;, best, ... 
e.g.: sorry to hear that, let me help with 
your concern, &lt;client-name&gt; 

Client: confirm identity 

call, number, phone, case, &lt;time&gt;, would, 
&lt;agent-name&gt;, pleas, &lt;phone&gt;, time, ... 
e.g.: &lt;agent-time&gt;, my phone number is 
&lt;phone&gt;. would you pleas call number... 

Agent: conversation closure 

anyth, els, welcom, help, &lt;client-name&gt;, 
today, assist, question, would, answer, ... 
e.g.: you are welcome, anything else today 
i would help/assist you, &lt;client-name&gt;? 

Agent: acknowledge identity 

give, minut, pleas, check, let, 
thank, moment, 3, one, 5, ... 
e.g.: thanks, one moment please, 
give me 3 minutes, let me check 

Client: report problem 

updat, window, install, &lt;agent-name&gt;, hello, 
error, get, problem, download, message, ... 
e.g.: hello, &lt;agent-name&gt;, i get problem/error 
when install/update/download in windows 

0.08 
0.1 

0.08 

0.07 

0.14 

0.05 

0.07 

0.06 

0.05 
0.05 

0.08 

0.06 

(4, 

</table></figure>

			<note place="foot" n="1"> Available for download at http://research.microsoft. com/en-us/events/dstc/ 2 Technical support human agents use many types of documentation-mainly checklists and guidelines, but in general, there are no flowcharts.</note>

			<note place="foot" n="3"> We used regular expression to map named entities, and Porter stemmer in NLTK to stem all tokens.</note>

			<note place="foot">M z 0,i w 1,i N 1 z 1,i w n,i N n z n,i T K h t g k s 0 w 0,i N 0 s 1 M z 0,i w 1,i N 1 z 1,i T K h t g k s 0 w 0,i N 0 s 1 M z 0,i N 1 z 1,i T K h t g k s 0 w 0,i N 0 s 1 M z 0,i T K h t g k s 0 w 0,i N 0 M z 0,i T K h t g k s 0 N 0 M z 0,i T K h t g k s 0 M T h t (a) TM-HMM s 0 w 0,i N 0 s 1 ... s n M</note>

			<note place="foot" n="4"> Note that a TM-HMMS model with state-specific topic models (instead of state-specific language models) would be subsumed by TM-HMM, since one topic could be used as the background topic in TM-HMMS.</note>

			<note place="foot" n="7"> We considered various ways of making a quantitative evaluation of the inferred state diagram, and proved difficult. Rather than attempt to justify a particular sub-division of each &quot;design states&quot;, we instead give several straightforward quantitative evaluations in the next section. 8 Recall in this corpus, state transitions occur after emitting each agent or client utterances, which does not necessarily alternate in a dialogue, so we display client request and agent response separately.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank anonymous reviewers and Jordan Boyd-Graber for their valuable comments. We are also grateful to Alan Ritter and Bill Dolan for their helpful discussions; and Kai (Anthony) Lui for providing TechSupport dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Distributed Gibbs sampling for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Triglia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A texttiling based approach to topic boundary detection in meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning the structure of taskdriven human-human dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">Di</forename><surname>Fabbrizio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Catching the drift: Probabilistic content models, with applications to generation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Variational Algorithms for Approximate Bayesian Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Conkie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Merigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Schubiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><forename type="middle">Eskenazi</forename></persName>
		</author>
		<title level="m">Spoken dialog challenge 2010: Comparison of live and control test results</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>SIGDIAL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>allocation. JMLR</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
<note type="report_type">Latent Dirichlet</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning the Structure of Task-oriented Conversations from the Corpus of In-domain Dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananlada</forename><surname>Chotimongkol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised classification of dialogue acts using a dirichlet process mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramn</forename><surname>Granell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>Pulman</surname></persName>
		</author>
		<editor>SIGDIAL</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
	<note>Bayesian query-focused summarization</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Verbal indicators of psychological distress in interactive dialogue with a virtual human</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Morbini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo methods in practice</title>
		<editor>Arnaud Doucet, Nando De Freitas, and Neil Gordon</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer Texts in Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discourse segmentation of multi-party conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Foslerlussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fully Bayesian approach to unsupervised part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mixture model POMDPs for efficient handling of uncertainty in dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online learning for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic segmentation of multiparty dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Yun</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In EACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A stochastic finite-state transducer approach to spoken dialog management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llu√≠s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Encarna</forename><surname>Planells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Segarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Griol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Switchboard SWBD-DAMSL shallowdiscourse-function annotation coders manual. Institute of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debra</forename><surname>Biasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science Technical Report</title>
		<imprint>
			<biblScope unit="page" from="97" to="99" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika Trust</title>
		<imprint>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information state and dialogue management in the TRINDI dialogue move engine toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Staffan</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Traum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="323" to="340" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A stochastic model of human-machine interaction for learning dialogue strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="23" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dialogue-oriented review summary generation for spoken dialogue recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Seneff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Polylingual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extractive summarization of meeting recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for Dirichlet process mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Slice sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="705" to="767" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised topic modelling for multi-party spoken discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>K√∂rding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural language generation as planning under uncertainty for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised modeling of twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2000-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The information state approach to dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Staffan</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current and new directions in discourse and dialogue</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="325" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Structured Topic Models for Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating probability of correctness for ASR N-best lists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhrid</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Challenges and opportunities for state tracking in statistical spoken dialog systems: Results from two public deployments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting response to political blog posts with topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using POMDPs for dialog management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IEEE/ACL Workshop on Spoken Language Technologies (SLT06)</title>
		<meeting>the 1st IEEE/ACL Workshop on Spoken Language Technologies (SLT06)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamad</forename><surname>Alkhouja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
