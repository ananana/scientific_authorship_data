<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Summarization by Extracting Sentences and Words</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
							<email>jianpeng.cheng@ed.ac.uk mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ILCC</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Summarization by Extracting Sentences and Words</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="484" to="494"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs 1. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The need to access and digest large amounts of textual data has provided strong impetus to de- velop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content. Much effort in automatic summarization has been de- voted to sentence extraction, where a summary is created by identifying and subsequently concate- nating the most salient text units in a document.</p><p>Most extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length ), the words in the title, the presence of proper nouns, content features such as word frequency ( <ref type="bibr" target="#b24">Nenkova et al., 2006</ref>), and event features such as action nouns <ref type="bibr" target="#b8">(Filatova and Hatzivassiloglou, 2004</ref>). Sentences are typically assigned a score indicating the strength of presence of these features. Several methods have been used in order to select the summary sen- tences ranging from binary classifiers ( <ref type="bibr" target="#b19">Kupiec et al., 1995)</ref>, to hidden Markov models <ref type="bibr" target="#b5">(Conroy and O'Leary, 2001</ref>), graph-based algorithms ( <ref type="bibr" target="#b7">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b22">Mihalcea, 2005)</ref>, and integer lin- ear programming <ref type="bibr" target="#b34">(Woodsend and Lapata, 2010)</ref>.</p><p>In this work we propose a data-driven approach to summarization based on neural networks and continuous sentence features. There has been a surge of interest recently in repurposing sequence transduction neural network architectures for NLP tasks such as machine translation <ref type="bibr" target="#b29">(Sutskever et al., 2014</ref>), question answering ( <ref type="bibr" target="#b12">Hermann et al., 2015)</ref>, and sentence compression ( <ref type="bibr" target="#b28">Rush et al., 2015)</ref>. Central to these approaches is an encoder- decoder architecture modeled by recurrent neu- ral networks. The encoder reads the source se- quence into a list of continuous-space representa- tions from which the decoder generates the target sequence. An attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> is often used to locate the region of focus during decoding.</p><p>We develop a general framework for single- document summarization which can be used to extract sentences or words. Our model includes a neural network-based hierarchical document reader or encoder and an attention-based content extractor. The role of the reader is to derive the meaning representation of a document based on its sentences and their constituent words. Our models adopt a variant of neural attention to extract sen- tences or words. Contrary to previous work where attention is an intermediate step used to blend hid- den units of an encoder to a vector propagating ad- ditional information to the decoder, our model ap- plies attention directly to select sentences or words of the input document as the output summary. Similar neural attention architectures have been previously used for geometry reasoning , under the name Pointer Networks.</p><p>One stumbling block to applying neural net- work models to extractive summarization is the lack of training data, i.e., documents with sen- tences (and words) labeled as summary-worthy. Inspired by previous work on summarization <ref type="bibr" target="#b34">(Woodsend and Lapata, 2010;</ref><ref type="bibr" target="#b30">Svore et al., 2007)</ref> and reading comprehension ( <ref type="bibr" target="#b12">Hermann et al., 2015)</ref> we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the infor- mation contained in the article (see <ref type="figure" target="#fig_1">Figure 1</ref> for an example). Using a number of transformation and scoring algorithms, we are able to match high- lights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous ap- proaches have used small scale training data in the range of a few hundred examples.</p><p>Our work touches on several strands of research within summarization and neural sequence model- ing. The idea of creating a summary by extracting words from the source document was pioneered in <ref type="bibr" target="#b1">Banko et al. (2000)</ref> who view summarization as a problem analogous to statistical machine transla- tion and generate headlines using statistical mod- els for selecting and ordering the summary words. Our word-based model is similar in spirit, how- ever, it operates over continuous representations, produces multi-sentence output, and jointly se- lects summary words and organizes them into sen- tences. A few recent studies ( <ref type="bibr" target="#b18">Kobayashi et al., 2015;</ref><ref type="bibr" target="#b35">Yogatama et al., 2015</ref>) perform sentence ex- traction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more di- rectly to perform the actual summarization task. <ref type="bibr" target="#b28">Rush et al. (2015)</ref> propose a neural attention model for abstractive sentence compression which is trained on pairs of headlines and first sentences in an article. In contrast, our model summarizes documents rather than individual sentences, pro- ducing multi-sentential discourse. A major archi- tectural difference is that our decoder selects out- put symbols from the document of interest rather than the entire vocabulary. This effectively helps us sidestep the difficulty of searching for the next output symbol under a large vocabulary, with low- frequency words and named entities whose rep- resentations can be challenging to learn. <ref type="bibr" target="#b9">Gu et al. (2016)</ref> and <ref type="bibr" target="#b11">Gulcehre et al. (2016)</ref> propose a similar "copy" mechanism in sentence compres- sion and other tasks; their model can accommo- date both generation and extraction by selecting which sub-sequences in the input sequence to copy in the output.</p><p>We evaluate our models both automatically (in terms of ROUGE) and by humans on two datasets: the benchmark DUC 2002 document summariza- tion corpus and our own DailyMail news high- lights corpus. Experimental results show that our summarizers achieve performance compara- ble to state-of-the-art systems employing hand- engineered features and sophisticated linguistic constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>In this section we formally define the summariza- tion tasks considered in this paper. Given a doc- ument D consisting of a sequence of sentences {s 1 , · · · , s m } and a word set {w 1 , · · · , w n }, we are interested in obtaining summaries at two levels of granularity, namely sentences and words.</p><p>Sentence extraction aims to create a sum- mary from D by selecting a subset of j sentences (where j &lt; m). We do this by scoring each sen- tence within D and predicting a label y L ∈ {0, 1} indicating whether the sentence should be in- cluded in the summary. As we apply supervised training, the objective is to maximize the likeli- hood of all sentence labels</p><formula xml:id="formula_0">y L = (y 1 L , · · · , y m L )</formula><p>given the input document D and model parameters θ:</p><formula xml:id="formula_1">log p(y L |D; θ) = m ∑ i=1 log p(y i L |D; θ)<label>(1)</label></formula><p>Although extractive methods yield naturally grammatical summaries and require relatively little linguistic analysis, the selected sentences make for long summaries containing much redun- dant information. For this reason, we also de- velop a model based on word extraction which seeks to find a subset of words 2 in D and their optimal ordering so as to form a summary</p><formula xml:id="formula_2">y s = (w 1 , · · · , w k ), w i ∈ D.</formula><p>Compared to sentence extraction which is a sequence labeling problem, this task occupies the middle ground between full abstractive summarization which can exhibit a wide range of rewrite operations and extractive AFL star blames vomiting cat for speeding ::::::::</p><p>Adelaide :::::: Crows ::::::::: defender :::::: Daniel ::::: Talia :::: has :::: kept ::: his ::::::: driving :::::::: license, :::::: telling :: a ::::: court ::: he :::: was :::::::: speeding • Adelaide Crows defender Daniel Talia admits to speeding but says he didn't see road signs be- cause his cat was vomiting in his car.</p><p>• 22-year-old Talia was fined $824 and four demerit points, instead of seven, because of his 'signif- icant' training commitments. summarization which exhibits none. We formu- late word extraction as a language generation task with an output vocabulary restricted to the original document. In our supervised setting, the training goal is to maximize the likelihood of the generated sentences, which can be further decomposed by enforcing conditional dependencies among their constituent words:</p><formula xml:id="formula_3">log p(y s |D; θ)= k ∑ i=1 log p(w i |D, w 1 ,· · ·, w i−1 ; θ) (2)</formula><p>In the following section, we discuss the data elici- tation methods which allow us to train neural net- works based on the above defined objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Data for Summarization</head><p>Data-driven neural summarization models require a large training corpus of documents with labels indicating which sentences (or words) should be in the summary. Until now such corpora have been limited to hundreds of examples (e.g., the DUC 2002 single document summarization cor- pus) and thus used mostly for testing <ref type="bibr" target="#b34">(Woodsend and Lapata, 2010)</ref>. To overcome the paucity of annotated data for training, we adopt a methodol- ogy similar to <ref type="bibr" target="#b12">Hermann et al. (2015)</ref> and create two large-scale datasets, one for sentence extrac- tion and another one for word extraction.</p><p>In a nutshell, we retrieved 3 hundreds of thou- sands of news articles and their corresponding highlights from DailyMail (see <ref type="figure" target="#fig_1">Figure 1</ref> for an ex- ample). The highlights (created by news editors) are genuinely abstractive summaries and therefore not readily suited to supervised training. To cre- ate the training data for sentence extraction, we reverse approximated the gold standard label of each document sentence given the summary based on their semantic correspondence <ref type="bibr" target="#b34">(Woodsend and Lapata, 2010)</ref>. Specifically, we designed a rule- based system that determines whether a document sentence matches a highlight and should be la- beled with 1 (must be in the summary), and 0 oth- erwise. The rules take into account the position of the sentence in the document, the unigram and bigram overlap between document sentences and highlights, the number of entities appearing in the highlight and in the document sentence. We ad- justed the weights of the rules on 9,000 documents with manual sentence labels created by <ref type="bibr" target="#b34">Woodsend and Lapata (2010)</ref>. The method obtained an accu- racy of 85% when evaluated on a held-out set of 216 documents coming from the same dataset and was subsequently used to label 200K documents. Approximately 30% of the sentences in each doc- ument were deemed summary-worthy.</p><p>For the creation of the word extraction dataset, we examine the lexical overlap between the high- lights and the news article. In cases where all high- light words (after stemming) come from the orig- inal document, the document-highlight pair con- stitutes a valid training example and is added to the word extraction dataset. For out-of-vocabulary (OOV) words, we try to find a semantically equiv- alent replacement present in the news article. Specifically, we check if a neighbor, represented by pre-trained 4 embeddings, is in the original doc- ument and therefore constitutes a valid substitu- tion. If we cannot find any substitutes, we discard the document-highlight pair. Following this pro- cedure, we obtained a word extraction dataset con- taining 170K articles, again from the DailyMail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Summarization Model</head><p>The key components of our summarization model include a neural network-based hierarchical doc- ument reader and an attention-based hierarchical content extractor. The hierarchical nature of our model reflects the intuition that documents are generated compositionally from words, sentences, paragraphs, or even larger units. We therefore em- ploy a representation framework which reflects the same architecture, with global information being discovered and local information being preserved. Such a representation yields minimum informa- tion loss and is flexible allowing us to apply neural attention for selecting salient sentences and words within a larger context. In the following, we first describe the document reader, and then present the details of our sentence and word extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document Reader</head><p>The role of the reader is to derive the meaning rep- resentation of the document from its constituent sentences, each of which is treated as a sequence of words. We first obtain representation vectors at the sentence level using a single-layer convo- lutional neural network (CNN) with a max-over- time pooling operation <ref type="bibr" target="#b14">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b36">Zhang and Lapata, 2014;</ref><ref type="bibr" target="#b15">Kim et al., 2016)</ref>. Next, we build representations for docu- ments using a standard recurrent neural network (RNN) that recursively composes sentences. The CNN operates at the word level, leading to the acquisition of sentence-level representations that are then used as inputs to the RNN that acquires document-level representations, in a hierarchical fashion. We describe these two sub-components of the text reader below.</p><p>Convolutional Sentence Encoder We opted for a convolutional neural network model for repre- senting sentences for two reasons. Firstly, single- layer CNNs can be trained effectively (without any long-term dependencies in the model) and secondly, they have been successfully used for sentence-level classification tasks such as senti- ment analysis <ref type="bibr" target="#b16">(Kim, 2014)</ref>. Let d denote the dimension of word embeddings, and s a docu- ment sentence consisting of a sequence of n words (w 1 , · · · , w n ) which can be represented by a dense column matrix W ∈ R n×d . We apply a tempo- ral narrow convolution between W and a kernel K ∈ R c×d of width c as follows:</p><formula xml:id="formula_4">f i j = tanh(W j: j+c−1 ⊗ K + b)<label>(3)</label></formula><p>where ⊗ equates to the Hadamard Product fol- lowed by a sum over all elements. f i j denotes the j-th element of the i-th feature map f i and b is the bias. We perform max pooling over time to obtain a single feature (the ith feature) representing the sentence under the kernel K with width c:</p><formula xml:id="formula_5">s i,K = max j f i j (4)</formula><p>In practice, we use multiple feature maps to compute a list of features that match the dimen- sionality of a sentence under each kernel width. In addition, we apply multiple kernels with different widths to obtain a set of different sentence vectors. Finally, we sum these sentence vectors to obtain the final sentence representation. The CNN model is schematically illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> (bottom). In the example, the sentence embeddings have six dimensions, so six feature maps are used under each kernel width. The blue feature maps have width two and the red feature maps have width three. The sentence embeddings obtained under each kernel width are summed to get the final sen- tence representation (denoted by green).</p><p>Recurrent Document Encoder At the docu- ment level, a recurrent neural network composes a sequence of sentence vectors into a document vec- tor. Note that this is a somewhat simplistic attempt at capturing document organization at the level of sentence to sentence transitions. One might view the hidden states of the recurrent neural network as a list of partial representations with each fo- cusing mostly on the corresponding input sentence given the previous context. These representations altogether constitute the document representation, which captures local and global sentential infor- mation with minimum compression.</p><p>The RNN we used has a Long Short-Term Memory (LSTM) activation unit for ameliorat- ing the vanishing gradient problem when train- ing long sequences (Hochreiter and Schmidhuber, 1997). Given a document d = (s 1 , · · · , s m ), the hidden state at time step t, denoted by h t , is up- dated as:</p><formula xml:id="formula_6">    i t f t o t ˆ c t     =     σ σ σ tanh     W · h t−1 s t<label>(5)</label></formula><formula xml:id="formula_7">c t = f t c t−1 + i t ˆ c t<label>(6)</label></formula><formula xml:id="formula_8">h t = o t tanh(c t )<label>(7)</label></formula><p>where W is a learnable weight matrix. Next, we discuss a special attention mechanism for extract- ing sentences and words given the recurrent docu- ment encoder just described, starting from the sen- tence extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence Extractor</head><p>In the standard neural sequence-to-sequence mod- eling paradigm ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, an atten- tion mechanism is used as an intermediate step to decide which input region to focus on in order to generate the next output. In contrast, our sen- tence extractor applies attention to directly extract salient sentences after reading them. The extractor is another recurrent neural net- work that labels sentences sequentially, taking into account not only whether they are individually relevant but also mutually redundant. The com- plete architecture for the document encoder and the sentence extractor is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. As can be seen, the next labeling decision is made with both the encoded document and the previ- ously labeled sentences in mind. Given encoder hidden states (h 1 , · · · , h m ) and extractor hidden states ( ¯ h 1 , · · · , ¯ h m ) at time step t, the decoder at- tends the t-th sentence by relating its current de- coding state to the corresponding encoding state:</p><formula xml:id="formula_9">¯ h t = LSTM(p t−1 s t−1 , ¯ h t−1 )<label>(8)</label></formula><formula xml:id="formula_10">p(y L (t) = 1|D) = σ(MLP( ¯ h t : h t ))<label>(9)</label></formula><p>where MLP is a multi-layer neural network with as input the concatenation of ¯ h t and h t . p t−1 repre- sents the degree to which the extractor believes the previous sentence should be extracted and memo- rized (p t−1 =1 if the system is certain; 0 otherwise).</p><p>In practice, there is a discrepancy between train- ing and testing such a model. During training we know the true label p t−1 of the previous sen- tence, whereas at test time p t−1 is unknown and has to be predicted by the model. The discrep- ancy can lead to quickly accumulating prediction errors, especially when mistakes are made early in the sequence labeling process. To mitigate this, we adopt a curriculum learning strategy ( : at the beginning of training when p t−1 cannot be predicted accurately, we set it to the true label of the previous sentence; as training goes on, we gradually shift its value to the pre- dicted label p(y L (t − 1) = 1|d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Extractor</head><p>Compared to sentence extraction which is a purely sequence labeling task, word extraction is closer to a generation task where relevant content must be selected and then rendered fluently and gram- matically. A small extension to the structure of the sequential labeling model makes it suitable for generation: instead of predicting a label for the next sentence at each time step, the model di- rectly outputs the next word in the summary. The model uses a hierarchical attention architecture: at time step t, the decoder softly 5 attends each document sentence and subsequently attends each word in the document and computes the probabil- ity of the next word to be included in the summary p(w t = w i |d, w 1 , · · · , w t−1 ) with a softmax classi- fier:</p><formula xml:id="formula_11">¯ h t = LSTM(w t−1 , ¯ h t−1 ) 6<label>(10)</label></formula><formula xml:id="formula_12">a t j = z T tanh(W e ¯ h t + W r h j ), h j ∈ D<label>(11)</label></formula><formula xml:id="formula_13">b t j = softmax(a t j )<label>(12)</label></formula><formula xml:id="formula_14">˜ h t = m ∑ j=1 b t j h j<label>(13)</label></formula><formula xml:id="formula_15">u t i = v T tanh(W e ˜ h t + W r w i ), w i ∈ D<label>(14)</label></formula><p>p(w t = w i |D, w 1 , · · · , w t−1 ) = softmax(u t i ) (15) In the above equations, w i corresponds to the vec- tor of the i-th word in the input document, whereas z, W e , W r , v, W e , and W r are model weights. The model architecture is shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>The word extractor can be viewed as a con- ditional language model with a vocabulary con- straint. In practice, it is not powerful enough to enforce grammaticality due to the lexical diversity and sparsity of the document highlights. A pos- sible enhancement would be to pair the extractor with a neural language model, which can be pre- trained on a large amount of unlabeled documents and then jointly tuned with the extractor during decoding ( <ref type="bibr" target="#b10">Gulcehre et al., 2015)</ref>. A simpler al- ternative which we adopt is to use n-gram features collected from the document to rerank candidate summaries obtained via beam decoding. We incor- porate the features in a log-linear reranker whose feature weights are optimized with minimum error rate training <ref type="bibr" target="#b25">(Och, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>In this section we present our experimental setup for assessing the performance of our summariza- tion models. We discuss the datasets used for <ref type="bibr">5</ref> A simpler model would use hard attention to select a sen- tence first and then a few words from it as a summary, but this would render the system non-differentiable for training. Al- though hard attention can be trained with the REINFORCE algorithm <ref type="bibr" target="#b33">(Williams, 1992)</ref>, it requires sampling of discrete actions and could lead to high variance. <ref type="bibr">6</ref> We empirically found that feeding the previous sentence- level attention vector as additional input to the LSTM would lead to small performance improvements. This is not shown in the equation. training and evaluation, give implementation de- tails, briefly introduce comparison models, and ex- plain how system output was evaluated.</p><p>Datasets We trained our sentence-and word- based summarization models on the two datasets created from DailyMail news. Each dataset was split into approximately 90% for training, 5% for validation, and 5% for testing. We evaluated the models on the DUC-2002 single document sum- marization task. In total, there are 567 documents belonging to 59 different clusters of various news topics. Each document is associated with two ver- sions of 100-word 7 manual summaries produced by human annotators. We also evaluated our mod- els on 500 articles from the DailyMail test set (with the human authored highlights as goldstan- dard). We sampled article-highlight pairs so that the highlights include a minimum of 3 sentences. The average byte count for each document is 278.</p><p>Implementation Details We trained our mod- els with Adam ( <ref type="bibr" target="#b17">Kingma and Ba, 2014</ref>) with ini- tial learning rate 0.001. The two momentum pa- rameters were set to 0.99 and 0.999 respectively. We performed mini-batch training with a batch size of 20 documents. All input documents were padded to the same length with an additional mask variable storing the real length for each document. The size of word, sentence, and document em- beddings were set to 150, 300, and 750, respec- tively. For the convolutional sentence model, we followed <ref type="bibr" target="#b15">Kim et al. (2016)</ref>  <ref type="bibr">8</ref> and used a list of ker- nel sizes {1, 2, 3, 4, 5, 6, 7}. For the recurrent doc- ument model and the sentence extractor, we used as regularization dropout with probability 0.5 on the LSTM input-to-hidden layers and the scoring layer. The depth of each LSTM module was 1. All LSTM parameters were randomly initialized over a uniform distribution within [-0.05, 0.05]. The word vectors were initialized with 150 dimen- sional pre-trained embeddings. <ref type="bibr">9</ref> Proper nouns pose a problem for embedding- based approaches, especially when these are rare or unknown (e.g., at test time). <ref type="bibr" target="#b28">Rush et al. (2015)</ref> address this issue by adding a new set of features and a log-linear model component to their sys- tem. As our model enjoys the advantage of gener- ation by extraction, we can force the model to in- spect the context surrounding an entity and its rel- ative position in the sentence in order to discover extractive patterns, placing less emphasis on the meaning representation of the entity itself. Specif- ically, we perform named entity recognition with the package provided by <ref type="bibr" target="#b12">Hermann et al. (2015)</ref> and maintain a set of randomly initialized entity embeddings. During training, the index of the en- tities is permuted to introduce some noise but also robustness in the data. A similar data augmenta- tion approach has been used for reading compre- hension ( <ref type="bibr" target="#b12">Hermann et al., 2015)</ref>.</p><p>A common problem with extractive methods based on sentence labeling is that there is no con- straint on the number of sentences being selected at test time. We address this by reranking the posi- tively labeled sentences with the probability scores obtained from the softmax layer (rather than the label itself). In other words, we are more inter- ested in is the relative ranking of each sentence rather than their exact scores. This suggests that an alternative to training the network would be to employ a ranking-based objective or a learning to rank algorithm. However, we leave this to future work. We use the three sentences with the highest scores as the summary (also subject to the word or byte limit of the evaluation protocol).</p><p>Another issue relates to the word extraction model which is challenging to batch since each document possesses a distinct vocabulary. We sidestep this during training by performing neg- ative sampling <ref type="bibr" target="#b23">(Mikolov et al., 2013</ref>) which trims the vocabulary of different documents to the same length. At each decoding step the model is trained to differentiate the true target word from 20 noise samples. At test time we still loop through the words in the input document (and a stop-word list) to decide which word to output next.</p><p>System Comparisons We compared the output of our models to various summarization meth- ods. These included the standard baseline of sim- ply selecting the "leading" three sentences from each document as the summary. We also built a sentence extraction baseline classifier using lo- gistic regression and human engineered features. The classifier was trained on the same datasets as our neural network models with the follow- ing features: sentence length, sentence position, number of entities in the sentence, sentence-to- sentence cohesion, and sentence-to-document rel- evance. Sentence-to-sentence cohesion was com- puted by calculating for every document sentence its embedding similarity with every other sentence in the same document. The feature was the nor- malized sum of these similarity scores. Sentence embeddings were obtained by averaging the con- stituent word embeddings. Sentence-to-document relevance was computed similarly. We calculated for each sentence its embedding similarity with the document (represented as bag-of-words), and nor- malized the score. The word embeddings used in this baseline are the same as the pre-trained ones used for our neural models.</p><p>In addition, we included a neural abstractive summarization baseline. This system has a similar architecture to our word extraction model except that it uses an open vocabulary during decoding. It can also be viewed as a hierarchical document- level extension of the abstractive sentence summa- rizer proposed by <ref type="bibr" target="#b28">Rush et al. (2015)</ref>. We trained this model with negative sampling to avoid the ex- cessive computation of the normalization constant.</p><p>Finally, we compared our models to three previ- ously published systems which have shown com- petitive performance on the DUC2002 single doc- ument summarization task. The first approach is the phrase-based extraction model of <ref type="bibr" target="#b34">Woodsend and Lapata (2010)</ref>. Their system learns to produce highlights from parsed input (phrase structure trees and dependency graphs); it selects salient phrases and recombines them subject to length, coverage, and grammar constraints enforced via integer linear programming (ILP). Like ours, this model is trained on document-highlight pairs, and produces telegraphic-style bullet points rather than full-blown summaries. The other two systems, TGRAPH <ref type="bibr" target="#b26">(Parveen et al., 2015)</ref> and URANK <ref type="bibr" target="#b32">(Wan, 2010)</ref>, produce more typical summaries and repre- sent the state of the art. TGRAPH is a graph-based sentence extraction model, where the graph is con- structed from topic models and the optimization is performed by constrained ILP. URANK adopts a unified ranking system for both single-and multi- document summarization.</p><p>Evaluation We evaluated the quality of the summaries automatically using ROUGE ( <ref type="bibr" target="#b21">Lin and Hovy, 2003</ref> lap (ROUGE-1,2) as a means of assessing infor- mativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. In addition, we evaluated the generated sum- maries by eliciting human judgments for 20 ran- domly sampled DUC 2002 test documents. Par- ticipants were presented with a news article and summaries generated by a list of systems. These include two neural network systems (sentence- and word-based extraction), the neural abstrac- tive system described earlier, the lead baseline, the phrase-based ILP model 10 of <ref type="bibr" target="#b34">Woodsend and Lapata (2010)</ref>, and the human authored summary. Subjects were asked to rank the summaries from best to worst (with ties allowed) in order of in- formativeness (does the summary capture impor- tant information in the article?) and fluency (is the summary written in well-formed English?). We elicited human judgments using Amazon's Me- chanical Turk crowdsourcing platform. Partici- pants (self-reported native English speakers) saw 2 random articles per session. We collected 5 re- sponses per document.  <ref type="table">Table 2</ref>: Rankings (shown as proportions) and mean ranks given to systems by human partici- pants (lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>NN-WE our word extraction model, and NN-ABS the neural abstractive baseline. The table also in- cludes results for the LEAD baseline, the logistic regression classifier (LREG), and three previously published systems (ILP, TGRAPH, and URANK).</p><p>The NN-SE outperforms the LEAD and LREG baselines with a significant margin, while per- forming slightly better than the ILP model. This is an encouraging result since our model has only access to embedding features obtained from raw text. In comparison, LREG uses a set of manually selected features, while the ILP system takes advantage of syntactic information and ex- tracts summaries subject to well-engineered lin- guistic constraints, which are not available to our models. Overall, our sentence extraction model achieves performance comparable to the state of the art without sophisticated constraint optimiza- tion (ILP, TGRAPH) or sentence ranking mech- anisms (URANK). We visualize the sentence weights of the NN-SE model in the top half of <ref type="figure">Fig- ure 4</ref>. As can be seen, the model is able to locate text portions which contribute most to the overall meaning of the document.</p><p>ROUGE scores for the word extraction model are less promising. This is somewhat expected given that ROUGE is n-gram based and not very well suited to measuring summaries which contain a significant amount of paraphrasing and may de- viate from the reference even though they express similar meaning. However, a meaningful com- parison can be carried out between NN-WE and NN-ABS which are similar in spirit. We observe that NN-WE consistently outperforms the purely abstractive model. As NN-WE generates sum- maries by picking words from the original docu- ment, decoding is easier for this model compared to NN-ABS which deals with an open vocabulary. The extraction-based generation approach is more robust for proper nouns and rare words, which pose a serious problem to open vocabulary mod-sentence extraction: a gang of at least three people poured gasoline on a car that stopped to fill up at entity5 gas station early on Saturday morning and set the vehicle on fire a gang of at least three people poured gasoline on a car that stopped to fill up at entity5 gas station early on Saturday morning and set the vehicle on fire the driver of the car, who has not been identified, said he got into an argument with the suspects while he was pumping gas at a entity13 in entity14 the driver of the car, who has not been identified, said he got into an argument with the suspects while he was pumping gas at a entity13 in entity14 the group covered his white entity16 in gasoline and lit it ablaze while there were two passengers inside the group covered his white entity16 in gasoline and lit it ablaze while there were two passengers inside at least three people poured gasoline on a car and lit it on fire at a entity14 gas station explosive situation the passengers and the driver were not hurt during the incident but the car was completely ruined the man's grandmother said the fire was lit after the suspects attempted to carjack her grandson, entity33 reported the man's grandmother said the fire was lit after the suspects attempted to carjack her grandson, entity33 reported she said:' he said he was pumping gas and some guys came up and asked for the car ' they pulled out a gun and he took off running ' they took the gas tank and started spraying ' no one was injured during the fire , but the car 's entire front end was torched , according to entity52 the entity53 is investigating the incident as an arson and the suspects remain at large the entity53 is investigating the incident as an arson and the suspects remain at large surveillance video of the incident is being used in the investigation before the fire , which occurred at 12:15am on Saturday , the suspects tried to carjack the man hot case the entity53 is investigating the incident at the entity67 station as an arson word extraction: gang poured gasoline in the car, entity5 Saturday morning. the driver argued with the suspects. his grandmother said the fire was lit by the suspects attempted to carjack her grandson. entities: entity5:California entity13:76-Station entity14: South LA entity16:Dodge Charger entity33:ABC entity52:NBC entity53:LACFD entity67:LA76 <ref type="figure">Figure 4</ref>: Visualization of the summaries for a DailyMail article. The top half shows the relative attention weights given by the sentence extraction model. Darkness indicates sentence importance. The lower half shows the summary generated by the word extraction.</p><p>els. An example of the generated summaries for NN-WE is shown at the lower half of <ref type="figure">Figure 4</ref>. <ref type="table" target="#tab_1">Table 1</ref> (lower half) also shows system results on the 500 DailyMail news articles (test set). In general, we observe similar trends to DUC 2002, with NN-SE performing the best in terms of all ROUGE metrics. Note that scores here are gener- ally lower compared to DUC 2002. This is due to the fact that the gold standard summaries (aka highlights) tend to be more laconic and as a result involve a substantial amount of paraphrasing.</p><p>The results of our human evaluation study are shown in <ref type="table">Table 2</ref>. Specifically, we show, propor- tionally, how often our participants ranked each system 1st, 2nd, and so on. Perhaps unsurpris- ingly, the human-written descriptions were con- sidered best and ranked 1st 27% of the time, how- ever closely followed by our NN-SE model which was ranked 1st 22% of the time. The ILP system was mostly ranked in 2nd place (38% of the time). The rest of the systems occupied lower ranks. We further converted the ranks to ratings on a scale of 1 to 6 (assigning ratings 6. . . 1 to rank placements 1. . . 6). This allowed us to perform Analysis of Variance (ANOVA) which revealed a reliable ef- fect of system type. Specifically, post-hoc Tukey tests showed that NN-SE and ILP are significantly (p &lt; 0.01) better than LEAD, NN-WE, and NN-ABS but do not differ significantly from each other or the human goldstandard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work we presented a data-driven summa- rization framework based on an encoder-extractor architecture. We developed two classes of mod- els based on sentence and word extraction. Our models can be trained on large scale datasets and learn informativeness features based on continu- ous representations without recourse to linguistic annotations. Two important ideas behind our work are the creation of hierarchical neural structures that reflect the nature of the summarization task and generation by extraction. The later effectively enables us to sidestep the difficulties of generat- ing under a large vocabulary, essentially covering the entire dataset, with many low-frequency words and named entities.</p><p>Directions for future work are many and var- ied. One way to improve the word-based model would be to take structural information into ac- count during generation, e.g., by combining it with a tree-based algorithm <ref type="bibr" target="#b4">(Cohn and Lapata, 2009)</ref>. It would also be interesting to apply the neural mod- els presented here in a phrase-based setting similar to <ref type="bibr" target="#b20">Lebret et al. (2015)</ref>. A third direction would be to adopt an information theoretic perspective and devise a purely unsupervised approach that selects summary sentences and words so as to minimize information loss, a task possibly achievable with the dataset created in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:</head><label></label><figDesc>:::: 36km :::: over :::: the :::: limit :::::::: because :: he :::: was ::::::::: distracted ::: by ::: his :::: sick :::: cat. The 22-year-old AFL star, who drove 96km/h in a 60km/h road works zone on the South Eastern expressway in February, said he didn't see the reduced speed sign because he was so distracted by his cat vomiting violently in the back seat of his car. :: In :::: the ::::::::: Adelaide ::::::::::: magistrates ::::: court ::: on :::::::::::: Wednesday, :::::::::: Magistrate ::::: Bob ::::::: Harrap :::::: fined ::::: Talia :::::: $824 ::: for ::::::::: exceeding ::: the :::::: speed :::: limit ::: by ::::: more :::: than :::::::: 30km/h. He lost four demerit points, instead of seven, because of his significant training commitments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: DailyMail news article with highlights. Underlined sentences bear label 1, and 0 otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A recurrent convolutional document reader with a neural sentence extractor.</figDesc><graphic url="image-1.png" coords="5,72.29,62.81,217.71,201.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Neural attention mechanism for word extraction.</figDesc><graphic url="image-2.png" coords="5,307.28,62.81,226.77,110.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 (</head><label>1</label><figDesc>top half) summarizes our results on the DUC 2002 test dataset using ROUGE. NN-SE represents our neural sentence extraction model, 10 We are grateful to Kristian Woodsend for giving us ac- cess to the output of his system. Unfortunately, we do not have access to the output of TGRAPH or URANK for inclusion in the human evaluation.</figDesc><table>Models 
1 st 2 nd 3 rd 4 th 5 th 6 th MeanR 

LEAD 

0.10 0.17 0.37 0.15 0.16 0.05 3.27 

ILP 

0.19 0.38 0.13 0.13 0.11 0.06 2.77 

NN-SE 

0.22 0.28 0.21 0.14 0.12 0.03 2.74 

NN-WE 

0.00 0.04 0.03 0.21 0.51 0.20 4.79 
NN-ABS 0.00 0.01 0.05 0.16 0.23 0.54 5.24 
Human 0.27 0.23 0.29 0.17 0.03 0.01 2.51 

</table></figure>

			<note place="foot" n="1"> Resources are available for download at http:// homepages.inf.ed.ac.uk/s1537177/resources.html</note>

			<note place="foot" n="2"> The vocabulary can also be extended to include a small set of commonly-used (high-frequency) words.</note>

			<note place="foot" n="3"> The script for constructing our datasets is modified from the one released in Hermann et al. (2015).</note>

			<note place="foot" n="4"> We used the Python Gensim library and the 300-dimensional GoogleNews vectors.</note>

			<note place="foot" n="7"> According to the DUC2002 guidelines http: //www-nlpir.nist.gov/projects/duc/guidelines/ 2002.html, the generated summary should be within 100 words. 8 The CNN-LSTM architecture is publicly available at https://github.com/yoonkim/lstm-char-cnn. 9 We used the word2vec (Mikolov et al., 2013) skip-gram model with context window size 6, negative sampling size 10 and hierarchical softmax 1. The model was trained on the Google 1-billion word benchmark (Chelba et al., 2014).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank three anonymous review-ers and members of the ILCC at the School of In-formatics for their valuable feedback. The support of the European Research Council under award number 681760 "Translating Multiple Modalities into Text" is gratefully acknowledged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2015</title>
		<meeting>ICLR 2015<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Headline generation based on statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vibhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th ACL</title>
		<meeting>the 38th ACL<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="318" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentence compression as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="637" to="674" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text summarization via hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conroy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th</title>
		<meeting>the 34th</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Acl</forename><surname>Annual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigir</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="406" to="407" />
			<pubPlace>New Oleans, Louisiana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lexpagerank: Prestige in multi-document text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Günesgünes¸günes¸erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 EMNLP</title>
		<meeting>the 2004 EMNLP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Event-based extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL04 Workshop</title>
		<editor>Stan Szpakowicz Marie-Francine Moens</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI</title>
		<meeting>the 30th AAAI<address><addrLine>Phoenix, Arizon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 EMNLP</title>
		<meeting>the 2014 EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Summarization based on embedding distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taichi</forename><surname>Yatsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A trainable document summarizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kupiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International ACM SIGIR</title>
		<meeting>the 18th Annual International ACM SIGIR<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Phrase-based image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ICML</title>
		<meeting>the 32nd ICML<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT NAACL</title>
		<meeting>HLT NAACL<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language independent extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Interactive Poster and Demonstration Sessions</title>
		<meeting>the ACL Interactive Poster and Demonstration Sessions<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual ACM SIGIR</title>
		<meeting>the 29th Annual ACM SIGIR<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
	<note>Washington</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st ACL</title>
		<meeting>the 41st ACL<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Topical coherence for graph-based extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daraksha</forename><surname>Parveen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hans-Martin Ramsl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1949" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mead-a platform for multidocument multilingual text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Blairgoldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arda</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanko</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Drabek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Columbia University Academic Commons</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhancing single-document summarization by combining RankNet and third-party sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krysta</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 EMNLPCoNLL</title>
		<meeting>the 2007 EMNLPCoNLL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="448" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards a unified approach to simultaneous single-document and multi-document summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd COLING</title>
		<meeting>the 23rd COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1137" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic generation of story highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th ACL</title>
		<meeting>the 48th ACL<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Extractive summarization by maximizing semantic volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1961" to="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2014 EMNLP</title>
		<meeting>2014 EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
