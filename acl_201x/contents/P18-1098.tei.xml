<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Architecture for Automated ICD Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">† Petuum Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Architecture for Automated ICD Coding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1066" to="1076"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding-which assigns a subset of ICD codes to a patient visit-is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error-prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) atten-tional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The International Classification of Diseases (ICD) is a healthcare classification system maintained by the World Health Organization ( <ref type="bibr" target="#b20">Organization et al., 1978)</ref>. It provides a hierarchy of diagnos- tic codes of diseases, disorders, injuries, signs, symptoms, etc. It is widely used for reporting diseases and health conditions, assisting in medi- cal reimbursement decisions, collecting morbidity and mortality statistics, to name a few.</p><p>While ICD codes are important for making clinical and financial decisions, medical coding -which assigns proper ICD codes to a patient visit -is time-consuming, error-prone, and expen- sive. Medical coders review the diagnosis descrip- tions written by physicians in the form of textual phrases and sentences, and (if necessary) other in- formation in the electronic health record of a clin- ical episode, then manually attribute the appro- priate ICD codes by following the coding guide- lines <ref type="bibr">(O'malley et al., 2005</ref>). Several types of er- rors frequently occur. First, the ICD codes are organized in a hierarchical structure. For a node representing a disease C, the children of this node represent the subtypes of C. In many cases, the difference between disease subtypes is very sub- tle. It is common that human coders select in- correct subtypes. Second, when writing diagno- sis descriptions, physicians often utilize abbrevia- tions and synonyms, which causes ambiguity and imprecision when the coders are matching ICD codes to those descriptions ( <ref type="bibr" target="#b23">Sheppard et al., 2008)</ref>. Third, in many cases, several diagnosis descrip- tions are closely related and should be mapped to a single ICD code. However, unexperienced coders may code each disease separately. Such errors are called unbundling. The cost incurred by coding errors and the financial investment spent on im- proving coding quality are estimated to be $25 bil- lion per year in the US <ref type="bibr" target="#b17">(Lang, 2007;</ref><ref type="bibr" target="#b4">Farkas and Szarvas, 2008)</ref>.</p><p>To reduce coding errors and cost, we aim at building an ICD coding model which automati- cally and accurately translates the free-text diag- nosis descriptions into ICD codes. To achieve this goal, several technical challenges need to be ad- dressed. First, there exists a hierarchical structure among the ICD codes. This hierarchy can be lever- aged to improve coding accuracy. On one hand, if code A and B are both children of C, then it is unlikely to simultaneously assign A and B to a patient. On the other hand, if the distance be-tween A and B in the code tree is smaller than that between A and C and we know A is the correct code, then B is more likely to be a correct code than C, since codes with smaller distance are more clinically relevant. How to explore this hierarchi- cal structure for better coding is technically de- manding. Second, the diagnosis descriptions and the textual descriptions of ICD codes are written in quite different styles even if they refer to the same disease. In particular, the textual description of an ICD code is formally and precisely worded, while diagnosis descriptions are usually written by physicians in an informal and ungrammatical way, with telegraphic phrases, abbreviations, and typos. Third, it is required that the assigned ICD codes are ranked according to their relevance to the patient. How to correctly determine this or- der is technically nontrivial. Fourth, as stated ear- lier, there does not necessarily exist an one-to- one mapping between diagnosis descriptions and ICD codes, and human coders should consider the overall health condition when assigning codes. In many cases, two closely related diagnosis descrip- tions need to be mapped onto a single combina- tion ICD code. On the other hand, physicians may write two health conditions into one diagnosis de- scription which should be mapped onto two ICD codes under such circumstances.</p><p>Contributions In this paper, we design a neural architecture to automatically perform ICD coding given the diagnosis descriptions. Specifically, we make the following contributions:</p><p>• We propose a tree-of-sequences LSTM archi- tecture to simultaneously capture the hierarchi- cal relationship among codes and the semantics of each code.</p><p>• We use an adversarial learning approach to rec- oncile the heterogeneous writing styles of diag- nosis descriptions and ICD code descriptions.</p><p>• We use isotonic constraints to preserve the im- portance order among codes and develop an al- gorithm based on ADMM and isotonic projec- tion to solve the constrained problem.</p><p>• We use an attentional matching mechanism to perform many-to-one and one-to-many map- pings between diagnosis descriptions and codes.</p><p>• On a clinical datasets with 59K patient visits, we demonstrate the effectiveness of the pro- posed methods.</p><p>The rest of the paper is organized as follows. Section 2 introduces related works. Section 3 and 4 present the dataset and methods. Section 5 gives experimental results. Section 6 presents conclu- sions and discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Larkey and Croft (1996) studied the automatic as- signment of ICD-9 codes to dictated inpatient dis- charge summaries, using a combination of three classifiers: k-nearest neighbors, relevance feed- back, and Bayesian independence classifiers. This method assigns a single code to each patient visit. However, in clinical practice, each patient is usually assigned with multiple codes. <ref type="bibr" target="#b5">Franz et al. (2000)</ref> investigated the automated coding of German-language free-text diagnosis phrases. This approach performs one-to-one mapping be- tween diagnosis descriptions and ICD codes. This is not in accordance with the coding practice where one-to-many and many-to-one mappings widely exist <ref type="bibr">(O'malley et al., 2005</ref>). <ref type="bibr" target="#b21">Pestian et al. (2007)</ref> studied the assignment of ICD-9 codes to radiology reports. <ref type="bibr" target="#b13">Kavuluru et al. (2013)</ref> proposed an unsupervised ensemble approach to automati- cally perform ICD-9 coding based on textual nar- ratives in electronic health records (EHRs) <ref type="bibr" target="#b14">Kavuluru et al. (2015)</ref> developed multi-label classifi- cation, feature selection, and learning to rank ap- proaches for ICD-9 code assignment of in-patient visits based on EHRs. <ref type="bibr" target="#b16">Koopman et al. (2015)</ref> ex- plored the automatic ICD-10 classification of can- cers from free-text death certificates. These meth- ods did not consider the hierarchical relationship or importance order among codes.</p><p>The tree LSTM network was first proposed by <ref type="bibr" target="#b25">(Tai et al., 2015</ref>) to model the constituent or dependency parse trees of sentences. <ref type="bibr" target="#b26">Teng and Zhang (2016)</ref> extended the unidirectional tree LSTM to a bidirectional one.  proposed a sequence-of-trees LSTM network to model a passage. In this network, a sequential LSTM is used to compose a sequence of tree LSTMs. The tree LSTMs are built on the con- stituent parse trees of individual sentences and the sequential LSTM is built on the sequence of sen- tences. Our proposed tree-of-sequences LSTM network differs from the previous works in two- fold. First, it is applied to a code tree to capture the hierarchical relationship among codes. Sec- ond, it uses a tree LSTM to compose a hierarchy Diagnosis Descriptions 1. Prematurity at 35 4/7 weeks gestation 2. Twin number two of twin gestation 3. Respiratory distress secondary to transient tachypnea of the newborn 4. Suspicion for sepsis ruled out Assigned ICD Codes 1. V31.00 (Twin birth, mate liveborn, born in hospital, delivered without mention of cesarean section) 2. 765.18 (Other preterm infants, 2,000-2,499 grams) 3. 775.6 (Neonatal hypoglycemia) 4. 770.6 (Transitory tachypnea of newborn) 5. V29.0 (Observation for suspected infectious condition) 6. V05.3 (Need for prophylactic vaccination and inoculation against viral hepatitis)  <ref type="bibr">Goodfellow et al., 2014</ref>), domain adap- tion ( <ref type="bibr" target="#b6">Ganin and Lempitsky, 2015)</ref>, feature learn- ing ( <ref type="bibr" target="#b3">Donahue et al., 2016</ref>), text generation ( <ref type="bibr" target="#b32">Yu et al., 2017)</ref>, to name a few. In this paper, we use adversarial learning for mitigating the discrepancy among the writing styles of a pair of sentences.</p><p>The attention mechanism was widely used in machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), im- age captioning ( <ref type="bibr" target="#b29">Xu et al., 2015)</ref>, reading compre- hension ( <ref type="bibr" target="#b22">Seo et al., 2016</ref>), text classification ( <ref type="bibr" target="#b31">Yang et al., 2016)</ref>, etc. In this work, we compute at- tention between sentences to perform many-to-one and one-to-many mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset and Preprocessing</head><p>We performed the study on the publicly available MIMIC-III dataset <ref type="bibr" target="#b12">(Johnson et al., 2016</ref>), which contains de-identified electronic health records (EHRs) of 58,976 patient visits in the Beth Israel Deaconess Medical Center from 2001 to 2012. Each EHR has a clinical note called discharge summary, which contains multiple sections of in- formation, such as 'discharge diagnosis', 'past medical history', etc. From the 'discharge diag- nosis' and 'final diagnosis' sections, we extracted the diagnosis descriptions (DDs) written by physi- cians. Each DD is a short phrase or a sentence, articulating a certain disease or condition. Med- ical coders perform ICD coding mainly based on DDs. Following such a practice, in this paper, we set the inputs of the automated coding model to be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder of diagnosis description</head><p>Tree-of-sequences LSTM encoder of ICD- code description Adversarial reconciliation module</p><p>Attentional matching module Isotonic constraints 1. Pneumonia 2. Acute kidney failure ......</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis descriptions</head><p>V31.00 775.6 765.18 770.6</p><p>Assigned ICD codes <ref type="figure">Figure 1</ref>: Architecture of the ICD Coding Model the DDs while acknowledging that other informa- tion in the EHRs is also valuable and is referred to by coders for code assignment. For simplicity, we leave the incorporation of non-DD information to future study.</p><p>Each patient visit is assigned with a list of ICD codes, ranked in descending order of importance and relevance. For each visit, the number of codes is usually not equal to the number of diagnosis de- scriptions. These ground-truth codes serve as the labels to train our coding model. The entire dataset contains 6,984 unique codes, each of which has a textual description, describing a disease, symp- tom, or condition. The codes are organized into a hierarchy where the top-level codes correspond to general diseases while the bottom-level ones rep- resent specific diseases. In the code tree, children of a node represent subtypes of a disease. <ref type="table" target="#tab_0">Table 1</ref> shows the DDs and codes of an exemplar patient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>In this section, we present a neural architecture for ICD coding. <ref type="figure">Figure 1</ref> shows the overview of our approach. The proposed ICD coding model consists of five mod- ules. The model takes the ICD-code tree and diagnosis descriptions (DDs) of a patient as in- puts and assigns a set of ICD codes to the pa- tient. The encoder of DDs generates a latent rep- resentation vector for a DD. The encoder of ICD codes is a tree-of-sequences long short-term mem- ory (LSTM) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref> The representation aims at simultaneously capturing the semantics of each code and the hi- erarchical relationship among codes. By incor- porating the code hierarchy, the model can avoid selecting codes that are subtypes of the same dis- ease and promote the selection of codes that are clinically correlated. The writing styles of DDs and code descriptions (CDs) are largely different, which makes the matching between a DD and a CD error-prone. To address this issue, we develop an adversarial learning approach to reconcile the writing styles. On top of the latent representa- tion vectors of the descriptions, we build a dis- criminative network to distinguish which ones are DDs and which are CDs. The encoders of DDs and CDs try to make such a discrimination impos- sible. By doing this, the learned representations are independent of the writing styles and facilitate more accurate matching. The representations of DDs and CDs are fed into an attentional match- ing module to perform code assignment. This at- tentional mechanism allows multiple DDs to be matched to a single code and allows a single DD to be matched to multiple codes. During training, we incorporate the order of importance among codes as isotonic constraints. These constraints regu- late the model's weight parameters so that codes with higher importance are given larger prediction scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tree-of-Sequences LSTM Encoder</head><p>This section introduces the encoder of ICD codes. Each code has a description (a sequence of words) that tells the semantics of this code. We use a sequential LSTM (SLSTM) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) to encode this description. To capture the hierarchical relationship among codes, we build a tree LSTM (TLSTM) <ref type="bibr" target="#b25">(Tai et al., 2015)</ref> along the code tree. At each TLSTM node, the input vector is the latent representation generated by the SLSTM. Combining these two types of LSTMs together, we obtain a tree-of-sequences LSTM network <ref type="figure">(Figure 2</ref>).</p><p>Sequential LSTM A sequential LSTM (SLSTM) <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref> network is a special type of recurrent neural network that (1) learns the latent representation (which usually reflects certain semantic infor- mation) of words, and (2) models the sequential structure among words. In the word sequence, each word t is allocated with an SLSTM unit, which consists of the following components: an input gate i t , a forget gate f t , an output gate o t , a memory cell c t , and a hidden state s t . These components (vectors) are computed as follows:</p><formula xml:id="formula_0">i t = σ(W (i) s t−1 + U (i) x t + b (i) ) f t = σ(W (f ) s t−1 + U (f ) x t + b (f ) ) o t = σ(W (o) s t−1 + U (o) x t + b (o) ) c t = i t tanh(W (c) s t−1 + U (c) x t + b (c) ) +f t c t−1 s t = o t tanh(c t )</formula><p>(1) where x t is the embedding vector of word t. W, U are component-specific weight matrices and b are bias vectors.</p><p>Tree-of-sequences LSTM We use a bidirec- tional tree LSTM (TLSTM) <ref type="bibr" target="#b25">(Tai et al., 2015;</ref> to capture the hierarchical rela- tionships among codes. The inputs of this LSTM include the code hierarchy and hidden states of in- dividual codes produced by the SLSTMs. It con- sists of a bottom-up TLSTM and a top-down TL- STM, which produce two hidden states h ↑ and h ↓ at each node in the tree.</p><p>In the bottom-up TLSTM, an internal node (rep- resenting a code C, having M children) is com- prised of these components: an input gate i ↑ , an output gate o ↑ , a memory cell c ↑ , a hidden state h ↑ and M child-specific forget gates {f</p><formula xml:id="formula_1">(m) ↑ } M m=1</formula><p>where f (m) ↑ corresponds to the m-th child. The transition equations among components are:</p><formula xml:id="formula_2">i ↑ = σ( M m=1 W (i,m) ↑ h (m) ↑ + U (i) s + b (i) ↑ ) ∀m, f (m) ↑ = σ(W (f,m) ↑ h (m) ↑ + U (f,m) s + b (f,m) ↑ ) o ↑ = σ( M m=1 W (o,m) ↑ h (m) ↑ + U (o) s + b (o) ↑ ) u ↑ = tanh( M m=1 W (u,m) ↑ h (m) ↑ + U (u) s + b (u) ↑ ) c ↑ = i ↑ u ↑ + M m=1 f (m) ↑ c (m) ↑ h ↑ = o ↑ tanh(c ↑ )<label>(2)</label></formula><p>where s is the SLSTM hidden state that en- codes the description of code C; {h</p><formula xml:id="formula_3">(m) ↑ } M m=1 and {c (m) ↑ } M m=1</formula><p>are the bottom-up TLSTM hidden states and memory cells of the children. W, U, b are component-specific weight matrices and bias vectors. For a leaf node having no children, its only input is the SLSTM hidden state s and no for- get gates are needed.</p><p>In the top-down TLSTM, for a non-root node, it has such components: an input gate i ↓ , a forget gate f ↓ , an output gate o ↓ , a memory cell c ↓ and a hidden state h ↓ . The transition equations are:</p><formula xml:id="formula_4">i ↓ = σ(W (i) ↓ h (p) ↓ + b (i) ↓ ) f ↓ = σ(W (f ) ↓ h (p) ↓ + b (f ) ↓ ) o ↓ = σ(W (o) ↓ h (p) ↓ + b (o) ↓ ) u ↓ = tanh(W (u) ↓ h (p) ↓ + b (u) ↓ ) c ↓ = i ↓ u ↓ + f ↓ c (p) ↓ h ↓ = o ↓ tanh(c ↓ )<label>(3)</label></formula><p>where h ↓ are the top-down TLSTM hid- den state and memory cell of the parent of this node. For the root node which has no parent, h ↓ cannot be computed using the above equations. In- stead, we set h ↓ to h ↑ (the bottom-up TLSTM hid- den state generated at the root node). h ↑ captures the semantics of all codes in this hierarchy, which is then propagated downwards to each individual code via the top-down TLSTM dynamics.</p><p>We concatenate the hidden states of the two di- rections to obtain the bidirectional TLSTM encod- ing of each code h = [h ↑ ; h ↓ ]. The bottom-up TL- STM composes the semantics of children (repre- senting sub-diseases) and merge them into the cur- rent node, which hence captures child-to-parent relationship. The top-down TLSTM makes each node inherit the semantics of its parent, which cap- tures parent-to-child relation. As a result, the hier- archical relationship among codes are encoded in the hidden states.</p><p>For the diagnosis descriptions of a patient, we use an SLSTM network to encode each descrip- tion individually. The weight parameters of this SLSTM are tied with those of the SLSTM used for encoding code descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attentional Matching</head><p>Next, we introduce how to map the DDs to codes. We denote the hidden representations of DDs and codes as {h m } M m=1 and {u n } N n=1 respectively, where M is the number of DDs of one patient and N is the total number of codes in the dataset. The mapping from DDs to codes is not one-to-one. In many cases, a code is assigned only when a certain combination of K (1 &lt; K ≤ M ) diseases simul- taneously appear within the M DDs and the value of K depends on this code. Among the K dis- eases, their importance of determining the assign- ment of this code is different. For the rest M − K DDs, we can consider their importance score to be zero. We use a soft-attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>) to calculate these importance scores. For a code u n , the importance of a DD h m to u n is calculated as a nm = u n h m . We normalize the scores {a nm } M m=1 of all DDs into a probabilistic simplex using the softmax opera- tion: ˜ a nm = exp(a nm )/ M l=1 exp(a nl ). Given these normalized importance scores {ã nm } M m=1 , we use them to weight the representations of DDs and get a single attentional vector of the M DDs: h n = M m=1ãm=1˜m=1ã nm h m . Then we concatenate h n and u n , and use a linear classifier to predict the probability that code n should be assigned:</p><formula xml:id="formula_5">p n = sigmoid(w n [ h n ; u n ] + b n ),</formula><p>where the coefficients w n and bias b n are specific to code n.</p><p>We train the weight parameters Θ of the pro- posed model using the data of L patient visits. Θ includes the sequential LSTM weights W s , tree LSTM weights W t and weights W p in the final prediction layer. Let c (l) ∈ R N be a binary vector where c (l) n = 1 if the n-th code is assigned to this patient and c (l) n = 0 if otherwise. Θ can be learned by minimizing the following prediction loss:</p><formula xml:id="formula_6">min Θ L pred (Θ) = L l=1 N n=1 CE(p (l) n , c (l) n ) (4)</formula><p>where p</p><formula xml:id="formula_7">(l)</formula><p>n is the predicted probability that code n is assigned to patient visit l and p</p><formula xml:id="formula_8">(l)</formula><p>n is a function of Θ. CE(·, ·) is the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Adversarial Reconciliation of Writing Styles</head><p>We use an adversarial learning ( <ref type="bibr">Goodfellow et al., 2014)</ref> approach to reconcile the different writing styles of diagnosis descriptions (DDs) and code descriptions (CDs). The basic idea is: after en- coded, if a description cannot be discerned to be a DD or a CD, then the difference in their writ- ing styles is eliminated. We build a discriminative network which takes the encoding vector of a de- scription as input and tries to identify it as a DD or CD. The encoders of DDs and CDs adjust their weight parameters so that such a discrimination is difficult to be achieved by the discriminative net- work. Consider all the descriptions {t r , y r } R r=1</p><p>where t r is a description and y r is a binary label. y r = 1 if t r is a DD and y r = 0 if otherwise. Let f (t r ; W s ) denote the sequential LSTM (SLSTM) encoder parameterized by W s . This encoder is shared by the DDs and CDs. Note that for CDs, a tree LSTM is further applied on top of the encod- ings produced by the SLSTM. We use the SLSTM encoding vectors of CDs as the input of the dis- criminative network rather than using the TLSTM encodings since the latter are irrelevant to writing styles. Let g(f (t r ; W s ); W d ) denote the discrim- inative network parameterized by W d . It takes the encoding vector f (t r ; W s ) as input and produces the probability that t r is a DD. Adversarial learn- ing is performed by solving this problem:</p><formula xml:id="formula_9">max Ws min W d L adv = R r=1 CE(g(f (t r ; W s ); W d ), y r )<label>(5)</label></formula><p>The discriminative network tries to differentiate DDs from CDs by minimizing this classification loss while the encoder maximizes this loss so that DDs and CDs are not distinguishable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Isotonic Constraints</head><p>Next, we incorporate the importance order among ICD codes. For the D (l) codes assigned to patient l, without loss of generality, we assume the order is 1 2 · · · D (l) (the order is given by human coders as ground-truth in the MIMIC-III dataset). We use the predicted probability p i <ref type="bibr">(1 ≤ i ≤ D (l)</ref> ) defined in Section 4.3 to characterize the impor- tance of code i. To incorporate the order, we im- pose an isotonic constraint on the probabilities:</p><formula xml:id="formula_10">p (l) 1 p (l) 2 · · · p (l) D (l)</formula><p>, and solve the following problem:</p><formula xml:id="formula_11">min Θ L pred (Θ) + max W d (−λL adv (W s , W d )) s.t. p (l) 1 p (l) 2 · · · p (l) D (l) ∀l = 1, · · · , L<label>(6)</label></formula><p>where the probabilities p (l) i are functions of Θ and λ is a tradeoff parameter.</p><p>We develop an algorithm based on the alternating direction method of multiplier (ADMM) <ref type="bibr" target="#b2">(Boyd et al., 2011</ref>) to solve the problem defined in Eq.(6). Let p (l) be a |D (l) |-dimensional vector where the i-th element is p (l) i . We first write the problem into an equivalent form </p><formula xml:id="formula_12">min Θ L pred (Θ) + max W d (−λL adv (W s , W d )) s.t. p (l) = q (l) q (l) 1 q (l) 2 · · · q (l) |D (l) | ∀l = 1, · · · , L<label>(7</label></formula><formula xml:id="formula_13">L pred (Θ) + max W d (−λL adv (W s , W d )) +p (l) − q (l) , v (l) + ρ 2 p (l) − q (l) 2 2 s.t. q (l) 1 q (l) 2 · · · q (l) |D (l) | ∀l = 1, · · · , L (8) We solve this problem by alternating between {p (l) } L l=1 , {q (l) } L l=1 and {v (l) } L l=1</formula><p>The sub- problem defined over q (l) is</p><formula xml:id="formula_14">min q (l) −−q (l) , v (l) + ρ 2 p (l) − q (l) 2 2 s.t. q (l) 1 q (l) 2 · · · q (l) |D (l) |<label>(9)</label></formula><p>which is an isotonic projection problem and can be solved via the algorithm proposed in ( <ref type="bibr" target="#b33">Yu and Xing, 2016)</ref>.</p><formula xml:id="formula_15">With {q (l) } L l=1 and {v (l) } L l=1 fixed, the sub-problem is min Θ L pred (Θ) + max W d (−λL adv (W s , W d ))</formula><p>which can be solved using stochastic gradient descent (SGD). The up- date of v (l) is simple:</p><formula xml:id="formula_16">v (l) = v (l) + ρ(p (l) − q (l) ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Out of the 6,984 unique codes, we selected 2,833 codes that have the top frequencies to perform the study. We split the data into a train/validation/test dataset with 40k/7k/12k patient visits respectively. The hyperparameters were tuned on the valida- tion set. The SLSTMs were bidirectional and dropout with 0.5 probability ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>) was used. The size of hidden states in all LSTMs was set to 100. The word embeddings were trained on the fly and their dimension was set to 200. The tradeoff parameter λ was set to 0.1. The parameter ρ in the ADMM algorithm was set to 1. In the SGD algorithm for solving</p><formula xml:id="formula_17">min Θ L pred (Θ)+max W d (−λL adv (W s , W d ))</formula><p>, we used the ADAM ( <ref type="bibr" target="#b15">Kingma and Ba, 2014</ref>) optimizer with an initial learning rate 0.001 and a mini- batch size 20. Sensitivity (true positive rate) and specificity (true negative rate) were used to eval- uate the code assignment performance. We cal- culated these two scores for each individual code on the test set, then took a weighted (proportional to codes' frequencies) average across all codes. To evaluate the ranking performance of codes, we used normalized discounted cumulative gain (NDCG) <ref type="bibr" target="#b11">(Järvelin and Kekäläinen, 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>We perform ablation study to verify the effective- ness of each module in our model. To evaluate module X, we remove it from the model without changing other modules and denote such a base- line by No-X. The comparisons of No-X with the full model are given in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Tree-of-sequences LSTM To evaluate this module, we compared with the two configu- rations: (1) No-TLSTM, which removes the tree LSTM and directly uses the hidden states produced by the sequential LSTM as final rep- resentations of codes; (2) Bottom-up TLSTM, which removes the hidden states generated by the top-down TLSTM. In addition, we compared with four hierarchical classification baselines including <ref type="formula">(1)</ref>  , by using them to replace the bidirec- tional tree LSTM while keeping other modules untouched. <ref type="table" target="#tab_2">Table 2</ref> shows the average sensitivity and specificity scores achieved by these methods on the test set. We make the following observa- tions. First, removing tree LSTM largely degrades performance: the sensitivity and specificity of No-TLSTM is 0.23 and 0.28 respectively while our full model (which uses bidirectional TLSTM) achieves 0.29 and 0.33 respectively. The reason is No-TLSTM ignores the hierarchical relationship among codes. Second, bottom-up tree LSTM alone performs less well than bidirectional tree LSTM. This demonstrates the necessity of the top-down TLSTM, which ensures every two codes are connected by directed paths and can more expressively capture code-relations in the hierarchy. Third, our method outperforms the four baselines. The possible reason is our method di- rectly builds codes' hierarchical relationship into their representations while the baselines perform representation-learning and relationship-capturing Sensitivity Specificity ( <ref type="bibr" target="#b18">Larkey and Croft, 1996)</ref> 0.15 0.17 ( <ref type="bibr" target="#b5">Franz et al., 2000)</ref> 0.19 0.21 ( <ref type="bibr" target="#b21">Pestian et al., 2007)</ref> 0.12 0.21 ( <ref type="bibr" target="#b13">Kavuluru et al., 2013)</ref> 0.09 0.11 ( <ref type="bibr" target="#b14">Kavuluru et al., 2015)</ref> 0.21 0.25 ( <ref type="bibr" target="#b16">Koopman et al., 2015)</ref> 0 Next, we present some qualitative results. For a patient (admission ID 147798) having a DD 'E Coli urinary tract infection', without using tree LSTM, two sibling codes 585.2 (chronic kidney disease, stage II (mild)) -which is the ground- truth -and 585.4 (chronic kidney disease, stage IV (severe)) are simultaneously assigned possibly because their textual descriptions are very similar (only differ in the level of severity). This is in- correct because 585.2 and 585.4 are the children of 585 (chronic kidney disease) and the severity level of this disease cannot simultaneously be mild and severe. After tree LSTM is added, the false prediction of 585.4 is eliminated, which demon- strates the effectiveness of tree LSTM in incorpo- rating one constraint induced by the code hierar- chy: among the nodes sharing the same parent, only one should be selected.</p><note type="other">.18 0.20 LET 0.23 0.29 HierNet 0.26 0.30 HybridNet 0.25 0.31 BranchNet 0.25 0.29 No-TLSTM 0.23 0.28 Bottom-up TLSTM 0.27 0.31 No-AL 0.26 0.31 No-IC 0.24 0.29 No-AM 0.27 0.29 Our full model 0.29 0.33</note><p>For patient 197205, No-TLSTM assigns the following codes: 462 (subacute sclerosing pa- nencephalitis), 790.29 (other abnormal glucose), 799.9 (unspecified viral infection), and 285.21 (anemia in chronic kidney disease). Among these codes, the first three are ground-truth and the fourth one is incorrect (the ground-truth is 401.9 (unspecified essential hypertension)). Adding tree LSTM fixes this error. The average distance be- tween 401.9 and the rest of ground-truth codes is 6.2. For the incorrectly assigned code 285.21, such a distance is 7.9. This demonstrates that tree LSTM is able to capture another constraint im- posed by the hierarchy: codes with smaller tree- distance are more likely to be assigned together.  Adversarial learning To evaluate the efficacy of adversarial learning (AL), we remove it from the full model and refer to this baseline as No-AL. Specifically, in Eq.(6), the loss term max . For any code c, we define the relevance score of c to l as 0 if c / ∈ M (l) and as |M (l) | − r(c) if otherwise, where r(c) is the ground-truth rank of c in M (l) . We rank codes in descending order of their corresponding prediction probabilities and obtain the predicted rank for each code. We calculate the NDCG scores at position 2, 4, 6, 8 based on the relevance scores and predicted ranks, which are shown in <ref type="table" target="#tab_4">Table 3</ref>. As can be seen, using IC achieves much higher NDCG than No- IC, which demonstrates the effectiveness of IC in capturing the importance order among codes.</p><formula xml:id="formula_18">W d (−L adv (W s , W d )) is taken away.</formula><p>We also evaluate how IC affects the sensitivity and specificity of code assignment. As can be seen from <ref type="table" target="#tab_2">Table 2</ref>, No-IC degrades the two scores from 0.29 and 0.33 to 0.24 and 0.29 respectively, which indicates that IC is helpful in training a model that can more correctly assign codes. This is because IC encourages codes that are highly relevant to the patients to be ranked at top positions, which pre- vents the selection of irrelevant codes.</p><p>Attentional matching (AM) In the evaluation of this module, we compare with a baseline - No-AM, which performs an unweighted average of the M DDs:</p><formula xml:id="formula_19">h n = 1 M M m=1 h m , concate- nates</formula><p>h n with u n and feeds the concatenated vec- tor into the final prediction layer. From <ref type="table" target="#tab_2">Table 2</ref>, we can see our full model (with AM) outperforms No-AM, which demonstrates the effectiveness of attentional matching. In determining whether a code should be assigned, different DDs have dif- ferent importance weights. No-AM ignores such weights, therefore performing less well.</p><p>AM can correctly perform many-to-one map- ping from multiple DDs to a CD. For example, patient 190236 was given two DDs: 'renal insuffi- ciency' and 'acute renal failure'. AM maps them to a combined ICD code: 403.91 (hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage V or end stage renal disease), which is in the ground-truth provided by medical coders. On the contrary, No-AM fails to assign this code. On the other hand, AM is able to cor- rectly map a DD to multiple CDs. For example, a DD 'congestive heart failure, diastolic' was given to patient 140851. AM successfully maps this DD to two codes: (1) 428.0 (congestive heart fail- ure, unspecified); (2) 428.30 (diastolic heart fail- ure, unspecified). Without AM, this DD is mapped only to 428.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Holistic Comparison with Other Baselines</head><p>In addition to evaluating the four modules individ- ually, we also compared our full model with four other baselines proposed by <ref type="bibr" target="#b18">(Larkey and Croft, 1996;</ref><ref type="bibr" target="#b5">Franz et al., 2000;</ref><ref type="bibr" target="#b21">Pestian et al., 2007;</ref><ref type="bibr" target="#b13">Kavuluru et al., 2013</ref><ref type="bibr" target="#b14">Kavuluru et al., , 2015</ref><ref type="bibr" target="#b16">Koopman et al., 2015)</ref> for ICD coding. <ref type="table" target="#tab_2">Table 2</ref> shows the results. As can be seen, our approach achieves much better sensi- tivity and specificity scores. The reason that our model works better is two-fold. First, our model is based on deep neural network, which has ar- guably better modeling power than linear methods used in the baselines. Second, our model is able to capture the hierarchical relationship and impor- tance order among codes, can alleviate the discrep- ancy in writing styles and allows flexible many-to- one and one-to-many mappings from DDs to CDs. These merits are not possessed by the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Discussions</head><p>In this paper, we build a neural network model for automated ICD coding. Evaluations on the MIMIC-III dataset demonstrate the following. First, the tree-of-sequences LSTM network effec- tively discourages the co-selection of sibling codes and promotes the co-assignment of clinically- relevant codes. Adversarial learning improves the matching accuracy by alleviating the discrepancy among the writing styles of DDs and CDs. Third, isotonic constraints promote the correct ranking of codes. Fourth, the attentional matching mecha- nism is able to perform many-to-one and one-to- many mappings.</p><p>In the coding practice of human coders, in addi- tion to the diagnosis descriptions, other informa- tion contained in nursing notes, lab values, and medical procedures are also leveraged for code as- signment. We have initiated preliminary investi- gation along this line and added two new input sources: (1) the rest of discharge summary and <ref type="formula" target="#formula_2">(2)</ref> lab values. The sensitivity is improved from 0.29 to 0.32 and the specificity is improved from 0.33 to 0.35. A full study is ongoing.</p><p>At present, the major limitations of this work include: (1) it does not perform well on infrequent codes; (2) it is less capable of dealing with abbre- viations. We will address these two issues in fu- ture by investigating diversity-promoting regular- ization (  and leveraging an exter- nal knowledge base that maps medical abbrevia- tions into their full names.</p><p>The proposed methods can be applied to other tasks in NLP. The tree-of-sequences model can be applied for ontology annotation. It takes the textual descriptions of concepts in the ontology and their hierarchical structure as inputs and pro- duces a latent representation for each concept. The representations can simultaneously capture the semantics of codes and their relationships. The proposed adversarial reconciliation of writ- ing styles and attentional matching can be applied for knowledge mapping or entity linking. For ex- ample, in tweets, we can use the method to map an informally written mention 'nbcbightlynews' to a canonical entity 'NBC Nightly News' in the knowledge base.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>) network.</head><label></label><figDesc>Figure 1 shows the overview of our approach. The proposed ICD coding model consists of five modules. The model takes the ICD-code tree and diagnosis descriptions (DDs) of a patient as inputs and assigns a set of ICD codes to the patient. The encoder of DDs generates a latent representation vector for a DD. The encoder of ICD codes is a tree-of-sequences long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) network. It takes the textual descriptions of the ICD codes and their hierarchical structure as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>hierarchical network (HierNet) (Yan et al., 2015), (2) HybridNet (Hou et al., 2017), (3) branch network (BranchNet) (Zhu and Bain, 2017), (4) label embedding tree (LET) (Bengio et al., 2010)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The diagnosis descriptions of a patient 
visit and the assigned ICD codes. Inside the paren-
theses are the descriptions of the codes. The codes 
are ranked according to descending importance. 

of sequential LSTMs. 
Adversarial learning (Goodfellow et al., 2014) 
has been widely applied to image genera-
tion (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sensitivity and Specificity on the Test Set 

separately. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Comparison of NDCG Scores in the Ab- lation Study of Isotonic Constraints.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the results, from which we observe that af-
ter AL is removed, the sensitivity and specificity 
are dropped from 0.29 and 0.33 to 0.26 and 0.31 
respectively. No-AL does not reconcile different 
writing styles of diagnosis descriptions (DDs) and 
code descriptions (CDs). As a result, a DD and 
a CD that have similar semantics may be mis-
matched because their writing styles are differ-
ent. For example, a patient (admission ID 147583) 
has a DD 'h/o DVT on anticoagulation', which 
contains abbreviation DVT (deep vein thrombo-
sis). Due to the presence of this abbreviation, 
it is difficult to assign a proper code to this DD 
since the textual descriptions of codes do not con-
tain abbreviations. With adversarial learning, our 
model can correctly map this DD to a ground-truth 
code: 443.9 (peripheral vascular disease, unspec-
ified). Without AL, this code is not selected. As 
another example, a DD 'coronary artery disease, 
STEMI, s/p 2 stents placed in RCA' was given to 
patient 148532. This DD is written informally and 
ungrammatically, and contains too much detailed 
information, e.g., 's/p 2 stents placed in RCA'. 
Such a writing style is quite different from that of 
CDs. With AL, our model successfully matches 
this DD to a ground-truth code: 414.01 (coronary 
atherosclerosis of native coronary artery). On the 
contrary, No-AL fails to achieve this. 

Isotonic constraint (IC) To evaluate this in-
gredient, we remove the ICs from Eq.(6) during 
training and denote this baseline as No-IC. We 
use NDCG to measure the ranking performance, 
which is calculated in the following way. Consider 
a testing patient-visit l where the ground-truth ICD 
codes are M (l) </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous review-ers for their very constructive and helpful com-ments and suggestions. Pengtao Xie and Eric P. Xing are supported by National Institutes of Health P30DA035778, Pennsylvania Department of Health BD4BH4100070287, and National Sci-ence Foundation IIS1617583.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label embedding trees for large multi-class tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic construction of rule-based icd-9-cm coding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated coding of diagnoses-three methods compared</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pius</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albrecht</forename><surname>Zaiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rüdiger</forename><surname>Klar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<imprint>
			<pubPlace>Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vegfru: A domain-specific dataset for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="541" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Scientific data</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of diagnosis codes from emrs using knowledge-based and extractive text summarization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian conference on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic icd-10 classification of cancers from free-text death certificates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bergheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narelle</forename><surname>Grayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of medical informatics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="956" to="965" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Consultant report-natural language processing in the health care industry. Cincinnati Children&apos;s Hospital Medical Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dee</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Winter</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining classifiers in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 19th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring diagnoses: Icd code accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kimberly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karon</forename><forename type="middle">F</forename><surname>O&amp;apos;malley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><forename type="middle">Raiford</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Wildes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hurdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health services research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5p2</biblScope>
			<biblScope unit="page" from="1620" to="1639" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">International classification of diseases:[9th] ninth revision, basic tabulation list with alphabetic index. World Health Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>World Health Organization</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A shared task involving multi-label classification of clinical free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>John P Pestian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Hovermale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Włodzisław</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ambiguous abbreviations: an audit of abbreviations in paediatric note keeping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><forename type="middle">E</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saher</forename><surname>Weidner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Zakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Fountain-Polley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of disease in childhood</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="204" to="206" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bidirectional tree-structured lstm with head lexicalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Uncorrelation and evenness: a new diversity-promoting regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3811" to="3820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A constituentcentric neural architecture for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1405" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exact algorithms for isotonic regression and related</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Liang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Physics: Conference Series</title>
		<imprint>
			<publisher>IOP Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">699</biblScope>
			<biblScope unit="page">12016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">B-cnn: Branch convolutional neural network for hierarchical classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09890</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
