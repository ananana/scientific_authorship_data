<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Nested Attention Neural Hybrid Model for Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongen</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Truong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">A Nested Attention Neural Hybrid Model for Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="753" to="762"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1070</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information , and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most successful approaches to grammat- ical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sen- tences to corrected ones ( <ref type="bibr" target="#b1">Brockett et al., 2006</ref>; <ref type="bibr" target="#b7">Gao et al., 2010;</ref><ref type="bibr" target="#b9">Junczys-Dowmunt and Grundkiewicz, 2016)</ref>. Such systems, which are based on phrase- based MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local er- rors in spelling and inflection. The approach has proven superior to systems based on local classi- fiers that can only fix focused errors in prepositions, determiners, or inflected forms <ref type="bibr" target="#b15">(Rozovskaya and Roth, 2016)</ref>.</p><p>Recently, neural machine translation (NMT) sys- tems have achieved substantial improvements in translation quality over phrase-based MT systems <ref type="bibr" target="#b19">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. Thus, there is growing interest in applying neu- ral systems to GEC <ref type="bibr" target="#b23">(Yuan and Briscoe, 2016;</ref><ref type="bibr" target="#b21">Xie et al., 2016)</ref>. In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC.</p><p>The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vec- tor. Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correct- ing global grammatical errors and helping users achieve native speaker fluency ( <ref type="bibr" target="#b16">Sakaguchi et al., 2016</ref>). Thus, the S2S model is expected to perform better on GEC than phrase-based models. However, as we will show in this paper, to achieve the best performance on GEC, we still need to extend the standard S2S model to address several task-specific challenges, which we will describe below.</p><p>First, a GEC model needs to deal with an ex- tremely large vocabulary that consists of a large number of words and their (mis)spelling variations. Second, the GEC model needs to capture structure at different levels of granularity in order to correct errors of different types. For example, while cor- recting spelling and local grammar errors requires only word-level or sub-word level information, e.g., violets → violates (spelling) or violate → violates (verb form), correcting errors in word order or us- age requires global semantic relationships among phrases and words.</p><p>Standard approaches in neural machine transla- tion, also applied to grammatical error correction by <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref>, address the large vo- cabulary problem by restricting the vocabulary to a limited number of high-frequency words and re-sorting to standard word translation dictionaries to provide translations for the words that are out of the vocabulary (OOV). However, this approach often fails to take into account the OOVs in con- text for making correction decisions, and does not generalize well to correcting words that are un- seen in the parallel training data. An alternative approach, proposed by <ref type="bibr" target="#b21">Xie et al. (2016)</ref>, applies a character-level sequence to sequence neural model. Although the model eliminates the OOV issue, it cannot effectively leverage word-level information for GEC, even if it is used together with a separate word-based language model. Our solution to the challenges mentioned above is a novel, hybrid neural model with nested at- tention layers that infuse both word-level and character-level information. The architecture of the model is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The word-level information is used for correcting global grammar and fluency errors while the character-level infor- mation is used for correcting local errors in spelling or inflected forms. Contextual information is cru- cial for GEC. Using the proposed model, by com- bining embedding vectors and attention at both word and character levels, we model all contextual words, including OOVs, in a unified context vector representation. In particular, as we will discuss in Section 5, the character-level attention layer cap- tures most useful information for correcting local errors that involve small edits in orthography.</p><p>Our model differs substantially from the word- level S2S model of <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref> and the character-level S2S model of <ref type="bibr" target="#b21">Xie et al. (2016)</ref> in the way we infuse information at both the word level and the character level. We extend the word- character hybrid model of <ref type="bibr" target="#b12">Luong and Manning (2016)</ref>, which was originally developed for ma- chine translation, by introducing a character atten- tion layer. This allows the model to learn substi- tution patterns at both the character level and the word level in an end-to-end fashion, using sentence- correction pairs.</p><p>We validate the effectiveness of our model on the CoNLL-14 benchmark dataset ( <ref type="bibr" target="#b13">Ng et al., 2014)</ref>. Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of <ref type="bibr" target="#b12">Luong and Manning (2016)</ref>, which we apply to GEC for the first time. When inte- grated with a large word-based n-gram language model, our GEC system achieves an F 0.5 of 45.15 on CoNLL-14, substantially exceeding the previ- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A variety of classifier-based and MT-based tech- niques have been applied to grammatical error cor- rection. The CoNLL-14 shared task overview paper of <ref type="bibr" target="#b13">Ng et al. (2014)</ref> provides a comparative evalu- ation of approaches. Two notable advances after the shared task have been in the areas of combining classifiers and phrase-based MT <ref type="bibr" target="#b15">(Rozovskaya and Roth, 2016)</ref> and adapting phrase-based MT to the GEC task <ref type="bibr" target="#b9">(Junczys-Dowmunt and Grundkiewicz, 2016)</ref>. The latter work has reported the highest per- formance to date on the task of 49.5 in F 0.5 score on the CoNLL-14 test set. This method integrates dis- criminative training toward the task-specific evalu- ation function, a rich set of features, and multiple large language models. Neural approaches to the task are less explored. We believe that the advances from Junczys-Dowmunt and <ref type="bibr" target="#b9">Grundkiewicz (2016)</ref> are complementary to the ones we propose for neu- ral MT, and could be integrated with neural models to achieve even higher performance.</p><p>Two prior works explored sequence to sequence neural models for GEC ( <ref type="bibr" target="#b21">Xie et al., 2016;</ref><ref type="bibr" target="#b23">Yuan and Briscoe, 2016)</ref>, while <ref type="bibr" target="#b5">Chollampatt et al. (2016)</ref> in- tegrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification ( <ref type="bibr" target="#b17">Schmaltz et al., 2016)</ref>. <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref> demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vo- cabulary coverage with a post-processing heuristic. <ref type="bibr" target="#b21">Xie et al. (2016)</ref> built a character-level sequence to sequence model, which achieves open vocabu- lary and character-level modeling, but has difficulty with global word-level decisions.</p><p>The primary focus of our work is integration of character and word-level reasoning in neural mod- els for GEC, to capture global fluency errors and local errors in spelling and closely related morpho- logical variants, while obtaining open vocabulary coverage. This is achieved with the help of charac- ter and word-level encoders and decoders with two nested levels of attention. Our model is inspired by advances in sub-word level modeling in neural machine translation. We build mostly on the hybrid model of <ref type="bibr" target="#b12">Luong and Manning (2016)</ref> to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task. Alternative methods for MT include modeling of word pieces to achieve open vocabulary <ref type="bibr" target="#b18">(Sennrich et al., 2016)</ref>, and more recently, fully character-level modeling ( <ref type="bibr" target="#b11">Lee et al., 2017)</ref>. None of these models integrate two nested levels of attention although an empirical evalua- tion of these approaches for GEC would also be interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Nested Attention Hybrid Model</head><p>Our model is hybrid, and uses both word-level and character-level representations. It consists of a word-based sequence-to-sequence model as a back- bone, and additional character-level encoder, de- coder, and attention components, which focus on words that are outside the word-level model's vo- cabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word-based sequence-to-sequence model as backbone</head><p>The word-based backbone closely follows the basic neural sequence-to-sequence architecture with at- tention as proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> and applied to grammatical error correction by <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref>. For completeness, we give a sketch here. It uses recurrent neural networks to encode the input sentence and to decode the output sentence. Given a sequence of embedding vectors, corre- sponding to a sequence of input words x:</p><formula xml:id="formula_0">x = (x 1 , . . . , x T ),<label>(1)</label></formula><p>the encoder creates a corresponding context- specific sequence of hidden state vectors e:</p><formula xml:id="formula_1">e = (h 1 , . . . , h T )</formula><p>The hidden state h t at time t is computed as:</p><formula xml:id="formula_2">f t = GRU enc f (f t−1 , x t ) , b t = GRU enc b (b t+1 , x t ), h t = [f t ; b t ],</formula><p>where GRU enc f and GRU enc b stand for gated recurrent unit functions as described in <ref type="bibr" target="#b3">Cho et al. (2014)</ref>. We use the symbol GRU with different subscripts to represent GRU functions us- ing different sets of parameters (for example, we used the enc f and enc b subscripts to denote the pa- rameters of the forward and backward word-level encoder units.) The decoder network is also an RNN using GRU units, and defines a sequence of hidden states ¯ d 1 , . . . , ¯ d S used to define the probability of an out- put sequence y 1 , . . . , y S as follows:</p><p>The context vector c s at time step s is computed as follows:</p><formula xml:id="formula_3">c s = T j=1 α sj h j (2)</formula><p>where:</p><formula xml:id="formula_4">α sk = u sk T j=1 u sj (3) u sk = φ 1 (d s ) T φ 2 (h k )<label>(4)</label></formula><p>Here φ 1 and φ 2 denote feedforward linear trans- formations followed by a tanh nonlinearity. The next hidden state ¯ d s is then defined as:</p><formula xml:id="formula_5">d s = GRU dec ( ¯ d s−1 , y s−1 ), ¯ d s = ReLU(W [c s ; d s ])</formula><p>where y s−1 is the embedding of the output token at time s-1. ReLU indicates rectified linear units ( <ref type="bibr" target="#b8">Hahnloser et al., 2000</ref>).</p><p>The probability of each target word y s is com- puted as:</p><formula xml:id="formula_6">p(y s |y &lt;s , x) = softmax(g( ¯ d s )),</formula><p>where g is a function that maps the decoder state into a vector of size the dimensionality of the target vocabulary.</p><p>The model is trained by minimizing the cross- entropy loss, which for a given (x, y) pair is:</p><formula xml:id="formula_7">Loss(x, y) = − S s=1 log p(y s |y &lt;s , x) (5)</formula><p>For parallel training data C, the loss is:</p><formula xml:id="formula_8">Loss = − (x,y)∈C S s=1</formula><p>log p(y s |y &lt;s , x)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hybrid encoder and decoder with two nested levels of attention</head><p>The word-level backbone models a limited vocab- ulary of source and target words, and represents out-of-vocabulary tokens with special UNK sym- bols. In the standard word-level NMT approach, valuable information is lost for source OOV words and target OOV words are predicted using post- processing heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid encoder</head><p>Our hybrid architecture overcomes the loss of source information in the word-level backbone by building up compositional representations of the source OOV words using a character-level recur- rent neural network with GRU units. These repre- sentations are used in place of the special source UNK embeddings in the backbone, and contribute to the contextual encoding of all source tokens. For example, a three word input sentence where the last term is out-of-vocabulary will be repre- sented as the following vector of embeddings in the word-level model: x = (x 1 , x 2 , x 3 ), where x 3 would be the embedding for the UNK symbol.</p><p>The hybrid encoder builds up a word embedding for the third word based on its character sequence: x c 1 , . . . , x c M . The encoder computes a sequence of hidden states e c for this character sequence, by a forward character-level GRU network:</p><formula xml:id="formula_9">e c = (h c 1 , . . . , h c M ),<label>(6)</label></formula><p>The last state h c M is used as an embedding of the unknown word. The sequence of embeddings for our example three-word sequence becomes: x = (x 1 , x 2 , h c M ). We use the same dimensional- ity for word embedding vectors x i and composed character sequence vectors h c M to ensure the two ways to define embeddings are compatible. Our hybrid source encoder architecture is similar to the one proposed by <ref type="bibr" target="#b12">Luong and Manning (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nested attention hybrid decoder</head><p>In traditional word-based sequence-to-sequence models special target UNK tokens are used to rep- resent outputs that are outside the target vocabu- lary. A post-processing UNK-replacement method is then used ( <ref type="bibr" target="#b23">Yuan and Briscoe, 2016)</ref> to replace these special tokens with target words. The hybrid model of <ref type="bibr" target="#b12">(Luong and Manning, 2016</ref>) uses a jointly trained character-level decoder to generate target words corresponding to UNK to- kens, and outperforms the traditional approach in the machine translation task.</p><p>However, unlike machine translation, models for grammar correction conduct "translation" in the same language, and often need to apply a small number of local edits to the character sequence of a source word corresponding to the target UNK word. For example, rare but correct words such as entity names need to be copied as is, and local spelling errors or errors in inflection need to be corrected. The architecture of <ref type="bibr" target="#b12">Luong and Manning (2016)</ref> does not have direct access to a source character se- quence, but only uses a single fixed-dimensionality embedding of source unknown words aggregated with additional contextual information from the source.</p><p>To address the needs of the grammatical error correction task, we propose a novel hybrid decoder with two nested levels of attention: word level and character-level. The character-level attention serves to provide the decoder with direct access to the relevant source character sequence.</p><p>More specifically, the probability of each target word is defined as follows: For words in the target vocabulary, the probability is defined by the word- level backbone. For words outside the vocabulary, the probability of each token is the probability of UNK according to the backbone, multiplied by the probability of the word's character sequence.</p><p>The probability of the target character sequence corresponding to an UNK token at position s in the target is defined using a character-level decoder. As in <ref type="bibr" target="#b12">Luong and Manning (2016)</ref>, the "separate path" architecture is used to capture the relevant context and define the initial state for the character-level decoder:</p><formula xml:id="formula_10">ˆ d s = ReLU( ˆ W [c s ; d s ])</formula><p>wherê W are parameters different from W , andˆdandˆ andˆd s is not used by the word-level model in predicting the subsequent tokens, but is only used to initialize the character-level decoder.</p><p>To be able to attend to the relevant source charac- ter sequence when generating the target character sequence, we use the concept of hard attention ( <ref type="bibr" target="#b22">Xu et al., 2015)</ref>, but use an arg-max approximation for inference instead of sampling. A similar approach to represent discrete hidden structure in a variety of architectures is used in <ref type="bibr" target="#b10">Kong et al. (2017)</ref>.</p><p>The source index z s corresponding to the target position s is defined according to the word-level attention model:</p><formula xml:id="formula_11">z s = arg max k∈0...T −1 α sk</formula><p>where α sk are the intermediate outputs of the word-level attention model we described in <ref type="figure">Eq.(3)</ref>.</p><p>The character-level decoder generates a charac- ter sequence y c = (y c 1 , . . . , y c N ), conditioned on the initial vectorˆdvectorˆ vectorˆd s and the source index z s . The characters are generated using a hidden state vec- tor d c n at each time step, via a softmax(gc <ref type="figure">(d c n )</ref>), where gc maps the state to the target character vo- cabulary space.</p><p>If the source word x zs is in the source vocabu- lary, the model is analogous to the one of <ref type="bibr" target="#b12">Luong and Manning (2016)</ref> and does not use character- level attention: the source context is available only in aggregated form to initialize the state of the de- coder. The state d c n for step n in the character- level decoder is defined as follows, where GRU c dec are parameters for the gated recurrent cell of this decoder:</p><formula xml:id="formula_12">d c n = GRU c dec ( ˆ d s , y c n−1 ) n = 0 GRU c dec (d c n−1 , y c n−1 ) n &gt; 0</formula><p>In contrast, if the corresponding token in the source x zs is also an out-of-vocabulary word, we define a second nested level of character atten- tion and use it in the character-level decoder. The character-level attention focuses on individual char- acters from the source word x zs . If e c are the source character hidden vectors computed as in Eq.(6), the recurrence equations for the character- level decoder with nested attention are:</p><formula xml:id="formula_13">¯ d c n = ReLU(W c [c c n ; d c n ]) d c n = GRU c decNested ( ˆ d s , y c n−1 ) n = 0 GRU c decNested ( ¯ d c n−1 , y c n−1 ) n &gt; 0</formula><p>where c c n is the context vector obtained using character-level attention on the sequence e c and the last state of the character-level decoder d c n , computed following equations 2, 3 and 4, but using a different set of parameters.</p><p>These equations show that the character-level de- coder with nested attention can use both the word- level statê d s , and the character-level context c c n and hidden state d c n to perform global and local editing operations. Since we introduced two architectures for the character-level decoder depending on whether the source word x zs is OOV, the combined loss func- tion is defined as follows for end-to-end training:</p><formula xml:id="formula_14">Loss total = Loss w + αLoss c1 + βLoss c2</formula><p>Here Loss w is the standard word-level loss in Eq. <ref type="formula">(5)</ref>; character level losses Loss c1 and Loss c2 are losses for target OOV words corresponding to source known and unknown tokens, respectively. α and β are hyper-parameters to balance the loss terms.</p><p>As seen, our proposed nested attention hybrid model uses character-level attention only when both a predicted target word and its correspond- ing source input word are OOV. While the model can be naturally generalized to integrate character- level attention for known words, the original hybrid model proposed by <ref type="bibr" target="#b12">Luong and Manning (2016)</ref> does not use any character-level information for known words. Thus for a controlled evaluation of the impact of the addition of character-level at- tention only, in this paper we limit character-level attention to OOV words, which already use charac- ters as a basis for building their embedding vectors. A thorough investigation of the impact of character- level information in the encoder, attention, and decoder for known words as well is an interesting topic for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoding for word-level and hybrid models</head><p>Beam-search is used to decode hypotheses accord- ing to the word-level backbone model. For the hybrid model architecture, word-level beam search is conducted first; for each target UNK token, character-level beam-search is used to generate a corresponding target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation</head><p>We use standard publicly available datasets for training and evaluation. One data source is the NUS Corpus of Learner English (NUCLE) ( <ref type="bibr" target="#b6">Dahlmeier et al., 2013</ref>), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks. From the original corpus of size about 60K parallel sen- tences, we randomly selected close to 5K sentence pairs for use as a validation set, and 45K parallel sentences for use in training. A second data source  is the Cambridge Learner Corpus (CLC) <ref type="bibr" target="#b14">(Nicholls, 2003)</ref>, from which we extracted a substantially larger set of parallel sentences. Finally, we used additional training examples from the Lang-8 Cor- pus of Learner English v1.0 ( <ref type="bibr" target="#b20">Tajiri et al., 2012</ref>). As Lang-8 data is crowd-sourced, we used heuristics to filter out noisy examples: we removed sentences longer than 100 words and sentence pairs where the correction was substantially shorter than the input text. <ref type="table" target="#tab_1">Table 2</ref> shows the number of sentence pairs from each source used for training.</p><p>We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task ( <ref type="bibr" target="#b13">Ng et al., 2014</ref>). We report final performance on the CoNLL-14 test set without alternatives, and an- alyze model performance on the CoNLL-13 devel- opment set ( <ref type="bibr" target="#b6">Dahlmeier et al., 2013)</ref>. We use the de- velopment and validation sets for model selection. The sizes of all datasets in number of sentences are shown in <ref type="table">Table 1</ref>. We report performance in F 0.5 -measure, as calculated by the m2scorer- the official implementation of the scoring metric in the shared task. <ref type="bibr">1</ref> Given system outputs and gold-standard edits, m2scorer computes the F 0.5 measure of a set of system edits against a set of gold-standard edits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline</head><p>We evaluate our model in comparison to the strong baseline of a word-based neural sequence- to-sequence model with attention, with post- processing for handling out-of-vocabulary words <ref type="bibr" target="#b23">(Yuan and Briscoe, 2016)</ref>; we refer to this model as word NMT+UNK replacement. Like <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref>, we use a traditional word- alignment model (GIZA++) to derive a word- correction lexicon from the parallel training set. However, in decoding, we don't use GIZA++ to find the corresponding source word for each tar-get OOV, but follow , Section 3.3 to use the NMT system's attention weights in- stead. The target OOV is then replaced by the most likely correction of the source word from the word- correction lexicon, or by the source word itself if there are no available corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details and Results</head><p>The embedding size for all word and character- level encoders and decoders is set to 1000, and the hidden unit size is also 1000. To reproduce the model of <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref>, we selected the word vocabulary for the baseline by choosing the 30K most frequent words in the source and target respectively to form the source and target vocabularies. In preliminary experiments for the hybrid models, we found that selecting the same vocabulary of 30K words for the source and target based on combined frequency was better (.003 in F 0.5 ) and use that method for vocabulary selection instead. However, there was no gain observed by using such a vocabulary selection method in the baseline. Although the source and target vocabu- laries in the hybrid models are the same, like in the word-level model, the embedding parameters for source and target words are not shared.</p><p>The hyper-parameters for the losses in our mod- els are selected based on the development set and set as follows: α = β = 0.5. All models are trained with mini-batch size of 128 (batches are shuffled), initial learning rate of 0.0003 and a 0.95 decay ratio if the cost increases in two consecutive 100 iterations. The gradient is rescaled whenever its norm exceeds 10, and dropout is used with a probability of 0.15. Parameters are uniformly ini-</p><formula xml:id="formula_15">tialized in [− √ (3) √ 1000 , √ (3) √ 1000</formula><p>]. We perform inference on the validation set every 5000 iterations to log word-level cost and character- level costs; we save parameter values for the model every 10000 iterations as well as the end of each epoch. The stopping point for training is selected based on development set F 0.5 among the top 20 parameter sets with best validation set value of the loss function. Training of the nested attention hybrid model takes approximately five days on a Tesla k40m GPU. The basic hybrid model trains in around four days and the word-level backbone trains in approximately three days. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of the baseline and our nested attention hybrid model on the devel- opment and test sets. In addition to the word-level  baseline, we include the performance of a hybrid model with a single level of attention, which fol- lows the work of <ref type="bibr" target="#b12">Luong and Manning (2016)</ref> for machine translation, and is the first application of a hybrid word/character-level model to grammatical error correction. Based on hyper-parameter selec- tion, the character-level component weight of the loss is α = 1 for the basic hybrid model. As shown in <ref type="table" target="#tab_3">Table 3</ref>, our implementation of the word NMT+UNK replacement baseline ap- proaches the performance of the one reported in <ref type="bibr">Yuan and Briscoe (2016) (38.77 versus 39.9</ref>). We attribute the difference to differences in the train- ing set and the word-alignment methods used. Our reimplementation serves to provide a controlled experimental evaluation of the impact of hybrid models and nested attention on the GEC task. As seen, our nested attention hybrid model substan- tially improves upon the baseline, achieving a gain of close to 3 points on the test set. The hybrid word/character model with a single level of atten- tion brings a large improvement as well, showing the importance of character-level information for this task. We delve deeper into the impact of nested attention for the hybrid model in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Integrating a Web-scale Language Model</head><p>The value of large language models for grammati- cal error correction is well known, and such mod- els have been used in classifier and MT-based sys- tems. To establish the potential of such models in word-based neural sequence-to-sequence sys- tems, we integrate a web-scale count-based lan- guage model. In particular, we use the modified Kneser-Ney 5-gram language model trained from Common Crawl ( <ref type="bibr" target="#b2">Buck et al., 2014</ref>), made available for download by <ref type="bibr" target="#b9">Junczys-Dowmunt and Grundkiewicz (2016)</ref>.</p><p>Candidates generated by neural models are re- ranked using the following linear interpolation of log probabilities: s y|x = log P N N (y|x) + λ log P LM (y). Here λ is a hyper-parameter that balances the weights of the neural network model and the language model. We tuned λ separately</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev Test</head><p>Character-based NMT + LM ( <ref type="bibr" target="#b21">Xie et al., 2016)</ref> 40.56</p><p>Word   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We analyze the impact of sub-word level informa- tion and the two nested levels of attention in more detail by looking at the performance of the mod- els on different segments of the data. In particular, we analyze the performance of the models on sen- tences containing OOV source words versus ones without OOV words, and corrections to orthograph- ically similar versus dissimilar word forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance by Segment: OOV versus Non-OOV</head><p>We present a comparative performance analysis of models on the CoNLL-13 development set. First, we divide the set into two segments: OOV and NonOOV, based on whether there is at least one OOV word in the given source input. <ref type="table" target="#tab_7">Table 5</ref> shows that both hybrid architectures substantially outper- form the word-level model in both segments of the data. The additional nested character-level atten- tion of our hybrid model brings a sizable improve- ment over the basic hybrid model in the OOV seg- ment and a small degradation in the non-OOV seg- ment. We should note that in future work character- level attention can be added for non-OOV source words in the nested attention model, which could improve performance on this segment as well.  This greatly violates the rights of people . <ref type="table">Table 6</ref>: An example sentence from the OOV segment where the nested attention hybrid model improves performance. <ref type="table">Table 6</ref> shows an example where the nested at- tention hybrid model successfully corrects a mis- spelling resulting in an OOV word on the source, whereas the baseline word-level system simply copies the source word without fixing the error (since this particular error is not observed in the parallel training set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of Nested Attention on Different Error Types</head><p>To analyze more precisely the impact of the addi- tional character-level attention introduced by our design, we continue to investigate the OOV seg- ment in more detail. The concept of edit, which is also used by the of- ficial M2 score metric, is defined as a minimal pair of corresponding sub-strings in a source sentence and a correction. For example, in the sentence frag- ment pair: "Even though there is a risk of causing harms to someone, people still are prefers to keep their pets without a leash." → "Even though there is a risk of causing harm to someone, people still prefer to keep their pets without a leash.", the min- imal edits are "harms → harm" and "are prefers → prefer". The F 0.5 score is computed using weighted precision and recall of the set of a system's edits against one or more sets of reference edits.</p><p>For our in-depth analysis, we classify edits in the OOV segment into two types: small changes and large changes, based on whether the source and target phrase of the edit are orthographically similar or not. More specifically, we say that the target and  <ref type="table">Table 7</ref>: Precision, Recall and F 0.5 results on CoNLL-13,on the "small changes" and "large changes" portions of the OOV segment.</p><p>source phrases are orthographically similar, iff: the character edit distance is at most 2 and the source or target is at most 8 characters long, or edit ratio &lt; 0.25, where edit ratio = character edit distance min(len(src),len(tar))+0.1 , len( * ) denotes number of characters in * , and src and tgt denote the pairs in the edit. There are 307 gold edits in the "small changes" portion of the CoNLL-13 OOV segment, and 481 gold edits in the "large changes" portion.</p><p>Our hypothesis is that the additional character- level attention layer is particularly useful to model edits among orthographically similar words. <ref type="table">Table  7</ref> contrasts the impact of character-level attention on the two portions of the data. We can see that the gains in the "small changes" portion are in- deed quite large, indicating that the fine-grained character-level attention empowers the model to more accurately correct confusions among phrases with high character-level similarity. The impact in the "large changes" portion is slightly positive in precision and slightly negative in recall. Thus most of the benefit of the additional character-level attention stems from improvements in the "small changes" portion. <ref type="table">Table 8</ref> shows an example input which illustrates the precision gain of the nested attention hybrid model. The input sentence has a source OOV word which is correct. The hybrid model introduces an error in this word, because it uses only a single source context vector, aggregating the character- level embedding of the source OOV word together with other source words. The additional character- level attention layer in the nested hybrid model en- ables the correct copying of this long source OOV word, without employing the heuristic mechanism of the word-level NMT system.  <ref type="table">Table 8</ref>: An example where the nested attention hybrid model outperforms the non-nested model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced a novel hybrid neural model with two nested levels of attention: word-level and character-level. The model addresses the unique challenges of the grammatical error correction task and achieves the best reported results on the CoNLL-14 benchmark among fully neural systems. Our nested attention hybrid model deeply com- bines the strengths of word and character level in- formation in all components of an end-to-end neu- ral model: the encoder, the attention layers, and the decoder. This enables it to correct both global word- level and local character-level errors in a unified way. The new architecture contributes substantial improvement in correction of confusions among rare or orthographically similar words compared to word-level sequence-to-sequence and non-nested hybrid models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of Nested Attention Hybrid Model ously reported top performance of 40.56 achieved by using a neural model and an external language model (Xie et al., 2016).</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,218.27,140.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4: F 0.5 results on the CoNLL-13 and CoNLL-14 test sets of main model architectures, when combined with a large language model. for each neural model variant, by exploring val- ues in the range [0.0, 2.0] with step size 0.1, and selecting according to development set F 0.5 . The selected values of λ are: 1.6 for word NMT + UNK replacement and 1.0 for the nested attention model. Table 4 shows the impact of the LM when com- bined with the neural models implemented in this work. The table also lists the results reported by Xie et al. (2016), for their character-level neural model combined with a large word-level language model. Our best results exceed the ones reported in the prior work by more than 4 points, although we should note that Xie et al. (2016) used a smaller parallel data set for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Training data by source. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F 0.5 results on the CoNLL-13 and 
CoNLL-14 test sets of main model architectures. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 : F 0.5 results on the CoNLL-13 set of</head><label>5</label><figDesc></figDesc><table>main 
</table></figure>

			<note place="foot" n="1"> http://www.comp.nus.edu.sg/ ˜ nlp/sw/ m2scorer.tar.gz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the ACL reviewers for their insightful suggestions, Victoria Zayats for her help with reproducing the baseline word-level NMT sys-tem and Yu Shi, Daxin Jiang and Michael Zeng for the helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correcting ESL errors using phrasal SMT techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>William B Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the Common Crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sébastien Jean Kyunghyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural network translation models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS corpus of learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A large scale rankerbased system for search query spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Xiaolong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Shiao-Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Micol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Richard Hr Hahnloser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><forename type="middle">A</forename><surname>Sarpeshkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Sebastian</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="issue">6789</biblScope>
			<biblScope unit="page" from="947" to="951" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dragnn: A transitionbased framework for dynamically connected neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bogatyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL Shared Task</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Corpus Linguistics</title>
		<meeting>the Corpus Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grammatical error correction: Machine translation and classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reassessing the goals of grammatical error correction: Fluency instead of grammaticality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sentence-level grammatical error identification as sequence-to-sequence correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart M</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04677</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tense and aspect error correction for ESL learners using global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshikazu</forename><surname>Tajiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural language correction with character-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>CoRR abs/1603.09727</idno>
		<ptr target="http://arxiv.org/abs/1603.09727" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
