<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-24, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjana</forename><surname>Vakil</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Paulus</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
						<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations <address><addrLine>Baltimore, Maryland USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="109" to="114"/>
							<date type="published">June 23-24, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper describes lex4all, an open-source PC application for the generation and evaluation of pronunciation lexicons in any language. With just a few minutes of recorded audio and no expert knowledge of linguistics or speech technology, individuals or organizations seeking to create speech-driven applications in low-resource languages can build lexicons enabling the recognition of small vocabularies (up to 100 terms, roughly) in the target language using an existing recognition engine designed for a high-resource source language (e.g. English). To build such lexicons , we employ an existing method for cross-language phoneme-mapping. The application also offers a built-in audio recorder that facilitates data collection, a significantly faster implementation of the phoneme-mapping technique, and an evaluation module that expedites research on small-vocabulary speech recognition for low-resource languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years it has been demonstrated that speech recognition interfaces can be extremely beneficial for applications in the developing world <ref type="bibr" target="#b6">(Sherwani and Rosenfeld, 2008;</ref><ref type="bibr" target="#b7">Sherwani, 2009;</ref><ref type="bibr" target="#b0">Bali et al., 2013)</ref>. Typically, such applications target low-resource languages (LRLs) for which large collections of speech data are unavailable, preventing the training or adaptation of recogni- tion engines for these languages. However, an ex- isting recognizer for a completely unrelated high- resource language (HRL), such as English, can be used to perform small-vocabulary recognition tasks in the LRL, given a pronunciation lexicon mapping each term in the target vocabulary to a sequence of phonemes in the HRL, i.e. phonemes which the recognizer can model. This is the motivation behind lex4all, 1 an open- source application that allows users to automati- cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog- nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and related work</head><p>Several commercial speech recognition systems offer high-level Application Programming Inter- faces (APIs) that make it extremely simple to add voice interfaces to an application, requiring very little general technical expertise and virtually no knowledge of the inner workings of the recogni- tion engine. If the target language is supported by the system -the Microsoft Speech Platform, 2 for example, supports over 20 languages -this makes it very easy to create speech-driven applications.</p><p>If, however, the target language is one of the many thousands of LRLs for which high-quality recognition engines have not yet been devel- oped, alternative strategies for developing speech- recognition interfaces must be employed. Though tools for quickly training recognizers for new lan- guages exist (e.g. CMUSphinx 3 ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt- ing multilingual or language-independent acoustic and language models to new languages from rela- tively small amounts of data ( <ref type="bibr" target="#b5">Schultz and Waibel, 2001;</ref><ref type="bibr" target="#b2">Kim and Khudanpur, 2003)</ref>, methods for building resources such as pronunciation dictio- naries from web-crawled data ( <ref type="bibr" target="#b4">Schlippe et al., 2014)</ref>, and even a web-based interface, the Rapid Language Adaptation Toolkit 4 (RLAT), which al- lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages ( <ref type="bibr" target="#b9">Vu et al., 2010</ref>). While they greatly reduce the amount of data needed to build new recognizers, these approaches still require non-trivial amounts of speech and text in the target language, which may be an obstacle for very low- or zero-resource languages. Furthermore, even high-level tools such as RLAT still demand some understanding of linguistics/language technology, and thus may not be accessible to all users.</p><p>However, many useful applications (e.g. for ac- cessing information or conducting basic transac- tions by telephone) only require small-vocabulary recognition, i.e. discrimination between a few dozen terms (words or short phrases). For ex- ample, VideoKheti ( <ref type="bibr" target="#b0">Bali et al., 2013</ref>), a text- free smartphone application that delivers agricul- tural information to low-literate farmers in In- dia, recognizes 79 Hindi terms. For such small- vocabulary applications, an engine designed to recognize speech in a HRL can be used as-is to perform recognition of the LRL terms, given a grammar describing the allowable combinations and sequences of terms to be recognized, and a pronunciation lexicon mapping each target term to at least one pronunciation (sequence of phonemes) in the HRL (see <ref type="figure">Fig. 1</ref> for an example). This is the thinking behind Speech-based Auto- mated Learning of Accent and Articulation Map- ping, or "Salaam" <ref type="bibr" target="#b7">(Sherwani, 2009;</ref><ref type="bibr" target="#b3">Qiao et al., 2010;</ref><ref type="bibr" target="#b1">Chan and Rosenfeld, 2012)</ref>, a method of cross-language phoneme-mapping that discovers accurate source-language pronunciations for terms in the target language. The basic idea is to discover the best pronunciation (phoneme sequence) for a target term by using the source-language recog- nition engine to perform phone decoding on one or more utterances of the term. As commercial &lt;lexicon version="1.0" xmlns="http://www . <ref type="bibr">w3</ref>.org/2005/01/pronunciation- lexicon" xml:lang="en-US" alphabet ="x-microsoft-ups"&gt; &lt;lexeme&gt; &lt;grapheme&gt;beeni&lt;/grapheme&gt; &lt;phoneme&gt;B E NG I&lt;/phoneme&gt; &lt;phoneme&gt;B EI N I I&lt;/phoneme&gt; &lt;/lexeme&gt; &lt;/lexicon&gt; <ref type="figure">Figure 1</ref>: Sample XML lexicon mapping the Yoruba word beeni ("yes") to two possible se- quences of American English phonemes.</p><p>recognizers such as Microsoft's are designed for word-decoding, and their APIs do not usually al- low users access to the phone-decoding mode, the Salaam approach uses a specially designed "super- wildcard" recognition grammar to mimic phone decoding and guide pronunciation discovery ( <ref type="bibr" target="#b3">Qiao et al., 2010;</ref><ref type="bibr" target="#b1">Chan and Rosenfeld, 2012)</ref>. This al- lows the recognizer to identify the phoneme se- quence best matching a given term, without any prior indication of how many phonemes that se- quence should contain.</p><p>Given this grammar and one or more audio recordings of the term, <ref type="bibr" target="#b3">Qiao et al. (2010)</ref> use an it- erative training algorithm to discover the best pro- nunciation(s) for that term, one phoneme at a time. Compared to pronunciations hand-written by a lin- guist, pronunciations generated automatically by this algorithm yield substantially higher recog- nition accuracy: <ref type="bibr" target="#b3">Qiao et al. (2010)</ref> report word recognition accuracy rates in the range of 75-95% for vocabularies of 50 terms. Chan and Rosen- feld (2012) improve accuracy on larger vocabu- laries (up to approximately 88% for 100 terms) by applying an iterative discriminative training al- gorithm, identifying and removing pronunciations that cause confusion between word types.</p><p>The Salaam method is fully automatic, demand- ing expertise neither in speech technology nor in linguistics, and requires only a few recorded utterances of each word. At least two projects have successfully used the Salaam method to add voice interfaces to real applications: an Urdu telephone-based health information system <ref type="bibr" target="#b7">(Sherwani, 2009)</ref>, and the VideoKheti application men- tioned above ( <ref type="bibr" target="#b0">Bali et al., 2013</ref>). What has not ex- isted before now is an interface that makes this ap- proach accessible to any user.</p><p>Given the established success of the Salaam method, our contribution is to create a more time- efficient implementation of the pronunciation- discovery algorithm and integrate it into an easy- to-use graphical application. In the following sec- tions, we describe this application and our slightly modified implementation of the Salaam method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System overview</head><p>We have developed lex4all as a desktop applica- tion for Microsoft Windows, 5 since it relies on the Microsoft Speech Platform (MSP) as explained in Section 4.1. The application and its source code are freely available via GitHub. <ref type="bibr">6</ref> The application's core feature is its lexicon- building tool, the architecture of which is illus- trated in <ref type="figure">Figure 2</ref>. A simple graphical user in- terface (GUI) allows users to type in the written form of each term in the target vocabulary, and select one or more audio recordings (.wav files) of that term. Given this input, the program uses the Salaam method to find the best pronuncia- tion(s) for each term. This requires a pre-trained recognition engine for a HRL as well as a series of dynamically-created recognition grammars; the engine and grammars are constructed and man- aged using the MSP. We note here that our imple- mentation of Salaam deviates slightly from that of <ref type="bibr" target="#b3">Qiao et al. (2010)</ref>, improving the time-efficiency and thus usability of the system (see <ref type="bibr">Sec. 4)</ref>.</p><p>Once pronunciations for all terms in the vocab- ulary have been generated, the application outputs a pronunciation lexicon for the given terms as an XML file conforming to the Pronunciation Lexi- con Specification. <ref type="bibr">7</ref> This lexicon can then be di- rectly included in a speech recognition application built using the MSP API or a similar toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Pronunciation mapping 4.1 Recognition engine</head><p>For the HRL recognizer we use the US English recognition engine of the MSP. The engine is used as-is, with no modifications to its underlying mod- els. We choose the MSP for its robustness and ease of use, as well as to maintain comparability with the work of <ref type="bibr" target="#b3">Qiao et al. (2010)</ref> and <ref type="bibr" target="#b1">Chan and Rosenfeld (2012)</ref>. Following these authors, we use an engine designed for server-side recognition <ref type="bibr">5</ref> Windows 7 or 8 (64-bit). of low-quality audio, since we aim to enable the creation of useful applications for LRLs, includ- ing those spoken in developing-world communi- ties, and such applications should be able to cope with telephone-quality audio or similar <ref type="bibr" target="#b6">(Sherwani and Rosenfeld, 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation of the Salaam method</head><p>Pronunciations (sequences of source-language phonemes) for each term in the target vocabu- lary are generated from the audio sample(s) of that term using the iterative Salaam algorithm (Sec. 2), which employs the source-language rec- ognizer and a special recognition grammar. In the first pass, the algorithm finds the best candi- date(s) for the first phoneme of the sample(s), then the first two phonemes in the second pass, and so on until a stopping criterion is met. In our im- plementation, we stop iterations if the top-scoring sequence for a term has not changed for three con- secutive iterations <ref type="bibr" target="#b1">(Chan and Rosenfeld, 2012)</ref>, or if the best sequence from a given pass has a lower confidence score than the best sequence from the previous pass <ref type="bibr" target="#b3">(Qiao et al., 2010</ref>). In both cases, at least three passes are required.</p><p>After the iterative training has completed, the n- best pronunciation sequences (with n specified by users -see Sec. 5.2) for each term are written to the lexicon, each in a &lt;phoneme&gt; element corre- sponding to the &lt;grapheme&gt; element containing the term's orthographic form (see <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Running time</head><p>A major challenge we faced in engineering a user- friendly application based on the Salaam algo- rithm was its long running time. The algorithm depends on a "super-wildcard" grammar that al- lows the recognizer to match each sample of a given term to a "phrase" of 0-10 "words", each word comprising any possible sequence of 1, 2, or 3 source-language phonemes ( <ref type="bibr" target="#b3">Qiao et al., 2010)</ref>. Given the 40 phonemes of US English, this gives over 65,000 possibilities for each word, resulting in a huge training grammar and thus a long pro- cessing time. For a 25-term vocabulary with 5 training samples per term, the process takes ap- proximately 1-2 hours on a standard modern lap- top. For development and research, this long train- ing time is a serious disadvantage.</p><p>To speed up training, we limit the length of each "word" in the grammar to only one phoneme, in- stead of up to 3, giving e.g. 40 possibilities in- stead of tens of thousands. The algorithm can still discover pronunciation sequences of an arbitrary length, since, in each iteration, the phonemes dis- covered so far are prepended to the super-wildcard grammar, such that the phoneme sequence of the first "word" in the phrase grows longer with each pass ( <ref type="bibr" target="#b3">Qiao et al., 2010)</ref>. However, the new imple- mentation is an order of magnitude faster: con- structing the same 25-term lexicon on the same hardware takes approximately 2-5 minutes, i.e. less than 10% of the previous training time.</p><p>To ensure that the new implementation's vastly improved running time does not come at the cost of reduced recognition accuracy, we evaluate and compare word recognition accuracy rates using lexicons built with the old and new implementa- tions. The data we use for this evaluation is a subset of the Yoruba data collected by <ref type="bibr" target="#b3">Qiao et al. (2010)</ref>, comprising 25 Yoruba terms (words) ut- tered by 2 speakers (1 male, 1 female), with 5 samples of each term per speaker. To determine same-speaker accuracy for each of the two speak- Old New p  The results of our evaluation, given in <ref type="table">Table 1</ref>, indicate no statistically significant difference in accuracy between the two implementations (all p- values are above 0.5 and thus clearly insignifi- cant). As our new, modified implementation of the Salaam algorithm is much faster than the original, yet equally accurate, lex4all uses the new imple- mentation by default, although for research pur- poses we leave users the option of using the origi- nal (slower) implementation (see Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discriminative training</head><p>Chan and Rosenfeld (2012) achieve increased ac- curacy (gains of up to 5 percentage points) by applying an iterative discriminative training al- gorithm. This algorithm takes as input the set of mapped pronunciations generated using the Salaam algorithm; in each iteration, it simulates recognition of the training audio samples using these pronunciations, and outputs a ranked list of the pronunciations in the lexicon that best match each sample. Pronunciations that cause "confu- sion" between words in the vocabulary, i.e. pro- nunciations that the recognizer matches to sam- ples of the wrong word type, are thus identified and removed from the lexicon, and the process is repeated in the next iteration.</p><p>We implement this accuracy-boosting algorithm in lex4all, and apply it by default. To enable fine-tuning and experimentation, we leave users the option to change the number of passes (4 by de- fault) or to disable discriminative training entirely, as mentioned in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">User interface</head><p>As mentioned above, we aim to make the creation and evaluation of lexicons simple, fast, and above all accessible to users with no expertise in speech or language technologies. Therefore, the applica- tion makes use of a simple GUI that allows users to quickly and easily specify input and output file paths, and to control the parameters of the lexicon- building algorithms. <ref type="figure">Figure 3</ref> shows the main interface of the lex4all lexicon builder. This window displays the terms that have been specified and the number of audio samples that have been selected for each word. Another form, accessed via the "Add word" or "Edit" buttons, allows users to add to or edit the vocabulary by simply typing in the desired ortho- graphic form of the word and selecting the audio sample(s) to be used for pronunciation discovery (see Sec. 5.1 for more details on audio input).</p><p>Once the target vocabulary and training audio have been specified, and the additional options have been set if desired, users click the "Build Lexicon" button and specify the desired name and target directory of the lexicon file to be saved, and pronunciation discovery begins. When all pronun- ciations have been generated, a success message displaying the elapsed training time is displayed, and users may either proceed to the evaluation module to assess the newly created lexicon (see Sec. 6), or return to the main interface to build an- other lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Audio input and recording</head><p>The GUI allows users to easily browse their file system for pre-recorded audio samples (.wav files) to be used for lexicon training. To simplify data collection and enable the development of lexi- cons even for zero-resource languages, lex4all also offers a simple built-in audio recorder to record new speech samples.</p><p>The recorder, built using the open-source library NAudio, 8 takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1).</p><p>8 http://naudio.codeplex.com/ <ref type="figure">Figure 3</ref>: Screenshot of the lexicon builder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Additional options</head><p>As seen in <ref type="figure">Figure 3</ref>, lex4all includes optional con- trols for quick and easy fine-tuning of the lexicon- building process (the default settings are pictured).</p><p>First of all, users can specify the maxi- mum number of pronunciations (&lt;phoneme&gt; el- ements) per word that the lexicon may contain; allowing more pronunciations per word may im- prove recognition accuracy ( <ref type="bibr" target="#b3">Qiao et al., 2010;</ref><ref type="bibr" target="#b1">Chan and Rosenfeld, 2012</ref>). Secondly, users may train the lexicon using our modified, faster imple- mentation of the Salaam algorithm or the origi- nal implementation. Finally, users may choose whether or not discriminative training is applied, and if so, how many passes are run (see Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation module for research</head><p>In addition to its primary utility as a lexicon- building tool, lex4all is also a valuable research aide thanks to an evaluation module that allows users to quickly and easily evaluate the lexicons they have created. The evaluation tool allows users to browse their file system for an XML lexicon file that they wish to evaluate; this may be a lexicon created using lex4all, or any other lexicon in the same format. As in the main interface, users then select one or more audio samples (.wav files) for each term they wish to evaluate. The system then attempts to recognize each sample using the given lexicon, and reports the counts and percent- ages of correct, incorrect, and failed recognitions.</p><p>Users may optionally save this report, along with a confusion matrix of word types, as a comma- separated values (.csv) file.</p><p>The evaluation module thus allows users to quickly and easily assess different configurations of the lexicon-building tool, by simply changing the settings using the GUI and evaluating the re- sulting lexicons. Furthermore, as the applica- tion's source code is freely available and modifi- able, researchers may even replace entire modules of the system (e.g. use a different pronunciation- discovery algorithm), and use the evaluation mod- ule to quickly assess the results. Therefore, lex4all facilitates not only application development but also further research into small-vocabulary speech recognition using mapped pronunciation lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and future work</head><p>We have presented lex4all, an open-source appli- cation that enables the rapid automatic creation of pronunciation lexicons in any (low-resource) language, using an out-of-the-box commercial recognizer for a high-resource language and the Salaam method for cross-language pronunciation mapping ( <ref type="bibr" target="#b3">Qiao et al., 2010;</ref><ref type="bibr" target="#b1">Chan and Rosenfeld, 2012</ref>). The application thus makes small- vocabulary speech recognition interfaces feasible in any language, since only minutes of training au- dio are required; given the built-in audio recorder, lexicons can be constructed even for zero-resource languages. Furthermore, lex4all's flexible and open design and easy-to-use evaluation module make it a valuable tool for research in language- independent small-vocabulary recognition.</p><p>In future work, we plan to expand the selection of source-language recognizers; at the moment, lex4all only uses US English as the source lan- guage, but any of the 20+ other HRLs supported by the MSP could be added. This would enable in- vestigation of the target-language recognition ac- curacy obtained using different source languages, though our initial exploration of this issue sug- gests that phonetic similarity between the source and target languages might not significantly affect accuracy <ref type="bibr" target="#b8">(Vakil and Palmer, 2014)</ref>. Another future goal is to improve and extend the functionality of the audio-recording tool to make it more flexible and user-friendly. Finally, as a complement to the application, it would be beneficial to create a cen- tral online data repository where users can upload the lexicons they have built and the speech sam- ples they have recorded. Over time, this could be- come a valuable collection of LRL data, enabling developers and researchers to share and re-use data among languages or language families.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Overview of the core components of the lex4all lexicon-building application.</figDesc><graphic url="image-1.png" coords="3,307.28,62.80,218.27,316.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Female</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="1"> http://lex4all.github.io/lex4all/ 2 http://msdn.microsoft.com/en-us/library/hh361572 3 http://www.cmusphinx.org</note>

			<note place="foot" n="4"> http://i19pc5.ira.uka.de/rlat-dev</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The first author was partially supported by a Deutschlandstipendium scholarship sponsored by IMC AG. We thank Roni Rosenfeld, Hao Yee Chan, and Mark Qiao for generously sharing their speech data and valuable advice, and Dietrich Klakow, Florian Metze, and the three anonymous reviewers for their feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Hindi speech recognizer for an agricultural video search application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Cuendet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indrani</forename><surname>Medhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM DEV &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative pronunciation learning for speech recognition for resource scarce languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM DEV &apos;12</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language model adaptation using cross-lingual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>In Eurospeech</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Small-vocabulary speech recognition for resource-scarce languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahanzeb</forename><surname>Sherwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM DEV &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Web-based tools and methods for rapid pronunciation dictionary creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Languageindependent and language-adaptive acoustic modeling for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="51" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The case for speech technology for developing regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahanzeb</forename><surname>Sherwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HCI for Community and International Development Workshop</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speech interfaces for information access by low literate users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahanzeb</forename><surname>Sherwani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crosslanguage mapping for small-vocabulary ASR in under-resourced languages: Investigating the impact of source language choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjana</forename><surname>Vakil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLTU &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rapid bootstrapping of five Eastern European languages using the Rapid Language Adaptation Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
