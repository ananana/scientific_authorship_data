<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressing Neural Language Models by Sparse Word Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
							<email>chenyunchuan11@mails.ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<address>
									<settlement>MoE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compressing Neural Language Models by Sparse Word Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="226" to="235"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time-and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent , but also improves the performance in terms of the perplexity measure. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs) play an important role in a variety of applications in natural language processing (NLP), including speech recognition and document recognition. In recent years, neu- ral network-based LMs have achieved signifi- cant breakthroughs: they can model language more precisely than traditional n-gram statistics (Mikolov et al., 2011); it is even possible to gen- erate new sentences from a neural LM, benefit- ing various downstream tasks like machine trans- lation, summarization, and dialogue systems <ref type="bibr" target="#b5">(Devlin et al., 2014;</ref><ref type="bibr" target="#b26">Rush et al., 2015;</ref><ref type="bibr" target="#b27">Sordoni et al., 2015;</ref><ref type="bibr" target="#b25">Mou et al., 2015b</ref>). <ref type="bibr">1</ref> Code released on https://github.com/chenych11/lm Existing neural LMs typically map a discrete word to a distributed, real-valued vector repre- sentation (called embedding) and use a neural model to predict the probability of each word in a sentence. Such approaches necessitate a large number of parameters to represent the em- beddings and the output layer's weights, which is unfavorable in many scenarios. First, with a wider application of neural networks in resource- restricted systems <ref type="bibr" target="#b10">(Hinton et al., 2015)</ref>, such ap- proach is too memory-consuming and may fail to be deployed in mobile phones or embedded sys- tems. Second, as each word is assigned with a dense vector-which is tuned by gradient-based methods-neural LMs are unlikely to learn mean- ingful representations for infrequent words. The reason is that infrequent words' gradient is only occasionally computed during training; thus their vector representations can hardly been tuned ade- quately.</p><p>In this paper, we propose a compressed neural language model where we can reduce the number of parameters to a large extent. To accomplish this, we first represent infrequent words' embeddings with frequent words' by sparse linear combina- tions. This is inspired by the observation that, in a dictionary, an unfamiliar word is typically defined by common words. We therefore propose an op- timization objective to compute the sparse codes of infrequent words. The property of sparseness (only 4-8 values for each word) ensures the effi- ciency of our model. Based on the pre-computed sparse codes, we design our compressed language model as follows. A dense embedding is assigned to each common word; an infrequent word, on the other hand, com- putes its vector representation by a sparse combi- nation of common words' embeddings. We use the long short term memory (LSTM)-based recur- rent neural network (RNN) as the hidden layer of our model. The weights of the output layer are also compressed in a same way as embeddings. Consequently, the number of trainable neural pa- rameters is a constant regardless of the vocabulary size if we ignore the biases of words. Even con- sidering sparse codes (which are very small), we find the memory consumption grows impercepti- bly with respect to the vocabulary.</p><p>We evaluate our LM on the Wikipedia corpus containing up to 1.6 billion words. During train- ing, we adopt noise-contrastive estimation (NCE) ( <ref type="bibr" target="#b8">Gutmann and Hyvärinen, 2012)</ref> to estimate the parameters of our neural LMs. However, dif- ferent from <ref type="bibr" target="#b22">Mnih and Teh (2012)</ref>, we tailor the NCE method by adding a regression layer (called ZRegressoion) to predict the normalization factor, which stabilizes the training process. Ex- perimental results show that, our compressed LM not only reduces the memory consumption, but also improves the performance in terms of the per- plexity measure.</p><p>To sum up, the main contributions of this paper are three-fold. (1) We propose an approach to rep- resent uncommon words' embeddings by a sparse linear combination of common ones'. (2) We pro- pose a compressed neural language model based on the pre-computed sparse codes. The memory increases very slowly with the vocabulary size (4- 8 values for each word). (3) We further introduce a ZRegression mechanism to stabilize the NCE algorithm, which is potentially applicable to other LMs in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Standard Neural LMs</head><p>Language modeling aims to minimize the joint probability of a corpus <ref type="bibr" target="#b13">(Jurafsky and Martin, 2014</ref>). Traditional n-gram models impose a Markov assumption that a word is only depen- dent on previous n − 1 words and independent of its position. When estimating the parameters, re- searchers have proposed various smoothing tech- niques including back-off models to alleviate the problem of data sparsity.  propose to use a feed- forward neural network (FFNN) to replace the multinomial parameter estimation in n-gram mod- els. Recurrent neural networks (RNNs) can also be used for language modeling; they are especially capable of capturing long range dependencies in sentences ( <ref type="bibr" target="#b18">Mikolov et al., 2010</ref>; Sundermeyer et In the above models, we can view that a neural LM is composed of three main parts, namely the Embedding, Encoding, and Prediction subnets, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The Embedding subnet maps a word to a dense vector, representing some abstract features of the word ( <ref type="bibr" target="#b20">Mikolov et al., 2013)</ref>. Note that this subnet usually accepts a list of words (known as history or context words) and outputs a sequence of word embeddings.</p><p>The Encoding subnet encodes the history of a target word into a dense vector (known as context or history representation). We may either leverage FFNNs ( ) or RNNs ( <ref type="bibr" target="#b18">Mikolov et al., 2010</ref>) as the Encoding subnet, but RNNs typically yield a better performance <ref type="bibr" target="#b29">(Sundermeyer et al., 2015)</ref>.</p><p>The Prediction subnet outputs a distribu- tion of target words as</p><formula xml:id="formula_0">p(w = w i |h) = exp(s(h, w i )) j exp(s(h, w j )) ,<label>(1)</label></formula><formula xml:id="formula_1">s(h, w i ) =W i h + b i ,<label>(2)</label></formula><p>where h is the vector representation of con- text/history h, obtained by the Encoding subnet.</p><formula xml:id="formula_2">W = (W 1 , W 2 , . . . , W V ) ∈ R C×V is the output weights of Prediction; b = (b 1 , b 2 , . . . , b V ) ∈ R C</formula><p>is the bias (the prior). s(h, w i ) is a scoring function indicating the degree to which the context h matches a target word w i . (V is the size of vo- cabulary V; C is the dimension of context/history, given by the Encoding subnet.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Complexity Concerns of Neural LMs</head><p>Neural network-based LMs can capture more pre- cise semantics of natural language than n-gram models because the regularity of the Embedding subnet extracts meaningful semantics of a word and the high capacity of Encoding subnet en- ables complicated information processing. Despite these, neural LMs also suffer from sev- eral disadvantages mainly out of complexity con- cerns.</p><p>Time complexity. Training neural LMs is typi- cally time-consuming especially when the vocab- ulary size is large. The normalization factor in Equation (1) contributes most to time complex- ity. <ref type="bibr" target="#b23">Morin and Bengio (2005)</ref> propose hierar- chical softmax by using a Bayesian network so that the probability is self-normalized. Sampling techniques-for example, importance sampling ( <ref type="bibr" target="#b1">Bengio and Senécal, 2003)</ref>, noise-contrastive es- timation ( <ref type="bibr" target="#b8">Gutmann and Hyvärinen, 2012)</ref>, and tar- get sampling <ref type="bibr" target="#b12">(Jean et al., 2014</ref>)-are applied to avoid computation over the entire vocabulary. In- frequent normalization maximizes the unnormal- ized likelihood with a penalty term that favors nor- malized predictions <ref type="bibr" target="#b0">(Andreas and Klein, 2014)</ref>.</p><p>Memory complexity and model complexity. The number of parameters in the Embedding and Prediction subnets in neural LMs increases linearly with respect to the vocabulary size, which is large <ref type="table" target="#tab_0">(Table 1)</ref>. As said in Section 1, this is sometimes unfavorable in memory-restricted sys- tems. Even with sufficient hardware resources, it is problematic because we are unlikely to fully tune these parameters. <ref type="bibr" target="#b4">Chen et al. (2015)</ref> pro- pose the differentiated softmax model by assign- ing fewer parameters to rare words than to fre- quent words. However, their approach only han- dles the output weights, i.e., W in Equation <ref type="formula" target="#formula_1">(2)</ref>; the input embeddings remain uncompressed in their approach.</p><p>In this work, we mainly focus on memory and model complexity, i.e., we propose a novel method to compress the Embedding and Prediction subnets in neural language models. Our work resembles little, if any, to the above methods as we compress embeddings and output weights using sparse word representations. Existing model compression typically works with a compromise of performance. On the contrary, our model im- proves the perplexity measure after compression. Sparse word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related Work</head><formula xml:id="formula_3">Sub-nets RNN-LSTM FFNN Embedding V E V E Encoding 4(CE + C 2 + C) nCE + C Prediction V (C + 1) V (C + 1) TOTAL † O((C + E)V ) O((E + C)V )</formula><p>We leverage sparse codes of words to compress neural LMs. <ref type="bibr" target="#b6">Faruqui et al. (2015)</ref> propose a sparse coding method to represent each word with a sparse vec- tor. They solve an optimization problem to ob- tain the sparse vectors of words as well as a dic- tionary matrix simultaneously. By contrast, we do not estimate any dictionary matrix when learning sparse codes, which results in a simple and easy- to-optimize model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Model</head><p>In this section, we describe our compressed lan- guage model in detail. Subsection 3.1 formal- izes the sparse representation of words, serving as the premise of our model. On such a basis, we compress the Embedding and Prediction subnets in Subsections 3.2 and 3.3, respectively. Finally, Subsection 3.4 introduces NCE for pa- rameter estimation where we further propose the ZRegression mechanism to stabilize our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparse Representations of Words</head><p>We split the vocabulary V into two disjoint subsets (B and C). The first subset B is a base set, con- taining a fixed number of common words (8k in our experiments). C = V\B is a set of uncommon words. We would like to use B's word embeddings to encode C's.</p><p>Our intuition is that oftentimes a word can be defined by a few other words, and that rare words should be defined by common ones. Therefore, it is reasonable to use a few common words' em- beddings to represent that of a rare word. Follow- ing most work in the literature ( <ref type="bibr" target="#b16">Lee et al., 2006;</ref><ref type="bibr" target="#b31">Yang et al., 2011</ref>), we represent each uncommon word with a sparse, linear combination of com-mon ones' embeddings. The sparse coefficients are called a sparse code for a given word.</p><p>We first train a word representation model like SkipGram ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) to obtain a set of embeddings for each word in the vocabulary, in- cluding both common words and rare words. Sup- pose U = (U 1 , U 2 , . . . , U B ) ∈ R E×B is the (learned) embedding matrix of common words, i.e., U i is the embedding of i-th word in B. (Here, B = |B|.)</p><p>Each word in B has a natural sparse code (de- noted as x): it is a one-hot vector with B elements, the i-th dimension being on for the i-th word in B.</p><p>For a word w ∈ C, we shall learn a sparse vector x = (x 1 , x 2 , . . . , x B ) as the sparse code of the word. Provided that x has been learned (which will be introduced shortly), the embedding of w isˆw</p><formula xml:id="formula_4">isˆ isˆw = B j=1 x j U j = U x,<label>(3)</label></formula><p>To learn the sparse representation of a certain word w, we propose the following optimization objective</p><formula xml:id="formula_5">min x U x − w 2 2 + αx 1 + β|1 x − 1| + γ1 max{0, −x},<label>(4)</label></formula><p>where max denotes the component-wise maxi- mum; w is the embedding for a rare word w ∈ C. The first term (called fitting loss afterwards) evaluates the closeness between a word's coded vector representation and its "true" representation w, which is the general goal of sparse coding.</p><p>The second term is an 1 regularizer, which en- courages a sparse solution. The last two regular- ization terms favor a solution that sums to 1 and that is nonnegative, respectively. The nonnegative regularizer is applied as in <ref type="bibr" target="#b9">He et al. (2012)</ref> due to psychological interpretation concerns.</p><p>It is difficult to determine the hyperparameters α, β, and γ. Therefore we perform several tricks. First, we drop the last term in the problem (4), but clip each element in x so that all the sparse codes are nonnegative during each update of training.</p><p>Second, we re-parametrize α and β by balanc- ing the fitting loss and regularization terms dy- namically during training. Concretely, we solve the following optimization problem, which is slightly different but closely related to the concep- tual objective (4):</p><formula xml:id="formula_6">min x L(x) + α t R 1 (x) + β t R 2 (x),<label>(5)</label></formula><p>where L(x) = U x − w 2 2 , R 1 (x) = x 1 , and R 2 (x) = |1 x−1|. α t and β t are adaptive param- eters that are resolved during training time. Sup- pose x t is the value we obtain after the update of the t-th step, we expect the importance of fitness and regularization remain unchanged during train- ing. This is equivalent to</p><formula xml:id="formula_7">α t R 1 (x t ) L(x t ) = w α ≡ const,<label>(6)</label></formula><formula xml:id="formula_8">β t R 2 (x t ) L(x t ) = w β ≡ const.<label>(7)</label></formula><p>or</p><formula xml:id="formula_9">α t = L(x t ) R 1 (x t ) w α and β t = L(x t ) R 2 (x t ) w β ,</formula><p>where w α and w β are the ratios between the regu- larization loss and the fitting loss. They are much easier to specify than α or β in the problem (4). We have two remarks as follows.</p><p>• To learn the sparse codes, we first train the "true" embeddings by word2vec 2 for both common words and rare words. However, these true embeddings are slacked during our language modeling.</p><p>• As the codes are pre-computed and remain unchanged during language modeling, they are not tunable parameters of our neural model. Considering the learned sparse codes, we need only 4-8 values for each word on av- erage, as the codes contain 0.05-0.1% non- zero values, which are almost negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Compression for the Embedding Subnet</head><p>One main source of LM parameters is the Embedding subnet, which takes a list of words (history/context) as input, and outputs dense, low- dimensional vector representations of the words. We leverage the sparse representation of words mentioned above to construct a compressed Embedding subnet, where the number of param- eters is independent of the vocabulary size.</p><p>By solving the optimization problem (5) for each word, we obtain a non-negative sparse code x ∈ R B for each word, indicating the degree to which the word is related to common words in B. Then the embedding of a word is given byˆw byˆ byˆw = U x.</p><p>We would like to point out that the embedding of a wordˆwwordˆ wordˆw is not sparse because U is a dense ma- trix, which serves as a shared parameter of learn- ing all words' vector representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter Compression for the Prediction Subnet</head><p>Another main source of parameters is the Prediction subnet. As <ref type="table" target="#tab_0">Table 1</ref> shows, the out- put layer contains V target-word weight vectors and biases; the number increases with the vocabu- lary size. To compress this part of a neural LM, we propose a weight-sharing method that uses words' sparse representations again. Similar to the com- pression of word embeddings, we define a base set of weight vectors, and use them to represent the rest weights by sparse linear combinations. Without loss of generality, we let D = W :,1:B be the output weights of B base target words, and c = b 1:B be bias of the B target words. <ref type="bibr">3</ref> The goal is to use D and c to represent W and b. How- ever, as the values of W and b are unknown before the training of LM, we cannot obtain their sparse codes in advance.</p><p>We claim that it is reasonable to share the same set of sparse codes to represent word vec- tors in Embedding and the output weights in the Prediction subnet. In a given corpus, an occurrence of a word is always companied by its context. The co-occurrence statistics about a word or corresponding context are the same. As both word embedding and context vectors cap- ture these co-occurrence statistics ( <ref type="bibr" target="#b17">Levy and Goldberg, 2014</ref>), we can expect that context vec- tors share the same internal structure as embed- dings. Moreover, for a fine-trained network, given any word w and its context h, the output layer's weight vector corresponding to w should spec- ify a large inner-product score for the context h; thus these context vectors should approximate the weight vector of w. Therefore, word embed- dings and the output weight vectors should share the same internal structures and it is plausible to use a same set of sparse representations for both words and target-word weight vectors. As we shall show in Section 4, our treatment of compressing the Prediction subnet does make sense and achieves high performance.</p><p>Formally, the i-th output weight vector is esti- mated byˆW byˆ byˆW i = Dx i ,  We apply NCE to estimate the parameters of the Prediction sub-network (dashed round rectan- gle). The SpUnnrmProb layer outputs a sparse, unnormalized probability of the next word. By "sparsity," we mean that, in NCE, the probability is computed for only the "true" next word (red) and a few generated negative samples.</p><p>The biases can also be compressed asˆb</p><formula xml:id="formula_11">asˆasˆb i = cx i .<label>(9)</label></formula><p>where x i is the sparse representation of the i-th word. (It is shared in the compression of weights and biases.) In the above model, we have managed to com- pressed a language model whose number of pa- rameters is irrelevant to the vocabulary size.</p><p>To better estimate a "prior" distribution of words, we may alternatively assign an indepen- dent bias to each word, i.e., b is not compressed. In this variant, the number of model parameters grows very slowly and is also negligible because each word needs only one extra parameter. Exper- imental results show that by not compressing the bias vector, we can even improve the performance while compressing LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Noise-Contrastive Estimation with ZRegression</head><p>We adopt the noise-contrastive estimation (NCE) method to train our model. Compared with the maximum likelihood estimation of softmax, NCE reduces computational complexity to a large de- gree. We further propose the ZRegression mechanism to stablize training. NCE generates a few negative samples for each positive data sample. During training, we only need to compute the unnormalized probability of these positive and negative samples. Interested readers are referred to ( <ref type="bibr" target="#b8">Gutmann and Hyvärinen, 2012</ref>) for more information.</p><p>Formally, the estimated probability of the word w i with history/context h is</p><formula xml:id="formula_12">P (w|h; θ) = 1 Z h P 0 (w i |h; θ) = 1 Z h exp(s(w i , h; θ)),<label>(10)</label></formula><p>where θ is the parameters and Z h is a context- dependent normalization factor. P 0 (w i |h; θ) is the unnormalized probability of the w (given by the SpUnnrmProb layer in <ref type="figure" target="#fig_3">Figure 2</ref>). The NCE algorithm suggests to take Z h as pa- rameters to optimize along with θ, but it is in- tractable for context with variable lengths or large sizes in language modeling. Following <ref type="bibr" target="#b22">Mnih and Teh (2012)</ref>, we set Z h = 1 for all h in the base model (without ZRegression).</p><p>The objective for each occurrence of con- text/history h is</p><formula xml:id="formula_13">J(θ|h) = log P (w i |h; θ) P (w i |h; θ) + kP n (w i ) + k j=1 log kP n (w j ) P (w j |h; θ) + kP n (w j ) ,</formula><p>where P n (w) is the probability of drawing a nega- tive sample w; k is the number of negative samples that we draw for each positive sample. The overall objective of NCE is</p><formula xml:id="formula_14">J(θ) = E h [J(θ|h)] ≈ 1 M M i=1 J(θ|h i ),</formula><p>where h i is an occurrence of the context and M is the total number of context occurrences. Although setting Z h to 1 generally works well in our experiment, we find that in certain sce- narios, the model is unstable. Experiments show that when the true normalization factor is far away from 1, the cost function may vibrate. To com- ply with NCE in general, we therefore propose a ZRegression layer to predict the normalization constant Z h dependent on h, instead of treating it as a constant.</p><p>The regression layer is computed by</p><formula xml:id="formula_15">Z −1 h = exp(W Z h + b Z ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partitions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running words</head><p>Train (n-gram) 1.6 B Train (neural LMs) 100 M Dev 100 K Test 5 M where W Z ∈ R C and b Z ∈ R are weights and bias for ZRegression. Hence, the estimated proba- bility by NCE with ZRegression is given by</p><formula xml:id="formula_16">P (w|h) = exp(s(h, w)) · exp(W Z h + b Z ).</formula><p>Note that the ZRegression layer does not guarantee normalized probabilities. During val- idation and testing, we explicitly normalize the probabilities by Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this part, we first describe our dataset in Subsec- tion 4.1. We evaluate our learned sparse codes of rare words in Subsection 4.2 and the compressed language model in Subsection 4.3. Subsection 4.4 provides in-depth analysis of the ZRegression mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We used the freely available Wikipedia 4 dump (2014) as our dataset. We extracted plain sen- tences from the dump and removed all markups. We further performed several steps of preprocess- ing such as text normalization, sentence splitting, and tokenization. Sentences were randomly shuf- fled, so that no information across sentences could be used, i.e., we did not consider cached language models. The resulting corpus contains about 1.6 billion running words.</p><p>The corpus was split into three parts for train- ing, validation, and testing. As it is typically time- consuming to train neural networks, we sampled a subset of 100 million running words to train neu- ral LMs, but the full training set was used to train the backoff n-gram models. We chose hyperpa- rameters by the validation set and reported model performance on the test set. <ref type="table" target="#tab_1">Table 2</ref> presents some statistics of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Analysis of Sparse Codes</head><p>To obtain words' sparse codes, we chose 8k com- mon words as the "dictionary," i.e., B = 8000. The sparse representations of selected words. The x-axis is the dictionary of 8k common words; the y-axis is the coefficient of sparse cod- ing. Note that algorithm, secret, and debate are common words, each being coded by itself with a coefficient of 1.</p><p>We had 2k-42k uncommon words in different set- tings. We first pretrained word embeddings of both rare and common words, and obtained 200d vectors U and w in Equation (5). The dimension was specified in advance and not tuned. As there is no analytic solution to the objective, we opti- mized it by Adam ( <ref type="bibr" target="#b15">Kingma and Ba, 2014)</ref>, which is a gradient-based method. To filter out small co- efficients around zero, we simply set a value to 0 if it is less than 0.015 · max{v ∈ x}. w α in Equa- tion (6) was set to 1 because we deemed fitting loss and sparsity penalty are equally important. We set w β in Equation (7) to 0.1, and this hyperparameter is insensitive. <ref type="figure" target="#fig_4">Figure 3</ref> plots the sparse codes of a few selected words. As we see, algorithm, secret, and debate are common words, and each is (sparsely) coded by itself with a coefficient of 1. We further notice that a rare word like algorithms has a sparse rep- resentation with only a few non-zero coefficient.</p><p>Moreover, the coefficient in the code of al- gorithms-corresponding to the base word algo- rithm-is large (∼ 0.6), showing that the words algorithm and algorithms are similar. Such phe- nomena are also observed with secret and debate.</p><p>The qualitative analysis demonstrates that our approach can indeed learn a sparse code of a word, and that the codes are meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Analysis of Compressed Language Models</head><p>We then used the pre-computed sparse codes to compress neural LMs, which provides quantita- tive analysis of the learned sparse representations of words. We take perplexity as the performance measurement of a language model, which is de- fined by</p><formula xml:id="formula_17">PPL = 2 − 1 N N i=1 log 2 p(w i |h i )</formula><p>where N is the number of running words in the test corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Settings</head><p>We leveraged LSTM-RNN as the Encoding sub- net, which is a prevailing class of neural networks for language modeling <ref type="bibr" target="#b29">(Sundermeyer et al., 2015;</ref><ref type="bibr" target="#b14">Karpathy et al., 2015</ref>). The hidden layer was 200d.</p><p>We used the Adam algorithm to train our neural models. The learning rate was chosen by valida- tion from {0.001, 0.002, 0.004, 0.006, 0.008}. Pa- rameters were updated with a mini-batch size of 256 words. We trained neural LMs by NCE, where we generated 50 negative samples for each pos- itive data sample in the corpus. All our model variants and baselines were trained with the same pre-defined hyperparameters or tuned over a same candidate set; thus our comparison is fair. We list our compressed LMs and competing methods as follows.</p><p>• KN3. We adopted the modified Kneser-Ney smoothing technique to train a 3-gram LM; we used the SRILM toolkit <ref type="bibr">(Stolcke and others, 2002</ref>) in out experiment.</p><p>• LBL5. A Log-BiLinear model introduced in <ref type="bibr" target="#b21">Mnih and Hinton (2007)</ref>. We used 5 preced- ing words as context. • LSTM-z,wb. Based on LSTM-z, we com- pressed word embeddings in Embedding and the output weights and biases in Prediction.</p><p>• LSTM-z,w. In this variant, we did not com- press the bias term in the output layer. For each word in C, we assigned an independent bias parameter. <ref type="table" target="#tab_3">Tables 3 shows the perplexity of our</ref>     model as well as the backoff 3-gram LM, even if the 3-gram LM is trained on a much larger cor- pus with 1.6 billion words. The ZRegression mechanism improves the performance of LSTM to a large extent, which is unexpected. Subsec- tion 4.4 will provide more in-depth analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Performance</head><p>Regarding the compression method proposed in this paper, we notice that LSTM-z,wb and LSTM-z,w yield similar performance to LSTM-z. In particular, LSTM-z,w outperforms LSTM-z in all scenarios of different vocabulary sizes. More- over, both LSTM-z,wb and LSTM-z,w can reduce the memory consumption by up to 80% <ref type="table" target="#tab_4">(Table 4)</ref>.</p><p>We further plot in <ref type="figure" target="#fig_6">Figure 4</ref> the model perfor- mance (lines) and memory consumption (bars) in a fine-grained granularity of vocabulary sizes. We see such a tendency that compressed LMs (LSTM- z,wb and LSTM-z,w, yellow and red lines) are generally better than LSTM-z (black line) when we have a small vocabulary. However, LSTM- z,wb is slightly worse than LSTM-z if the vocabu- lary size is greater than, say, 20k. The LSTM-z,w remains comparable to LSTM-z as the vocabulary grows.</p><p>To explain this phenomenon, we may imagine that the compression using sparse codes has two effects: it loses information, but it also enables more accurate estimation of parameters especially for rare words. When the second factor dominates, we can reasonably expect a high performance of the compressed LM.</p><p>From the bars in <ref type="figure" target="#fig_6">Figure 4</ref>, we observe that tra- ditional LMs have a parameter space growing lin- early with the vocabulary size. But the number of parameters in our compressed models does not increase-or strictly speaking, increases at an ex- tremely small rate-with vocabulary.</p><p>These experiments show that our method can largely reduce the parameter space with even per- formance improvement. The results also verify that the sparse codes induced by our model indeed capture meaningful semantics and are potentially useful for other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of ZRegression</head><p>We next analyze the effect of ZRegression for NCE training. As shown in <ref type="figure" target="#fig_8">Figure 5a</ref>, the training process becomes unstable after processing 70% of the dataset: the training loss vibrates significantly, whereas the test loss increases.</p><p>We find a strong correlation between unsta- bleness and the Z h factor in Equation (10), i.e., the sum of unnormalized probability <ref type="figure" target="#fig_8">(Figure 5b</ref>). Theoretical analysis shows that the Z h factor tends to be self-normalized even though it is not forced to <ref type="bibr" target="#b8">(Gutmann and Hyvärinen, 2012)</ref>. However, problems would occur, should it fail.</p><p>In traditional methods, NCE jointly estimates normalization factor Z and model parameters ( <ref type="bibr" target="#b8">Gutmann and Hyvärinen, 2012</ref>). For language modeling, Z h dependents on context h. <ref type="bibr" target="#b22">Mnih and Teh (2012)</ref> propose to estimate a separate Z h based on two history words (analogous to 3-gram), but their approach hardly scales to RNNs because of the exponential number of different combina- tions of history words.</p><p>We propose the ZRegression mechanism in Section 3.4, which can estimate the Z h factor well <ref type="figure" target="#fig_8">(Figure 5d</ref>) based on the history vector h. In this way, we manage to stabilize the training pro- cess <ref type="figure" target="#fig_8">(Figure 5c</ref>) and improve the performance by  a large margin, as has shown in <ref type="table" target="#tab_3">Table 3</ref>. It should be mentioned that ZRegression is not specific to model compression and is generally applicable to other neural LMs trained by NCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed an approach to repre- sent rare words by sparse linear combinations of common ones. Based on such combinations, we managed to compress an LSTM language model (LM), where memory does not increase with the vocabulary size except a bias and a sparse code for each word. Our experimental results also show that the compressed LM has yielded a better per- formance than the uncompressed base LM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of a neural networkbased language model.</figDesc><graphic url="image-1.png" coords="2,359.73,62.80,113.38,111.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Existing work on model compression for neural networks. BuciluˇaBuciluˇa et al. (2006) and Hinton et al. (2015) use a well-trained large network to guide the training of a small network for model compres- sion. Jaderberg et al. (2014) compress neural mod- els by matrix factorization, Gong et al. (2014) by quantization. In NLP, Mou et al. (2015a) learn an embedding subspace by supervised training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3</head><label></label><figDesc>W:,1:B is the first B columns of W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Compressing the output of neural LM. We apply NCE to estimate the parameters of the Prediction sub-network (dashed round rectangle). The SpUnnrmProb layer outputs a sparse, unnormalized probability of the next word. By "sparsity," we mean that, in NCE, the probability is computed for only the "true" next word (red) and a few generated negative samples.</figDesc><graphic url="image-2.png" coords="5,326.06,62.81,181.05,160.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The sparse representations of selected words. The x-axis is the dictionary of 8k common words; the y-axis is the coefficient of sparse coding. Note that algorithm, secret, and debate are common words, each being coded by itself with a coefficient of 1.</figDesc><graphic url="image-3.png" coords="7,72.53,62.80,217.23,104.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>LSTM-s. A standard LSTM-RNN language model which is applied in Sundermeyer et al. (2015) and Karpathy et al. (2015). We im- plemented the LM ourselves based on Theano (Theano Development Team, 2016) and also used NCE for training. • LSTM-z. An LSTM-RNN enhanced with the ZRegression mechanism described in Section 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fine-grained plot of performance (perplexity) and memory consumption (including sparse codes) versus the vocabulary size.</figDesc><graphic url="image-4.png" coords="8,72.13,334.64,217.50,100.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(b) The validation perplexity and normalization factor Z h w/o ZRegression. (c) Training loss vs. training time w/ ZRegression of different runs. (d) The validation perplexity and normalization factor Z h w/ ZRegression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Analysis of ZRegression.</figDesc><graphic url="image-7.png" coords="9,105.27,208.96,149.50,111.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Number of parameters in different neural 
network-based LMs. E: embedding dimension; 
C: context dimension; V : vocabulary size.  † Note 
that V C (or E). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of our corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Perplexity of our compressed language 
models and baselines.  † Trained with the full cor-
pus of 1.6 billion running words. 

Vocabulary 
10k 
22k 
36k 
50k 

LSTM-z,w 
17.76 59.28 73.42 79.75 
LSTM-z,wb 17.80 59.44 73.61 79.95 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Memory reduction (%) by our proposed 
methods in comparison with the uncompressed 
model LSTM-z. The memory of sparse codes are 
included. 

</table></figure>

			<note place="foot" n="2"> https://code.google.com/archive/p/word2vec</note>

			<note place="foot" n="4"> http://en.wikipedia.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When and why are log-linear models self-normalizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="244" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quick training of probabilistic neural nets by importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Ninth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Buciluˇabuciluˇa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Welin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04906</idno>
		<title level="m">Strategies for training large vocabulary neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse overcomplete word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1491" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document summarization based on data reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanying</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 26th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="620" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2007</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Speech and Language Processing</title>
		<imprint>
			<publisher>Pearson</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>the IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine learning</title>
		<meeting>the 24th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distilling word embeddings: An encoding approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04488</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Backward and forward language modeling for constrained natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.06612</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SRILM-An extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From feedforward to recurrent LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust sparse coding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
