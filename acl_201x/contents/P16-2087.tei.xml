<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonparametric Spherical Topic Modeling with Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nematollah</forename><surname>Kayhan Batmanghelich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Saeedi</surname></persName>
							<email>ardavans@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><forename type="middle">R</forename><surname>Narasimhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
							<email>gershman@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Nonparametric Spherical Topic Modeling with Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="537" to="542"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Vari-ational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Prior work on topic modeling has mostly involved the use of categorical likelihoods ( <ref type="bibr" target="#b2">Blei et al., 2003;</ref><ref type="bibr" target="#b1">Blei and Lafferty, 2006;</ref><ref type="bibr" target="#b18">Rosen-Zvi et al., 2004</ref>). Applications of topic models in the tex- tual domain treat words as discrete observations, ignoring the semantics of the language. Recent developments in distributional representations of words ( <ref type="bibr" target="#b13">Mikolov et al., 2013;</ref><ref type="bibr" target="#b16">Pennington et al.</ref>, * Authors contributed equally and listed alphabetically. <ref type="bibr">1</ref> Code is available at https://github.com/ Ardavans/sHDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2014</head><p>) have succeeded in capturing certain seman- tic regularities, but have not been explored exten- sively in the context of topic modeling. In this pa- per, we propose a probabilistic topic model with a novel observational distribution that integrates well with directional similarity metrics.</p><p>One way to employ semantic similarity is to use the Euclidean distance between word vectors, which reduces to a Gaussian observational distri- bution for topic modeling ( <ref type="bibr" target="#b5">Das et al., 2015)</ref>. The cosine distance between word embeddings is an- other popular choice and has been shown to be a good measure of semantic relatedness <ref type="bibr" target="#b13">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b16">Pennington et al., 2014</ref>). The von Mises-Fisher (vMF) distribution is well-suited to model such directional data ( <ref type="bibr" target="#b6">Dhillon and Sra, 2003;</ref><ref type="bibr" target="#b0">Banerjee et al., 2005</ref>) but has not been pre- viously applied to topic models.</p><p>In this work, we use vMF as the observational distribution. Each word can be viewed as a point on a unit sphere with topics being canonical di- rections. More specifically, we use a Hierarchi- cal Dirichlet Process (HDP) ( <ref type="bibr" target="#b19">Teh et al., 2006</ref>), a Bayesian nonparametric variant of Latent Dirich- let Allocation (LDA), to automatically infer the number of topics. We implement an efficient infer- ence scheme based on Stochastic Variational Infer- ence (SVI) <ref type="bibr" target="#b9">(Hoffman et al., 2013)</ref>.</p><p>We perform experiments on two different English text corpora: 20 NEWSGROUPS and NIPS and compare against two baselines -HDP and Gaussian LDA. Our model, spherical HDP (sHDP), outperforms all three systems on the mea- sure of topic coherence. For instance, sHDP ob- tains gains over Gaussian LDA of 97.5% on the NIPS dataset and 65.5% on the 20 NEWSGROUPS dataset. Qualitative inspection reveals consistent topics produced by sHDP. We also empirically demonstrate that employing SVI leads to efficient 537 topic inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Topic modeling and word embeddings <ref type="bibr" target="#b5">Das et al. (2015)</ref> proposed a topic model which uses a Gaussian distribution over word embeddings. By performing inference over the vector representa- tions of the words, their model is encouraged to group words that are semantically similar, lead- ing to more coherent topics. In contrast, we pro- pose to utilize von Mises-Fisher (vMF) distribu- tions which rely on the cosine similarity between the word vectors instead of euclidean distance.</p><p>vMF in topic models The vMF distribution has been used to model directional data by plac- ing points on a unit sphere <ref type="bibr" target="#b6">(Dhillon and Sra, 2003)</ref>. <ref type="bibr" target="#b17">Reisinger et al. (2010)</ref> propose an admix- ture model that uses vMF to model documents rep- resented as vector of normalized word frequen- cies. This does not account for word level seman- tic similarities. Unlike their method, we use vMF over word embeddings. In addition, our model is nonparametric.</p><p>Nonparametric topic models HDP and its vari- ants have been successfully applied to topic mod- eling ( <ref type="bibr" target="#b15">Paisley et al., 2015;</ref><ref type="bibr" target="#b3">Blei, 2012;</ref><ref type="bibr" target="#b8">He et al., 2013)</ref>; however, all these models assume a cate- gorical likelihood in which the words are encoded as one-hot representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we describe the generative process for documents. Rather than one-hot representa- tion of words, we employ normalized word em- beddings ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>) to capture seman- tic meanings of associated words. Word n from document d is represented by a normalized M - dimensional vector x dn and the similarity between words is quantified by the cosine of angle between the corresponding word vectors.</p><p>Our model is based on the Hierarchical Dirich- let Process (HDP). The model assumes a collec- tion of "topics" that are shared across documents in the corpus. The topics are represented by the topic centers µ k ∈ R M . Since word vectors are normalized, the µ k can be viewed as a direction on unit sphere. Von Mises−Fisher (vMF) is a distri- bution that is commonly used to model directional data. The likelihood of the topic k for word</p><formula xml:id="formula_0">x dn D x dn z dn ⇡ d µk, k 1 ⇤ ↵ ' dn k , k ✓ d N d (µ 0 , C 0 ) (m, )</formula><p>Figure 1: Graphical representation of our spheri- cal HDP (sHDP) model. The symbol next to each random variable denotes the parameter of its vari- ational distribution. We assume D documents in the corpus, each document contains N d words and there are countably infinite topics represented by</p><formula xml:id="formula_1">(µ k , κ k ).</formula><p>is:</p><formula xml:id="formula_2">f (x dn ; µ k ; κ k ) = exp κ k µ T k x dn C M (κ k )</formula><p>where κ k is the concentration of the topic k, the</p><formula xml:id="formula_3">C M (κ k ) := κ M/2−1 k / (2π) M/2 I M/2−1 (κ k ) is</formula><p>the normalization constant, and I ν (·) is the mod- ified Bessel function of the first kind at order ν. Interestingly, the log-likelihood of the vMF is pro- portional to µ T k x dn (up to a constant), which is equal to the cosine distance between two vectors. This distance metric is also used in <ref type="bibr" target="#b13">Mikolov et al. (2013)</ref> to measure semantic proximity.</p><p>When sampling a new document, a subset of topics determine the distribution over words. We let z dn denote the topic selected for the word n of document d. Hence, z dn is drawn from a categori- cal distribution: z dn ∼ Mult(π d ), where π d is the proportion of topics for document d. We draw π d from a Dirichlet Process which enables us to esti- mate the the number of topics from the data. The generative process for the generation of new doc- ument is as follows:</p><formula xml:id="formula_4">β ∼ GEM(γ) π d ∼ DP(α, β) κ k ∼ log-Normal(m, σ 2 ) µ k ∼ vMF(µ 0 , C 0 ) z dn ∼ Mult(π d ) x dn ∼ vMF(µ k , κ k )</formula><p>where GEM(γ) is the stick-breaking distribution with concentration parameter γ, DP(α, β) is a Dirichlet process with concentration parameter α and stick proportions β ( <ref type="bibr">Teh et al., 2012</ref>). We use log-normal and vMF as hyper-prior distributions for the concentrations (κ k ) and centers of the top- ics (µ k ) respectively. <ref type="figure">Figure 1</ref> provides a graphical illustration of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic variational inference</head><p>In the rest of the paper, we use bold symbols to denote the vari- ables of the same kind (e.g.,</p><formula xml:id="formula_5">x d = {x dn } n , z := {z dn } d,n ).</formula><p>We employ stochastic variational mean-field inference (SVI) <ref type="bibr" target="#b9">(Hoffman et al., 2013)</ref> to estimate the posterior distributions of the latent variables. SVI enables us to sequentially process batches of documents which makes it appropriate in large-scale settings.</p><p>To approximate the posterior distribution of the latent variables, the mean-field approach finds the optimal parameters of the fully factorizable q (i.e., q(z, β, π, µ, κ) := q(z)q(β)q(π)q(µ)q(κ)) by maximizing the Evidence Lower Bound (ELBO),</p><formula xml:id="formula_6">L(q) = E q [log p(X, z, β, π, µ, κ)] − E q [log q]</formula><p>where E q <ref type="bibr">[·]</ref> is expectation with respect to q, p(X, z, β, π, µ, κ) is the joint likelihood of the model specified by the HDP model.</p><p>The variational distributions for z, π, µ have the following parametric forms,</p><formula xml:id="formula_7">q(z) = Mult(z|ϕ) q(π) = Dir(π|θ) q(µ) = vMF(µ|ψ, λ),</formula><p>where Dir denotes the Dirichlet distribution and ϕ, θ, ψ and λ are the parameters we need to op- timize the ELBO. Similar to <ref type="bibr" target="#b4">(Bryant and Sudderth, 2012</ref>), we view β as a parameter; hence, q(β) = δ β * (β). The prior distribution κ does not follow a conjugate distribution; hence, its poste- rior does not have a closed-form. Since κ is only one dimensional variable, we use importance sam- pling to approximate its posterior. For a batch size of one (i.e., processing one document at time), the update equations for the parameters are:</p><formula xml:id="formula_8">ϕ dwk ∝ exp{E q [log vMF(x dw |ψ k , λ k )] + E q [log π dk ]} θ dk ← (1 − ρ)θ dk + ρ(αβ k + D W n=1 ω wj ϕ dwk ) t ← (1 − ρ)t + ρs(x d , ϕ dk ) ψ ← t/t 2 , λ ← t 2</formula><p>where D, ω wj , W , ρ are the total number of docu- ments, number of word w in document j, the total number of words in the dictionary, and the step size, respectively. t is a natural parameter for vMF and s(x d , ϕ dk ) is a function computing the suffi- cient statistics of vMF distribution of the topic k. We use numerical gradient ascent to optimize for β * . For exact forms of E q log[vMF(x dw |ψ k , λ k )] and E q [log π dk ], see Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Setup We perform experiments on two different text corpora: 11266 documents from 20 NEWS- GROUPS 2 and 1566 documents from the NIPS cor- pus <ref type="bibr">3</ref> . We utilize 50-dimensional word embeddings trained on text from Wikipedia using word2vec 4 . The vectors are normalized to have unit 2 -norm, which has been shown to provide superior perfor- mance ( <ref type="bibr" target="#b12">Levy et al., 2015)</ref>).</p><p>We evaluate our model using the measure of topic coherence <ref type="bibr" target="#b14">(Newman et al., 2010)</ref>, which has been shown to effectively correlate with human judgement ( <ref type="bibr" target="#b11">Lau et al., 2014</ref>). For this, we com- pute the Pointwise Mutual Information (PMI) us- ing a reference corpus of 300k documents from Wikipedia. The PMI is calculated using co- occurence statistics over pairs of words (u i , u j ) in 20-word sliding windows:</p><formula xml:id="formula_9">PMI(u i , u j ) = log p(u i , u j ) p(u i ) · p(u j )</formula><p>Additionally, we also use the metric of normalized PMI (NPMI) to evaluate the models in a similar fashion:</p><formula xml:id="formula_10">NPMI(u i , u j ) = log p(u i ,u j ) p(u i )·p(u j ) − log p(u i , u j )</formula><p>We compare our model with two baselines: HDP and the Gaussian LDA model. We ran G-LDA with various number of topics (k).</p><p>Results <ref type="table" target="#tab_0">Table 2</ref> details the topic coherence av- eraged over all topics produced by each model. We observe that our sHDP model outperforms G- LDA by 0.08 points on 20 NEWSGROUPS and by 0.17 points in terms of PMI on the NIPS dataset. The NPMI scores also show a similar trend with sHDP obtaining the best scores on both datasets. We can also see that the individual topics inferred <ref type="table">Gaussian LDA  vector  shows  network  hidden  performance  net  figure  size  image  feature  learning  term  work  references  shown  average  gaussian  show  model  rule  press  introduction  neurons  present  equation  motion  neural  word  tion  statistical  point  family  generalization  action  input  means  ing  related  large  versus  images  spike  data  words  eq  comparison  neuron  spread  gradient  series  function  approximate  performed  source  small  median  theory  final  time  derived  em  statistics  fig  physiology  dimensional  robot  set  describe  vol  free  cells</ref>  <ref type="table">Table 1</ref>: Examples of top words for the most coherent topics (column-wise) inferred on the NIPS dataset by Gaussian LDA (k=40) and Spherical HDP. The last row for each model is the topic coherence (PMI) computed using Wikipedia documents as reference.  by sHDP make sense qualitatively and have higher coherence scores than G-LDA <ref type="table">(Table 1)</ref>. This sup- ports our hypothesis that using the vMF likelihood helps in producing more coherent topics. sHDP produces 16 topics for the 20 NEWSGROUPS and 92 topics on the NIPS dataset. <ref type="figure">Figure 2</ref> shows a plot of normalized log- likelihood against the runtime of sHDP and G- LDA. <ref type="bibr">5</ref> We calculate the normalized value of log- likelihood by subtracting the minimum value from it and dividing it by the difference of maximum <ref type="bibr">5</ref> Our sHDP implementation is in Python and the G-LDA code is in Java. G-LDA sHDP <ref type="figure">Figure 2</ref>: Normalized log-likelihood (in percent- age) over a training set of size 1566 documents from the NIPS corpus. Since the log-likelihood values are not comparable for the Gaussian LDA and the sHDP, we normalize them to demon- strate the convergence speed of the two inference schemes for these models.</p><note type="other">children 1.16 0.4 0.35 0.29 0.25 0.25 0.21 0.2 Spherical HDP neural function analysis press pattern problem noise algorithm layer linear theory cambridge fig process gradient error neurons functions computational journal temporal method propagation parameters neuron vector statistical vol shape optimal signals computation activation random field eds smooth solution frequency algorithms brain probability simulations trans surface complexity feedback compute cells parameter simulation springer horizontal estimation electrical binary cell dimensional nonlinear volume vertical prediction filter mapping synaptic equation dynamics review posterior solve detection optimization 1.87 1.73 1.51 1.44 1.41 1.19 1.12 1.03</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>and minimum values. We can see that sHDP con- verges faster than G-LDA, requiring only around five iterations while G-LDA takes longer to con- verge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Classical topic models do not account for semantic regularities in language. Recently, distributional representations of words have emerged that exhibit semantic consistency over directional metrics like cosine similarity. Neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correla- tions. In this work, we demonstrate the use of the von Mises-Fisher distribution to model words as points over a unit sphere. We use HDP as the base topic model and propose an efficient algorithm based on Stochastic Variational Inference. Our model naturally exploits the semantic structures of word embeddings while flexibly inferring the number of topics. We show that our method out- performs three competitive approaches in terms of topic coherence on two different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yee Whye Teh, Michael I Jordan, Matthew J Beal, and</head><p>David M Blei. 2012. Hierarchical dirichlet pro- cesses. Journal of the american statistical associ- ation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendinx</head><p>Mean field update equations</p><p>In this section, we provide the mean field update equations. The SVI update equations can be de- rived from the mean field update ( <ref type="bibr" target="#b9">Hoffman et al., 2013)</ref>. The following term is computed for the update equations:</p><formula xml:id="formula_11">E q [log vMF(x dn |µ k , κ k )] = E q [log C M (κ k )]+ E q [κ k ]x T dn E q [µ k ]</formula><p>where C M (·) is explained in Section 3. The difficulty here lies in computing E q [κ k ] and E q [C M (κ k )]. However, κ is a scalar value. Hence, to compute E q [κ k ], we divide a reasonable interval of κ k into grids and compute the weight for each grid point as suggested by <ref type="bibr" target="#b7">Gopal and Yang (2014)</ref>:</p><formula xml:id="formula_12">p(κ k |· · ·) ∝ exp (n k log C M (κ k )+ κ k D d=1 N d n=1 [ϕ dn ] k x dn , E q [µ k ] × logNormal(κ k |m, σ 2 )</formula><p>where <ref type="bibr">[a]</ref> k denotes the k'th element of vector a. After computing the normalized weights, we can compute E q [κ k ] or expectation of any other function of κ k (e.g., E q [C M (κ k )]). The rest of the terms can be com- puted as follows:</p><formula xml:id="formula_13">n k = D d=1 N d d=1 [ϕ dn ] k and</formula><formula xml:id="formula_14">E q [µ k ] = E q I M/2 (κ k ) I M/2−1 (κ k ) ψ k , ψ k = E q [κ k ] D d=1 N d n=1 [ϕ dn ] k x dn + C 0 µ 0 ψ k ← ψ k ψ k 2 , [E q [log(π d )]] k = Ψ([θ d ] k ) − Ψ k [θ d ] k , [ϕ dn ] k ∝ exp (E q [log vMF(x dn |µ k , κ k )] + E q [log([π d ] k )]) , [θ d ] k =α + N d n=1 [ϕ dn ] k Ψ(·) is the digamma function.</formula><p>To find β * , similar to <ref type="bibr" target="#b10">Johnson and Willsky (2014)</ref>, we use the gradient expression of ELBO with respect to β and take a truncated gradient step on β ensuring β * ≥ 0.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average topic coherence for various base-
lines (HDP, Gaussian LDA (G-LDA)) and sHDP. 
k=number of topics. Best scores are shown in 
bold. 

</table></figure>

			<note place="foot" n="2"> http://qwone.com/ ˜ jason/20Newsgroups/ 3 http://www.cs.nyu.edu/ ˜ roweis/data. html 4 https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to Rajarshi Das for helping with the Gaus-sian LDA experiments and Matthew Johnson for his help with the HDP code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von mises-fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1345" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Truly nonparametric online variational inference for hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2699" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaussian LDA for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modeling data using directional distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
		<idno>TR-03-06</idno>
		<ptr target="ftp://ftp.cs.utexas.edu/pub/techreports/tr03-06.ps.gz" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Sciences, The University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Von misesfisher clustering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddarth</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic joint sentiment-topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Matthew D Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic variational inference for bayesian time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1854" to="1862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nested hierarchical dirichlet processes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chingyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="270" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spherical topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Silverthorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="903" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th conference on Uncertainty in artificial intelligence</title>
		<meeting>the 20th conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
