<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Resource Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21211</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21211</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21211</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Resource Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1177" to="1187"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We explore the extent to which high-resource manual annotations such as tree-banks are necessary for the task of semantic role labeling (SRL). We examine how performance changes without syntactic supervision, comparing both joint and pipelined methods to induce latent syntax. This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax. Our best models obtain competitive results in the high-resource setting and state-of-the-art results in the low resource setting, reaching 72.48% F1 averaged across languages. We release our code for this work along with a larger toolkit for specifying arbitrary graphical structure. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of semantic role labeling (SRL) is to identify predicates and arguments and label their semantic contribution in a sentence. Such labeling defines who did what to whom, when, where and how. For example, in the sentence "The kids ran the marathon", ran assigns a role to kids to denote that they are the runners; and a role to marathon to denote that it is the race course.</p><p>Models for SRL have increasingly come to rely on an array of NLP tools (e.g., parsers, lem- matizers) in order to obtain state-of-the-art re- sults <ref type="bibr" target="#b2">(Björkelund et al., 2009;</ref><ref type="bibr" target="#b34">Zhao et al., 2009)</ref>. Each tool is typically trained on hand-annotated data, thus placing SRL at the end of a very high- resource NLP pipeline. However, richly annotated data such as that provided in parsing treebanks is expensive to produce, and may be tied to specific domains (e.g., newswire). Many languages do <ref type="bibr">1</ref> http://www.cs.jhu.edu/ ˜ mrg/software/ not have such supervised resources (low-resource languages), which makes exploring SRL cross- linguistically difficult.</p><p>The problem of SRL for low-resource lan- guages is an important one to solve, as solutions pave the way for a wide range of applications: Ac- curate identification of the semantic roles of enti- ties is a critical step for any application sensitive to semantics, from information retrieval to machine translation to question answering.</p><p>In this work, we explore models that minimize the need for high-resource supervision. We ex- amine approaches in a joint setting where we marginalize over latent syntax to find the optimal semantic role assignment; and a pipeline setting where we first induce an unsupervised grammar. We find that the joint approach is a viable alterna- tive for making reasonable semantic role predic- tions, outperforming the pipeline models. These models can be effectively trained with access to only SRL annotations, and mark a state-of-the-art contribution for low-resource SRL.</p><p>To better understand the effect of the low- resource grammars and features used in these models, we further include comparisons with (1) models that use higher-resource versions of the same features; (2) state-of-the-art high resource models; and (3) previous work on low-resource grammar induction. In sum, this paper makes several experimental and modeling contributions, summarized below.</p><p>Experimental contributions:</p><p>• Comparison of pipeline and joint models for SRL.</p><p>• Subtractive experiments that consider the re- moval of supervised data.</p><p>• Analysis of the induced grammars in un- supervised, distantly-supervised, and joint training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling contributions:</head><p>• Simpler joint CRF for syntactic and semantic dependency parsing than previously reported.</p><p>• New application of unsupervised grammar induction: low-resource SRL.</p><p>• Constrained grammar induction using SRL for distant-supervision.</p><p>• Use of Brown clusters in place of POS tags for low-resource SRL.</p><p>The pipeline models are introduced in § 3.1 and jointly-trained models for syntactic and semantic dependencies (similar in form to <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref>) are introduced in § 3.2. In the pipeline models, we develop a novel approach to unsu- pervised grammar induction and explore perfor- mance using SRL as distant supervision. The joint models use a non-loopy conditional random field (CRF) with a global factor constraining latent syn- tactic edge variables to form a tree. Efficient exact marginal inference is possible by embedding a dy- namic programming algorithm within belief prop- agation as in <ref type="bibr" target="#b26">Smith and Eisner (2008)</ref>.</p><p>Even at the expense of no dependency path fea- tures, the joint models best pipeline-trained mod- els for state-of-the-art performance in the low- resource setting <ref type="bibr">( § 4.4)</ref>. When the models have ac- cess to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource set- ting on some languages <ref type="bibr">( § 4.3)</ref>.</p><p>Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar in- duction has an even greater impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work builds upon research in both seman- tic role labeling and unsupervised grammar in- duction ( <ref type="bibr" target="#b13">Klein and Manning, 2004;</ref><ref type="bibr" target="#b27">Spitkovsky et al., 2010a</ref>). Previous related approaches to se- mantic role labeling include joint classification of semantic arguments ( <ref type="bibr" target="#b31">Toutanova et al., 2005;</ref><ref type="bibr" target="#b11">Johansson and Nugues, 2008)</ref>, latent syntax induc- tion ( <ref type="bibr" target="#b4">Boxwell et al., 2011;</ref><ref type="bibr" target="#b21">Naradowsky et al., 2012)</ref>, and feature engineering for SRL ( <ref type="bibr" target="#b34">Zhao et al., 2009;</ref><ref type="bibr" target="#b2">Björkelund et al., 2009)</ref>. <ref type="bibr" target="#b31">Toutanova et al. (2005)</ref> introduced one of the first joint approaches for SRL and demon- strated that a model that scores the full predicate- argument structure of a parse tree could lead to significant error reduction over independent clas- sifiers for each predicate-argument relation. <ref type="bibr" target="#b11">Johansson and Nugues (2008)</ref> and <ref type="bibr" target="#b16">Lluís et al. (2013)</ref> extend this idea by coupling predictions of a dependency parser with predictions from a se- mantic role labeler. In the model from <ref type="bibr" target="#b11">Johansson and Nugues (2008)</ref>, the outputs from an SRL pipeline are reranked based on the full predicate- argument structure that they form. The candidate set of syntactic-semantic structures is reranked us- ing the probability of the syntactic tree and seman- tic structure. <ref type="bibr" target="#b16">Lluís et al. (2013)</ref> use a joint arc- factored model that predicts full syntactic paths along with predicate-argument structures via dual decomposition. <ref type="bibr" target="#b4">Boxwell et al. (2011)</ref> and <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref> observe that syntax may be treated as la- tent when a treebank is not available. <ref type="bibr" target="#b4">Boxwell et al. (2011)</ref> describe a method for training a se- mantic role labeler by extracting features from a packed CCG parse chart, where the parse weights are given by a simple ruleset. <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref> marginalize over latent syntactic depen- dency parses.</p><p>Both <ref type="bibr" target="#b4">Boxwell et al. (2011) and</ref><ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref> suggest methods for SRL without supervised syntax, however, their features come largely from supervised resources. Even in their lowest resource setting, <ref type="bibr" target="#b4">Boxwell et al. (2011)</ref> re- quire an oracle CCG tag dictionary extracted from a treebank. <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref> limit their exploration to a small set of basic features, and included high-resource supervision in the form of lemmas, POS tags, and morphology available from the CoNLL 2009 data.</p><p>There has not yet been a comparison of tech- niques for SRL that do not rely on a syntactic treebank, and no exploration of probabilistic mod- els for unsupervised grammar induction within an SRL pipeline that we have been able to find.</p><p>Related work for the unsupervised learning of dependency structures separately from semantic roles primarily comes from <ref type="bibr" target="#b13">Klein and Manning (2004)</ref>, who introduced the Dependency Model with Valence (DMV). This is a robust generative model that uses a head-outward process over word classes, where heads generate arguments. <ref type="bibr" target="#b27">Spitkovsky et al. (2010a)</ref> show that Viterbi (hard) EM training of the DMV with simple uni- form initialization of the model parameters yields higher accuracy models than standard soft-EM  <ref type="figure" target="#fig_3">Figure 1</ref>: Pipeline approach to SRL. In this sim- ple pipeline, the first stage syntactically parses the corpus, and the second stage predicts semantic predicate-argument structure for each sentence us- ing the labels of the first stage as features. In our low-resource pipelines, we assume that the syntac- tic parser is given no labeled parses-however, it may optionally utilize the semantic parses as dis- tant supervision. Our experiments also consider 'longer' pipelines that include earlier stages: a morphological analyzer, POS tagger, lemmatizer.</p><p>training. In Viterbi EM, the E-step finds the max- imum likelihood corpus parse given the current model parameters. The M-step then finds the maximum likelihood parameters given the corpus parse. We utilize this approach to produce unsu- pervised syntactic features for the SRL task. Grammar induction work has further demon- strated that distant supervision in the form of ACE-style relations (Naseem and Barzilay, 2011) or HTML markup ( <ref type="bibr" target="#b28">Spitkovsky et al., 2010b</ref>) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has sup- planted these methods with even higher accuracies ( <ref type="bibr" target="#b30">Spitkovsky et al., 2013</ref>) by arranging optimiz- ers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5.</p><p>In both pipeline and joint models, we use fea- tures adapted from state-of-the-art approaches to SRL. This includes <ref type="bibr" target="#b34">Zhao et al. (2009)</ref> features, who use feature templates from combinations of word properties, syntactic positions including head and children, and semantic properties; and features from <ref type="bibr" target="#b2">Björkelund et al. (2009)</ref>, who utilize features on syntactic siblings and the dependency path concatenated with the direction of each edge. Features are described further in § 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>We consider an array of models, varying:</p><p>1. Pipeline vs. joint training <ref type="figure" target="#fig_1">(Figures 1 and 2)</ref> 2. Types of supervision 3. The objective function at the level of syntax</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Syntax in the Pipeline</head><p>Typical SRL systems are trained following a pipeline where the first component is trained on supervised data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role la- beler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly super- vised fashion.</p><p>Brown Clusters We use fully unsupervised Brown clusters <ref type="bibr" target="#b5">(Brown et al., 1992</ref>) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition ( <ref type="bibr" target="#b20">Miller et al., 2004</ref>) and dependency parsing ( <ref type="bibr" target="#b14">Koo et al., 2008;</ref><ref type="bibr" target="#b29">Spitkovsky et al., 2011</ref>). The clusters are formed by a greedy hierachi- cal clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a fine- grained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsen- ing of the clusters by truncating the bit-strings. We train 1000 Brown clusters for each of the CoNLL- 2009 languages on Wikipedia text. <ref type="bibr">2</ref> Unsupervised Grammar Induction Our first method for grammar induction is fully unsuper- vised Viterbi EM training of the Dependency Model with Valence (DMV) ( <ref type="bibr" target="#b13">Klein and Manning, 2004)</ref>, with uniform initialization of the model pa- rameters. We define the DMV such that it gener- ates sequences of word classes: either POS tags or Brown clusters as in <ref type="bibr" target="#b29">Spitkovsky et al. (2011)</ref>. The DMV is a simple generative model for pro- jective dependency trees. Children are generated recursively for each node. Conditioned on the par- ent class, the direction (right or left), and the cur- rent valence (first child or not), a coin is flipped to decide whether to generate another child; the dis- tribution over child classes is conditioned on only the parent class and direction.</p><p>Constrained Grammar Induction Our second method, which we will refer to as DMV+C, in- duces grammar in a distantly supervised fashion by using a constrained parser in the E-step of Viterbi EM. Since the parser is part of a pipeline, we constrain it to respect the downstream SRL an- notations during training. At test time, the parser is unconstrained.</p><p>Dependency-based semantic role labeling can be described as a simple structured prediction problem: the predicted structure is a labeled di- rected graph, where nodes correspond to words in the sentence. Each directed edge indicates that there is a predicate-argument relationship between the two words; the parent is the predicate and the child the argument. The label on the edge indi- cates the type of semantic relationship. Unlike syntactic dependency parsing, the graph is not re- quired to be a tree, nor even a connected graph. Self-loops and crossing arcs are permitted.</p><p>The constrained syntactic DMV parser treats the semantic graph as observed, and constrains the syntactic parent to be chosen from one of the se- mantic parents, if there are any. In some cases, imposing this constraint would not permit any pro- jective dependency parses-in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm <ref type="bibr" target="#b33">(Younger, 1967;</ref><ref type="bibr" target="#b0">Aho and Ullman, 1972)</ref> by utilizing a PCFG corresponding to the DMV ( <ref type="bibr" target="#b6">Cohn et al., 2010)</ref>. Each chart cell al- lows only non-terminals compatible with the con- strained sets. This can be viewed as a variation of <ref type="bibr" target="#b23">Pereira and Schabes (1992)</ref>. Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We de- fine a conditional random field (CRF) ( <ref type="bibr" target="#b15">Lafferty et al., 2001</ref>) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n has n 2 possible edges. We define a single L+1-ary variable for each edge, whose value can be any of L semantic labels or a special label indi- cating there is no predicate-argument relationship between the two words. In this way, we jointly perform identification (determining whether a se- mantic relationship exists) and classification (de- termining the semantic label). This use of an L+1- ary variable is in contrast to the model of <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref>, which used a more complex  set of binary variables and required a constraint factor permitting AT-MOST-ONE. We include one unary factor for each variable.</p><p>We optionally include additional variables that perform word sense disambiguation for each pred- icate. Each has a unary factor and is completely disconnected from the semantic edge (similar to <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref>). These variables range over all the predicate senses observed in the train- ing data for the lemma of that predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Syntactic and Semantic Parsing Model</head><p>In Section 3.1, we introduced pipeline-trained models for SRL, which used grammar induction to predict unlabeled syntactic parses. In this sec- tion, we define a simple model for joint syntactic and semantic dependency parsing. This model extends the CRF model in Section 3.1 to include the projective syntactic dependency parse for a sentence. This is done by includ- ing an additional n 2 binary variables that indicate whether or not a directed syntactic dependency edge exists between a pair of words in the sen- tence. Unlike the semantic dependencies, these syntactic variables must be coupled so that they produce a projective dependency parse; this re- quires an additional global constraint factor to en- sure that this is the case ( <ref type="bibr" target="#b26">Smith and Eisner, 2008)</ref>. The constraint factor touches all n 2 syntactic-edge variables, and multiplies in 1.0 if they form a pro- jective dependency parse, and 0.0 otherwise. We couple each syntactic edge variable to its semantic edge variable with a binary factor. <ref type="figure" target="#fig_1">Figure 2</ref> shows the factor graph for this joint model.</p><p>Note that our factor graph does not contain any loops, thereby permitting efficient exact marginal inference just as in <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref>. We  word form all word forms 2 lower case word form all lower-case forms 3</p><p>5-char word form prefixes all 5-char form prefixes 4 capitalization True, False 5 top-800 word form top-800 word forms 6 brown cluster 000, 1100, 010110001, ... 7</p><p>brown cluster, length 5 length 5 prefixes of brown clusters 8 lemma all word lemmas 9 POS tag NNP, CD, JJ, DT, ..  train our CRF models by maximizing conditional log-likelihood using stochastic gradient descent with an adaptive learning rate (AdaGrad) (Duchi et al., 2011) over mini-batches. The unary and binary factors are defined with exponential family potentials. In the next section, we consider binary features of the observations (the sentence and labels from previous pipeline stages) which are conjoined with the state of the variables in the factor.</p><note type="other">. 10 morphological features Gender, Case, Number, ... (different across languages) 11 dependency label SBJ, NMOD, LOC, ... 12 edge direction Up, Down</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Features for CRF Models</head><p>Our feature design stems from two key ideas. First, for SRL, it has been observed that fea- ture bigrams (the concatenation of simple fea- tures such as a predicate's POS tag and an ar- gument's word) are important for state-of-the-art ( <ref type="bibr" target="#b34">Zhao et al., 2009;</ref><ref type="bibr" target="#b2">Björkelund et al., 2009</ref>). Sec- ond, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data ( <ref type="bibr" target="#b14">Koo et al., 2008)</ref>.</p><p>We create binary indicator features for each model using feature templates. Our feature tem- plate definitions build from those used by the top performing systems in the CoNLL-2009 Shared Task, <ref type="bibr" target="#b34">Zhao et al. (2009)</ref> and <ref type="bibr" target="#b2">Björkelund et al. (2009)</ref> and from features in syntactic dependency parsing <ref type="bibr" target="#b19">(McDonald et al., 2005;</ref><ref type="bibr" target="#b14">Koo et al., 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template</head><p>Possible values relative position before, after, on distance, continuity Z + binned distance &gt; 2, 5, 10, 20, 30, or 40 geneological relationship parent, child, ancestor, descendant path-grams the NN went  <ref type="formula">(2009)</ref>, we include the notion of verb and noun supports and sections of the dependency path. Also following <ref type="bibr" target="#b34">Zhao et al. (2009)</ref>, properties from a set of positions can be put together in three possible orders: as the given sequence, as a sorted list of unique strings, and re- moving all duplicated neighbored strings. We con- sider both template unigrams and bigrams, com- bining two templates in sequence. Additional templates we include are the relative position <ref type="bibr" target="#b2">(Björkelund et al., 2009)</ref>, geneological re- lationship, distance ( <ref type="bibr" target="#b34">Zhao et al., 2009)</ref>, and binned distance ( <ref type="bibr" target="#b14">Koo et al., 2008</ref>) between two words in the path. From <ref type="bibr" target="#b16">Lluís et al. (2013)</ref>, we use 1, 2, 3- gram path features of words/POS tags (path-grams), and the number of non-consecutive token pairs in a predicate-argument path (continuity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature Selection</head><p>Constructing all feature template unigrams and bi- grams would yield an unwieldy number of fea- tures. We therefore determine the top N template bigrams for a dataset and factor a according to an information gain measure <ref type="bibr" target="#b18">(Martins et al., 2011)</ref>:</p><formula xml:id="formula_0">IG a,m = f ∈Tm xa p(f, x a ) log 2 p(f, x a ) p(f )p(x a )</formula><p>where T m is the mth feature template, f is a par- ticular instantiation of that template, and x a is an assignment to the variables in factor a. The proba- bilities are empirical estimates computed from the training data. This is simply the mutual informa- tion of the feature template instantiation with the variable assignment. This filtering approach was treated as a sim- ple baseline in <ref type="bibr" target="#b18">Martins et al. (2011)</ref> to contrast with increasingly popular gradient based regular- ization approaches. Unlike the gradient based ap-proaches, this filtering approach easily scales to many features since we can decompose the mem- ory usage over feature templates.</p><p>As an additional speedup, we reduce the dimen- sionality of our feature space to 1 million for each clique using a common trick referred to as fea- ture hashing <ref type="bibr" target="#b32">(Weinberger et al., 2009)</ref>: we map each feature instantiation to an integer using a hash function 3 modulo the desired dimentionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We are interested in the effects of varied super- vision using pipeline and joint training for SRL. To compare to prior work (i.e., submissions to the CoNLL-2009 Shared Task), we also consider the joint task of semantic role labeling and predicate sense disambiguation. Our experiments are sub- tractive, beginning with all supervision available and then successively removing (a) dependency syntax, (b) morphological features, (c) POS tags, and (d) lemmas. Dependency syntax is the most expensive and difficult to obtain of these various forms of supervision. We explore the importance of both the labels and structure, and what quantity of supervision is useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>The <ref type="bibr">CoNLL-2009</ref><ref type="bibr">Shared Task (Hajič et al., 2009</ref>) dataset contains POS tags, lemmas, morpholog- ical features, syntactic dependencies, predicate senses, and semantic roles annotations for 7 lan- guages: Catalan, Chinese, Czech, English, Ger- man, Japanese, 4 Spanish. The <ref type="bibr">CoNLL-2005 and</ref> Shared Task datasets provide English SRL annotation, and for cross dataset comparability we consider only verbal predicates (more details in § 4.4). To compare with prior approaches that use semantic supervision for grammar induction, we utilize Section 23 of the WSJ portion of the Penn Treebank (Marcus et al., 1993).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Template Sets</head><p>Our primary feature set IG C consists of 127 tem- plate unigrams that emphasize coarse properties (i.e., properties 7, 9, and 11 in <ref type="table" target="#tab_1">Table 1</ref>). We also explore the 31 template unigrams 5 IG B described <ref type="bibr">3</ref> To reduce hash collisions, We use MurmurHash v3 https://code.google.com/p/smhasher. <ref type="bibr">4</ref> We do not report results on Japanese as that data was only made freely available to researchers that competed in CoNLL 2009. <ref type="bibr">5</ref> Because we do not include a binary factor between pred- icate sense and semantic role, we do not include sense as a by <ref type="bibr" target="#b2">Björkelund et al. (2009)</ref>. Each of IG C and IG B also include 32 template bigrams selected by in- formation gain on 1000 sentences-we select a different set of template bigrams for each dataset.</p><p>We compare against the language-specific fea- ture sets detailed in the literature on high-resource top-performing SRL systems <ref type="bibr">: From Björkelund et al. (2009)</ref>, these are feature sets for German, En- glish, Spanish and Chinese, obtained by weeks of forward selection (B de,en,es,zh ); and from <ref type="bibr" target="#b34">Zhao et al. (2009)</ref>, these are features for Catalan Z ca . <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">High-resource SRL</head><p>We first compare our models trained as a pipeline, using all available supervision (syntax, morphol- ogy, POS tags, lemmas) from the CoNLL-2009 data. <ref type="table" target="#tab_5">Table 4</ref>(a) shows the results of our model with gold syntax and a richer feature set than that of <ref type="bibr" target="#b21">Naradowsky et al. (2012)</ref>, which only looked at whether a syntactic dependency edge was present. This highlights an important advan- tage of the pipeline trained model: the features can consider any part of the syntax (e.g., arbitrary sub- trees), whereas the joint model is limited to those features over which it can efficiently marginalize (e.g., short dependency paths). This holds true even in the pipeline setting where no syntactic su- pervision is available. <ref type="table" target="#tab_5">Table 4</ref>(b) contrasts our high-resource results for the task of SRL and sense disambiguation with the top systems in the CoNLL-2009 Shared Task, giving further insight into the performance of the simple information gain feature selection technique. With supervised syntax, our sim- ple information gain feature selection technique ( § 3.4) performs admirably. However, the orig- inal unigram Björkelund features (B de,en,es,zh ), which were tuned for a high-resource model, ob- tain higher F1 than our information gain set us- ing the same features in unigram and bigram tem- plates (IG B ). This suggests that further work on feature selection may improve the results. We find that IG B obtain higher F1 than the original Björkelund feature sets (B de,en,es,zh ) in the low- resource pipeline setting with constrained gram- mar induction (DMV+C).</p><p>feature for argument prediction.     In the low-resource setting, training and decod- ing times for the pipeline and joint methods are similar as computation time tends to be dominated by feature extraction.</p><p>These results begin to answer a key research question in this work: The joint models outper- form the pipeline models in the low-resource set- ting. This holds even when using the same feature selection process. Further, the best-performing low-resource features found in this work are those based on coarse feature templates and selected by information gain. Templates for these fea- tures generalize well to the high-resource setting. However, analysis of the induced grammars in the pipeline setting suggests that the book is not closed on the issue. We return to this in § 4.5. <ref type="bibr">-2008, -2005</ref> To finish out comparisons with state-of-the-art SRL, we contrast our ap- proach with that of <ref type="bibr" target="#b4">Boxwell et al. (2011)</ref>, who evaluate on SRL in isolation (without sense disam- biguation, as in CoNLL-2009). They report results on Prop-CCGbank ( <ref type="bibr" target="#b3">Boxwell and White, 2008)</ref>, which uses the same training/testing splits as the CoNLL-2005 Shared Task. Their results are there- fore loosely 7 comparable to results on the CoNLL- 2005 dataset, which we can compare here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoNLL</head><p>There is an additional complication in com- paring SRL approaches directly: The CoNLL- 2005 dataset defines arguments as spans instead of <ref type="bibr">7</ref> The comparison is imperfect for two reasons: first, the CCGBank contains only 99.44% of the original PTB sen- tences <ref type="bibr" target="#b10">(Hockenmaier and Steedman, 2007)</ref>; second, because PropBank was annotated over CFGs, after converting to CCG only 99.977% of the argument spans were exact matches <ref type="bibr" target="#b3">(Boxwell and White, 2008)</ref>. However, this comparison was adopted by <ref type="bibr" target="#b4">Boxwell et al. (2011)</ref>, so we use it here. heads, which runs counter to our head-based syn- tactic representation. This creates a mismatched train/test scenario: we must train our model to pre- dict argument heads, but then test on our models ability to predict argument spans. <ref type="bibr">8</ref> We therefore train our models on the CoNLL-2008 argument heads, 9 and post-process and convert from heads to spans using the conversion algorithm available from <ref type="bibr" target="#b11">Johansson and Nugues (2008)</ref>. <ref type="bibr">10</ref> The heads are either from an MBR tree or an oracle tree. This gives Boxwell et al. (2011) the advantage, since our syntactic dependency parses are optimized to pick out semantic argument heads, not spans. <ref type="table" target="#tab_6">Table 5</ref> presents our results. Boxwell et al. (2011) (B'11) uses additional supervision in the form of a CCG tag dictionary derived from su- pervised data with (tdc) and without (tc) a cut- off. Our model does very poorly on the '05 span- based evaluation because the constituent bracket- ing of the marginalized trees are inaccurate. This is elucidated by instead evaluating on the ora- cle spans, where our F1 scores are higher than <ref type="bibr" target="#b4">Boxwell et al. (2011)</ref>. We also contrast with rela- vant high-resource methods with span/head con- versions from <ref type="bibr" target="#b11">Johansson and Nugues (2008)</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>: Pun- yakanok et al. (2008) (PRY'08) and Johansson and Nugues (2008) (JN'08).</head><p>Subtractive Study In our subsequent experi- ments, we study the effectiveness of our models as the available supervision is decreased. We in- crementally remove dependency syntax, morpho- logical features, POS tags, then lemmas. For these experiments, we utilize the coarse-grained feature set (IG C ), which includes Brown clusters.</p><p>Across languages, we find the largest drop in F1 when we remove POS tags; and we find a gain in F1 when we remove lemmas. This indi- cates that lemmas, which are a high-resource an- notation, may not provide a significant benefit for this task. The effect of removing morphological features is different across languages, with little change in performance for Catalan and Spanish, <ref type="bibr">8</ref> We were unable to obtain the system output of <ref type="bibr" target="#b4">Boxwell et al. (2011)</ref> in order to convert their spans to dependencies and evaluate the other mismatched train/test setting.</p><p>9 <ref type="bibr">CoNLL-2005</ref><ref type="bibr">CoNLL- , -2008</ref> were derived from Prop- Bank and share the same source text; -2008 and -2009 use argument heads. <ref type="bibr">10</ref> Specifically, we use their Algorithm 2, which produces the span dominated by each argument, with special handling of the case when the argument head dominates that of the predicate. Also following <ref type="bibr" target="#b11">Johansson and Nugues (2008)</ref>, we recover the '05 sentences missing from the '08 evaluation set.   but a drop in performance for German. This may reflect a difference between the languages, or may reflect the difference between the annotation of the languages: both the Catalan and Spanish data orig- inated from the Ancora project, 11 while the Ger- man data came from another source. <ref type="figure" target="#fig_6">Figure 3</ref> contains the learning curve for SRL su- pervision in our lowest resource setting for two example languages, Catalan and German. This shows how F1 of SRL changes as we adjust the number of training examples. We find that the joint training approach to grammar induction yields consistently higher SRL performance than its distantly supervised counterpart. <ref type="table" target="#tab_10">Table 7</ref> shows grammar induction accuracy in low-resource settings. We find that the gap be- tween the supervised parser and the unsupervised methods is quite large, despite the reasonable ac- curacy both methods achieve for the SRL end task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Grammar Induction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parser</head><p>Avg. ca cs de en es zh     This suggests that refining the low-resource gram- mar induction methods may lead to gains in SRL. Interestingly, the marginalized grammars best the DMV grammar induction method; however, this difference is less pronounced when the DMV is constrained using SRL labels as distant super- vision. This could indicate that a better model for grammar induction would result in better perfor- mance for SRL. We therefore turn to an analysis of other approaches to grammar induction in <ref type="table">Table 8</ref>, evaluated on the Penn Treebank. We contrast with methods using distant supervision ( <ref type="bibr" target="#b22">Naseem and Barzilay, 2011;</ref><ref type="bibr" target="#b28">Spitkovsky et al., 2010b</ref>) and fully unsupervised dependency parsing ( <ref type="bibr" target="#b30">Spitkovsky et al., 2013)</ref>. Following prior work, we exclude punctuation from evaluation and convert the con- stituency trees to dependencies. <ref type="bibr">12</ref> The approach from <ref type="bibr" target="#b30">Spitkovsky et al. (2013)</ref> 12 <ref type="bibr" target="#b22">Naseem and Barzilay (2011)</ref> and our results use the Penn converter <ref type="bibr" target="#b24">(Pierre and Heiki-Jaan, 2007)</ref>. <ref type="bibr" target="#b28">Spitkovsky et al. (2010b;</ref> use Collins (1999) head percolation rules.</p><p>(SAJ'13) outperforms all other approaches, in- cluding our marginalized settings. We therefore may be able to achieve further gains in the pipeline model by considering better models of latent syn- tax, or better search techniques that break out of local optima. Similarly, improving the non- convex optimization of our latent-variable CRF (Marginalized) may offer further gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>We have compared various approaches for low- resource semantic role labeling at the state-of-the- art level. We find that we can outperform prior work in the low-resource setting by coupling the selection of feature templates based on informa- tion gain with a joint model that marginalizes over latent syntax.</p><p>We utilize unlabeled data in both generative and discriminative models for dependency syntax and in generative word clustering. Our discriminative joint models treat latent syntax as a structured- feature to be optimized for the end-task of SRL, while our other grammar induction techniques op- timize for unlabeled data likelihood-optionally with distant supervision. We observe that careful use of these unlabeled data resources can improve performance on the end task.</p><p>Our subtractive experiments suggest that lemma annotations, a high-resource annotation, may not provide a large benefit for SRL. Our grammar in- duction analysis indicates that relatively low accu- racy can still result in reasonable SRL predictions; still, the models do not outperform those that use supervised syntax, and we aim to explore how well the pipeline models in particular improve when we apply higher accuracy unsupervised grammar in- duction techniques.</p><p>We have utilized well studied datasets in order to best understand the quality of our models rela- tive to prior work. In future work, we hope to ex- plore the effectiveness of our approaches on truly low resource settings by using crowdsourcing to develop semantic role datasets in other languages and domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Factor graph for the joint syntactic/semantic dependency parsing model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4</head><label></label><figDesc>.4 Low-Resource SRL CoNLL-2009 Table 4(c) includes results for our low-resource approaches and Naradowsky et al. (2012) on predicting semantic roles as well as sense. In the low-resource setting of the CoNLL- 2009 Shared task without syntactic supervision, our joint model (Joint) with marginalized syntax obtains state-of-the-art results with features IG C described in § 4.2. This model outperforms prior work (Naradowsky et al., 2012) and our pipeline model (Pipeline) with contrained (DMV+C) and unconstrained grammar induction (DMV) trained on brown clusters (bc).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curve for semantic dependency supervision in Catalan and German. F1 of SRL only (without sense disambiguation) shown as the number of training sentences is increased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Supervised</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>8 :</head><label>8</label><figDesc>Comparison of grammar induction ap- proaches. We contrast the DMV trained with Viterbi EM+uniform initialization (DMV), our constrained DMV (DMV+C), and our model's MBR decoding of latent syntax (Marginalized) with other recent work: Spitkovsky et al. (2010a) (SAJM'10), Spitkovsky et al. (2010b) (SJA'10), Naseem and Barzilay (2011) (NB'11), and the CS model of Spitkovsky et al. (2013) (SAJ'13).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Word and edge properties in templates. 

i, i-1, i+1 
noFarChildren(wi) linePath(wp, wc) 
parent(wi) 
rightNearSib(wi) 
depPath(wp, wc) 
allChildren(wi) 
leftNearSib(wi) 
depPath(wp, w lca ) 
rightNearChild(wi) firstVSupp(wi) 
depPath(wc, w lca ) 
rightFarChild(wi) lastVSupp(wi) 
depPath(w lca , wroot) 
leftNearChild(wi) firstNSupp(wi) 
leftFarChild(wi) 
lastNSupp(wi) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Word positions used in templates. Based 
on current word position (i), positions related to 
current word w i , possible parent, child (w p , w c ), 
lowest common ancestor between parent/child 
(w lca ), and syntactic root (w root ). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Additional standalone templates.</head><label>3</label><figDesc></figDesc><table>Template Creation Feature templates are de-
fined over triples of property, positions, order. 
Properties, listed in Table 1, are extracted from 
word positions within the sentence, shown in Ta-
ble 2. Single positions for a word w i include 
its syntactic parent, its leftmost farthest child 
(leftFarChild), its rightmost nearest sibling (rightNearSib), 
etc. Following Zhao et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource 
settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are 
ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision. 

*Indicates partial averages for the language-specific feature sets (Zca and B de,en,es,zh ), for which we show results only on the 
languages for which the sets were publicly available. 

train 
test 

2008 
heads 
2005 
spans 
2005 
spans 
(oracle 
tree) 

PRY'08 

2005 
spans 

84.32 79.44 
B'11 (tdc) 
-
71.5 
B'11 (td) 
-
65.0 
JN'08 

2008 
heads 

85.93 79.90 
Joint, IGC 
72.9 
35.0 
72.0 
Joint, IGB 
67.3 
37.8 
67.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>F1 for SRL approaches (without sense 
disambiguation) in matched and mismatched 
train/test settings for CoNLL 2005 span and 2008 
head supervision. We contrast low-resource () 
and high-resource settings ( ), where latter uses a 
treebank. See  § 4.4 for caveats to this comparison. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Subtractive experiments. Each row con-
tains the F1 for SRL only (without sense disam-
biguation) where the supervision type of that row 
and all above it have been removed. Removed su-
pervision types (Rem) are: syntactic dependencies 
(Dep), morphology (Mor), POS tags (POS), and 
lemmas (Lem). #FT indicates the number of fea-
ture templates used (unigrams+bigrams). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Unlabeled directed dependency accuracy 
on CoNLL'09 test set in low-resource settings. 
DMV models are trained on either POS tags (pos) 
or Brown clusters (bc). *Indicates the supervised parser 

outputs provided by the CoNLL'09 Shared Task. 

WSJ ∞ Distant 
Supervision 
SAJM'10 
44.8 none 
SAJ'13 
64.4 none 
SJA'10 
50.4 HTML 
NB'11 
59.4 ACE05 
DMV (bc) 
24.8 none 
DMV+C (bc) 
44.8 SRL 
Marginalized, IGC 
48.8 SRL 
Marginalized, IGB 
58.9 SRL 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> The Wikipedia text was tokenized for Polyglot (Al-Rfou&apos; et al., 2013): http://bit.ly/embeddings</note>

			<note place="foot" n="6"> This covers all CoNLL languages but Czech, where feature sets were not made publicly available in either work. In Czech, we disallowed template bigrams involving path-grams.</note>

			<note place="foot" n="11"> http://clic.ub.edu/corpus/ancora</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Theory of Parsing, Translation, and Compiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Computational Natural Language Learning</title>
		<meeting>the 17th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilingual semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Love</forename><surname>Hafdell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning</title>
		<meeting>the 13th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Shared Task</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Projecting propbank roles onto the CCGbank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2008). European Language Resources Association</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2008). European Language Resources Association</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic role labeling without treebanks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Mehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP). Asian Federation of Natural Language Processing</title>
		<meeting>the 5th International Joint Conference on Natural Language Processing (IJCNLP). Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inducing tree-substitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning</title>
		<meeting>the 13th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Shared Task</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dependency-based semantic role labeling of PropBank</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CorpusBased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL 2004). Association for Computational Linguistics</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL 2004). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint arc-factored parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured sparsity in structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Aguiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Name tagging with word clusters and discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jethran</forename><surname>Guinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zamanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004: Main Proceedings. Association for Computational Linguistics</title>
		<editor>Susan Dumais, Daniel Marcu, and Salim Roukos</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving NLP through marginalization of hidden syntactic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2012 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using semantic cues to learn syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Insideoutside reestimation from partially bracketed corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 30th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extended constituent-to-dependency conversion for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nugues</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalep</forename><surname>Heiki-Jaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NODALIDA 2007 Proceedings</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference on Computational Natural Language Learning</title>
		<meeting>the 14th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Profiting from mark-up: Hyper-text annotations for guided parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing without gold part-of-speech tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint learning improves semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005). Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<editor>Léon Bottou and Michael Littman</editor>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Recognition and parsing of context-free languages in time n 3. Information and Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Younger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Computational Natural Language Learning</title>
		<meeting>the 13th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Shared Task. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
