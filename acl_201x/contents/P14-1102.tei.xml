<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bootstrapping into Filler-Gap: An Acquisition Story</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bootstrapping into Filler-Gap: An Acquisition Story</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1084" to="1093"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives. Therefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g. SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language. Specifically, this model, trained on part-of-speech tags, represents the preferred locations of semantic roles relative to a verb as Gaussian mixtures over real numbers. This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance. Additionally, this model is shown to be able to account for a characteristic error made by learners during this period (A and B gorped interpreted as A gorped B).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The phenomenon of filler-gap, where the argument of a predicate appears outside its canonical posi- tion in the phrase structure (e.g. [the apple] i that the boy ate t i or [what] i did the boy eat t i ), has long been an object of study for syntacticians <ref type="bibr" target="#b28">(Ross, 1967)</ref> due to its apparent processing complexity. Such complexity is due, in part, to the arbitrary length of the dependency between a filler and its gap (e.g. [the apple] i that Mary said the boy ate t i ).</p><p>Recent studies indicate that comprehension of filler-gap constructions begins around 15 months ( <ref type="bibr" target="#b30">Seidl et al., 2003;</ref><ref type="bibr" target="#b12">Gagliardi et al., 2014</ref>). This finding raises the question of how such a complex phenomenon could be acquired so early since chil- dren at that age do not yet have a very advanced grasp of language (e.g. ditransitives do not seem to be generalized until at least 31 months; <ref type="bibr" target="#b16">Goldberg et al. 2004</ref><ref type="bibr" target="#b1">, Bello 2012</ref>). This work shows that filler-gap comprehension in English may be The developmental timeline of subject (Wh-S) and object (Wh-O) wh-clause extraction comprehension suggested by experimental results ( <ref type="bibr" target="#b30">Seidl et al., 2003;</ref><ref type="bibr" target="#b12">Gagliardi et al., 2014</ref>). Paren- theses indicate weak comprehension. The final row shows the timeline of 1-1 role bias errors <ref type="bibr" target="#b24">(Naigles, 1990;</ref><ref type="bibr" target="#b13">Gertner and Fisher, 2012</ref>). Missing nodes de- note a lack of studies. acquired through learning word orderings rather than relying on hierarchical syntactic knowledge.</p><p>This work describes a cognitive model of the de- velopmental timecourse of filler-gap comprehension with the goal of setting a lower bound on the mod- eling assumptions necessary for an ideal learner to display filler-gap comprehension. In particular, the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles. These orderings then permit the model to successfully resolve filler-gap depen- dencies. 1 Further, the model presented here is also shown to initially reflect an idiosyncratic role as- signment error observed in development (e.g. A and B kradded interpreted as A kradded B ; Gert- ner and Fisher, 2012), though after training, the model is able to avoid the error. As such, this work may be said to model a learner from 15 months to between 25 and 30 months.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The developmental timeline during which children acquire the ability to process filler-gap construc- tions is not well-understood. Language comprehen- sion precedes production, and the developmental literature on the acquisition of filler-gap construc- tions is sparsely populated due to difficulties in de- signing experiments to test filler-gap comprehen- sion in preverbal infants. Older studies typically looked at verbal children and the mistakes they make to gain insight into the acquisition process (de <ref type="bibr" target="#b9">Villiers and Roeper, 1995)</ref>.</p><p>Recent studies, however, indicate that filler- gap comprehension likely begins earlier than pro- duction ( <ref type="bibr" target="#b30">Seidl et al., 2003;</ref><ref type="bibr" target="#b11">Gagliardi and Lidz, 2010;</ref><ref type="bibr" target="#b12">Gagliardi et al., 2014</ref>). Therefore, studies of verbal children are probably actually testing the acquisition of production mechanisms (plan- ning, motor skills, greater facility with lexical ac- cess, etc) rather than the acquisition of filler- gap. Note that these may be related since filler- gap could introduce greater processing load which could overwhelm the child's fragile production ca- pacity <ref type="bibr" target="#b25">(Phillips, 2010)</ref>. <ref type="bibr" target="#b30">Seidl et al. (2003)</ref> showed that children are able to process wh-extractions from subject position (e.g. [who] i t i ate pie) as young as 15 months while similar extractions from object position (e.g.</p><p>[what] i did the boy eat t i ) remain unparseable until around 20 months of age. <ref type="bibr">2</ref> This line of investiga- tion has been reopened and expanded by <ref type="bibr" target="#b12">Gagliardi et al. (2014)</ref> whose results suggest that the ex- perimental methodology employed by <ref type="bibr" target="#b30">Seidl et al. (2003)</ref> was flawed in that it presumed infants have ideal performance mechanisms. By providing more trials of each condition and controlling for the prag- matic felicity of test statements, <ref type="bibr" target="#b12">Gagliardi et al. (2014)</ref> provide evidence that 15-month old infants can process wh-extractions from both subject and object positions. Object extractions are more diffi- cult to comprehend than subject extractions, how- ever, perhaps due to additional processing load in object extractions <ref type="bibr" target="#b14">(Gibson, 1998;</ref><ref type="bibr" target="#b25">Phillips, 2010)</ref>. Similarly, <ref type="bibr" target="#b11">Gagliardi and Lidz (2010)</ref> show that rel- ativized extractions with a wh-relativizer (e.g. find [the boy] i who t i ate the apple) are easier to com- prehend than relativized extractions with that as the relativizer (e.g. find [the boy] i that t i ate the apple). <ref type="bibr" target="#b35">Yuan et al. (2012)</ref> demonstrate that 19-month olds use their knowledge of nouns to learn both verbs and their associated argument structure. In their study, infants were shown video of a person talking on a phone using a nonce verb with ei- ther one or two nouns (e.g. Mary kradded Susan). Under the assumption that infants look longer at things that correspond to their understanding of a prompt, the infants were then shown two im- ages that potentially depicted the described action -one picture where two actors acted independently (reflecting an intransitive proposition) and one pic- ture where one actor acted on the other (reflecting a transitive proposition). 3 Even though the infants had no extralinguistic knowledge about the verb, they consistently treated the verb as transitive if two nouns were present and intransitive if only one noun was present.</p><p>Similarly, <ref type="bibr" target="#b13">Gertner and Fisher (2012)</ref> show that intransitive phrases with conjoined subjects (e.g. John and Mary gorped ) are given a transitive in- terpretation (i.e. John gorped Mary) at 21 months (henceforth termed '1-1 role bias'), though this ef- fect is no longer present at 25 months <ref type="bibr" target="#b24">(Naigles, 1990)</ref>. This finding suggests both that learners will ignore canonical structure in favor of using all possible arguments and that children have a bias to assign a unique semantic role to each argu- ment. It is important to note, however, that cross- linguistically children do not seem to generalize be- yond two arguments until after at least 31 months of age ( <ref type="bibr" target="#b16">Goldberg et al., 2004;</ref><ref type="bibr" target="#b1">Bello, 2012)</ref>, so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive.</p><p>Computational modeling provides a way to test the computational level of processing <ref type="bibr" target="#b23">(Marr, 1982)</ref>. That is, given the input (child-directed speech, adult-directed speech, and environmental experi- ences), it is possible to probe the computational processes that result in the observed output. How- ever, previous computational models of grammar induction ( <ref type="bibr" target="#b20">Klein and Manning, 2004</ref>), including in- fant grammar induction ( <ref type="bibr" target="#b21">Kwiatkowski et al., 2012)</ref>, have not addressed filler-gap comprehension. <ref type="bibr">4</ref> The closest work to that presented here is the work on BabySRL ( <ref type="bibr" target="#b5">Connor et al., 2008;</ref><ref type="bibr" target="#b6">Connor et al., 2009;</ref><ref type="bibr" target="#b7">Connor et al., 2010)</ref>. BabySRL is a com- putational model of semantic role acquistion using a similar set of assumptions to the current work. BabySRL learns weights over ordering constraints (e.g. preverbal, second noun, etc.) to acquire se- mantic role labelling while still exhibiting 1-1 role bias. However, no analysis has evaluated the abil-Susan said John gave girl book -3 -2 -1 0 1 2 <ref type="table">Table 1</ref>: An example of a chunked sentence (Su- san said John gave the girl a red book ) with the sentence positions labelled. Nominal heads of noun chunks are in bold.</p><p>ity of BabySRL to acquire filler-gap constructions. Further comparison to BabySRL may be found in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Assumptions</head><p>The present work restricts itself to acquiring filler- gap comprehension in English. The model pre- sented here learns a single, non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple lin- ear grammars that are dictated by semantic roles <ref type="bibr" target="#b10">(Diessel and Tomasello, 2001</ref>; Jackendoff and Wit- tenberg, in press). This work assumes learners can already identify nouns and verbs, which is sup- ported by <ref type="bibr" target="#b31">Shi et al. (1999)</ref> who show that chil- dren at an extremely young age can distinguish be- tween content and function words and by <ref type="bibr" target="#b34">Waxman and Booth (2001)</ref> who show that children can dis- tinguish between different types of content words. Further, since <ref type="bibr" target="#b34">Waxman and Booth (2001)</ref> demon- strate that, by 14 months, children are able to dis- tinguish nouns from modifiers, this work assumes learners can already chunk nouns and access the nominal head. To handle recursion, this work as- sumes that children treat the final verb in each sentence as the main verb (implicitly assuming sen- tence segmentation), which ideally assigns roles to each of the nouns in the sentence. Due to the findings of <ref type="bibr" target="#b35">Yuan et al. (2012)</ref>, this work adopts a 'syntactic bootstrapping' the- ory of acquisition <ref type="bibr" target="#b15">(Gleitman, 1990)</ref>, where struc- tural properties (e.g. number of nouns) inform the learner about semantic properties of a predicate (e.g. how many semantic roles it confers). Since infants infer the number of semantic roles, this work further assumes they already have expecta- tions about where these roles tend to be realized in sentences, if they appear. These positions may correspond to different semantic roles for different predicates (e.g. the subject of run and of melt); however, the role for predicates with a single argu- ment is usually assigned to the noun that precedes the verb while a second argument is usually as- signed after the verb. The semantic properties of these roles may be learned lexically for each pred- icate, but that is beyond the scope of this work. Therefore, this work uses syntactic and semantic roles interchangeably (e.g. subject and agent). <ref type="table">Table 2</ref>: Initial values for the mean (µ), standard deviation (σ), and prior (π) of each Gaussian as well as the skip penalty (Φ) used in this paper.</p><formula xml:id="formula_0">µ σ π G SC -1 0.5 .999 G SN -1 3 .001 G OC 1 0.5 .999 G ON 1 3 .001 Φ .00001</formula><p>Finally, following the finding by Gertner and Fisher (2012) that children interpret intransitives with conjoined subjects as transitives, this work as- sumes that semantic roles have a one-to-one corre- spondence with nouns in a sentence (similarly used as a soft constraint in the semantic role labelling work of <ref type="bibr" target="#b33">Titov and Klementiev, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>The model represents the preferred locations of semantic roles relative to the verb as distribu- tions over real numbers. This idea is adapted from Boersma (1997) who uses it to learn constraint rankings in optimality theory.</p><p>In this work, the final (main) verb is placed at position 0; words (and chunks) before the verb are given progressively more negative positions, and words after the verb are given progressively more positive positions (see <ref type="table">Table 1</ref>). Learner expecta- tions of where an argument will appear relative to the verb are modelled as two-component Gaus- sian mixtures: one mixture of Gaussians (G S· ) cor- responds to the subject argument, another (G O· ) corresponds to the object argument. There is no mixture for a third argument since children do not generalize beyond two arguments until later in de- velopment ( <ref type="bibr" target="#b16">Goldberg et al., 2004;</ref><ref type="bibr" target="#b1">Bello, 2012)</ref>.</p><p>One component of each mixture learns to repre- sent the canonical position for the argument (G ·C ) while the other (G ·N ) represents some alternate, non-canonical position such as the filler position in filler-gap constructions. To reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap, the mixture is initialized so that there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture. 5 Fi- nally, the one-to-one role bias is explicitly encoded such that the model cannot use a label that has already been used elsewhere in the sentence.  <ref type="table">Table 2</ref> and (Right) the converged model's expectations of where arguments will appear.</p><p>Thus, the initial model conditions (see <ref type="figure" target="#fig_1">Figure 2</ref>) are most likely to realize an SVO ordering, al- though it is possible to obtain SOV (by sampling a negative number from the blue curve) or even OSV (by also sampling the red curve very close to 0). The model is most likely to hypothesize a preverbal object when it has already assigned the subject role to something and, in addition, there is no postverbal noun competing for the object label. In other words, the model infers that an object ex- traction may have occurred if there is a 'missing' postverbal argument.</p><p>Finally, the probability of a given sequence is the product of the label probabilities for the compo- nent argument positions (e.g. G SC generating an argument at position -2, etc). Since many sentences have more than two nouns, the model is allowed to skip nouns by multiplying a penalty term (Φ) into the product for each skipped noun; the cost is set at 0.00001 for this study, though see Section 7 for a discussion of the constraints on this parameter. See <ref type="table">Table 2</ref> for initialization parameters and <ref type="figure" target="#fig_1">Figure 2</ref> for a visual representation of the initial expecta- tions of the model. This work uses a model with 2-component mix- tures for both subjects and objects (termed the symmetric model ). This formulation achieves the best fit to the training data according to the Bayesian Information Criterion (BIC). <ref type="bibr">6</ref> However, follow-up experiments find that the non-canonical subject Gaussian only improves the likelihood of the data by erroneously modeling postverbal nouns in imperative statements. The lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data by using the non-canonical subject Gaussian to capture ficti-tious postverbal arguments. When imperatives are filtered out of the training corpus, the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian. There- fore, if one makes the assumption that impera- tives are prosodically-marked for learners (e.g. the learner is the implicit subject), the best model is one that lacks a non-canonical subject. 7 The re- mainder of this paper assumes a symmetric model to demonstrate what happens if such an assump- tion is not made; for the evaluations described in this paper, the results are similar in either case.</p><p>This model differs from other non-recursive computational models of grammar induction (e.g. <ref type="bibr" target="#b17">Goldwater and Griffiths, 2007</ref>) since it is not based on Hidden Markov Models. Instead, it determines the best ordering for the sentence as a whole. This approach bears some similarity to a Generalized Mallows model <ref type="bibr" target="#b4">(Chen et al., 2009</ref>), but the current formulation was chosen due to being independently posited as cognitively plausible <ref type="bibr" target="#b3">(Boersma, 1997)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> (Right) shows the converged, final state of the model. The model expects the first argu- ment (usually agent) to be assigned preverbally and expects the second (say, patient) to be assigned postverbally; however, there is now a larger chance that the second argument will appear preverbally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The model in this work is trained using transcribed child-directed speech (CDS) from the BabySRL portions ( <ref type="bibr" target="#b5">Connor et al., 2008</ref>) of CHILDES <ref type="bibr" target="#b22">(MacWhinney, 2000)</ref>. Chunking is performed us- 7 This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects if imperative filtering were employed.  ing a basic noun-chunker from NLTK ( <ref type="bibr" target="#b2">Bird et al., 2009</ref>). Based on an initial analysis of chunker per- formance, yes is hand-corrected to not be a noun. Poor chunker perfomance is likely due to a mis- match in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise may be a good estimation of learner uncertainty, so the remaining text is left uncorrected. All noun phrase chunks are then re- placed with their final noun (presumed the head) to approximate the ability of children to distin- guish nouns from modifiers ( <ref type="bibr" target="#b34">Waxman and Booth, 2001</ref>). Finally, for each sentence, the model assigns sentence positions to each word with the final verb at zero. Viterbi Expectation-Maximization is performed over each sentence in the corpus to infer the pa- rameters of the model. During the Expectation step, the model uses the current Gaussian param- eters to label the nouns in each sentence with ar- gument roles. Since the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object. The model then chooses the best label sequence for each sentence.</p><note type="other">Eve (n = 4820) Adam (n = 4461) P</note><p>These newly labelled sentences are used during the Maximization step to determine the Gaussian parameters that maximize the likelihood of that labelling. The mean of each Gaussian is updated to the mean position of the words it labels. Sim- ilarly, the standard deviation of each Gaussian is updated with the standard deviation of the posi- tions it labels. A learning rate of 0.3 is used to prevent large parameter jumps. The prior proba- bility of each Gaussian is updated as the ratio of that Gaussian's labellings to the total number of labellings from that mixture in the corpus:</p><formula xml:id="formula_1">π ρθ = | G ρθ | | G ρ· |<label>(1)</label></formula><p>where ρ ∈ {S, O} and θ ∈ {C, N }.</p><p>Best results seem to be obtained when the skip- penalty is loosened by an order of magnitude dur-  ing testing. Essentially, this forces the model to tightly adhere to the perceived argument struc- ture during training to learn more rigid parame- ters, but the model is allowed more leeway to skip arguments it has less confidence in during testing. Convergence (see <ref type="figure" target="#fig_1">Figure 2</ref>) tends to occur after four iterations but can take up to ten iterations depending on the initial parameters.</p><note type="other">Subject Extraction filter: S x V . . . Object Extraction filter:</note><p>Since the model is unsupervised, it is trained on a given corpus (e.g. Eve) before being tested on the role annotations of that same corpus. The Eve corpus was used for development purposes, 8 and the Adam data was used only for testing.</p><p>For testing, this study uses the semantic role annotations in the BabySRL corpus. These anno- tations were obtained by automatically semantic role labelling portions of CHILDES with the sys- tem of <ref type="bibr" target="#b27">Punyakanok et al. (2008)</ref> before roughly hand-correcting them ( <ref type="bibr" target="#b5">Connor et al., 2008</ref>). The BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles. Therefore, overall accuracy results (see <ref type="table" target="#tab_1">Table 3</ref>) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables).</p><p>Since children do not generalize above two ar- guments during the modelled age range ( <ref type="bibr" target="#b16">Goldberg et al., 2004;</ref><ref type="bibr" target="#b1">Bello, 2012)</ref>, the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers. The increase in accuracy obtained from collapsing non-agent ar- guments indicates that children may initially gen- eralize incorrectly to some verbs and would need to learn lexically-specific role assignments (e.g. double-object constructions of give). Since the cur- rent work is interested in general filler-gap com- prehension at this age, including over unknown verbs, the remaining analyses in this paper con-  <ref type="table">Table 5</ref>: (Left) Subject-extraction accuracy and object-extraction accuracy and (Right) Wh-relative ac- curacy and that-relative accuracy; calculated over the Eve and Adam sections of the BabySRL corpus with non-agent roles collapsed into a single role. † p = .02 * p &lt;&lt; .01 sider performance when non-agent arguments are collapsed. <ref type="bibr">9</ref> Next, a filler-gap version of the BabySRL cor- pus is created using a coarse filtering process: the new corpus is comprised of all sentences where an associated object precedes the final verb and all sentences where the relevant subject is not imme- diately followed by the final verb (see <ref type="table" target="#tab_3">Table 4</ref>). For these filler-gap evaluations, the model is trained on the full version of the corpus in question (e.g. Eve) before being tested on the filler-gap subset of that corpus. The overall results of the filler-gap evalua- tion (see <ref type="table" target="#tab_3">Table 4</ref>) indicate that the model improves significantly at parsing filler-gap constructions af- ter training.</p><p>The performance of the model on role- assignment in filler-gap constructions may be analyzed further in terms of how the model performs on subject-extractions compared with object-extractions and in terms of how the model performs on that-relatives compared with wh- relatives (see <ref type="table">Table 5</ref>).</p><p>The model actually performs worse at subject- extractions after training than before training. This is unsurprising because, prior to training, subjects have little-to-no competition for prever- bal role assignments; after training, there is a pre- verbal extracted object category, which the model can erroneously use. This slight, though signifi- cant in Eve, deficit is counter-balanced by a very substantial and significant improvement in object- extraction labelling accuracy.</p><p>Similarly, training confers a large and significant improvement for role assignment in wh-relative constructions, but it yields less of an improve- ment for that-relative constructions. This differ- ence mimics a finding observed in the developmen- tal literature where children seem slower to ac- quire comprehension of that-relatives than of wh- relatives ( <ref type="bibr" target="#b11">Gagliardi and Lidz, 2010)</ref>. <ref type="bibr">9</ref> Though performance is slightly worse when argu- ments are not collapsed, all the same patterns emerge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Comparison to BabySRL</head><p>The acquisition of semantic role labelling (SRL) by the BabySRL model ( <ref type="bibr" target="#b5">Connor et al., 2008;</ref><ref type="bibr" target="#b6">Connor et al., 2009;</ref><ref type="bibr" target="#b7">Connor et al., 2010</ref>) bears many sim- ilarities to the current work and is, to our knowl- edge, the only comparable line of inquiry to the current one. The primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make <ref type="bibr" target="#b13">(Gertner and Fisher, 2012)</ref>, the 1-1 role bias error (John and Mary gorped interpreted as John gorped Mary). Similar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments.</p><p>This section will demonstrate that the model in this paper initially reflects 1-1 role bias comparably to BabySRL, though it progresses beyond this bias after training. 10 Further, the model in this paper is able to reflect the concurrent acquisition of filler- gap whereas BabySRL does not seem well-suited to such a task. Finally, BabySRL performs unde- sirably in intransitive settings whereas the model in this paper does not. <ref type="bibr" target="#b5">Connor et al. (2008)</ref> demonstrate that a super- vised perceptron classifier, based on positional fea- tures and trained on the silver role label annota- tions of the BabySRL corpus, manifests 1-1 role bias errors. Follow-up studies show that supervi- sion may be lessened ( <ref type="bibr" target="#b6">Connor et al., 2009</ref>) or re- moved ( <ref type="bibr" target="#b7">Connor et al., 2010)</ref> and BabySRL will still reflect a substantial 1-1 role bias. <ref type="bibr" target="#b5">Connor et al. (2008)</ref> and <ref type="bibr" target="#b6">Connor et al. (2009)</ref> run direct analyses of how frequently their mod- els make 1-1 role bias errors. A comparable eval- uation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling (see <ref type="table">Table 6</ref>). 11 <ref type="bibr">10</ref> All evaluations in this section are preceded by training on the chunked Eve corpus. 11 While <ref type="table">Table 6</ref> analyzes erroneous labellings of NNV structure, the 'Obj' column of <ref type="table" target="#tab_1">Table 5 (Left) Error rate  Initial  .36  Trained  .11  Initial (given 2 args)</ref> .66 Trained (given 2 args)</p><p>.13 2008 arg-arg position .65 2008 arg-verb position 0 2009 arg-arg position .82 2009 arg-verb position .63 <ref type="table">Table 6</ref>  <ref type="formula">(2009)</ref> has a unique argument constraint, similar to the model in this paper, in order to make comparison as di- rect as possible. The 1-1 role bias error rate (before training) of the model presented in this paper is comparable to that of <ref type="bibr" target="#b5">Connor et al. (2008)</ref> and <ref type="bibr" target="#b6">Connor et al. (2009)</ref>, which shows that the current model pro- vides comparable developmental modeling benefits to the BabySRL models. Further, similar to real children (see <ref type="figure" target="#fig_0">Figure 1</ref>) the model presented in this paper develops beyond this error by the end of its training, 12 whereas the BabySRL models still make this error after training.</p><p>Connor et al. <ref type="formula" target="#formula_1">(2010)</ref> look at how frequently their model correctly labels the agent in transitive and intransitive sentences with unknown verbs (to demonstrate that it exhibits an agent-first bias). This evaluation can be replicated for the current study by generating 1,000 sentences with the tran- sitive form of NVN and a further 1,000 sentences with the intransitive form of NV (see <ref type="table">Table 7</ref>).</p><p>Since <ref type="bibr" target="#b7">Connor et al. (2010)</ref> investigate the effects shows model accuracy on NNV structures. <ref type="bibr">12</ref> It is important to note that the unique argument constraint prevents the current model from actually getting the correct, conjoined-subject parse, but it no longer exhibits agent-first bias, an important step for acquiring passives, which occurs between 3 and 4 years ( <ref type="bibr" target="#b32">Thatcher et al., 2008</ref>).   of different initial lexicons, this evaluation com- pares against the resulting BabySRL from each ini- tializer: they initially seed their part-of-speech tag- ger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NVN NV</head><p>As with subject extraction, the model in this paper gets less accurate after training because of the newly minted extracted object category that can be mistakenly used in these canonical settings. While the model of <ref type="bibr" target="#b7">Connor et al. (2010)</ref> outper- forms the model presented here when in a tran- sitive setting, their model does much worse in an intransitive setting. The difference in transitive set- tings stems from increased lexicalization, as is ap- parent from their results alone; the model pre- sented here initially performs close to their weakly lexicalized model, though training impedes agent- prediction accuracy due to an increased probability of non-canonical objects.</p><p>For the intransitive case, however, whereas the model presented in this paper is generally able to successfully label the lone noun as the subject, the model of <ref type="bibr" target="#b7">Connor et al. (2010)</ref> chooses to label lone nouns as objects about 40% of the time. This likely stems from their model's reliance on argument- argument relative position as a feature; when there is no additional argument to use for reference, the model's accuracy decreases. This is borne out by their model (not shown in <ref type="table">Table 7</ref>) that omits the argument-argument relative position feature and solely relies on verb-argument position, which achieves up to 70% accuracy in intransitive set- tings. Even in that case, however, BabySRL still chooses to label lone nouns as objects 30% of the time. The fact that intransitive sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus sug- gests that learners should be more likely to assign correct roles in an intransitive setting, which is not reflected in the BabySRL results. The overall reason for the different results be- tween the current work and BabySRL is that BabySRL relies on positional features that mea- sure the relative position of two individual ele- ments (e.g. where a given noun is relative to the verb). Since the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected.</p><p>Further, while BabySRL consistently reflects 1- 1 role bias (corresponding to a pre 25-month old learner), it also learns to productively label five roles, which developmental studies have shown does not take place until at least 31 months <ref type="bibr" target="#b16">(Goldberg et al., 2004;</ref><ref type="bibr" target="#b1">Bello, 2012)</ref>. Finally, it does not seem likely that BabySRL could be easily extended to capture filler-gap acquisition. The argument- verb position features impede acquisition of filler- gap by classifying preverbal arguments as agents, and the argument-argument position features in- hibit accurate labelling in intransitive settings and result in an agent-first bias which would tend to label extracted objects as agents. In fact, these ob- servations suggest that any linear classifier which relies on positioning features will have difficulties modeling filler-gap acquisition.</p><p>In sum, the unlexicalized model presented in this paper is able to achieve greater labelling accuracy than the lexicalized BabySRL models in intran- sitive settings, though this model does perform slightly worse in the less common transitive set- ting. Further, the unsupervised model in this pa- per initially reflects developmental 1-1 role bias as well as the supervised BabySRL models, and it is able to progress beyond this bias. Finally, un- like BabySRL, the model presented here provides a cognitive model of the acquisition of filler-gap com- prehension, which BabySRL does not seem well- suited to model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>This paper has presented a simple cognitive model of filler-gap acquisition, which is able to capture several findings from developmental psychology. Training significantly improves role labelling in the case of object-extractions, which improves the overall accuracy of the model. This boost is ac- companied by a slight decrease in labelling ac- curacy in subject-extraction settings. The asym- metric ease of subject versus object comprehen- sion is well-documented in both children and adults <ref type="bibr" target="#b14">(Gibson, 1998)</ref>, and while training improves the model's ability to process object-extractions, there is still a gap between object-extraction and subject-extraction comprehension even after train- ing.</p><p>Further, the model exhibits better comprehen- sion of wh-relatives than that-relatives similar to children <ref type="bibr" target="#b11">(Gagliardi and Lidz, 2010)</ref>. This could also be an area where a lexicalized model could do better. As <ref type="bibr" target="#b11">Gagliardi and Lidz (2010)</ref> point out, whereas wh-relatives such as who or which always signify a filler-gap construction, that can occur for many different reasons (demonstrative, determiner, complementizer, etc) and so is a much weaker filler-gap cue. A lexical model could poten- tially pick up on clues which could indicate when that is a relativizer or simply improve on its com- prehension of wh-relatives even more.</p><p>It is interesting to note that the cuurent model does not make use of that as a cue at all and yet is still slower at acquiring that-relatives than wh-relatives. This fact suggests that the findings of <ref type="bibr" target="#b11">Gagliardi and Lidz (2010)</ref> may be partially ex- plained by a frequency effect: perhaps the input to children is simply biased such that wh-relatives are much more common than that-relatives (as shown in <ref type="table">Table 5</ref>).</p><p>This model also initially reflects the 1-1 role bias observed in children <ref type="bibr" target="#b13">(Gertner and Fisher, 2012)</ref> as well as previous models <ref type="bibr" target="#b5">(Connor et al., 2008;</ref><ref type="bibr" target="#b6">Connor et al., 2009;</ref><ref type="bibr" target="#b7">Connor et al., 2010</ref>) without sac- rificing accuracy in canonical intransitive settings.</p><p>Finally, this model is extremely robust to differ- ent initializations. The canonical Gaussian expec- tations can begin far from the verb (±3) or close to the verb (±0.1), and the standard deviations of the distributions and the skip-penalty can vary widely; the model always converges to give compa- rable results to those presented here. The only con- straint on the initial parameters is that the proba- bility of the extracted object occurring preverbally must exceed the skip-penalty (i.e. extraction must be possible). In short, this paper describes a sim- ple, robust cognitive model of the development of a learner between 15 months until somewhere be- tween 25-and 30-months old (since 1-1 role bias is no longer present but no more than two arguments are being generalized).</p><p>In future, it would be interesting to incorporate lexicalization into the model presented in this pa- per, as this feature seems likely to bridge the gap between this model and BabySRL in transitive set- tings. Lexicalization should also help further dis- tinguish modifiers from arguments and improve the overall accuracy of the model. It would also be interesting to investigate how well this model generalizes to languages besides English. Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax.</p><p>Ordering is one of the definining characteris- tics of a language that must be acquired by learn- ers (e.g. SVO vs SOV), and this work shows that filler-gap comprehension can be acquired as a by- product of learning orderings rather than having to resort to higher-order syntax. Note that this model cannot capture the constraints on filler-gap usage which require a hierarchical grammar (e.g. subja- cency), but such knowledge is really only needed for successful production of filler-gap construc- tions, which occurs much later (around 5 years; <ref type="bibr" target="#b9">de Villiers and Roeper, 1995)</ref>. Further, the kind of ordering system proposed in this paper may form an initial basis for learning such grammars <ref type="bibr">(Jackendoff and Wittenberg, in press</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The developmental timeline of subject (Wh-S) and object (Wh-O) wh-clause extraction comprehension suggested by experimental results (Seidl et al., 2003; Gagliardi et al., 2014). Parentheses indicate weak comprehension. The final row shows the timeline of 1-1 role bias errors (Naigles, 1990; Gertner and Fisher, 2012). Missing nodes denote a lack of studies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visual representations of (Left) the initial model's expectations of where arguments will appear, given the initial parameters in Table 2 and (Right) the converged model's expectations of where arguments will appear.</figDesc><graphic url="image-1.png" coords="4,80.82,62.81,224.64,168.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>: 1-1 role bias error in this model compared to the models of Connor et al. (2008) and Connor et al. (2009). That is, how frequently each model labelled an NNV sentence SOV. Since the Connor et al. models are perceptron-based, they require both arguments be labelled. The model presented in this paper does not share this restriction, so the raw error rate for this model is presented in the first two lines; the error rate once this additional restriction is imposed is given in the second two lines. The results of Connor et al. (2008) and Connor et al. (2009) depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model). Further, the model presented here from Connor et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 :</head><label>7</label><figDesc>Agent-prediction recall accuracy in tran- sitive (NVN) and intransitive (NV) settings of the model presented in this paper (middle) and the combined model of Connor et al. (2010) (bottom), which has features for argument-argument relative position as well as argument-predicate relative po- sition and so is closest to the model presented in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Overall accuracy on the Eve and Adam 
sections of the BabySRL corpus. Bottom rows re-
flect accuracy when non-agent roles are collapsed 
into a single role. Note that improvements are nu-
merically slight since filler-gap is relatively rare 
(Schuler, 2011).  *  p &lt;&lt; .01 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>(Above) Filters to extract filler-gap con-
structions: A) the subject and verb are not ad-
jacent, B) the object precedes the verb. (Below) 
Filler-gap accuracy on the Eve and Adam sections 
of the BabySRL corpus when non-agent roles are 
collapsed into a single role.  *  p &lt;&lt; .01 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> This model does not explicitly learn gap positions, but rather assigns thematic roles to arguments based on where those arguments are expected to manifest. This approach to filler-gap comprehension is supported by findings that show people do not actually link fillers to gap positions but instead link the filler to a verb with missing arguments (Pickering and Barry, 1991)</note>

			<note place="foot" n="2"> Since the wh-phrase is in the same (or a very similar) position as the original subject when the wh-phrase takes subject position, it is not clear that these constructions are true extractions (Culicover, 2013), however, this paper will continue to refer to them as such for ease of exposition.</note>

			<note place="foot" n="3"> There were two actors in each image to avoid biasing the infants to look at the image with more actors. 4 As one reviewer notes, Joshi et al. (1990) and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms; these have the virtue of scaling up to adult grammar, but due to their complexity, do not seem to have been described as models of early acquisition.</note>

			<note place="foot" n="5"> Akhtar (1999) finds that learners may not have strong expectations of canonical argument positions until four years of age, but the results of the current study are extremely robust to changes in initialization, as discussed in Section 7 of this paper, so this assumption is mostly adopted for ease of exposition.</note>

			<note place="foot" n="6"> The BIC rewards improved log-likelihood but penalizes increased model complexity.</note>

			<note place="foot" n="8"> This is included for transparency, though the initial parameters have very little bearing on the final results as stated in Section 7, so the danger of overfitting to development data is very slight.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Acquiring basic word order: evidence for data-driven learning of syntactic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nameera</forename><surname>Akhtar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Language</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="339" to="356" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying indirect objects in French: An elicitation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 annual conference of the Canadian Linguistic Association</title>
		<meeting>the 2012 annual conference of the Canadian Linguistic Association</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly, Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How we learn variation, optionality, and probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Boersma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Institute of Phonetic Sciences of the University of Amsterdam</title>
		<meeting>the Institute of Phonetic Sciences of the University of Amsterdam</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="43" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Content modeling using latent permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Harr Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="129" to="163" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Baby srl: Modeling early language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Gertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimally supervised model of early language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Gertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Starting from scratch in semantic role labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Gertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010</title>
		<meeting>ACL 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Explaining syntax: representations, structures, and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Culicover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Barriers, binding, and acquisition of the dp-np distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>De Villiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roeper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Acquisition</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="104" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The acquisition of finite complement clauses in english: A corpus-based analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Diessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tomasello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morphosyntactic cues impact filler-gap dependency resolution in 20-and 30-month-olds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Gagliardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lidz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Poster session of BUCLD35</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Discontinuous development in the acquisition of filler-gap dependencies: Evidence from 15-and 20-montholds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Gagliardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">M</forename><surname>Mease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Lidz</surname></persName>
		</author>
		<ptr target="http://www.people.fas.harvard.edu/∼gagliardi" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicted errors in children&apos;s early sentence comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Gertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic complexity: Locality of syntactic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="76" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The structural sources of verb meanings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lila</forename><forename type="middle">R</forename><surname>Gleitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Acquisition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="55" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning argument structure generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adele</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Casenhiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitya</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Linguistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="316" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fully Bayesian approach to unsupervised partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What you can say without syntax: A hierarchy of grammatical complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Jackendoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Wittenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measuring Linguistic Complexity</title>
		<editor>Fritz Newmeyer and Lauren Preston</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The convergence of mildly contextsensitive grammar formalisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Vijay</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shanker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weir</surname></persName>
		</author>
		<idno>MS-CIS-90-01</idno>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corpus-based induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2012</title>
		<meeting>EACL 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The CHILDES project: Tools for analyzing talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Macwhinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lawrence Elrbaum Associates</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vision. A Computational Investigation into the Human Representation and Processing of Visual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>W.H. Freeman and Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Children use syntax to learn verb meanings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Letitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naigles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal Child Language</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="357" to="374" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some arguments and nonarguments for reductionist accounts of syntactic phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="156" to="187" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentence processing without empty categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Barry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="259" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="287" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Constraints on Variables in Syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effects of filler-gap dependencies on working memory requirements for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COGSCI</title>
		<meeting>COGSCI<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="501" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Early understanding of subject and object wh-questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Hollich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Jusczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infancy</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="423" to="436" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Newborn infants&apos; sensitivity to perceptual cues to lexical and grammatical words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Children&apos;s early acquisition of the passive: Evidence from syntactic priming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Thatcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holly</forename><surname>Branigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonella</forename><surname>Sorace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Child Language Seminar</title>
		<meeting>the Child Language Seminar</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="195" to="205" />
		</imprint>
		<respStmt>
			<orgName>University of Reading</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crosslingual induction of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL2011)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL2011)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seeing pink elephants: Fourteen-month-olds&apos; interpretations of novel nouns and adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">E</forename><surname>Waxman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Booth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Psychology</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="217" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Counting the nouns: Simple structural cues to verb meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Snedeker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1382" to="1399" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
