<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Metric Validation for Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
							<email>leshem.choshen@mail.huji.ac.il, oabend@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Cognitive Sciences</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Metric Validation for Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1372" to="1382"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1372</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Metric validation in Grammatical Error Correction (GEC) is currently done by observing the correlation between human and metric-induced rankings. However , such correlation studies are costly, methodologically troublesome, and suffer from low inter-rater agreement. We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties with existing practices. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M 2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that correcting some types of errors is consistently penalized by existing metrics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much recent effort has been devoted to auto- matic evaluation, both within GEC ( <ref type="bibr" target="#b25">Napoles et al., 2015;</ref><ref type="bibr" target="#b15">Felice and Briscoe, 2015;</ref><ref type="bibr" target="#b27">Ng et al., 2014;</ref><ref type="bibr">Dahlmeier and Ng, 2012, see §2)</ref>, and more gen- erally in text-to-text generation tasks. Within Ma- chine Translation (MT), an annual shared task is devoted to automatic metric development, accom- panied by an extensive analysis of metric behav- ior ( <ref type="bibr" target="#b3">Bojar et al., 2017)</ref>. Metric validation is also raising interest in GEC, with several recent works on the subject ( <ref type="bibr" target="#b19">Grundkiewicz et al., 2015;</ref><ref type="bibr" target="#b25">Napoles et al., 2015</ref><ref type="bibr">Napoles et al., , 2016b</ref><ref type="bibr" target="#b29">Sakaguchi et al., 2016</ref>), all us- ing correlation with human rankings (henceforth, CHR) as their methodology.</p><p>Human rankings are often considered as ground truth in text-to-text generation, but using them re- liably can be challenging. Other than the costs of compiling a sizable validation set, human rank- ings are known to yield poor inter-rater agree- ment in MT <ref type="bibr" target="#b4">(Bojar et al., 2011;</ref><ref type="bibr" target="#b23">Lopez, 2012;</ref><ref type="bibr" target="#b17">Graham et al., 2012)</ref>, and to introduce a number of methodological problems that are difficult to over- come, notably the treatment of ties in the rankings and uncomparable sentences (see §3). These dif- ficulties have motivated several proposals to alter the MT metric validation protocol <ref type="bibr" target="#b20">(Koehn, 2012;</ref><ref type="bibr" target="#b14">Dras, 2015)</ref>, leading to a recent abandoning of evaluation by human rankings due to its unreli- ability ( <ref type="bibr" target="#b18">Graham et al., 2015;</ref>. These conclusions have not yet been implemented in GEC, despite their relevance. In §3 we show that human rankings in GEC also suffer from low inter-rater agreement, motivating the development of alternative methodologies.</p><p>The main contribution of this paper is an auto- matic methodology for metric validation in GEC called MAEGE (Methodology for Automatic Eval- uation of GEC Evaluation), which addresses these difficulties. MAEGE requires no human rankings, and instead uses a corpus with gold standard GEC annotation to generate lattices of corrections with similar meanings but varying degrees of grammat- icality. For each such lattice, MAEGE generates a partial order of correction quality, a quality score for each correction, and the number and types of edits required to fully correct each. It then com- putes the correlation of the induced partial order with the metric-induced rankings.</p><p>MAEGE addresses many of the problems with existing methodology:</p><p>• Human rankings yield low inter-rater and intra-rater agreement ( §3). Indeed, <ref type="bibr" target="#b9">Choshen and Abend (2018a)</ref> show that while annota- tors often generate different corrections given a sentence, they generally agree on whether a correction is valid or not. Unlike CHR, MAEGE bases its scores on human correc- tions, rather than on rankings.</p><p>• CHR uses system outputs to obtain human rankings, which may be misleading, as sys- tems may share similar biases, thus neglect- ing to evaluate some types of valid correc- tions ( §7). MAEGE addresses this issue by systematically traversing an inclusive space of corrections.</p><p>• The difficulty in handling ties is addressed by only evaluating correction pairs where one contains a sub-set of the errors of the other, and is therefore clearly better.</p><p>• MAEGE uses established statistical tests for determining the significance of its results, thereby avoiding ad-hoc methodologies used in CHR to tackle potential biases in human rankings ( §5, §6).</p><p>In experiments on the standard NUCLE test set ( <ref type="bibr" target="#b12">Dahlmeier et al., 2013)</ref>, we find that MAEGE often disagrees with CHR as to the quality of existing metrics. For example, we find that the standard GEC metric, M 2 , is a poor predictor of corpus- level ranking, but a good predictor of sentence- level pair-wise rankings. The best predictor of corpus-level quality by MAEGE is the reference- less LT metric <ref type="bibr" target="#b24">(Miłkowski, 2010;</ref><ref type="bibr">Napoles et al., 2016b</ref>), while of the reference-based metrics, GLEU ( <ref type="bibr" target="#b25">Napoles et al., 2015</ref>) fares best.</p><p>In addition to measuring metric reliability, MAEGE can also be used to analyze the sensitivi- ties of the metrics to corrections of different types, which to our knowledge is a novel contribution of this work. Specifically, we find that not only are valid edits of some error types better rewarded than others, but that correcting certain error types is consistently penalized by existing metrics (Sec- tion 7). The importance of interpretability and de- tail in evaluation practices (as opposed to just pro- viding bottom-line figures), has also been stressed in MT evaluation (e.g., <ref type="bibr" target="#b1">Birch et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Examined Metrics</head><p>We turn to presenting the metrics we experiment with. The standard practice in GEC evaluation is to define differences between the source and a cor- rection (or a reference) as a set of edits ( <ref type="bibr" target="#b13">Dale et al., 2012</ref>). An edit is a contiguous span of tokens to be edited, a substitute string, and the corrected error type. For example: "I want book" might have an edit (2-3, "a book", ArtOrDet); applying the edit results in "I want a book". Edits are defined (by the annotation guidelines) to be maximally inde- pendent, so that each edit can be applied indepen- dently of the others. We denote the examined set of metrics with METRICS.</p><p>BLEU. BLEU ( <ref type="bibr" target="#b28">Papineni et al., 2002</ref>) is a reference-based metric that averages the output- reference n-gram overlap precision values over different ns. While commonly used in MT and other text generation tasks ( <ref type="bibr" target="#b31">Sennrich et al., 2017;</ref><ref type="bibr" target="#b21">Krishna et al., 2017;</ref><ref type="bibr" target="#b34">Yu et al., 2017)</ref>, BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sen- tence should remain unchanged ( <ref type="bibr" target="#b33">Xu et al., 2016)</ref>. We use the NLTK implementation of BLEU, using smoothing method 3 by <ref type="bibr" target="#b8">Chen and Cherry (2014)</ref>.</p><p>GLEU. GLEU ( <ref type="bibr" target="#b25">Napoles et al., 2015</ref>) is a reference-based GEC metric inspired by BLEU. Recently, it was updated to better address multi- ple references <ref type="bibr">(Napoles et al., 2016a</ref>). GLEU re- wards n-gram overlap of the correction with the reference and penalizes unchanged n-grams in the correction that are changed in the reference.</p><p>iBLEU. iBLEU (Sun and Zhou, 2012) was in- troduced to monolingual translation in order to balance BLEU, by averaging it with the BLEU score of the source and the output. This yields a metric that rewards similarity to the source, and not only overlap with the reference:</p><formula xml:id="formula_0">iBLEU (S, R, O) = αBLEU (O, R)−(1−α)BLEU (O, S)</formula><p>We set α = 0.8 as suggested by Sun and Zhou.</p><p>F -Score computes the overlap of edits to the source in the reference, and in the output. As system edits can be constructed in multiple ways, the standard M 2 scorer ( <ref type="bibr" target="#b11">Dahlmeier and Ng, 2012)</ref> computes the set of edits that yields the maximum F -score. As M 2 requires edits from the source to the reference, and as MAEGE generates new source sentences, we use an established protocol to auto- matically construct edits from pairs of strings <ref type="bibr" target="#b16">(Felice et al., 2016;</ref><ref type="bibr" target="#b6">Bryant et al., 2017)</ref>. The protocol was shown to produce similar M 2 scores to those produced with manual edits. Following common practice, we use the Precision-oriented F 0.5 .</p><p>SARI. SARI ( <ref type="bibr" target="#b33">Xu et al., 2016</ref>) is a reference- based metric proposed for sentence simplification. SARI averages three scores, measuring the ex- tent to which n-grams are correctly added to the source, deleted from it and retained in it. Where multiple references are present, SARI's score is determined not as the maximum single-reference score, but some averaging over them. As this may lead to an unintuitive case, where a correction which is identical to the output gets a score of less than 1, we experiment with an additional metric, MAX-SARI, which coincides with SARI for a sin- gle reference, and computes the maximum single- reference SARI score for multiple-references.</p><p>Levenshtein Distance. We use the Levenshtein distance ( <ref type="bibr" target="#b22">Kruskal and Sankoff, 1983)</ref>, i.e., the number of character edits needed to convert one string to another, between the correction and its closest reference (M inLD O→R ). To enrich the discussion, we also report results with a measure of conservatism, LD S→O , i.e., the Levenshtein distance between the correction and the source. Both distances are normalized by the number of characters in the second string (R, O respectively). In order to convert these distance measures into measures of similarity, we report 1 − LD(c1,c2) len(c1) . Grammaticality is a reference-less metric, which uses grammatical error detection tools to assess the grammaticality of GEC system outputs. We use LT <ref type="bibr" target="#b24">(Miłkowski, 2010)</ref>, the best performing non-proprietary grammaticality metric ( <ref type="bibr">Napoles et al., 2016b</ref>). The detection tool at the base of LT can be much improved. Indeed, <ref type="bibr">Napoles et al. (2016b)</ref> reported that the proprietary tool they used detected 15 times more errors than LT. A sentence's score is defined to be 1 − #errors #tokens . See ( <ref type="bibr" target="#b0">Asano et al., 2017;</ref><ref type="bibr" target="#b10">Choshen and Abend, 2018b)</ref> for additional reference-less measures, published concurrently with this work.</p><p>I-Measure. I-Measure <ref type="bibr" target="#b15">(Felice and Briscoe, 2015</ref>) is a weighted accuracy metric over tokens. I-measure rank determines whether a correction is better than the source and to what extent. Unlike in this paper, I-measure assumes that every pair of intersecting edits (i.e., edits whose spans of tokens overlap) are alternating, and that non-intersecting edits are independent. Consequently, where multi- ple references are present, it extends the set of ref- erences, by generating every possible combination of independent edits. As the number of combina- tions is generally exponential in the number of ref- erences, the procedure can be severely inefficient. Indeed, a sentence in the test set has 3.5 billion references on average, where the median is 512 (See <ref type="figure" target="#fig_0">Figure 1</ref>). I-measure can also be run without generating new references, but despite paralleliza- tion efforts, this version did not terminate after 140 CPU days, while the cumulative CPU time of the rest of the metrics was less than 1.5 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Human Ranking Experiments</head><p>Correlation with human rankings (CHR) is the standard methodology for assessing the validity of GEC metrics. While informative, human rank- ings are costly to produce, present low inter-rater agreement (shown for MT evaluation in <ref type="bibr" target="#b4">(Bojar et al., 2011;</ref><ref type="bibr" target="#b14">Dras, 2015)</ref>), and introduce method- ological difficulties that are hard to overcome. We begin by showing that existing sets of human rank- ings produce inconsistent results with respect to the quality of different metrics, and proceed by proposing an improved protocol for computing this correlation in the future.</p><p>There are two existing sets of human rankings for GEC that were compiled concurrently: GJG15 by <ref type="bibr" target="#b19">Grundkiewicz et al. (2015)</ref>, and NSPT15 by <ref type="bibr" target="#b25">Napoles et al. (2015)</ref>. Both sets are based on system outputs from the CoNLL 2014 ( <ref type="bibr" target="#b27">Ng et al., 2014</ref>) shared task, using sentences from the NUCLE test set. We compute CHR against each. System-level correlations are computed by <ref type="bibr">TrueSkill (Sakaguchi et al., 2014</ref>), which adopts its methodology from MT. 1 <ref type="table">Table 1</ref> shows CHR with Spearman ρ (Pear- son r shows similar trends). Results on the two datasets diverge considerably, despite their use of the same systems and corpus (albeit a different sub-set thereof). For example, BLEU receives a high positive correlation on GJG15, but a nega- tive one on NSPT15; GLEU receives a correlation of 0.51 against GJG15 and 0.76 against NSPT15; and M 2 ranges between 0.4 (GJG15) and 0.7 (NSPT15). In fact, this variance is already appar- ent in the published correlations of GLEU, e.g., <ref type="bibr" target="#b25">Napoles et al. (2015)</ref> reported a ρ of 0.56 against <ref type="bibr">NSPT15 and Napoles et al. (2016b)</ref> reported a ρ of 0.85 against GJG15. <ref type="bibr">2</ref> This variance in the met- rics' scores is an example of the low agreement be- tween human rankings, echoing similar findings in MT <ref type="bibr" target="#b4">(Bojar et al., 2011;</ref><ref type="bibr" target="#b23">Lopez, 2012;</ref><ref type="bibr" target="#b14">Dras, 2015)</ref>.</p><p>Another source of inconsistency in CHR is that the rankings are relative and sampled, so datasets rank different sets of outputs ( <ref type="bibr" target="#b23">Lopez, 2012)</ref>. For example, if a system is judged against the best sys- tems more often then others, it may unjustly re- ceive a lower score. TrueSkill is the best known practice to tackle such issues ( <ref type="bibr" target="#b2">Bojar et al., 2014</ref>), but it produces a probabilistic corpus-level score, which can vary between runs ( <ref type="bibr" target="#b29">Sakaguchi et al., 2016)</ref>. <ref type="bibr">3</ref> This makes CHR more difficult to inter- pret, compared to classic correlation coefficients.</p><p>We conclude by proposing a practice for report- ing CHR in future work. First, we combine both sets of human judgments to arrive at the statis- tically most powerful test. Second, we compute the metrics' corpus-level rankings according to the same subset of sentences used for human rankings. The current practice of allowing metrics to rank systems based on their output on the entire CoNLL test set (while human rankings are only collected for a sub-set thereof), may bias the results due to potential non-uniform system performance on the test set. We report CHR according to the proposed protocol in <ref type="table">Table 1</ref> (left column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Constructing Lattices of Corrections</head><p>In the following sections we present MAEGE an al- ternative methodology to CHR, which uses human corrections to induce more reliable and scalable rankings to compare metrics against. We begin our presentation by detailing the method MAEGE <ref type="bibr">2</ref> The difference between our results and previously re- ported ones is probably due to a recent update in GLEU to better tackles multiple references ( <ref type="bibr">Napoles et al., 2016a)</ref>. <ref type="bibr">3</ref> The standard deviation of the results is about 0.02.   j is the j-th perfect correction of Oi (i.e., the perfect correction that result from applying all the edits of the j-th annotation of Oi).</p><formula xml:id="formula_1">R (1) 1 R (1) k · · · O 1 R (n) 1 R (n) k · · · · · · O n</formula><p>uses to generate source-correction pairs and a par- tial order between them. MAEGE operates by us- ing a corpus with gold annotation, given as edits, to generate lattices of corrections, each defined by a sub-set of the edits. Within the lattice, every pair of sentences can be regarded as a potential source and a potential output. We create sentence chains, in an increasing order of quality, taking a source sentence and applying edits in some order one af- ter the other (see <ref type="figure" target="#fig_2">Figure 2 and 3)</ref>.</p><p>Formally, for each sentence s in the corpus and each annotation a, we have a set of typed edits edits(s, a) = {e s,a } of size n s,a . We call 2 edits(s,a) the corrections lattice, and denote it with E s,a . We call, s, the correction correspond- ing to ∅ the original. We define a partial order relation between x, y ∈ E s,a such that x &lt; y if x ⊂ y. This order relation is assumed to be the gold standard ranking between the corrections.</p><p>For our experiments, we use the NUCLE test data ( <ref type="bibr" target="#b27">Ng et al., 2014</ref>). Each sentence is paired with two annotations. The other eight available Social media makes our life patten so fast and left us less time to think about our life.</p><p>Social media makes our life patten so fast and leave us less time to think about our life.</p><p>Social media make our life patten so fast and leave us less time to think about our life.</p><p>Social media make our pace of life so fast and leave us less time to think about our life. left leave makes make life patten pace of life  references, produced by <ref type="bibr" target="#b7">Bryant and Ng (2015)</ref>, are used as references for the reference-based metrics. Denote the set of references for s with R s .</p><p>Sentences which require no correction accord- ing to at least one of the two annotations are dis- carded. In 26 cases where two edit spans intersect in the same annotation (out of a total of about 40K edits), the edits are manually merged or split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Corpus-level Analysis</head><p>We conduct a corpus-level analysis, namely test- ing the ability of metrics to determine which cor- pus of corrections is of better quality. In practice, this procedure is used to rank systems based on their outputs on the test corpus.</p><p>In order to compile corpora corresponding to systems of different quality levels, we define sev- eral corpus models, each applying a different ex- pected number of edits to the original. Models are denoted with the expected number of edits they apply to the original which is a positive number M ∈ R + . Given a corpus model M , we generate a corpus of corrections by traversing the original sentences, and for each sentence s uniformly sam- ple an annotation a (i.e., a set of edits that results in a perfect correction), and the number of edits applied n edits , which is sampled from a clipped binomial probability with mean M and variance 0.9. Given n edits , we uniformly sample from the lattice E s,a a sub-set of edits of size n edits , and ap- ply this set of edits to s. The corpus of M = 0 is the set of originals.</p><p>The corpus of source sentences, against which all other corpora are compared, is sampled by traversing the original sentences, and for each sen- tence s, uniformly sample an annotation a, and given s, a, uniformly sample a sentence from E s,a .</p><p>Given a metric m ∈ METRICS, we compute its score for each sampled corpus. Where corpus- level scores are not defined by the metrics them- selves, we use the average sentence score instead. We compare the rankings induced by the scores of m and the ranking of systems according to their corpus model (i.e., systems that have a higher M should be ranked higher), and report the correla- tion between these rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments</head><p>Setup. For each model, we sample one correc- tion per NUCLE sentence, noting that it is possi- ble to reduce the variance of the metrics' corpus- level scores by sampling more. Corpus models of integer values between 0 and 10 are taken. We re- port Spearman ρ, commonly used for system-level rankings ( <ref type="bibr" target="#b3">Bojar et al., 2017</ref>). 4</p><p>Results. Results, presented in <ref type="table">Table 2</ref> (left part), shows that LT correlates best with the rankings in- duced by MAEGE, where GLEU is second. M 2 's correlation is only 0.06. We note that the LT re- quires a complementary metric to penalize gram- matical outputs that diverge in meaning from the source ( <ref type="bibr">Napoles et al., 2016b)</ref>. See §8.</p><p>Comparing the metrics' quality in corpus-level evaluation with their quality according to CHR ( §3), we find they are often at odds. <ref type="figure" target="#fig_5">Figure 4</ref>   <ref type="table">Table 2</ref>: Corpus-level Spearman ρ, sentence-level Pearson r and Kendall τ with the metrics (left). † represents P-value &lt; 0.001.</p><p>LT correlates best at the corpus level and has the highest sentence-level τ , while iBLEU has the highest sentence-level r. showing correlations are slightly correlated, but disagreements as to metric quality are frequent and substantial (e.g., with iBLEU or SARI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Sentence-level Analysis</head><p>We proceed by presenting a method for assessing the correlation between metric-induced scores of corrections of the same sentence, and the scores given to these corrections by MAEGE. Given a sentence s and an annotation a, we sample a ran- dom permutation over the edits in edits(s, a). We denote the permutation with σ ∈ S ns,a , where S ns,a is the permutation group over {1, · · · , n s,a }. Given σ, we define a monotonic chain in E i,j as:</p><formula xml:id="formula_2">chain(s, a, σ) = ∅ &lt; {e (σ(1)) s,a } &lt; {e (σ(1)) s,a , e (σ(2)) s,a } &lt; . . . &lt; edits(s, a)</formula><p>For each chain, we uniformly sample one of its el- ements, mark it as the source, and denote it with src. In order to generate a set of chains, MAEGE traverses the original sentences and annotations, and for each sentence-annotation pair, uniformly samples n ch chains without repetition. It then uni- formly samples a source sentence from each chain. If the number of chains in E s,a is smaller than n ch , MAEGE selects all the chains. Given a metric m ∈ METRICS, we compute its score for every correction in each sampled chain against the sampled source and available ref- erences. We compute the sentence-level correla- tion of the rankings induced by the scores of m and the rankings induced by &lt;. For computing rank correlation (such as Spearman ρ or Kendall τ ), such a relative ranking is sufficient.</p><p>We report Kendall τ , which is only sensitive to the relative ranking of correction pairs within the same chain. Kendall is minimalistic in its assump- tions, as it does not require numerical scores, but only assuming that &lt; is well-motivated, i.e., that applying a set of valid edits is better in quality than applying only a subset of it.</p><p>As &lt; is a partial order, and as Kendall τ is stan- dardly defined over total orders, some modifica- tion is required. τ is a function of the number of compared pairs and of discongruent pairs (ordered differently in the compared rankings):</p><formula xml:id="formula_3">τ = 1 − 2 |discongruent pairs| |all pairs| .</formula><p>To compute these quantities, we extract all unique pairs of corrections that can be compared with &lt; (i.e., one applies a sub-set of the edits of the other), and count the number of discongruent ones between the metric's ranking and &lt;. Signif- icance is modified accordingly. <ref type="bibr">5</ref> Spearman ρ is less applicable in this setting, as it compares total orders whereas here we compare partial orders.</p><p>To compute linear correlation with Pearson r, we make the simplifying assumption that all edits contribute equally to the overall quality. Specifi- cally, we assume that a perfect correction (i.e., the top of a chain) receives a score of 1. Each original sentence s (the bottom of a chain), for which there exists annotations a 1 , . . . , a n , receives a score of</p><formula xml:id="formula_4">1 − min i |edits(s, a i )| |tokens(s)| .</formula><p>The scores of partial (non-perfect) corrections in each chain are linearly spaced between the score of the perfect correction and that of the original. This scoring system is well-defined, as a partial correction receives the same score according to all chains it is in, as all paths between a partial cor- rection and the original have the same length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiments</head><p>Setup. We experiment with n ch = 1, yielding 7936 sentences in 1312 chains (same as the num- ber of original sentences in the NUCLE test set). We report the Pearson correlation over the scores of all sentences in all chains (r), and Kendall τ over all pairs of corrections within the same chain.</p><p>Results. Results are presented in <ref type="table">Table 2 (right  part)</ref>. No metric scores very high, neither ac- cording to Pearson r nor according to Kendall τ . iBLEU correlates best with &lt; according to r, ob- taining a correlation of 0.23, whereas LT fares best according to τ , obtaining 0.222.</p><p>Results show a discrepancy between the low corpus-level and sentence-level r correlations of M 2 and its high sentence-level τ . It seems that although M 2 orders pairs of corrections well, its scores are not a linear function of MAEGE's scores. This may be due to M 2 's assignment of the min- imal possible score to the source, regardless of its quality. M 2 thus seems to predict well the rela- tive quality of corrections of the same sentence, but to be less effective in yielding a globally co- herent score (cf. <ref type="bibr" target="#b15">Felice and Briscoe (2015)</ref>).</p><p>GLEU shows the inverse behaviour, failing to correctly order pairs of corrections of the same sentence, while managing to produce globally co- herent scores. We test this hypothesis by comput- ing the average difference in GLEU score between all pairs in the sampled chains, and find it to be slightly negative (-0.00025), which is in line with GLEU's small negative τ . On the other hand, plot- ting the GLEU scores of the originals grouped by the number of errors they contain, we find they correlate well <ref type="figure" target="#fig_6">(Figure 5</ref>), indicating that GLEU performs well in comparing the quality of correc- tions of different sentences. Four sentences with considerably more errors than the others were con- sidered outliers and removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Metric Sensitivity by Error Type</head><p>MAEGE's lattice can be used to analyze how the examined metrics reward corrections of errors of different types. For each edit type t, we denote with S t the set of correction pairs from the lattice that only differ in an edit of type t. For each such pair (c, c ) and for each metric m, we compute the difference in the score assigned by m to c and c . The average difference is denoted with ∆ m,t .</p><formula xml:id="formula_5">∆ m,t = 1 |S t | (c,c )∈St m(src, c, R)−m(src, c , R)</formula><p>R is the corresponding reference set. A neg- ative (positive) ∆ m,t indicates that m penalizes (awards) valid corrections of type t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experiments</head><p>Setup. We sample chains using the same sam- pling method as in §6, and uniformly sample a source from each chain. For each edit type t, we detect all pairs of corrections in the sampled chains that only differ in an edit of type t, and use them to compute ∆ m,t . We use the set of 27 edit types given in the NUCLE corpus.</p><p>Results. <ref type="table" target="#tab_3">Table 3</ref> presents the results, showing that under all metrics, some edits types are penal- ized and others rewarded. iBLEU and LT penalize the least edit types, and GLEU penalizes the most, providing another perspective on GLEU's negative Kendall τ ( §6). Certain types are penalized by al- most all metrics. One such type is Vm, wrong verb modality (e.g., "as they [∅ ; may] not want to know"). Another such type is Npos, a problem in noun possessive (e.g., "their [facebook's ; Face- book] page"). Other types, such as Mec, mechani- cal (e.g., "[real-life ; real life]"), and V0, missing verb (e.g., "'Privacy', this is the word that [∅ ; is] popular"), are often rewarded by the metrics.</p><p>In general, the tendency of reference-based met- rics (the vast majority of GEC metrics) to penal- ize edits of various types suggests that many edit  types are under-represented in available reference sets. Automatic evaluation of systems that per- form these edit types may, therefore, be unreliable. Moreover, not addressing these biases in the met- rics may hinder progress in GEC. Indeed, M 2 and GLEU, two of the most commonly used metrics, only award a small sub-set of edit types, thus of- fering no incentive for systems to improve perfor- mance on such types. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We revisit the argument that using system outputs to perform metric validation poses a methodolog- ical difficulty. Indeed, as GEC systems are de- veloped, trained and tested using available met- rics, and as metrics tend to reward some correc- tion types and penalize others ( §7), it is possible that GEC development adjusts to the metrics, and neglects some error types. Resulting tendencies in GEC systems would then yield biased sets of outputs for human rankings, which in turn would result in biases in the validation process.</p><p>To make this concrete, GEC systems are often precision-oriented: trained to prefer not to cor- rect than to invalidly correct. Indeed, Choshen and 6 LDS→O tends to award valid corrections of almost all types. As source sentences are randomized across chains, this indicates that on average, corrections with more applied ed- its tend to be more similar to comparable corrections on the lattice. This is also reflected by the slightly positive sentence- level correlation of LDS→O ( §6). <ref type="bibr" target="#b9">Abend (2018a)</ref> show that modern systems tend to be highly conservative, often performing an order of magnitude fewer changes to the source than ref- erences do. Validating metrics on their ability to rank conservative system outputs (as is de facto the common practice) may produce a different picture of metric quality than when considering a more in- clusive set of corrections.</p><p>We use MAEGE to mimic a setting of ranking against precision-oriented outputs. To do so, we perform corpus-level and sentence-level analyses, but instead of randomly sampling a source, we in- variably take the original sentence as the source. We thereby create a setting where all edits applied are valid (but not all valid edits are applied).</p><p>Comparing the results to the regular MAEGE correlation <ref type="table">(Table 4)</ref>, we find that LT remains re- liable, while M 2 , that assumes the source receives the worst possible score, gains from this unbal- anced setting. iBLEU drops, suggesting it may need to be retuned to this setting and give less weight to BLEU (O, S), thus becoming more like BLEU and GLEU. The most drastic change we see is in SARI and MAX-SARI, which flip their sign and present strong performance. Interest- ingly, the metrics that benefit from this precision- oriented setting in the corpus-level are the same metrics that perform better according to CHR than to MAEGE <ref type="figure" target="#fig_5">(Figure 4)</ref>. This indicates the different trends produced by MAEGE and CHR, may result <ref type="bibr">Corpus-level</ref> Sentence-level ρ P-val r P-val  <ref type="table">Table 4</ref>: Corpus-level Spearman ρ, sentence-level Pearson r and Kendall τ correlations using origin as the source with the various metrics (left). Correlations using a random source are found in parenthesis. † represents P − value &lt; 0.001. LT is the best corpus correlated, and has the best τ while iBLEU has the best r from the latter's use of precision-oriented outputs.</p><p>Drawbacks. Like any methodology MAEGE has its simplifying assumptions and drawbacks; we wish to make them explicit. First, any biases in- troduced in the generation of the test corpus are in- herited by MAEGE (e.g., that edits are contiguous and independent of each other). Second, MAEGE does not include errors that a human will not per- form but machines might, e.g., significantly al- tering the meaning of the source. This partially explains why LT, which measures grammaticality but not meaning preservation, excels in our ex- periments. Third, MAEGE's scoring system ( §6) assumes that all errors damage the score equally. While this assumption is made by GEC metrics, we believe it should be refined in future work by collecting user information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we show how to leverage existing annotation in GEC for performing validation re- liably. We propose a new automatic methodol- ogy, MAEGE, which overcomes many of the short- comings of the existing methodology. Experi- ments with MAEGE reveal a different picture of metric quality than previously reported. Our anal- ysis suggests that differences in observed metric quality are partly due to system outputs sharing consistent tendencies, notably their tendency to under-predict corrections. As existing methodol- ogy ranks system outputs, these shared tendencies bias the validation process. The difficulties in bas- ing validation on system outputs may be applica- ble to other text-to-text generation tasks, a ques- tion we will explore in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histogram and rug plot of the log number of references under I-measure assumptions, i.e. overlapping edits alternate as valid corrections of the same error. There are billions of ways to combine 8 references on average.</figDesc><graphic url="image-1.png" coords="3,307.28,62.81,196.43,147.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Combined</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the generated corrections lattices. The Ois are the original sentences, directed edges represent an application of an edit and R (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example chain from a corrections lattice-each sentence is the result of applying a single edit to the sentence below it. The top sentence is a perfect correction, while the bottom is the original.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A scatter plot of the corpus-level correlation of metrics according to the different methodologies. The x-axis corresponds to the correlation according to human rankings (Combined setting), and the y-axis corresponds to the correlation according to MAEGE. While some get similar correlation (e.g., GLEU), other metrics change drastically (e.g., SARI).</figDesc><graphic url="image-2.png" coords="5,72.00,290.18,196.43,147.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average GLEU score of originals (y-axis), plotted against the number of errors they contain (x-axis). Their substantial correlation indicates that GLEU is globally reliable.</figDesc><graphic url="image-3.png" coords="6,72.00,250.47,196.43,147.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>plots the Spearman correlation of the different metrics according to the two validation methodologies,</figDesc><table>Corpus-level 
Sentence-level 
ρ 
P-val 
r 
P-val 
τ 
P-val 

iBLEU 
0.418 0.200 
0.230  † 
0.050  † 
M 2 
0.060 0.853 -0.025 0.024 
0.213  † 
LT 
0.973 
 † 
0.167  † 
0.222  † 
BLEU 
0.564 0.071 
0.214  † 
0.111  † 
M inLDO→R -0.867 
 † 
0.011 0.327 -0.183  † 
GLEU 
0.736 0.001 
0.189  † 
-0.028  † 
MAX-SARI 
-0.809 0.003 
0.027 0.015 -0.070  † 
SARI 
-0.545 0.080 
0.061  † 
-0.039  † 

LDS→O 
-0.118 0.729 
0.109  † 
0.094  † 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average change in metric score by metric and edit types (∆m,t; see text). Rows correspond to edit types (abbreviations 
in Dahlmeier et al. (2013)); columns correspond to metrics. Some edit types are consistently penalized. 

</table></figure>

			<note place="foot" n="1"> There&apos;s a minor problem in the output of the NTHU system: a part of the input is given as sentence 39 and sentence 43 is missing. We corrected it to avoid unduly penalizing NTHU for all the sentences in this range.</note>

			<note place="foot" n="4"> Using Pearson correlation shows similar trends.</note>

			<note place="foot" n="5"> Code can be found in https://github.com/ borgr/EoE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Israel Science Foundation (grant No. 929/17), and by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister's Office. We thank Joel Tetreault and Courtney Napoles for helpful feedback and inspir-ing conversations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="343" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hume: Human ucca-based evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1264" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (wmt17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A grain of salt for the wmt manual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Ercegovčevi´ercegovčevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Results of the wmt16 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojevi´cstanojevi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic annotation and evaluation of error types for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="793" to="805" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How far are we from fully automatic high quality grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="697" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A systematic comparison of smoothing techniques for sentencelevel bleu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="362" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inherent biases in reference-based evaluation for grammatical error correction and text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Referenceless measure of faithfulness for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The nus corpus of learner english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hoo 2012: A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating human pairwise preference judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="345" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards a standard evaluation method for grammatical error detection and correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic extraction of learner errors in esl sentences using linguistically enhanced alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="835" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measurement of progress in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate evaluation of segment-level machine translation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human evaluation of grammatical error correction systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gillian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simulating human judgment in machine translation evaluation campaigns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation (IWSLT)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sankoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Putting human assessments of machine translation systems in order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Developing an open-source, rule-based proofreading tool. Software: Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Miłkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="543" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="588" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reference-less evaluation metrics in grammatical error correction</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2109" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL Shared Task</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reassessing the goals of grammatical error correction: Fluency instead of grammaticality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient elicitation of annotations for human evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mokry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04357</idno>
		<title level="m">Nematus: a toolkit for neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint learning of a dual smt system for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
