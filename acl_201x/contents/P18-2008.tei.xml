<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active learning for deep semantic parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Voicebox Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Afshar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Voicebox Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Estival</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Voicebox Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Pink</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Voicebox Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Voicebox Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Voicebox Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Active learning for deep semantic parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="43" to="48"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic parsing requires training data that is expensive and slow to collect. We apply active learning to both traditional and &quot;overnight&quot; data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We evaluate several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing maps a natural language query to a logical form (LF) <ref type="bibr">Collins, 2005, 2007;</ref><ref type="bibr" target="#b6">Haas and Riezler, 2016;</ref><ref type="bibr" target="#b11">Kwiatkowksi et al., 2010)</ref>. Producing training data for seman- tic parsing is slow and costly. Active learning is effective in reducing costly data requirements for many NLP tasks. In this work, we apply active learning to deep semantic parsing and show that we can substantially reduce the data required to achieve state-of-the-art results.</p><p>There are two main methods for generating se- mantic parsing training data. The traditional ap- proach first generates the input natural language utterances and then labels them with output LFs. We show that active learning based on uncertainty sampling works well for this approach.</p><p>The "overnight" annotation approach ( <ref type="bibr" target="#b17">Wang et al., 2015)</ref> generates output LFs from a grammar, and uses crowd workers to paraphrase these LFs into input natural language queries. This approach is faster and cheaper than traditional annotation. However, the difficulty and cost of data genera- tion and validation are still substantial if we need a large amount of data for the system to achieve high accuracy; if the logical forms can express complex combinations of semantic primitives that must be covered; or if the target language is one with rela- tively few crowd workers.</p><p>Applying active learning to the overnight ap- proach is even more compelling, since the unla- belled LFs can be generated essentially for free by a grammar. However, conventional active learning strategies are not compatible with the overnight approach, since the crowd annotators produce in- puts (utterances) rather than labels (LFs).</p><p>In order to apply active learning to deep se- mantic parsing, we need a way of selecting hy- perparameters without requiring the full training dataset. For optimal performance, we should re- run hyperparameter tuning for each active learning round, but this is prohibitively expensive compu- tationally. We show that hyperparameters selected using a random subset of the data (about 20%) per- form almost as well as those from the full set.</p><p>Our contributions are (1) a simple hyperparam- eter selection technique for active learning ap- plied to semantic parsing, and (2) straightforward active learning strategies for both traditional and overnight data collection that significantly reduce data annotation requirements. To the best of our knowledge we are the first to investigate active learning for overnight data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Sequence-to-sequence models are currently the state-of-the-art for semantic parsing <ref type="bibr" target="#b8">(Jia and Liang, 2016;</ref><ref type="bibr" target="#b3">Dong and Lapata, 2016;</ref><ref type="bibr" target="#b4">Duong et al., 2017)</ref>. In this paper, we also exploit a sequence- to-sequence model to minimise the amount of la-belled training data required to achieve state-of- the-art semantic parsing results.</p><p>Active learning has been applied to a variety of machine learning and NLP tasks <ref type="bibr" target="#b15">(Thompson et al., 1999;</ref><ref type="bibr" target="#b14">Tang et al., 2002;</ref><ref type="bibr" target="#b1">Chenguang Wang, 2017</ref>) employing various algorithms such as least confidence score <ref type="bibr" target="#b2">(Culotta and McCallum, 2005</ref>), large margin ( <ref type="bibr" target="#b13">Settles and Craven, 2008)</ref>, entropy based sampling, density weighting method <ref type="bibr" target="#b12">(Settles, 2012)</ref>, and reinforcement learning <ref type="bibr" target="#b5">(Fang et al., 2017)</ref>. Nevertheless, there has been limited work applying active learning for deep semantic parsing with the exception of <ref type="bibr" target="#b7">Iyer et al. (2017)</ref>. Different from conventional active learning, they used crowd workers to select what data to annotate for traditional semantic parsing data collection.</p><p>In this paper, we apply active learning for both traditional and overnight data collection with the focus on overnight approach. In addition, a limi- tation of prior active learning work is that the hy- perparameters are usually predefined in some way, mostly from different work on the same or simi- lar dataset, or from the authors experience ( <ref type="bibr" target="#b5">Fang et al., 2017)</ref>. In this paper, we investigate how to efficiently set the hyperparam- eters for the active learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Base S2S Model</head><p>We base our approach on the attentional sequence- to-sequence model (S2S) of <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref>. This attentional model uses a bidirec- tional recurrent neural network (RNN) to encode a source as a sequence of vectors, which are used by another RNN to generates output. Given the source utterance x = [x 1 , x 2 , ...x n ] and target LF y = [y 1 , y 2 , ...y m ], we train the model to minimize the loss under model parameters ✓.</p><formula xml:id="formula_0">loss = m X i=1 log P(y i |y 1 , ..y i1 , x; ✓)<label>(1)</label></formula><p>Additionally, we apply the UNK replacement tech- nique in <ref type="bibr" target="#b4">Duong et al. (2017)</ref>, keeping the original sentence in the data. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Active learning models</head><p>There is a diversity of strategies for active learn- ing. A simple and effective active learning strat- egy is based on least confidence score <ref type="bibr" target="#b2">(Culotta and McCallum, 2005</ref>). This strategy selects ut- terance x 0 to label from the unlabelled data U x as follows:</p><formula xml:id="formula_1">x 0 = argmin x2Ux ⇥ max y ⇤ P(y ⇤ |x; ✓) ⇤</formula><p>where y ⇤ is the most likely output. We found that this least confidence score works well across datasets, even better than more complicated strate- gies in traditional data collection (described be- low).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Traditional data collection</head><p>In the traditional (forward) approach, we start with the list of unlabelled utterances and an initial seed of utterances paired with LFs. We gradually select utterances to annotate with the aim of maximizing the test score as early as possible. We use forward S2S sentence loss as defined in Equation <ref type="formula" target="#formula_0">(1)</ref> as the least confidence score measurement (i.e. select the instance with higher loss).</p><p>The drawback of a least confidence score strat- egy (and strategies based on other measurements such as large margin), is that they only leverage a single measurement to select utterances <ref type="bibr" target="#b13">(Settles and Craven, 2008)</ref>. To combine multiple measure- ments, we build a classifier to predict if the model will wrongly generate the LF given the utterance, and select those utterances for annotation. The classifier is trained on the data generated by run- ning 5-fold cross validation on annotated data. <ref type="bibr">2</ref> We exploit various features, including sentence log loss, the margin between the best and second best solutions, source sentence frequency, source encoder last hidden state and target decoder last hidden state (see supplementary material §A.1 for more detail) and various classifier architectures in- cluding logistic regression, feedforward networks and multilayer convolutional neural networks. On the development corpus, we observed that the least confidence score works as well as the classifier strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overnight data collection</head><p>In the overnight (backward) approach, we start with the set of all unlabelled LFs (U y ), and an ini- tial randomly-selected seed of LFs paired with ut- terances (i.e. labelled LFs L y ). The aim is to select LFs for which we should obtain utterances, max- imizing the test score as early as possible. In the overnight approach, we can't use the least confi- dence score (i.e. the forward S2S sentence loss) directly since we can't estimate P(y|x) because we don't know the utterance x. We have to some- how approximate this probability with regard to the performance on test.</p><p>A simple strategy is just to apply the backward S2S model and estimate P(x|y), e.g. we select LF y 0 to label from the unlabelled data U y as follows:</p><formula xml:id="formula_2">y 0 = argmin y2Uy ⇥ max x ⇤ P(x ⇤ |y; ✓) ⇤</formula><p>Essentially, we train the S2S model to predict the utterance given the LF. The motivation is that if we can reconstruct the utterance from the LF then we could possibly generate LFs from utterances. However, this strategy ignores one important as- pect of semantic parsing, which is that LFs are an abstraction of utterances. One utterance is mapped to only one LF, but one LF corresponds to many utterances.</p><p>Since the forward S2S loss performs so well, another strategy is to approximate the selections made by this score. We train a linear binary clas- sifier 3 to predict selections, using features which can be computed from LFs only. We extract two set of features from the LF model and the back- ward S2S model. The LF model is an RNN lan- guage model but trained on LFs ( <ref type="bibr" target="#b18">Zaremba et al., 2014</ref>). <ref type="bibr">4</ref> We extract the LF sentence log proba- bility i.e. log P(y), feature from this model. The backward S2S model, as mentioned above, is the model trained to predict an utterance given a LF. We extracted the same set of features as mentioned in §4.1 including LF sentence log loss, margin be- tween best and second best solutions, and LF fre- quencies.</p><p>On the development corpus, we first run one ac- tive learning round using forward S2S model sen- tence loss (i.e. modelling P(y|x)) on the initial an- notated data L y . The set of selected LFs based on forward S2S loss will be the positive exam- ples, and all other LFs that are not selected will be the negative examples for training the binary classifier. Our experiments show that the classi- fier which uses the combination of two features (source LF frequencies and the margin of best and second best solution) are the best predictor of what is selected by forward S2S model log loss (i.e. modelling P(y|x)). It is interesting to see that ab- solute score of backward S2S model loss is not a good indicator as it is not selected. This may be due to the fact that utterance-LF mapping is one- to-many and the model probability is distributed to all valid output utterances. Hence, low proba- bility is not necessary an indicator of bad predic- tion. We use the linear combination of the two features mentioned above with the weights from the binary classifier as a means of selecting the LF for overnight active learning on different corpora without retraining the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We experiment with the NLMaps corpus <ref type="bibr" target="#b6">(Haas and Riezler, 2016)</ref> which was collected using the traditional approach. We tokenize follow- ing Kočisk´ <ref type="bibr" target="#b10">Kočisk´y et al. (2016)</ref>. We also experiment with the Social Network corpus from the Overnight dataset ( <ref type="bibr" target="#b17">Wang et al., 2015</ref>) (which was collected using the overnight approach). Social Network was chosen as being the largest dataset available. Since neither corpora have a separate development set, we use 10% of the training set as development data for early stopping. We select ATIS <ref type="bibr" target="#b19">(Zettlemoyer and Collins, 2007)</ref> as our development cor- pus for all feature selection and experiments with classifiers in §4.1 and §4.2.</p><p>For evaluation, we use full LF exact match ac- curacy for all experiments <ref type="bibr" target="#b10">(Kočisk´Kočisk´y et al., 2016)</ref>. Note that this is a much stricter evaluation com- pared with running through database evaluator as in <ref type="bibr" target="#b17">Wang et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyperparameter tuning</head><p>Hyperparameter tuning is important for good per- formance. We tune the base S2S model ( §3) on the development data by generating 100 con- figurations using Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2014)</ref> and a permutation of different source and target RNN sizes, RNN cell types, initializer, dropout rates and mini-batch sizes.</p><p>As mentioned, hyperparameter tuning is often overlooked in active learning. The common ap- proach is just to use the configuration from a sim- ilar problem, from prior work on the same dataset,  or based on the authors own experience. How- ever, in practice we don't have any prior work to copy the configuration from. <ref type="table" target="#tab_0">Table 1</ref> shows the experiments with the NLMap and Social Network corpora with configurations: 1) copied from an- other dataset (ATIS), 2) tuned on a small subset (10% of train data plus development data) and 3) tuned on the full dataset. We can see that copy- ing from a different dataset results in a subopti- mal solution, which is expected since the different datasets are significantly different. It is surprising that tuning on small subset of the data performs as well as tuning on all the data and, more impor- tantly, it achieves similar results as the state of the art (SOTA). <ref type="figure" target="#fig_0">Figure 1</ref> shows the active learning curve for NLMap, ATIS and Overnight (Social Network) datasets. 10% of data is randomly selected as initial seed data for active learning and hyperpa- rameter tuning. We run active learning for 10 rounds, selecting 10% of the data at each round. Round 0 reports the result trained on the initial seed data and round 9 is the result on the whole training data. For reference, we also report Fw S2S for Social Network, treating that corpus as if they were collected using the traditional approach, and Bw S2S/classifier for NLMap and ATIS treating those corpora as if they were collected using the overnight approach. For traditional data collection (forward direc- tion), S2S loss consistently outperforms the ran- dom baselines on both datasets. The differences are as high as 9% for NLMap (at round 4). Apply- ing this strategy for ATIS, we reach SOTA results at round 4, using only 50% of data. We also exper- imented with the large margin baseline and classi- fier strategies as mentioned in §4.1. The least con- fidence strategy using S2S loss outperforms large margin and achieves similar performance with the more complicated classifier strategy, thus we omit those results for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Active Learning Results</head><p>On the overnight data collection active learn- ing (backward direction), the results are split. The backward S2S loss performs particularly well on the NLMap corpus, approximating the forward S2S performance. However, it performs similar to the random baseline in the other corpora. On the other hand, the classifier strategy performs well on both ATIS and Social Network but poorly on NLMap. Using this strategy, we approximate the SOTA for both ATIS and Social Network at round 5 and 6 respectively (saving 40% and 30% of data). We suspect that backward S2S loss per- forms so well on NLMap since there is a one-to- one mapping between utterance and LF. The num- ber of unique LFs in the training data for NLMap, ATIS and Overnight are 95.4%, 28.4% and 19.5% respectively. All in all, our proposed strategies for "overnight" active learning are nearly as good as traditional active learning, showing in similar area under the curve value in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have discussed practical active learning for deep semantic parsing. We have empirically shown that it is possible to get good hyperpa- rameters from only a small subset of annotated data. We applied active learning for both tradi- tional and overnight semantic parsing data collec- tion. For traditional data collection, we show that least confidence score based on S2S log loss per- forms well across datasets. Applying active learn- ing for overnight data collection is challenging, and the best performing strategy depends on the domain. We recommend that applications explore both the backward S2S and classifier strategies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Active learning for various selection criteria. Random baseline randomly select the training data at each round. Fw S2S is used for traditional data collection using forward S2S loss score. Bw S2S is used for overnight data collection using backward S2S loss score. Bw classifier is used also for the overnight approach but linearly combines several scores together as mentioned in §4.2. The scores in parentheses measure the area under the curve. The dashed lines are the SOTA from Table 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>NLMap Social ATIS 

From ATIS 
76.0 
65.8 
86.0 
Small subset 
84.2 
68.9 
85.7 
Full data 
84.2 
69.1 
86.0 

SOTA 
84.1 
68.8 
86.1 

Table 1: The LF exact match accuracy on NLMap, 
Social Network and ATIS with configurations 
from ATIS, from hyperparameter tuning on small 
subset of data (10% + dev) or on the full train-
ing data. The supervised SOTA for NLMap and 
ATIS (Duong et al., 2017) and Social Network (Jia 
and Liang, 2016) are provided for reference. 5 

</table></figure>

			<note place="foot" n="1"> We call S2S model applied to traditional data collection and overnight data collection as forward S2S and backward S2S respectively. The forward S2S model estimates P(y|x), the backward S2S model estimates P(x|y).</note>

			<note place="foot" n="2"> This classifier is complementary to the approach proposed in Iyer et al. (2017) where we use this classifier instead of user feedback.</note>

			<note place="foot" n="3"> Instead of binary classifier, it would also be possible to train a logistic model. However, we leave this for future work. 4 We use the configuration from Zaremba et al. (2014).</note>

			<note place="foot" n="5"> The exact match accuracy for Social Network is extracted from logs from (Jia and Liang, 2016).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active learning for black-box semantic role labeling with neural factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura Chiticariu Yunyao Li Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.24963/ijcai.2017/405</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/405" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2908" to="2914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing labeling effort for structured prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th National Conference on Artificial Intelligence</title>
		<meeting>the 20th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual semantic parsing and code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Estival</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/K17-1038</idno>
		<ptr target="https://doi.org/10.18653/v1/K17-1038" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning how to active learn: A deep reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D17-1063" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A corpus and semantic parser for multilingual natural language querying of openstreetmap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolin</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning a neural semantic parser from user feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR abs/1704.08760</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowksi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active learning for statistical natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073105</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL &apos;02</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Active learning for natural language parsing and information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><forename type="middle">A</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Elaine</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning</title>
		<meeting>the Sixteenth International Conference on Machine Learning<address><addrLine>San Francisco, CA, USA, ICML &apos;99</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cost-effective active learning for deep image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/1701.03551</idno>
		<ptr target="http://arxiv.org/abs/1701.03551" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1332" to="1342" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured /classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI &apos;05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07-26" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
