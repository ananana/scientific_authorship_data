<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Normalized Log-Linear Interpolation of Backoff Language Models is Efficient</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
							<email>kheafiel@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street Edinburgh</addrLine>
									<postCode>EH8 9AB</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chase</forename><surname>Geigle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street Edinburgh</addrLine>
									<postCode>EH8 9AB</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Massung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street Edinburgh</addrLine>
									<postCode>EH8 9AB</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street Edinburgh</addrLine>
									<postCode>EH8 9AB</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Normalized Log-Linear Interpolation of Backoff Language Models is Efficient</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="876" to="886"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
					<note>707 S. Mathews Ave. Urbana, IL 61801 United States</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model, contradicting Hsu (2007). While prior work reported that log-linear interpolation yields lower per-plexity than linear interpolation, normalizing at query time was impractical. We normalize the model offline in advance, which is efficient due to a recurrence relationship between the normalizing factors. To tune interpolation weights, we apply Newton&apos;s method to this convex problem and show that the derivatives can be computed efficiently in a batch process. These findings are combined in new open-source interpolation tool, which is distributed with KenLM. With 21 out-of-domain corpora, log-linear interpolation yields 72.58 per-plexity on TED talks, compared to 75.91 for linear interpolation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Log-linearly interpolated backoff language mod- els yielded better perplexity than linearly interpo- lated models <ref type="bibr" target="#b15">(Klakow, 1998;</ref><ref type="bibr" target="#b6">Gutkin, 2000)</ref>, but experiments and adoption were limited due the im- practically high cost of querying. This cost is due to normalizing to form a probability distribution by brute-force summing over the entire vocabu- lary for each query. Instead, we prove that the log-linearly interpolated model can be normalized offline in advance and exactly expressed as an or- dinary backoff language model. This contradicts <ref type="bibr" target="#b12">Hsu (2007)</ref>, who claimed that log-linearly inter- polated models "cannot be efficiently represented as a backoff n-gram model."</p><p>We show that offline normalization is efficient due to a recurrence relationship between the normalizing factors ( <ref type="bibr" target="#b27">Whittaker and Klakow, 2002</ref>). This forms the basis for our open- source implementation, which is part of KenLM: https://kheafield.com/code/kenlm/.</p><p>Linear interpolation <ref type="bibr" target="#b13">(Jelinek and Mercer, 1980)</ref>, combines several language models p i into a single model p L p L (w n | w n−1</p><formula xml:id="formula_0">1 ) = i λ i p i (w n | w n−1 1 )</formula><p>where λ i are weights and w n 1 are words. Because each component model p i is a probability distri- bution and the non-negative weights λ i sum to 1, the interpolated model p L is also a probability dis- tribution. This presumes that the models have the same vocabulary, an issue we discuss in §3.1.</p><p>A log-linearly interpolated model p LL uses the weights λ i as powers <ref type="bibr" target="#b15">(Klakow, 1998)</ref>. The weights λ i are unconstrained real numbers, allowing parameters to soften or sharpen distribu- tions. Negative weights can be used to divide a mixed-domain model by an out-of-domain model. To form a probability distribution, the product is normalized The sum is taken over all words x in the combined vocabulary of the underlying models, which can number in the millions or even billions. Comput- ing Z efficiently is a key contribution in this work.</p><p>Our proofs assume the component models p i are backoff language models <ref type="bibr" target="#b14">(Katz, 1987)</ref> that mem- orize probability for seen n-grams and charge a backoff penalty b i for unseen n-grams. p i (w n | w n−1 1 ) = p i (w n | w n−1 1 ) if w n 1 is seen p i (w n | w n−1 2 )b i (w n−1 1 ) o.w.</p><p>While linearly or log-linearly interpolated mod- els can be queried online by querying the compo- nent models <ref type="bibr" target="#b22">(Stolcke, 2002;</ref><ref type="bibr" target="#b5">Federico et al., 2008)</ref>, doing so costs RAM to store duplicated n-grams and CPU time to perform lookups. Log-linear in- terpolation is particularly slow due to normalizing over the entire vocabulary. Instead, it is preferable to combine the models offline into a single back- off model containing the union of n-grams. Do- ing so is impossible for linear interpolation ( §3.2); SRILM <ref type="bibr" target="#b22">(Stolcke, 2002</ref>) and MITLM ( <ref type="bibr" target="#b11">Hsu and Glass, 2008</ref>) implement an approximation. In con- trast, we prove that offline log-linear interpolation requires no such approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Instead of building separate models then weight- ing, <ref type="bibr" target="#b29">Zhang and Chiang (2014)</ref> show how to train Kneser-Ney models <ref type="bibr" target="#b16">(Kneser and Ney, 1995)</ref> on weighted data. Their work relied on prescriptive weights from domain adaptation techniques rather than tuning weights, as we do here.</p><p>Our exact normalization approach relies on the backoff structure of component models. Sev- eral approximations support general models: ig- noring normalization ), noise- contrastive estimation <ref type="bibr" target="#b25">(Vaswani et al., 2013)</ref>, and self-normalization ( <ref type="bibr" target="#b0">Andreas and Klein, 2015)</ref>. In future work, we plan to exploit the structure of other features in high-quality unnormalized log- linear language models ( <ref type="bibr" target="#b20">Sethy et al., 2014</ref>).</p><p>Ignoring normalization is particularly common in speech recognition and machine translation. This is one of our baselines. Unnormalized mod- els can also be compiled into a single model by multiplying the weighted probabilities and back- offs. <ref type="bibr">1</ref> Many use unnormalized models because weights can be jointly tuned along with other fea- ture weights. However, <ref type="bibr" target="#b7">Haddow (2013)</ref> showed that linear interpolation weights can be jointly tuned by pairwise ranked optimization ( <ref type="bibr" target="#b10">Hopkins and May, 2011</ref>). In theory, normalized log-linear interpolation weights can be jointly tuned in the same way. Dynamic interpolation weights <ref type="bibr" target="#b26">(Weintraub et al., 1996)</ref> give more weight to models famil- iar with a given query. Typically the weights are a function of the contexts that appear in the combined language model, which is compatible with our approach. However, normalizing factors would need to be calculated in each context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linear Interpolation</head><p>To motivate log-linear interpolation, we examine two issues with linear interpolation: normalization when component models have different vocabular- ies and offline interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vocabulary Differences</head><p>Language models are normalized with respect to their vocabulary, including the unknown word.</p><formula xml:id="formula_1">x∈vocab(p 1 ) p 1 (x) = 1</formula><p>If two models have different vocabularies, then the combined vocabulary is larger and the sum is taken over more words. Component models as- sign their unknown word probability to these new words, leading to an interpolated model that sums to more than one. An example is shown in <ref type="table">Table 1</ref>.  <ref type="table">Table 1</ref>: Linearly interpolating two models p 1 and p 2 with equal weight yields an unnormalized model p L . If gaps are filled with zeros instead, the model is normalized.</p><p>To work around this problem, SRILM <ref type="bibr" target="#b22">(Stolcke, 2002</ref>) uses zero probability instead of the un- known word probability for new words. This pro- duces a model that sums to one, but differs from what users might expect.</p><p>IRSTLM <ref type="bibr" target="#b5">(Federico et al., 2008</ref>) asks the user to specify a common large vocabulary size. The un- known word probability is downweighted so that all models sum to one over the large vocabulary.</p><p>A component model can also be renormalized with respect to a larger vocabulary. For unigrams, the extra mass is the number of new words times the unknown word probability. For longer con- texts, if we assume the typical case where the unknown word appears only as a unigram, then queries for new words will back off to unigrams. The total mass in context w n−1</p><formula xml:id="formula_2">1 is 1 + |new|p(&lt;unk&gt;) n−1 i=1 b(w n−1 i )</formula><p>where new is the set of new words. This is effi- cient to compute online or offline. While there are tools to renormalize models, we are not aware of a tool that does this for linear interpolation. Log-linear interpolation is normalized by con- struction. Nonetheless, in our experiments we ex- tend IRSTLM's approach by training models with a common vocabulary size, rather than retrofitting it at query time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Linear Interpolation</head><p>Given an interpolated model, offline interpolation seeks a combined model meeting three criteria: (i) encoding the same probability distribution, (ii) be- ing a backoff model, and (iii) containing the union of n-grams from component models. Proof. By counterexample. Consider the models given in <ref type="table">Table 2</ref> interpolated with equal weight. <ref type="table">Table 2</ref>: Counterexample models.</p><formula xml:id="formula_3">p 1 p 2 p L p(A) 0.4 0.2 0.3 p(B) 0.3 0.3 0.3 p(C) 0.3 0.5 0.4 p(C | A) 0.4 0.8 0.6 b(A) 6 7 ≈ 0.857 0.4 2 3 ≈ 0.667</formula><p>The probabilities shown for p L result from encod- ing the same distribution. Taking the union of n- grams implies that p L only has entries for A, B, C, and A C. Since the models have the same vocabu- lary, they are all normalized to one.</p><formula xml:id="formula_4">p(A | A) + p(B | A) + p(C | A) = 1</formula><p>Since all models have backoff structure,</p><formula xml:id="formula_5">p(A)b(A) + p(B)b(A) + p(C | A) = 1</formula><p>which when solved for backoff b(A) gives the val- ues shown in <ref type="table">Table 2</ref>. We then query p L (B | A) online and offline. Online interpolation yields</p><formula xml:id="formula_6">p L (B | A) = 1 2 p 1 (B | A) + 1 2 p 2 (B | A) = 1 2 p 1 (B)b 1 (A) + 1 2 p 2 (B)b 2 (A) = 33 175</formula><p>Offline interpolation yields</p><formula xml:id="formula_7">p L (B | A) = p L (B)b L (A) = 0.2 = 33 175 ≈ 0.189</formula><p>The same problem happens with real language models. To understand why, we attempt to solve for the backoff b L (w n−1</p><formula xml:id="formula_8">1 ). Supposing w n 1 is not in either model, we query p L (w n | w n−1 1 ) offline pL(wn|w n−1 1 ) =pL(wn|w n−1 2 )bL(w n−1 1 ) =(λ1p1(wn|w n−1 2 ) + λ2p2(wn|w n−1 2 ))bL(w n−1 1 )</formula><p>while online interpolation yields</p><formula xml:id="formula_9">pL(wn|w n−1 1 ) =λ1p1(wn|w n−1 1 ) + λ2p2(wn|w n−1 1 ) =λ1p1(wn|w n−1 2 )b1(w n−1 1 ) + λ1p2(wn|w n−1 2 )b2(w n−1 1 ) Solving for b L (w n−1 1 ) we obtain λ1p1(wn|w n−1 2 )b1(w n−1 1 ) + λ2p2(wn|w n−1 2 )b2(w n−1 1 ) λ1p1(wn|w n−1 2 ) + λ2p2(wn|w n−1 2 )</formula><p>which is a weighted average of the backoff weights b 1 (w n−1 1 ) and b 2 (w n−1 1 ). The weights depend on w n , so b L is no longer a function of w n−1 1 .</p><p>In the SRILM approximation <ref type="bibr" target="#b22">(Stolcke, 2002)</ref>, probabilities for n-grams that exist in the model are computed exactly. The backoff weights are chosen to produce a model that sums to one. However, newer versions of SRILM (Stolcke et al., 2011) interpolate by ingesting one component model at a time. For example, the first two mod- els are approximately interpolated before adding a third model. An n-gram appearing only in the third model will have an approximate probabil- ity. Therefore, the output depends on the order in which users specify models. Moreover, weights were optimized for correct linear interpolation, not the approximation. <ref type="bibr" target="#b22">Stolcke (2002)</ref> find that the approximation actu- ally decreases perplexity, which we also see in the experiments ( §6). However, approximation only happens when the model backs off, which is less likely to happen in fluent sentences used for per- plexity scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Offline Log-Linear Interpolation</head><p>Log-linearly interpolated backoff models p i can be collapsed into a single offline model p LL . The combined model takes the union of n-grams in component models. <ref type="bibr">2</ref> For those n-grams, it mem- orizes correct probability p LL .</p><formula xml:id="formula_10">p LL (w n | w n−1 1 ) = i p i (w n | w n−1 1 ) λ i Z(w n−1 1 )<label>(1)</label></formula><p>When w n 1 does not appear, the backoff b LL (w n−1 1 ) modifies p LL (w n | w n−1 2 ) to make an appropri- ately normalized probability. To do so, it can- cels out the shorter query's normalization term Z(w n−1 2 ) then applies the correct term Z(w n−1 1 ). It also applies the component backoff terms.</p><formula xml:id="formula_11">b LL (w n−1 1 ) = Z(w n−1 2 ) Z(w n−1 1 ) i b i (w n−1 1 ) λ i<label>(2)</label></formula><p>Almost by construction, the model satisfies two of our criteria ( §3.2): being a backoff model and containing the union of n-grams. However, back- off models require that the backoff weight of an unseen n-gram be implicitly 1. </p><formula xml:id="formula_12">Z(w n−1 1 ) = x i p i (x | w n−1 1 ) λ i = x i p i (x | w n−1 2 ) λ i b i (w n−1 1 ) λ i = x i p i (x | w n−1 2 ) λ i = Z(w n−1 2 )</formula><p>All of the models back off because w n−1 1 x is un- seen, being a superstring of w n−1 </p><formula xml:id="formula_13">Z(w n−1 2 ) Z(w n−1 1 ) i b i (w n−1 1 ) λ i = Z(w n−1 2 ) Z(w n−1 1 ) = 1</formula><p>We now have a backoff model containing the union of n-grams. It remains to show that the of- fline model produces correct probabilities.</p><p>Theorem 2. The proposed offline model agrees with online log-linear interpolation.</p><p>Proof. By induction on the number of words backed off in offline interpolation. To disam- biguate, we will use p on to refer to online inter- polation and p off to refer to offline interpolation. Base case: the queried n-gram is in the offline model and we have memorized the online prob- ability by construction. Inductive case: Let p off (w n | w n−1 1 ) be a query that backs off. In online interpolation,</p><formula xml:id="formula_14">p on (w n | w n−1 1 ) = i p i (w n | w n−1 1 ) λ i Z(w n−1 1 )</formula><p>Because w n 1 is unseen in the offline model and we took the union, it is unseen in every model p i .</p><formula xml:id="formula_15">= i p i (w n | w n−1 2 ) λ i b i (w n−1 1 ) λ i Z(w n−1 1 ) = i p i (w n | w n−1 2 ) λ i i b i (w n−1 1 ) λ i Z(w n−1 1 )</formula><p>Recognizing the unnormalized probability</p><formula xml:id="formula_16">Z(w n−1 2 )p on (w n | w n−1 2 ), = Z(w n−1 2 )p on (w n | w n−1 2 ) i b i (w n−1 1 ) λ i Z(w n−1 1 ) = p on (w n | w n−1 2 ) Z(w n−1 2 ) Z(w n−1 1 ) i b i (w n−1 1 ) λ i = p on (w n | w n−1 2 )b off (w n−1 1 )</formula><p>The last equality follows from the definition of b off and Lemma 1, which extended the domain of b off to any w n−1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>. By the inductive hypothesis,</p><formula xml:id="formula_17">p on (w n | w n−1 2 ) = p off (w n | w n−1 2</formula><p>) because it backs off one less time.</p><formula xml:id="formula_18">= p off (w n | w n−1 2 )b off (w n−1 1 ) = p off (w n | w n−1 1 )</formula><p>The offline model p off (w n | w n−1 1 ) backs off be- cause that is the case we are considering. Combin- ing our chain of equalities,</p><formula xml:id="formula_19">p on (w n | w n−1 1 ) = p off (w n | w n−1 1 )</formula><p>By induction, the claim holds for all w n 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Normalizing Efficiently</head><p>In order to build the offline model, the normaliza- tion factor Z needs to be computed in every seen context. To do so, we extend the tree-structure method of <ref type="bibr" target="#b27">Whittaker and Klakow (2002)</ref>, which they used to compute and cache normalization fac- tors on the fly. It exploits the sparsity of language models: when summing over the vocabulary, most queries will back off. Formally, we define s(w n 1 ) to be the set of words x where p i (x | w n 1 ) does not back off in some model. s(w n 1 ) = {x : w n 1 x is seen in any model} To exploit this, we use the normalizing factor Z(w n 2 ) from a lower order and patch it up by sum- ming over s(w n 1 ). Theorem 3. The normalization factors Z obey a recurrence relationship:</p><formula xml:id="formula_20">Z(w n 1 ) = x∈s(w n 1 ) i p i (x | w n 1 ) λ i +   Z(w n 2 ) − x∈s(w n 1 ) i p i (x | w n 2 ) λ i   i b i (w n 1 ) λ i</formula><p>Proof. The first term handles seen n-grams while the second term handles unseen n-grams. The definition of Z</p><formula xml:id="formula_21">Z(w n 1 ) = x∈vocab i p i (x | w n 1 ) λ i</formula><p>can be partitioned by cases.</p><formula xml:id="formula_22">x∈s(w n 1 ) i p i (x | w n 1 ) λ i + x ∈s(w n 1 ) i p i (x | w n 1 ) λ i</formula><p>The first term agrees with the claim, so we focus on the case where x ∈ s(w n 1 ). By definition of s, all models back off.</p><p>x ∈s(w n 1 )</p><formula xml:id="formula_23">i p i (x | w n 1 ) λ i = x ∈s(w n 1 ) i p i (x | w n 2 ) λ i b i (w n 1 ) λ i =   x ∈s(w n 1 ) i p i (x | w n 2 ) λ i   i b i (w n 1 ) λ i =   Z(w n 2 ) − x∈s(w n 1 ) i p i (x | w n 2 ) λ i   i b i (w n 1 ) λ i</formula><p>This is the second term of the claim. Context sort w n 1 , m(w n 1 ), i p i (w n |w n−1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM</head><formula xml:id="formula_24">m i (w n 1 ) ) λ i ), i p i (w n |w n−1 m i (w n 2 ) ) λ i ) In context order w n 1 , i b i (w n−1 1 ) λ i , i p i (w n | w n−1 1 ) λ i , i p i (w n | w n−1 2 ) λ i In suffix order b LL (w n 1 )</formula><p>Suffix sort The recurrence structure of the normalization factors suggests a computational strategy: com- pute Z() by summing over the unigrams, Z(w n ) by summing over bigrams w n x, Z(w n n−1 ) by sum- ming over trigrams w n n−1 x, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Streaming Computation</head><p>Part of the point of offline interpolation is that there may not be enough RAM to fit all the com- ponent models. Moreover, with compression tech- niques that rely on immutable models <ref type="bibr" target="#b28">(Whittaker and Raj, 2001;</ref><ref type="bibr" target="#b23">Talbot and Osborne, 2007)</ref>, a mu- table version of the combined model may not fit in RAM. Instead, we construct the offline model with disk-based streaming algorithms, using the frame- work we designed for language model estimation ( <ref type="bibr" target="#b8">Heafield et al., 2013)</ref>. Our pipeline <ref type="figure" target="#fig_3">(Figure 1</ref>) has four conceptual steps: merge probabilities, apply backoffs, normalize, and output. Applying back- offs and normalization are performed in the same pass, so there are three total passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Merge Probabilities</head><p>This step takes the union of n-grams and multi- plies probabilities from component models. We assume that the component models are sorted in suffix order <ref type="figure">(Figure 4)</ref>, which is true of models produced by lmplz ( <ref type="bibr" target="#b8">Heafield et al., 2013</ref>) or stored in a reverse trie. Moreover, despite having different word indices, the models are consistently sorted using the string word, or a hash thereof. <ref type="table">Table 3</ref>: Merging probabilities processes n-grams in lexicographic order by suffix. Column headings indicate precedence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">2 1 A A A A A A B A A B</head><p>The algorithm processes n-grams in lexico- graphic (depth-first) order by suffix <ref type="table">(Table 3)</ref>. In this way, the algorithm processes p i (A) before it might be used as a backoff point for p i (A | B) in one of the models. It jointly streams through all models, so that p 1 (A | B) and p 2 (A | B) are avail- able at the same time. Ideally, we would compute unnormalized probabilities.</p><formula xml:id="formula_25">i p i (w n | w n−1 1 ) λ i</formula><p>However, these queries back off when models con- tain different n-grams. The appropriate backoff weights b i (w n−1 1 ) are not available in a stream- ing fashion. Instead, we proceed without charging backoffs</p><formula xml:id="formula_26">i p i (w n | w n−1 m i (w n 1 ) ) λ i</formula><p>where m i (w n 1 ) records what backoffs should be charged later.</p><p>The normalization step ( §4.2.3) also uses lower- order probabilities</p><formula xml:id="formula_27">i p i (w n | w n−1 2 ) λ i</formula><p>and needs to access them in a streaming fashion, so we also output <ref type="table">Table 4</ref>: Sorting orders ( <ref type="bibr" target="#b8">Heafield et al., 2013</ref>). In suffix order, the last word is primary. In context order, the penultimate word is primary. Column headings indicate precedence.</p><formula xml:id="formula_28">i p i (w n | w n−1 m i (w n 2 ) ) λ i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suffix 3 2 1 Z B A Z A B B B B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context 2 1 3 Z A B B B B Z B A</head><p>Each output tuple has the form</p><formula xml:id="formula_29">w n 1 , m(w n 1 ), i p i (w n |w n−1 m i (w n 1 ) ) λ i , i p i (w n |w n−1 m i (w n 2 ) ) λ i</formula><p>where m(w n 1 ) is a vector of backoff requests, from which m(w n 2 ) can be computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Apply Backoffs</head><p>This step fulfills the backoff requests from merg- ing probabilities. The merged probabilities are sorted in context order <ref type="table">(Table 4)</ref>  The same applies to the lower order.</p><formula xml:id="formula_30">i p i (w n |w n−1 2 ) λ i</formula><p>This step also merges backoffs from component models, with output still in context order.</p><formula xml:id="formula_31">w n 1 , i b i (w n−1 1 ) λ i , i p i (w n |w n−1 1 ) λ i i p i (w n |w n−1 2 ) λ i</formula><p>The implementation is combined with normaliza- tion, so the tuple is only conceptual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Normalize</head><p>This step computes normalization factor Z for all contexts, which it applies to produce p LL and b LL . Recalling §4.1, Z(w n− <ref type="formula" target="#formula_10">1   1</ref> ) is efficient to com- pute in a batch process by processing suffixes Z(), Z(w n ), . . . Z(w n−1 2 ) first. In order to min- imize memory consumption, we chose to evaluate the contexts in depth-first order by suffix, so that Z(A) is computed immediately before it is needed to compute Z(A A) and forgotten at Z(B).</p><p>Computing Z(w n−1 1 ) by applying Theorem 3 requires the sum</p><formula xml:id="formula_32">x∈s(w n−1 1 ) i p i (x | w n−1 1 ) λ i</formula><p>where s(w n−1 1 ) restricts to seen n-grams. For this, we stream through the output of the apply backoffs step in context order, which makes the various val- ues of x consecutive. Theorem 3 also requires a sum over the lower-order unnormalized probabili- ties</p><formula xml:id="formula_33">x∈s(w n−1 1 ) i p i (x | w n−1 2 ) λ i</formula><p>We placed these terms in the input tuple for w n−1 1 x. Otherwise, it would be hard to access these values while streaming in context order.</p><p>While we have shown how to compute Z(w n−1 1 ), we still need to normalize the probabil- ities. Unfortunately, Z(w n−1 1 ) is only known after streaming through all records of the form w n−1 1 x, which are the very same records to normalize. We therefore buffer up to the vocabulary size for each order in memory to allow rewinding. Processing context w n−1 1 thus yields normalized probabilities p LL (x | w n−1 1 ) for all seen w n−1</p><formula xml:id="formula_34">1 x. w n 1 , p LL (x | w n−1 1 )</formula><p>These records are generated in context order, the same order as the input. The normalization step also computes backoffs.</p><formula xml:id="formula_35">b LL (w n−1 1 ) = Z(w n−1 2 ) Z(w n−1 1 ) i b i (w n−1 1 ) λ i Normalization Z(w n−1 1</formula><p>) is computed by this step, numerator Z(w n−1 2 ) is available due to depth-first search, and the backoff terms i b i (w n−1 1 ) λ i are present in the input. The backoffs b LL are gener- ated in suffix order, since each context produces a backoff value. These are written to a sidechannel stream as bare values without keys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Output</head><p>Language model toolkits store probability p LL (w n | w n−1 1 ) and backoff b LL (w n 1 ) together as values for the key w n 1 . To reunify them, we sort w n 1 , p LL (w n | w n−1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>) in suffix order and merge with the backoff sidechannel from normalization, which is already in suffix order. Suffix order is also preferable because toolkits can easily build a reverse trie data structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tuning</head><p>Weights are tuned to maximize the log probability of held-out data. This is a convex optimization problem <ref type="bibr" target="#b15">(Klakow, 1998)</ref>. Iterations are expensive due to the need to normalize over the vocabulary at least once. However, the number of weights is small, which makes the Hessian matrix cheap to store and invert. We therefore selected Newton's method. <ref type="bibr">3</ref> The log probability of tuning data w is The gradient with respect to λ i has a compact form n log p i (w n | w n−1</p><formula xml:id="formula_36">1 ) + CH(p LL , p i | w n−1 1 )</formula><p>where CH is cross entropy. However, comput- ing the cross entropy directly would entail a sum over the vocabulary for every word in the tun- ing data. Instead, we apply Theorem 3 to ex- press Z(w n− <ref type="formula" target="#formula_10">1   1</ref> ) in terms of Z(w n−1 2 ) before tak- ing the derivative. This allows us to perform the same depth-first computation as before ( §4. we apply Theorem 3 to compute the Hessian for w n 1 in terms of the Hessian for w n 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We perform experiments for perplexity, query speed, memory consumption, and effectiveness in a machine translation system. Individual language models were trained on En- glish corpora from the WMT 2016 news transla- tion shared task ( <ref type="bibr" target="#b2">Bojar et al., 2016)</ref>. This includes the seven newswires (afp, apw, cna, ltw, nyt, wpb, xin) from English Gigaword Fifth Edition <ref type="figure" target="#fig_3">(Parker et al., 2011)</ref>; the 2007-2015 news crawls; 4 News discussion; News commmentary v11; En- glish from Europarl v8 ( <ref type="bibr" target="#b18">Koehn, 2005)</ref>; the English side of the French-English parallel corpus ( <ref type="bibr" target="#b1">Bojar et al., 2013)</ref>; and the English side of SETIMES2 <ref type="bibr" target="#b24">(Tiedemann, 2009)</ref>. We additionally built one lan- guage model trained on the concatenation of all of the above corpora. All corpora were prepro- cessed using the standard Moses ( <ref type="bibr" target="#b17">Koehn et al., 2007</ref>) scripts to perform normalization, tokeniza- tion, and truecasing. To prevent SRILM from run- ning out of RAM, we excluded the large mono- lingual CommonCrawl data, but included English from the parallel CommonCrawl data.</p><p>All language models are 5-gram backoff lan- guage models trained with modified Kneser-Ney smoothing <ref type="bibr" target="#b3">(Chen and Goodman, 1998</ref>) using lmplz ( <ref type="bibr" target="#b8">Heafield et al., 2013</ref>). Also to prevent SRILM from running out of RAM, we pruned sin- gleton trigrams and above.</p><p>For linear interpolation, we tuned weights us- ing IRSTLM. To work around SRILM's limitation of ten models, we interpolated the first ten then carried the combined model and added nine more component models, repeating this last step as nec- essary. Weights were normalized within batches to achieve the correct final weighting. This simply extends the way SRILM internally carries a com- bined model and adds one model at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Perplexity experiments</head><p>We experiment with two domains: TED talks, which is out of domain, and news, which is in- domain for some corpora. For TED, we tuned on the IWSLT 2010 English dev set and test on the 2010 test set. For news, we tuned on the English side of the WMT 2015 Russian-English evaluation set and test on the WMT 2014 Russian- English evaluation set. To measure generalization, we also evaluated news on models tuned for TED and vice-versa. Results are shown in   Log-linear interpolation performs better on TED (72.58 perplexity versus 75.91 for offline lin- ear interpolation). However, it performs worse on news. In future work, we plan to investigate whether log-linear wins when all corpora are out- of-domain since it favors agreement by all models. <ref type="table" target="#tab_3">Table 6</ref> compares the speed and memory per- formance of the competing methods. While the log-linear tuning is much slower, its compilation is faster compared to the offline linear model's long run time. Since the model formats are the same for the concatenation and log-linear, they share the fastest query speeds. Query speed was mea- sured using <ref type="bibr">KenLM's (Heafield, 2011</ref>) faster prob- ing data structure. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MT experiments</head><p>We trained a statistical phrase-based machine translation system for Romanian-English on the Romanian-English parallel corpora released as part of the 2016 WMT news translation shared task. We trained three variants of this MT system. The first used a single language model trained on the concatenation of the 21 individual LM train- ing corpora. The second used 22 language mod- els, with each LM presented to Moses as a sep- arate feature. The third used a single language model which is an interpolation of all 22 mod- els. This variant was run with offline linear, online linear, and log-linear interpolation. All MT sys- tem variants were optimized using IWSLT 2011 Romanian-English TED test as the development set, and were evaluated using the IWSLT 2012 Romanian-English TED test set.</p><p>As shown in <ref type="table">Table 7</ref>, no significant difference in MT quality as measured by BLEU was observed; the best BLEU score came from separate features at 18.40 while log-linear scored 18.15. We leave  <ref type="table">Table 7</ref>: Machine translation performance com- parison in an end-to-end system.</p><p>jointly tuned normalized log-linear interpolation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Normalized log-linear interpolation is now a tractable alternative to linear interpolation for backoff language models. Contrary to <ref type="bibr" target="#b12">Hsu (2007)</ref>, we proved that these models can be exactly col- lapsed into a single backoff language model. This solves the query speed problem. Empiri- cally, compiling the log-linear model is faster than SRILM can collapse its approximate offline linear model. In future work, we plan to improve per- formace of feature weight tuning and investigate more general features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>p</head><label></label><figDesc>LL (w n | w n−1 1 ) ∝ i p i (w n | w n−1 1 ) λ i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 1 .</head><label>1</label><figDesc>The three offline criteria cannot be satisfied for general linearly interpolated backoff models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 1 .</head><label>1</label><figDesc>If w n−1 1 is unseen in the combined model, then the backoff weight b LL (w n−1 1 ) = 1. Proof. Because we have taken the union of en- tries, w n−1 1 is unseen in component models. These components are backoff models, so implicitly b i (w n−1 1 ) = 1 ∀i. Focusing on the normalization term Z(w n−1 1 ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .</head><label>1</label><figDesc>Relevant back- off weights b i (w n−1 1 ) = 1 as noted earlier. Recall- ing the definition of b LL (w n−1 1 ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-stage streaming pipeline for offline log-linear interpolation. Bold arrows indicate sorting is performed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>so that n- grams w n 1 sharing the same context w n−1 1 are consecutive. Moreover, contexts w n−1 1 appear in suffix order. We use this property to stream through the component models again in their native suffix order, this time reading backoff weights b i (w n−1 1 ), b i (w n−1 2 ), . . . , b i (w n−1 ). Mul- tiplying the appropriate backoff weights by i p i (w n |w n−1 m i (w n 1 ) ) λ i yields unnormalized proba- bility i p i (w n |w n−1 1 ) λ i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>log n p LL (w n | w n−1 1 ) which expands according to the definition of p LL n i λ i log p i (w n | w n−1 1 ) − log Z(w n−1 1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The same argument applies when taking the Hessian with respect to λ i and λ j . Rather than compute it directly in the form n − x pLL(x|w n−1 1 ) log pi(x|w n−1 1 ) log pj(x|w n−1 1 ) + CH(pLL, pi | w n−1 1 )CH(pLL, pj | w n−1 1 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>4 For News Crawl 2014, we used version 2. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Test set perplexities. In the middle ta-
ble, weights are optimized for TED and include 
a model trained on all concatenated text. In the 
bottom table, weights are optimized for news and 
exclude the model trained on all concatenated text. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 : Speed and memory consumption of LM combination methods.</head><label>6</label><figDesc></figDesc><table>Interpolated models include the 
</table></figure>

			<note place="foot" n="1"> Missing probabilities are found from the backoff algorithm and missing backoffs are implicitly one.</note>

			<note place="foot" n="2"> We further assume that every substring of a seen n-gram is also seen. This follows from estimating on text, except in the case of adjusted count pruning by SRILM. In such cases, we add the missing entries to component models, with no additional memory cost in trie data structures.</note>

			<note place="foot" n="3"> We also considered minibatches, though grouping tuning data to reduce normalization cost would introduce bias.</note>

			<note place="foot" n="5"> KenLM does not natively implement online linear interpolation, so we wrote a custom wrapper, which is faster than querying IRSTLM.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to João Sedoc, Grant Erdmann, Jeremy Gwinnup, Marcin Junczys-Dowmunt, Chris Dyer, Jon Clark, and MT Marathon attendees for discus-sions. Partial funding was provided by the Ama-zon Academic Research Awards program. This material is based upon work supported by the NSF GRFP under Grant Number DGE-1144245.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When and why are log-linear models self-normalizing?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the 2013 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liane</forename><surname>Guillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<title level="m">Proceedings of the First Conference on Machine Translation (WMT&apos;16)</title>
		<meeting>the First Conference on Machine Translation (WMT&apos;16)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
	<note>Findings of the 2016 Conference on Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998-08" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Topic adaptation for language modeling using unnormalized exponential models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Seymore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 IEEE International Conference on</title>
		<meeting>the 1998 IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
	<note>Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">IRSTLM: an open source toolkit for handling large scale language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Log-linear interpolation of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gutkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-11" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Applying pairwise ranked optimisation to improve the interpolation of translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iterative language model estimation: Efficient data structure &amp; algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized linear interpolation of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition &amp; Understanding, 2007. ASRU. IEEE Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="136" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpolated estimation of Markov source parameters from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Pattern Recognition in Practice</title>
		<meeting>the Workshop on Pattern Recognition in Practice</meeting>
		<imprint>
			<date type="published" when="1980-05" />
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slava</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987-03" />
		</imprint>
	</monogr>
	<note>ASSP</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Log-linear interpolation of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Conference on Spoken Language Processing</title>
		<meeting>5th International Conference on Spoken Language Processing<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="page" from="1695" to="1699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
	<note>Czech Republic</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit</title>
		<meeting>MT Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">English gigaword fifth edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Static interpolation of exponential n-gram models using features of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustic, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SRILM at sixteen: Update and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Abrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2011 IEEE Workshop on Automatic Speech Recognition &amp; Understanding (ASRU)</title>
		<meeting>2011 IEEE Workshop on Automatic Speech Recognition &amp; Understanding (ASRU)<address><addrLine>Waikoloa, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Spoken Language Processing</title>
		<meeting>the Seventh International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Randomised language modelling for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">News from OPUS-A collection of multilingual parallel corpora with tools and interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
		<editor>N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov</editor>
		<meeting><address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
	<note>John Benjamins, Amsterdam/Philadelphia</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Weintraub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaman</forename><surname>Aksu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satya</forename><surname>Dharanipragada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liz</forename><surname>Shriberg</surname></persName>
		</author>
		<title level="m">LM95 project report: Fast training and portability. Research Note 1, Center for Language and Speech Processing</title>
		<imprint>
			<date type="published" when="1996-02" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient construction of long-range language models using log-linear interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="905" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quantization-based language model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech<address><addrLine>Aalborg, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-12" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kneser-Ney smoothing on expected counts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
