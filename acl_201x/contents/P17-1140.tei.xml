<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1524" to="1534"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1140</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based N-MT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) ( <ref type="bibr" target="#b2">Brown et al., 1993;</ref><ref type="bibr" target="#b15">Koehn et al., 2003;</ref><ref type="bibr" target="#b3">Chiang, 2005</ref>) which provides word reordering knowledge to ensure reason- able translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style.</p><p>In recent years, end-to-end NMT <ref type="bibr" target="#b13">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) has made tremen- dous progress <ref type="bibr" target="#b11">(Jean et al., 2015;</ref><ref type="bibr" target="#b18">Luong et al., 2015b;</ref><ref type="bibr" target="#b25">Shen et al., 2016;</ref><ref type="bibr" target="#b24">Sennrich et al., 2016;</ref><ref type="bibr" target="#b30">Tu et al., 2016;</ref><ref type="bibr" target="#b8">Zhou et al., 2016;</ref><ref type="bibr" target="#b12">Johnson et al., 2016</ref>). An encoder-decoder framework ( <ref type="bibr" target="#b5">Cho et al., 2014b;</ref>) with attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> is widely used, in which an encoder compresses the source sentence, an attention mechanism evaluates related source words and a decoder generates target words.</p><p>The attention mechanism evaluates the dis- tribution of to-be-translated source words in a content-based addressing fashion ( <ref type="bibr" target="#b10">Graves et al., 2014</ref>) which tends to attend to the source words regarding the content relation with current translation status. Lack of ex- plicit models to exploit the word reordering knowledge may lead to attention faults and generate fluent but inaccurate or inadequate translations. <ref type="table">Table 1</ref> shows a translation in- stance and <ref type="figure" target="#fig_0">Figure 1</ref> depicts the corresponding word alignment matrix that produced by the attention mechanism. In this example, even though the word "zuixin (latest)" is a common adjective in Chinese and its following word should be translated soon in Chinese to En- glish translation direction, the word "yiju (ev- idence)" does not obtain appropriate attention which leads to the incorrect translation. src youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de('s) zuixin(latest) yiju(evidence) . ref the report is the latest evidence that supports their arguments .</p><p>NMT the report supports their perception of the latest . count zuixin yiju {0} <ref type="table">Table 1</ref>:</p><p>An instance in Chinese-English translation task. The row "count" represents the frequency of the word collocation in the training corpus. The collocation "zuixin yiju" does not appear in the training data. To enhance the attention mechanism, im- plicit word reordering knowledge needs to be incorporated into attention-based NMT. In this paper, we introduce three distortion mod- els that originated from SMT ( <ref type="bibr" target="#b2">Brown et al., 1993;</ref><ref type="bibr" target="#b15">Koehn et al., 2003;</ref><ref type="bibr" target="#b20">Och et al., 2004;</ref><ref type="bibr" target="#b29">Tillmann, 2004;</ref><ref type="bibr" target="#b0">Al-Onaizan and Papineni, 2006)</ref>, so as to model the word reordering knowledge as the probability distribution of the relative jump distances between the newly translated source word and the to-be-translated source word. Our focus is to extend the attention mechanism to attend to source words regard- ing both the semantic requirement and the word reordering penalty.</p><p>Our models have three merits:</p><p>1. Extended word reordering knowledge. Our models capture explicit word reordering knowledge to guide the attending process for attention mechanism.</p><p>2. Convenient to be incorporated into attention-based NMT. Our distortion models are differentiable and can be trained in the end-to-end style. The inter- polation approach ensures that the pro- posed models can coordinately work with the original attention mechanism.</p><p>3. Flexible to utilize variant context for com- puting the word reordering penalty. In this paper, we exploit three categories of in- formation as distortion context conditions to compute the word reordering penalty, but variant context information can be u- tilized due to our model's flexibility.</p><p>We validate our models on the Chinese- English translation task and achieve notable improvements:</p><p>• On 16K vocabularies, NMT models are usually inferior in comparison with the phrase-based SMT, but our model sur- passes phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points.</p><p>• On 30K vocabularies, the improvements over the phrase-based Moses and the attention-based NMT baseline system are average 6.06 and 1.57 BLEU points re- spectively.</p><p>• Compared with previous work on identi- cal corpora, we achieve the state-of-the- art translation performance on average.</p><p>The word alignment quality evaluation shows that our model can effectively improve the word alignment quality that is crucial for im- proving translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We aim to capture word reordering knowledge for the attention-based NMT by incorporat- ing distortion models. This section briefly in- troduces attention-based NMT and distortion models in SMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention-based Neural Machine Translation</head><p>Formally, given a source sentence x = x 1 , ..., x m and a target sentence y = y 1 , ..., y n , NMT models the translation probability as</p><formula xml:id="formula_0">P (y|x) = n ∏ t=1 P (y t |y &lt;t , x),<label>(1)</label></formula><p>where y &lt;t = y 1 , ..., y t−1 . The generation probability of y t is</p><formula xml:id="formula_1">P (y t |y &lt;t , x) = g(y t−1 , c t , s t ),<label>(2)</label></formula><p>where g(·) is a softmax regression function, y t−1 is the newly translated target word and   s t is the hidden states of decoder which repre- sents the translation status. The attention c t denotes the related source words for generating y t and is computed as the weighted-sum of source representation h upon an alignment vector α t shown in Eq. <ref type="formula">(3)</ref> where the align(·) function is a feedforward network with sof tmax normalization.</p><formula xml:id="formula_2">c t = m ∑ j=1 α t,j h j α t,j = align(s t , h j ) (3)</formula><p>The hidden states s t is updated as</p><formula xml:id="formula_3">s t = f (s t−1 , y t−1 , c t ),<label>(4)</label></formula><p>where f (·) is a recurrent function. We adopt a varietal attention mechanism 1 in our in-house RNNsearch model which is im- plemented as</p><formula xml:id="formula_4">s t = f 1 (s t−1 , y t−1 ), α t,j = align( s t , h j ), s t = f 2 ( s t , c t ),<label>(5)</label></formula><p>where f 1 (·) and f 2 (·) are recurrent functions. As shown in Eq.(3), the attention mecha- nism attends to source words in a content- based addressing way without considering any explicit word reordering knowledge. We in- troduce distortion models to capture explicit word reordering knowledge for enhancing the attention mechanism and improving transla- tion quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distortion Models in SMT</head><p>In SMT, distortion models are linearly com- bined with other features, as follows,</p><formula xml:id="formula_5">y * = arg max y exp[λ d d(x, y, b)+ R−1 ∑ r=1 λ r h r (x, y, b)],<label>(6)</label></formula><p>where d(·) is the distortion feature, h r (·) repre- sents other features, λ d and λ r are the weights, b is the latent variable that represents trans- lation knowledge and R is the number of fea- tures.</p><p>IBM Models ( <ref type="bibr" target="#b2">Brown et al., 1993</ref>) depict- ed the word reordering knowledge as position- al relations between source and target word- s. <ref type="bibr" target="#b15">Koehn et al. (2003)</ref> proposed a distortion model for phrase-based SMT based on jump distances between the newly translated phras- es and to-be-translated phrases which does not consider specific lexical information. <ref type="bibr" target="#b20">Och et al. (2004)</ref> and <ref type="bibr" target="#b29">Tillmann (2004)</ref> proposed orientation-based distortion models that con- sider translation orientations. <ref type="bibr" target="#b0">Yaser and Papineni (2006)</ref> proposed a distortion model to estimate probability distribution on possible relative jumps conditioned on source words.</p><p>These models are proposed for SMT and separately trained as sub-components. In- spired by these previous work, we introduce the distortion models into NMT model for modeling the word reordering knowledge. Our proposed models are designed for NMT which can be trained in the end-to-end style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distortion Models for attention-based NMT</head><p>The basic idea of our proposed distortion mod- els is to estimate the probability distribution of the possible relative jump distances between the newly translated source word and the to- be-translated source word upon the context condition. <ref type="figure" target="#fig_2">Figure 2</ref> shows the general archi- tecture of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Architecture</head><p>We employ an interpolation approach to incor- porate distortion models into attention-based NMT as</p><formula xml:id="formula_6">α t = λ · d t + (1 − λ)ˆ α t ,<label>(7)</label></formula><p>Figure 3: Illustration of shift actions of the alignment vector α t−1 . If α t is the left shift of α t−1 , it represents the translation orienta- tion of the source sentence is backward and if α t is the right shift of α t−1 , the translation orientation is forward.</p><p>where α t is the ultimate alignment vector for computing the related source context c t , d t is the alignment vector calculated by the distor- tion model, ˆ α t is the alignment vector com- puted by the basic attention mechanism and λ is a hyper-parameter to control the weight of the distortion model.</p><p>In the proposed distortion model, relative jumps on source words are depicted as the "shift" actions of the alignment vector α t−1 which is shown in the <ref type="figure">Figure 3</ref>. The right shift of α t−1 indicates that the translation orienta- tion of source words is forward and the left shift represents that the translation orienta- tion is backward. The extent of a shift action measures the word reordering distance. Align- ment vector d t , which is produced by the dis- tortion model, is the expectation of all possible shifts of α t−1 conditioned on certain context. Formally, the proposed distortion model is</p><formula xml:id="formula_7">d t = E[Γ(α t−1 )] = l ∑ k=−l P (k|Ψ) · Γ(α t−1 , k),<label>(8)</label></formula><p>where k ∈ [−l, l] is the possible relative jump distance, l is the window size parameter and P (k|Ψ) stands for the probability of jump dis- tance k that conditioned on the context Ψ. Function Γ(·) for shifting the alignment vec- tor is defined as</p><formula xml:id="formula_8">Γ(α t−1 , k) =      {α t−1,−k , ..., α t−1,m , 0, ..., 0}, k&lt;0 α t−1 , k= 0 {0, ..., 0, α t−1,1 , ..., α t−1,m−k }, k&gt;0<label>(9)</label></formula><p>which can be implemented as matrix multi- plication computations.</p><p>We respectively exploit source context, target context and translation status con- text (hidden states of decoder) as Ψ and derive three distortion models: Source-based Distortion (S-Distortion) model , Target- based Distortion (T-Distortion) model and Translation-status-based Distortion (H-Distortion) model. Our framework is capable of utilizing arbitrary context as the condition Ψ to predict the relative jump distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">S-Distortion model</head><p>S-Distortion model adopts previous source context c t−1 as the context Ψ with the intu- ition that certain source word indicate certain jump distance. The to-be-translated source word have intense positional relations with the newly translated one.</p><p>The underlying linguistic intuition is that synchronous grammars <ref type="bibr">(Yamada and Knight, 2001;</ref><ref type="bibr" target="#b9">Galley et al., 2004</ref>) can be extracted from language pairs. Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry specific word reordering knowledge.</p><p>To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in <ref type="table">Ta- ble 1</ref> as follows,</p><formula xml:id="formula_9">N P −→ JJ N N | JJ N N JJ −→ zuixin | latest.<label>(10)</label></formula><p>From the above grammar, we can conjecture the speculation that after the word "zuix- in(latest)" is translated, the translation orien- tation is forward with shift distance 1. The probability function in S-Distortion model is defined as follows,</p><formula xml:id="formula_10">P (·|Ψ) = z(c t−1 ) = sof tmax(W c c t−1 + b c ),<label>(11)</label></formula><p>where W c ∈ R (2l+1)×dim(c t−1 ) and b c ∈ R 2l+1 are weight matrix and bias parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">T-Distortion Model</head><p>T-Distortion model exploits the embedding of the previous generated target word y t−1 as the context condition to predict the probability distribution of distortion distances. It focuses on the word reordering knowledge upon target word context. As illustrated in Eq.(10), the target word "latest" possesses word reordering knowledge that is identical with source word "zuixin". The probability function in T-Distortion model is defined as follows,</p><formula xml:id="formula_11">P (·|Ψ) = z(y t−1 ) = sof tmax(W y emb(y t−1 ) + b y ),<label>(12)</label></formula><p>where emb(y t−1 ) is the embedding of y t−1 , W y ∈ R (2l+1)×dim(emb(y t−1 )) and b y ∈ R 2l+1 are weight matrix and bias parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">H-Distortion Model</head><p>The hidden states˜sstates˜ states˜s t−1 reflect the translation status and contains both source context and target context information. Therefore, we ex- ploit˜sploit˜ ploit˜s t−1 as context Ψ in the H-Distortion model to predict shift distances. The probability function in H-Distortion model is defined as follows,</p><formula xml:id="formula_12">P (·|Ψ) = z(˜ s t−1 ) = sof tmax(W s ˜ s t−1 + b s )<label>(13)</label></formula><p>where W s ∈ R (2l+1)×dim(˜ s t−1 ) and b s ∈ R 2l+1 are the weight matrix and bias parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We carry the translation task on the Chinese- English direction to evaluate the effectiveness of our models. To investigate the word align- ment quality, we take the word alignmen- t quality evaluation on the manually aligned corpus. We also conduct the experiments to observe effects of hyper-parameters and the training strategies.  <ref type="bibr">[2003]</ref><ref type="bibr">[2004]</ref><ref type="bibr">[2005]</ref><ref type="bibr">[2006]</ref> are used as test sets. To assess the word alignment quality, we employ Tsinghua dataset ( <ref type="bibr" target="#b16">Liu and Sun, 2015</ref>) which contains 900 manually aligned sentence pairs. Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU <ref type="bibr">3 (Papineni et al., 2002</ref>). Sign-test <ref type="bibr" target="#b7">(Collins et al., 2005</ref>) is exploited for statistical signifi- cance test. Alignment error rate (AER) <ref type="bibr" target="#b21">(Och and Ney, 2003</ref>) is calculated to assess the word alignment quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Systems</head><p>We compare our approaches with three base- line systems: Moses ( <ref type="bibr" target="#b14">Koehn et al., 2007</ref>): An open source phrase-based SMT system with default set- tings. Words are aligned with GIZA++ (Och and Ney, 2003). The 4-gram language mod- el with modified Kneser-Ney smoothing is trained on the target portion of training da- ta by SRILM ( <ref type="bibr" target="#b27">Stolcke et al., 2002</ref>). Groundhog <ref type="bibr">4</ref> : An open source attention- based NMT system with default settings. RNNsearch * : Our in-house implementation of NMT system with the varietal attention mechanism and other settings that presented in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any re- strictions. Following <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>, we use bi-directional Gated Recurrent Unit (GRU) as the encoder. The forward repre- sentation and the backward representation are concatenated at the corresponding position as the ultimate representation of a source word. The word embedding dimension is set to 620 and the hidden layer size is 1000. The interpo- lation parameter λ is 0.5 and the window size l is set to 3.  Gaussian distribution with mean 0 and vari- ance 0.01 2 . All bias are initialized to 0. Parameters are updated by Mini-batch Gra- dient Descent and the learning rate is con- trolled by the AdaDelta <ref type="bibr">(Zeiler, 2012)</ref> algorith- m with decay constant ρ = 0.95 and denomi- nator constant ϵ = 1e − 6. The batch size is 80. Dropout strategy ( <ref type="bibr" target="#b26">Srivastava et al., 2014</ref>) is applied to the output layer with the dropout rate 0.5 to avoid over-fitting. The gradients of the cost function which have L2 norm larger than a predefined threshold 1.0 is normalized to the threshold to avoid gradients explosion ( <ref type="bibr" target="#b23">Pascanu et al., 2013)</ref>. We exploit length nor- malization ( <ref type="bibr" target="#b4">Cho et al., 2014a</ref>) on candidate translations and the beam size for decoding is 12. For NMT with distortion models, we use trained RNNsearch * model to initialize param- eters except for those related to distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>The translation quality experiment results are shown in <ref type="table" target="#tab_3">Table 2</ref>. We carry the experiments on different vocabulary sizes for that different vocabulary sizes cause different degrees of the rare word collocations. Through this way, we can validate the effects of our proposed models in alleviating the rare word collocations prob- lem that leads to incorrect word alignments. On 16K vocabularies: The phrase-based Moses performs better than the basic NMTs including Groundhog and RNNsearch * . Be- sides the differences between model archi- tectures, restricted vocabularies and sentence length also affect the performance of NMTs. However, RNNsearch * with distortion models surpass phrase-based Moses by average 3.60, 4.27 and 4.43 BLEU points. RNNsearch * out- performs Groundhog by average 1.96 BLEU points due to the varietal attention mech- anism, length normalization and dropout s- trategies. Distortion models bring about re- markable improvements as 4.26, 4.92 and 5.09 BLEU points over the RNNsearch * model. On 30K vocabularies: RNNsearch * with distortion models yield average gains by 1.57, 1.21 and 1.45 BLEU points over RNNsearch * and outperform phrase-based Moses by aver- age 6.06, 5.70 and 5.94 BLEU points and sur- pass GroundHog by average 5.56, 5.20 and 5.44 BLEU points. RNNsearch * (16K) with distortion models achieve close performances with RNNsearch * (30K). The improvements on 16K vocabularies are larger than that on 30K vocabularies for the intuition that more "UN- K" words lead to more rare word collocations, which results in serious attention ambiguities.</p><p>The RNNsearch * with distortion models yield tremendous improvements on BLEU s- cores proves the effectiveness of proposed ap- proaches in improving translation quality. Comparison with previous work: We present the performance comparison with pre-     <ref type="table">Table 4</ref>: BLEU-4 scores (%) and AER scores on Tsinghua manually aligned Chinese-English evaluation set. The lower the AER score, the better the alignment quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Length MT03 MT04 MT05 MT06 Average</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU AER</head><p>vious work that employ identical training cor- pora in <ref type="table" target="#tab_5">Table 3</ref>. Our work evidently outper- forms previous work on average performance. Although we restrict the maximum length of sentence to 50, our model achieves the state- of-the-art BLEU scores on almost all test sets except NIST2006.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>We investigate the effects on the alignment quality of our models and conduct the exper- iments to evaluate the influence of the hyper- parameter settings and the training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Alignment Quality</head><p>Distortion models concentrate on attending to to-be-translated words based on the word reordering knowledge and can intuitively en- hance the word alignment quality. To in- vestigate the effect on word alignment qual- ity, we apply the BLEU and AER evalua- tions on Tsinghua manually aligned data set.   <ref type="table">Table 5</ref>: Comparison between pre-training and no pre-training H-Distortion model. The per- formances are consistent. <ref type="table">Table 4</ref> lists the BLEU and AER scores of Chinese-English translation with 30K vocabu- lary. RNNsearch*(30K) with distortion mod- els achieve significant improvements on BLEU scores and obvious decrease on AER scores. The results shows that the proposed model can effectively improve the word alignment quality <ref type="figure" target="#fig_3">Figure 4</ref> shows the output of distortion model and ultimate alignment matrix of the above-mentioned instance. Compared with <ref type="figure" target="#fig_0">Figure 1</ref>, the alignment matrix produced by NMT with distortion models is more concen- trated and accurate. The output of distortion model shows its capacity of modeling word re- ordering knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effect of Hyper-parameters</head><p>To investigate the effect of the weight hyper- parameter λ and window hyper-parameter l in the proposed model, we carry experiments on H-Distortion model with variable hyper- parameter settings. We fix l = 3 for exploring the effect of λ and fix λ = 0.5 for observing the effect of l. <ref type="figure" target="#fig_4">Figure 5</ref> presents the trans- lation performances with respect to hyper- parameters.</p><p>With the increase of weight λ, the BLEU scores first rise and then drop, which shows the distortion model provides ad- ditional helpful information while can not fully cover the attention mechanism for its insuffi- cient content searching ability. For window l, the experiments show that larger windows bring slight further improvements, which in- dicates that distortion model pays more at- tention to the short-distance reordering knowl- edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Pre-training VS No Pre-training</head><p>We conduct the experiment without using pre- training strategy to observe the effect of the initialization. As is shown in <ref type="table">Table 5</ref>, the no-pre-training model achieves consistent im- provements with the pre-training one which verifies the stable effectiveness of our ap- proach. Initialization with pre-training strate- gy provides a fast approach to obtain the mod- el for it needs fewer training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is inspired by the distortion model- s that widely used in SMT. The most related work in SMT is the distortion model proposed by <ref type="bibr" target="#b0">Yaser and Papineni (2006)</ref>. Their mod- el is identical to our S-Distortion model that captures the relative jump distance knowledge on source words. However, our approach is deliberately designed for the attention-based NMT system and is capable of exploiting vari- ant context information to predict the relative jump distances.</p><p>Our work is related to the work ( <ref type="bibr" target="#b17">Luong et al., 2015a;</ref><ref type="bibr" target="#b8">Feng et al., 2016;</ref><ref type="bibr" target="#b30">Tu et al., 2016;</ref><ref type="bibr" target="#b6">Cohn et al., 2016;</ref><ref type="bibr" target="#b19">Meng et al., 2016;</ref><ref type="bibr">Wang et al., 2016</ref>) that concentrate on the improve- ment of the attention mechanism. To remit the computing cost of the attention mecha- nism when dealing with long sentences, <ref type="bibr" target="#b17">Luong et al. (2015a)</ref> proposed the local atten- tion mechanism by just focusing on a sub- scope of source positions. <ref type="bibr" target="#b6">Cohn et al. (2016)</ref> incorporated structural alignment biases in- to the attention mechanism and obtained improvements across several challenging lan- guage pairs in low-resource settings. <ref type="bibr" target="#b8">Feng et al. (2016)</ref> passed the previous attention con- text to the attention mechanism by adding re- current connections as the implicit distortion model. <ref type="bibr" target="#b30">Tu et al. (2016)</ref> maintained a cover- age vector for keeping the attention history to acquire accurate translations. <ref type="bibr" target="#b19">Meng et al. (2016)</ref> proposed the interactive attention with the attentive read and attentive write opera- tion to keep track of the interaction history. <ref type="bibr">Wang et al. (2016)</ref> utilized an external memo- ry to store additional information for guiding the attention computation. These works are different from ours, as our distortion models explicitly capture word reordering knowledge through estimating the probability distribu- tion of relative jump distances on source words to incorporate word reordering knowledge into the attention-based NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented three distortion models to enhance attention-based NMT through incor- porating the word reordering knowledge. The basic idea of proposed distortion models is to enable the attention mechanism to attend to the source words regarding both semantic re- quirement and the word reordering penalty. Experiments show that our models can ev- idently improve the word alignment quality and translation performance. Compared with previous work on identical corpora, our mod- el achieves the state-of-the-art performance on average. Our model is convenient to be ap- plied in the attention-based NMT and can be trained in the end-to-end style. We also in- vestigated the effect of hyper-parameters and pre-training strategy and further proved the stable effectiveness of our model. In the fu- ture, we plan to validate the effectiveness of our model on more language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mingxuan Wang, Zhengdong Lu, Hang Li, and</head><p>Qun Liu. 2016. Memory-enhanced decoder for neural machine translation. In Proceedings of EMNLP2016. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The source word "yiju" does not obtain appropriate attention and its word sense is completely neglected.</figDesc><graphic url="image-1.png" coords="2,80.33,62.41,201.94,181.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Encoder</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The general architecture of our proposed models. The dash line represents variant context can be utilized to determine the word reordering penalty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) is the output of the distortion model and is calculated on shift actions of previous alignment vector. (b) is the ultimate word alignment matrix of attention-based NMT with H-Distortion model. Compared with Figure 1, (b) is more centralized and accurate.</figDesc><graphic url="image-4.png" coords="7,329.46,249.88,187.72,197.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Translation performance on the test sets with respect to the hyper-parameter λ and l.</figDesc><graphic url="image-6.png" coords="8,310.72,72.61,165.80,112.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU-4 scores (%) on NIST test set 03-06 of Moses (default settings), Groundhog 
(default settings), RNNsearch  *  and RNNsearch  *  with distortion models respectively. The val-
ues in brackets are increases on RNNsearch  *  , Moses and Groundhog respectively.  ‡ indicates 
statistical significant difference (p&lt;0.01) from RNNsearch  *  and  † means statistical significant 
difference (p&lt;0.05) from RNNsearch  *  . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison with previous work on identical training corpora. Coverage (Tu et al., 
2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and 
under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality 
with external memory. NMT IA (Meng et al., 2016) exploits a readable and writable attention 
mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion 
model. The vocabulary sizes of all work are 30K and maximum lengths of sentence differ. 

</table></figure>

			<note place="foot" n="1"> https://github.com/nyu-dl/dl4mttutorial/tree/master/session2</note>

			<note place="foot" n="2"> The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.</note>

			<note place="foot" n="3"> ftp://jaguar.ncsl.nist.gov/mt/resources/ mteval-v11b.pl 4 https://github.com/lisa-groundhog/ GroundHog</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>Qun Liu's work is partially supported by Science Foundation Ireland in the ADAP-T Centre for Digital Content Technology (www.adaptcentre.ie) at Dublin City Univer-sity funded under the SFI Research Centres Programme (Grant 13/RC/2106) co-funded under the European Regional Development Fund. We are grateful to Qiuye Zhao, Fan-dong Meng and Daqi Zheng for their helpful suggestions. We thank the anonymous review-ers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distortion models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2006</title>
		<meeting>ACL2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR2015</title>
		<meeting>ICLR2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation:parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2005</title>
		<meeting>ACL2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax,Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL2016</title>
		<meeting>NAACL2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kučerová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2005</title>
		<meeting>ACL2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Implicit distortion and fertility models for attention-based encoder-decoder nmt model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Shu Jie Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03317</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2014</title>
		<meeting>ACL2014</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<idno>arXiv preprint arX- iv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP2013</title>
		<meeting>EMNLP2013<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL2007 Demo and Poster Sessions</title>
		<meeting>the ACL2007 Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings NAACL2003</title>
		<meeting>NAACL2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contrastive unsupervised word alignment with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI2015</title>
		<meeting>AAAI2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2295" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP2015. Lisbon, Portugal</title>
		<meeting>EMNLP2015. Lisbon, Portugal</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2015</title>
		<meeting>ACL2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="82" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING2016</title>
		<meeting>COLING2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A smorgasbord of features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2002. Association for Computational Linguistics</title>
		<meeting>ACL2002. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2016</title>
		<meeting>ACL2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2016</title>
		<meeting>ACL2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on spoken language processing</title>
		<meeting>the international conference on spoken language processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS2014</title>
		<meeting>NIPS2014</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A unigram orientation model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2004: Short Papers</title>
		<meeting>HLT-NAACL 2004: Short Papers</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
