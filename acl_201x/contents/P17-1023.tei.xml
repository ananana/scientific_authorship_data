<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Obtaining referential word meanings from visual and distributional information: Experiments on object naming</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
						</author>
						<title level="a" type="main">Obtaining referential word meanings from visual and distributional information: Experiments on object naming</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="243" to="254"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1023</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Expressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., house in <ref type="figure">Figure 1 (a)</ref>, or, as a very general type, thingy in <ref type="figure">Figure 1</ref> (d). Determin- ing such a name is a crucial step for referring expression generation (REG) systems, as many other decisions concerning, e.g., the selection of attributes follow from it ( <ref type="bibr" target="#b1">Dale and Reiter, 1995;</ref><ref type="bibr" target="#b17">Krahmer and Van Deemter, 2012</ref>). For a long time, however, research on REG mostly assumed the availability of symbolic representations of ref- (a)"house" (b)"buildings"</p><p>(c)"large structure" (d)"roof thingy"</p><p>Figure 1: Examples of object names in the REFERIT corpus referring to instances of buildings erent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.</p><p>Recent advances in image processing promise to fill this gap, with state-of-the-art computer vi- sion systems being able to classify images into thousands of different categories (e.g. <ref type="bibr">Szegedy et al. (2015)</ref>). However, classification is not nam- ing ( <ref type="bibr" target="#b27">Ordonez et al., 2016)</ref>. Standard object clas- sification schemes are inherently "flat", and treat object labels as mutually exclusive <ref type="bibr" target="#b3">(Deng et al., 2014)</ref>. A state-of-the-art object recognition sys- tem would be trained to classify the object in e.g. <ref type="figure">Figure 1</ref> (a) as either house or building, ignoring the lexical similarity between these two names. In contrast, humans seem to be more flexible as to the chosen level of generality. Depending on the prototypicality of the object to name, and possi- bly other visual properties, a general name might be more or less appropriate. For instance, a robin can be named bird, but a penguin is better referred to as "penguin" <ref type="bibr" target="#b28">(Rosch, 1978)</ref>; along the same lines, the rather unusual building in <ref type="figure">Figure 1</ref> (c) that is not easy to otherwise categorise was named "structure".</p><p>Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a continuous representation of word meaning, i.e. through cross-modal transfer into distributional vector spaces ( <ref type="bibr" target="#b26">Norouzi et al., 2013)</ref>. Here, the idea is to exploit a powerful model of lexical similarity induced from large amounts text for being able to capture inherent lex- ical relations between object categories. Thus, un- der the assumption that such semantic spaces rep- resent, in some form at least, taxonomic knowl- edge, this makes labels on different levels of speci- ficity available for a given object. Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if dur- ing training of the mapping this label has not been seen (zero-shot learning).</p><p>While cross-modal transfer seems to be a con- ceptually attractive model for learning object names, it is based on an important assumption that, in our view, has not received sufficient attention in previous works: it assumes that a given distri- butional vector space constitutes an optimal target representation that visual instances of objects can be mapped to. However, distributional represen- tations of word meaning are known to capture a rather fuzzy notion of lexical similarity, e.g. car is similar to van and to street. A cross-modal transfer model is "forced" to learn to map objects into the same area in the semantic space if their names are distributionally similar, but regardless of their ac- tual visual similarity. Indeed, we have found in a recent study that the contribution of distributional information to learning referential word meanings is restricted to certain types of words and does not generalize across the vocabulary ( <ref type="bibr" target="#b38">Zarrieß and Schlangen, 2017)</ref>.</p><p>The goal of this work is to learn a model of referential word meaning that makes accurate ob- ject naming predictions and goes beyond treat- ing words as independent, mutually exclusive la- bels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions ( <ref type="bibr" target="#b38">Zarrieß and Schlangen, 2017</ref>) that treats words as individual predictors capturing referential appropriateness. We explore different ways of linking these predic- tors to distributional knowledge, during applica- tion and during training. We find that these differ- ent models achieve very similar performance in a standard object naming task, though experiments on model combination suggest that they capture complementary aspects of referential meaning. In a zero-shot setup of an object naming task, we find that combining lexical and visual information dur- ing training is most beneficial, outperforming vari- ants of cross-modal transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Grounding and Reference An early example for work in REG that goes beyond <ref type="bibr" target="#b1">Dale and Reiter (1995)</ref>'s dominant symbolic paradigm is Deb Roy's work from the early 2000s ( <ref type="bibr" target="#b31">Roy, 2002</ref><ref type="bibr" target="#b29">Roy, , 2005</ref>).  use com- puter vision techniques to process a video feed, and to compute colour, positional and spatial fea- tures. These features are then associated in a learn- ing process with certain words, resulting in an association of colour features with colour words, spatial features with prepositions, etc., and based on this, these words can be interpreted with refer- ence to the scene currently presented to the video feed. Whereas Roy's work still looked at relatively simple scenes with graphical objects, research on REG has recently started to investigate set-ups based on real-world images ( <ref type="bibr" target="#b13">Kazemzadeh et al., 2014;</ref><ref type="bibr" target="#b9">Gkatzia et al., 2015;</ref><ref type="bibr" target="#b37">Zarrieß and Schlangen, 2016;</ref><ref type="bibr" target="#b24">Mao et al., 2015)</ref>. Importantly, the low- level visual features that can be extracted from these scenes correspond less directly to particu- lar word classes. Moreover, the visual scenes con- tain many different types of objects, which poses new challenges for REG. For instance, <ref type="bibr" target="#b37">Zarrieß and Schlangen (2016)</ref> find that semantic errors related to mismatches between nouns (e.g. the system generates tree vs. man) are particularly disturb- ing for users. Whereas <ref type="bibr" target="#b37">Zarrieß and Schlangen (2016)</ref> propose a strategy to avoid object names when the systems confidence is low, we focus on improving the generation of object names, using distributional knowledge as an additional source. Similarly, <ref type="bibr" target="#b27">Ordonez et al. (2016)</ref> have studied the problem of deriving appropriate object names, or so-called entry-level categories, from the output of an object recognizer. Their approach focusses on linking abstract object categories in ImageNet to actual words via various translation procedures. We are interested in learning referential appropri- ateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction.</p><p>Multi-modal distributional semantics Distri- butional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks ( <ref type="bibr" target="#b36">Turney and Pantel, 2010;</ref><ref type="bibr" target="#b5">Erk, 2016)</ref>. Recent work on multi- modal distributional vector spaces <ref type="bibr" target="#b7">(Feng and Lapata, 2010;</ref><ref type="bibr" target="#b33">Silberer and Lapata, 2014;</ref><ref type="bibr" target="#b15">Kiela and Bottou, 2014;</ref><ref type="bibr" target="#b22">Lazaridou et al., 2015b;</ref>) has aimed at capturing semantic simi- larity even more accurately by integrating distri- butional and perceptual features associated with words (mostly taken from images) into a single representation.</p><p>Cross-modal transfer Rather than fusing dif- ferent modalities into a single, joint space, other work has looked at cross-modal mapping between spaces. Herbelot and Vecchi (2015) present a model that learns to map vectors in a distributional space to vectors in a set-theoretic space, showing that there is a functional relationship between dis- tributional information and conceptual knowledge representing quantifiers and predicates. More re- lated to our work are cross-modal mapping mod- els,that learn to transfer from a representation of an object or image in the visual space to a vec- tor in a distributional space <ref type="bibr" target="#b34">(Socher et al., 2013;</ref><ref type="bibr" target="#b26">Norouzi et al., 2013;</ref><ref type="bibr" target="#b20">Lazaridou et al., 2014</ref>). Here, the motivation is to exploit the rich lexical knowledge encoded in a distribu- tional space for learning visual classifications. In practice, these models are mostly used for zero- shot learning where the test set contains object categories not observed during training. When tested on standard object recognition tasks, trans- fer, however, comes at a price.  and <ref type="bibr" target="#b26">Norouzi et al. (2013)</ref> both find that it slightly degrades performance as compared to a plain object classification using standard accu- racy metrics (called flat "hit @k metric" in their paper). Interestingly though,  report better performance using "hierarchical pre- cision", which essentially means that transfer pre- dicts words that are ontologically closer to the gold label and makes "semantically more reasonable er- rors". To the best of our knowledge, this pattern has not been systematically investigated any fur- ther. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors ob- served during training ( <ref type="bibr" target="#b21">Lazaridou et al., 2015a)</ref>. In this work, we present a model that exploits distri- butional knowledge for learning referential word meaning as well, but explore and compare differ- ent ways of combining visual and lexical aspects of referential word meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task and Data</head><p>We define object naming as follows: Given an ob- ject x in an image, the task is to predict a word w that could be used as the head noun of a real- istic referring expression. (Cf. discussion above: "bird" when naming a robin, but "penguin" when naming a penguin.) To get at this, we develop our approach using a corpus of referring expressions produced by human users under natural, interac- tive conditions ( <ref type="bibr" target="#b13">Kazemzadeh et al., 2014)</ref>, and train and test on the corresponding head nouns in these REs. This is similar to picture naming setups used in psycholinguistic research (cf. <ref type="bibr" target="#b23">Levelt et al. (1991)</ref>) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image.</p><p>We now summarise the details of our setup:</p><p>Corpus We train and test on the REFERIT cor- pus ( <ref type="bibr" target="#b13">Kazemzadeh et al., 2014</ref>), which is based on the SAIAPR image collection ( <ref type="bibr" target="#b10">Grubinger et al., 2006</ref>) (99.5k image regions;120K REs). We fol- low (  and select words with a minimum frequency of 40 in these two data sets, which gives us a vocabulary of 793 words.</p><p>Names For most of our experiments, we only use a subset of this vocabulary, namely the set of object names. As the REs contain nouns that can- not be considered to be object names (background, bottom, etc.), we extract a list of names from the semantically annotated held-out set released with the REFERIT. These correspond to 'entry-level' nouns mentioned in <ref type="bibr" target="#b13">Kazemzadeh et al. (2014)</ref>. This gives us a list of 159 names. This set cor- responds to the majority of object names in the corpus: out of the 99.5K available image regions, we use 80K for training and testing. Thus, our experiments are on a smaller scale as compared to ( <ref type="bibr" target="#b27">Ordonez et al., 2016)</ref>. Nevertheless, the data is challenging, as the corpus contains references to objects that fall outside of the object labeling scheme that available object recognition systems are typically optimized for, cf. <ref type="bibr" target="#b12">Hu et al. (2015)</ref>'s discussion on "stuff" entities such "sky" or "grass" in the REFERIT data. For testing, we remove re- lational REs (containing a relational preposition such as 'left of X'), because here we cannot be sure that the head noun of the target is fully infor- mative; we also remove REs with more than one head noun from our list (i.e. these are mostly rela- tional expressions as well such as 'girl laughing at boy'). We pair each image region from the test set with its corresponding names from the remaining REs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image and Word Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Three Models of Interfacing Visual and Distributional Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Direct Cross-Modal Mapping</head><p>Following <ref type="bibr" target="#b20">Lazaridou et al. (2014)</ref>, referential meaning can be represented as a translation func- tion that projects visual representations of objects to linguistic representations of words in a distribu- tional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work. This model will be called TRANSFER below. During training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the trans- formation. <ref type="bibr" target="#b20">Lazaridou et al. (2014)</ref> and <ref type="bibr" target="#b21">Lazaridou et al. (2015a)</ref> test a range of technical tweaks and different algorithms for cross-modal mapping. For ease of comparison with other models, we stick with simple Ridge Regression in this work.</p><p>For decoding, we map an object into the dis- tributional space, and retrieve the nearest neigh- bors of the predicted vector using cosine similar- ity. In theory, the model should generalize easily to words that it has not observed in a pair with an object during training as it can map an object any- where in the distributional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lexical Mapping Through Individual Word Classifiers</head><p>Another approach is to keep visual and distribu- tional information separate, by training a separate visual classifier for each word w in the vocabu- lary. Predictions can then be mapped into distribu- tional space during application time via the vectors of the predicted words. Here, we use 's WAC model, building the training set for each word w as follows: all visual objects in a corpus that have been referred to as w are used as positive instances, the remaining objects as negative instances. Thus, the classifiers learn to predict referential appropriateness for individ- ual words based on the visual features of the ob- jects they refer to, in isolation of other words. During decoding, we apply all word classifiers from the model's vocabulary to the given object, and take the argmax over the individual word probabilities. The model predicts names directly, without links into a distributional space.</p><p>In order to extend the model's vocabulary for zero-shot learning, we follow <ref type="bibr" target="#b26">Norouzi et al. (2013)</ref> and associate the top n words with their corre- sponding distributional vector and compute the convex combination of these vectors. Then, in par- allel to cross-modal mapping, we retrieve the near- est neighbors of the combined embedding from the distributional space. Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers.</p><p>We did some small-scale experiments to find an optimal value for n, similar to <ref type="bibr" target="#b26">Norouzi et al. (2013)</ref>. In our case, performance started to de- crease systematically with n &gt; 10, but did not dif- fer significantly for values below 10. In Section 5, we will report results for n set to 5 and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Prediction via Cross-Modal Similarity Mapping</head><p>Finally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space. Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names. When building the training set for such a word predictor w, instead of simply di- viding objects into w and ¬w instances, we label each object with a real-valued similarity obtained from cosine similarity between w and v in a dis- tributional vector space, where v is the word that was used to refer to the object. Thus, we task the model with jointly learning similarities and refer- ential appropriateness, by training it with Ridge regression on a continuous output space. Object instances where v = w (i.e., the positive instances in the binary setup) have maximal similarity; the remaining instances have a lower value which is more or less close to maximal similarity. This is the SIM-WAP model, recently proposed in <ref type="bibr" target="#b38">Zarrieß and Schlangen (2017)</ref>. Importantly, and going beyond <ref type="bibr" target="#b38">Zarrieß and Schlangen (2017)</ref>, this model allows for an in- novative treatment of words that only exist in a distributional space (without being paired with vi- sual referents in the image corpus): as the predic- tors are trained on a continuous output space, no genuine positive instances of a word's referent are needed. When training a predictor for such a word w, we use all available objects from our corpus and annotate them with the expected lexical similarity between w and the actual object names v, which for all objects will be below the maximal value that marks genuine positive instances. During decod- ing, this model does not need to project its pre- dictions into a distributional space, but it simply applies all available predictors to the object, and takes the argmax over the predicted referential ap- propriateness scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment 1: Naming Objects</head><p>This Section reports on experiments in a stan- dard setup of the object naming task where all object names are paired with visual instances of their referents during training. In a compara- ble task, i.e. object recognition with known ob- ject categories, cross-modal projection or trans- fer approaches have been reported to perform worse than standard object classification methods ( <ref type="bibr" target="#b26">Norouzi et al., 2013)</ref>. This seems to suggest that lexical or at least distri- butional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model comparison</head><p>Setup We use the train/test split of REFERIT data as in ( ). We consider image regions with non-relational referring expressions that contain at least one of the 159 head nouns from the list of entry-level nouns (see section 3). This amounts to 6208 image regions for testing and 73K instances for training.</p><p>Results <ref type="table" target="#tab_1">Table 1</ref> shows accuracies in the object naming task for the TRANSFER, WAC and SIM- WAP models according to their accuracies in the top n, including two variants of WAC where its top 5 and top 10 predictions are projected into the distributional space. Overall, the models achieve very similar performance. However, there is an in- teresting pattern when comparing accuracies @1 and @2 to accuracies in the top 5 predictions. Thus, looking at accuracies for the top (two) pre- dictions, the various models that link referential meaning to word representations in the distribu- tional space all perform slightly worse than the plain WAC model, i.e. individual word classifiers trained on visual features only. This might sug- gest that certain aspects of referential word mean- ing are learned less accurately when mapping from visual to distributional space (which replicates re- sults reported in the literature on standard object recognition benchmarks). On the other hand, the SIM-WAP model is on a par with WAC in terms of the @5 accuracy. This effect suggests that dis- tributional knowledge that SIM-WAP has access to during training sometimes distracts the model from predicting the exact name chosen by a hu- man speaker, but that SIM-WAP is still able to rank it among the most probable names. As a simple accuracy-based evaluation is not suited to fully ex- plain this pattern, we carry out a more detailed analysis in Section 5.3.    Setup We combine TRANSFER, SIM-WAP and WAC by aggregating the scores they predict for different object names for a given object. Dur- ing testing, we apply all models to an image re- gion and consider words ranked among the top 10. We first normalize the referential appropri- ateness scores in each top-10 list and then com- pute their sum. This aggregation scheme will give more weight to words that appear in the top 10 list of different models, and less weight to words that only get top-ranked by a single model. We test on the same data as in Section 5.1.</p><p>Results <ref type="table" target="#tab_2">Table 2</ref> shows that all model combi- nations improve over the results of their isolated models in <ref type="table" target="#tab_1">Table 1</ref>, suggesting that WAC, TRANS- FER and SIM-WAP indeed do capture complemen- tary aspects of referential word meaning. On their own, the distributionally informed models are less tuned to specific word occurrences than the visual word classifiers in the WAC model, but they can add useful information which leads to a clear over- all improvement. We take this as a promising find- ing, supporting our initial hypothesis that knowl- edge on lexical distributional meaning should and Av. cosine similarity among top k gold -top k  can be exploited when learning how to use words for reference. <ref type="figure">Figure 2</ref> illustrates objects from our test set where the combination of TRANSFER, SIM-WAP and WAC predicts an accurate name, whereas the mod- els in isolation do not. These examples give some interesting insight into why the models capture different aspects of referential word meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>Word Similarities Many of the examples in <ref type="figure">Figure 2</ref> suggest that the object names ranked among the top 3 by the TRANSFER and SIM- WAP model are semantically similar to each other, whereas WAC generates object names on top that describe very different underlying object cate- gories, such as seal / rock in <ref type="figure">Figure 2</ref>(a), animal / lamp in <ref type="figure">Figure 2</ref>(g) or chair / shirt in <ref type="figure">Figure 2</ref>(c).</p><p>To quantify this general impression, <ref type="table" target="#tab_3">Table 3</ref> shows cosine similarities among words in the top n gen- erated by our models, using their word2vec em- beddings. The average cosine similarity between words in our vocabulary is 0.17. The TRANSFER and SIM-WAP model rank words on top that are clearly more similar to each other than word pairs on average, whereas words ranked top by the WAC model are more dissimilar to each other. Another remarkable finding is that the words generated by TRANSFER and SIM-WAP are not only more simi- lar among the top predictions, but also more sim- ilar to the gold name <ref type="table" target="#tab_3">(Table 3</ref> , right columns). This result is noteworthy since the accuracies for the top predictions shown in <ref type="table" target="#tab_1">Table 1</ref> are slightly below WAC. In general, this suggests that there is a trade-off between optimizing a model of refer- ential word meaning to exact naming decisions, or tailoring it to make lexically consistent pre- dictions. This parallels findings by  who found that their transfer-based object recognition made "semantically more reasonable" errors than a standard convolutional network while not improving accuracies for object recognition, see discussion in Section 2. Additional evaluation metrics, such as success rates in a human evalua- tion (cf. <ref type="bibr" target="#b37">Zarrieß and Schlangen (2016)</ref>), would be an interesting direction for more detailed investi- gation here.</p><p>Word Use But even though the WAC classifiers lack knowledge on lexical similarities, they seem to able to detect relatively specific instances of word use such as hut in <ref type="figure">Figure 2</ref>(b), shirt in 2(c) or lamp in 2(h). Here, the combination with TRANS- FER and SIM-WAP is helpful to give more weight to the object name that is taxonomically correct (sometimes pushing up words below the top-3 and hence not shown in <ref type="figure">Figure 2</ref>). In <ref type="figure">Figure 1</ref>(e), SIM- WAP and TRANSFER give more weight to typical names for persons, whereas WAC top-ranks more unusual names, reflecting that the person is diffi- cult to identify visually. Another observation is that the mapping models have difficulties deal- ing with object names in singular and plural. As these words have very similar representations in the distributional space, they are often predicted as likely variants among the top 10 by SIM-WAP and TRANSFER, whereas the WAC model seems to pre- dict inappropriate plural words less often among the top 3. Such specific phenomena at the intersec- tion of visual and semantic similarity have found very little attention in the literature. We will in- vestigate them further in our Experiments on zero- shot naming in the following Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Zero-Shot Naming</head><p>Zero-shot learning is an attractive prospect for REG from images, as it promises to overcome de- pendence on pairings of visual instances and nat- ural names being available for all names, if vi- sual/referential data can be generalised from other types of information. Previous work has looked at the feasibility of zero-shot learning as a func- tion of semantic similarity or ontological close- ness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones ( <ref type="bibr" target="#b26">Norouzi et al., 2013)</ref>. Our experiments on object naming in Section 5 suggest that lexical similarities encoded in a dis- tributional space might not always fully carry over to referential meaning. This could constitute an additional challenge for zero-shot learning, as dis- tributional similarities might be misleading when the model has to fully rely on them for learning referential word meanings. Therefore, the fol- lowing experiments investigate the performance of our models in zero-shot naming as a function of the lexical relation between unknown and known object names, i.e. namely hypernyms and singu- lar/plurals. Both relations are typically captured by distributional models of word meaning in terms of closeness in the vector space, but their visual and referential relation is clearly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Vocabulary Splits and Testsets</head><p>Random As in previous work on zero-shot learning, we consider zero-shot naming for words of varying degrees of similarity. We randomly split our 159 names from Experiment 1 into 10 subsets. We train the models on 90% of the nouns (and all their visual instances in the image cor- pus) and test on the set of image regions that are named with words which the model did not ob- serve during training. Results reported in <ref type="table" target="#tab_5">Table  4</ref> on the random test set correspond to averaged scores from cross-validation over the 10 splits.</p><p>Hypernyms We manually split the model's vo- cabulary into set of hypernyms (see Appendix A) and the remaining nouns. We train the models on those 84K image regions that where not named with a hypernym, and test on 8895 image regions that were named with a hypernym in the corpus. We checked that for each of these hypernyms, the vocabulary contains at least one or two names that can be considered as hyponyms, i.e. the model sees objects during training that are instances of vehicle for example, but never encounters actual uses of that name. This test set is particularly inter- esting from an REG perspective, as objects named with very general terms by human speakers are of- ten difficult to describe with more common, but more specific terms, as is illustrated by the uses of structure and thingy in <ref type="figure">Figure 1</ref>.</p><p>Singulars/Plurals We pick 68 words from our vocabulary that can be grouped into 34 singular- plural noun pairs (see Appendix A). From each pair, we randomly include the singular or plural noun in the set of zero-shot nouns. Thus, we make sure that the model encounters singular and plu- ral names during training, but it never encounters both variants of a name. This results training split of 23K image regions and a test split of 13825 in- stances.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation</head><p>Some previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word ( ). We fol- low <ref type="bibr" target="#b20">Lazaridou et al. (2014)</ref> and let the model de- cide whether to refer to an object by a known or unknown name. Related to that, distinct evalua- tion procedures have been used in the literature on zero-shot learning:</p><p>Testing on full vocabulary A realistic way to test zero-shot learning performance is to consider all words from a given vocabulary during testing, though the testset only contains instances of ob- jects that have been named with a 'zero-shot word' (for which no visual instances were seen during training). Accuracies in this setup reflect how well the model is able to generalize, i.e. how often it decides to deviate from the words it was trained on, and (implicitly) predicts that the given object requires a "new" name. In case of the (i) hyper- nym and (ii) singular/plural test set, this accuracy also reflects to what extent the model is able to de- tect cases where (i) a more general or vague term is needed, where (ii) an unknown singular/plural counterpart of a known object type occurs.</p><p>Testing on disjoint vocabulary Alternatively, the model's vocabulary can be restricted during testing to zero-shot words only, such that names encountered during training and testing are dis- joint, see e.g. ( <ref type="bibr" target="#b18">Lampert et al., 2009</ref><ref type="bibr" target="#b19">Lampert et al., , 2013</ref>. This setup factors out the generalization problem, and assesses to what extent a model is able to cap- ture the referential meaning of a word that does not have instances in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>As compared to Experiment 1 where models achieved similar performance, differences are more pronounced in the zero-shot setup, as shown in <ref type="table" target="#tab_5">Table 4</ref>. In particular, we find that the SIM- WAP model which induces individual predictors for words that have not been observed in the train- ing data is clearly more successful than TRANS- FER or WAC that project predictions into the dis- tributional space. When tested on the full vocabu- lary, we find that TRANSFER and WAC very rarely generate names whose referents were excluded from training, which is in line with observations made by <ref type="bibr" target="#b21">Lazaridou et al. (2015a)</ref>. The SIM-WAP predictors generalize much better, in particular on the singular/plural testset. An interesting exception is the good perfor- mance of the TRANSFER model on the hypernym test set, when evaluated with a disjoint vocabu- lary. This corroborates evidence from Experiment 1, namely that the transfer model captures tax- onomic aspects of object names better than the other models. Projection via individual word clas- sifiers, on the other hand, seems to generalize bet- ter than TRANSFER, at least when looking at ac- curacies @2 ... @10. Thus, combining several vectors predicted by a model of referential word meaning can provide additional information, as compared to mapping an object to a single vec- tor in distributional space. More work is needed to establish how these approaches can be integrated more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusion</head><p>In this paper, we have investigated models of refer- ential word meaning, using different ways of com- bining visual information about a word's referent and distributional knowledge about its lexical sim- ilarities. Previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity. We found that cross-modal mapping produces seman- tically appropriate and mutually highly similar ob- ject names in its top-n list, but does not preserve differences in referential word use (e.g. appropri- atness of person vs. woman) especially within the same semantic field. We have shown that it is beneficial for performance in standard and zero- shot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during ap- plication or through cross-modal similarity map- ping during training. As we have tested these ap- proaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g. recently collected through conver- sational agents ( <ref type="bibr" target="#b2">Das et al., 2016</ref>) that circumvent the need for human-human interaction data. Also from an REG perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>251</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, 'GoogleNet' (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features. As dis- tributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>hit</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2: Examples from object naming experiment where model combination is accurate</figDesc><graphic url="image-12.png" coords="8,303.91,366.48,113.39,86.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Accuracies in object naming 

hit @k(%) 
1 
5 
10 

sim-wap + transfer 
49.10 61.78 75.81 
sim-wap + wac 
51.10 63.45 77.92 
transfer + wac 
51.13 63.76 77.84 
wac + transfer + sim-wap 
52.19 64.71 78.40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Object naming acc., combined models 5.2 Model combination In order to get more insight into why the TRANS- FER and SIM-WAP models produce slightly worse results than individual visual word classifiers, we now test to what extent the different models are complementary and combine them by aggregating over their naming predictions. If the models are complementary, their combination should lead to more confident and accurate naming decisions.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Cosine similarities between word2vec 
embeddings of nouns generated in the top k 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Accuracies in zero-shot object naming on different vocabulary splits</head><label>4</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge support by the Cluster of Excellence "Cognitive Interaction Technology" (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG). We thank the anonymous reviewers for their very valuable, very detailed and highly in-teresting comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-1023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computational interpretations of the gricean maxims in the generation of referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="263" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno>abs/1611.08669</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What do you know about an alligator when you know the company it keeps?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantics and Pragmatics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="doi">10.3765/sp.9.17</idno>
		<ptr target="https://doi.org/10.3765/sp.9.17" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual information in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From the virtual to the realworld: Referring to objects in real-world spatial scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Gkatzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Bartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mackaness</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1224" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1936" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The IAPR TC-12 benchmark: a new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building a shared world: mapping distributional to modeltheoretic semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1003" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>CoRR abs/1511.04164</idno>
		<ptr target="http://arxiv.org/abs/1511.04164" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qatar</forename><surname>Doha</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning image embeddings using convolutional neural networks for improved multi-modal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1005" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4985" to="4994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computational generation of referring expressions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="218" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1403" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1027" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The time course of lexical access in speech production: A study of picture naming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Willem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Levelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Schriefers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vorberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Pechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Havinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">122</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>ArXiv / CoRR abs/1511.02283</idno>
		<ptr target="http://arxiv.org/abs/1511.02283" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems. Curran Associates Inc., USA, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems. Curran Associates Inc., USA, NIPS&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to name objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="108" to="115" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Principles of Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognition and Categorization</title>
		<editor>Eleanor Rosch and Barbara B. Lloyd</editor>
		<meeting><address><addrLine>Lawrence Erlbaum, Hillsdale, N.J., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="27" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grounding words in perception and action: Computational insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciene</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="389" to="396" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A trainable spoken language understanding system for visual object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gorniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Juster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Speech and Language Processing</title>
		<meeting>the International Conference on Speech and Language Processing<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Colorado</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning visually-grounded words and syntax for a scene description task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Resolving references to objects in photographs using the words-as-classifiers model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarriess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Kennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54rd Annual Meeting of the Association for Computational Linguistics (ACL</title>
		<meeting>the 54rd Annual Meeting of the Association for Computational Linguistics (ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-1068" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Easy things first: Installments improve referring expression generation for objects in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1058" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="610" to="620" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Is this a child, a girl or a car? exploring the contribution of distributional similarity to learning referential word meanings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-2014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="86" to="91" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A Vocabulary Splits for Zero-Shot Naming Hypernyms animal, animals, plant, plants, vehicle, person, persons, food, thing</title>
		<imprint/>
	</monogr>
	<note>object, area, things, thingy, toy, anyone, clothes, dish, building, land, structure, item, water</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Singulars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plurals</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">trees, man, kid, guy, girl, boy, flower, bird, hill, orange, cloud, curtain, window, shrub, apple, light, house, glass, bottle, dude, leg, book, wall, bananas, carrots, pillows, bushes, mountains, bags</title>
	</analytic>
	<monogr>
		<title level="m">training on instances of: animals, plants, cars, people, buildings</title>
		<imprint/>
	</monogr>
	<note>testing on instances of: animal, plant, car, person, building, tree, men, kids, guys, girls, boys, flowers, birds, hills, oranges, clouds, curtains, windows, shrubs, apples, lights, houses, glasses, bottles, dudes, legs, books, walls, banana, carrot, pillow, bush, mountain, bag</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
