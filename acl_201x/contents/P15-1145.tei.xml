<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">York University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
							<email>siwei@iflytek.com, zhling@ustc.edu.cn, yuhu@iflytek.com</email>
							<affiliation key="aff2">
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">iFLYTEK Research</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1501" to="1511"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a general framework to incorporate semantic knowledge into the popular data-driven learning process of word embeddings to improve the quality of them. Under this framework, we represent semantic knowledge as many ordinal ranking inequalities and formulate the learning of semantic word embed-dings (SWE) as a constrained optimization problem, where the data-derived objective function is optimized subject to all ordinal knowledge inequality constraints extracted from available knowledge resources such as Thesaurus and Word-Net. We have demonstrated that this constrained optimization problem can be efficiently solved by the stochastic gradient descent (SGD) algorithm, even for a large number of inequality constraints. Experimental results on four standard NLP tasks, including word similarity measure, sentence completion, name entity recognition , and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be significantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed word representation (i.e., word embed- ding) is a technique that represents words as con- tinuous vectors, which is an important research topic in natural language processing (NLP) <ref type="bibr" target="#b13">(Hinton et al., 1986;</ref><ref type="bibr" target="#b31">Turney et al., 2010)</ref>. In re- cent years, it has been widely used in various NLP tasks, including neural language model <ref type="bibr" target="#b0">(Bengio et al., 2003;</ref><ref type="bibr" target="#b28">Schwenk, 2007)</ref>, sequence la- belling tasks <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b5">Collobert et al., 2011</ref>), machine translation <ref type="bibr" target="#b6">(Devlin et al., 2014;</ref><ref type="bibr" target="#b29">Sutskever et al., 2014)</ref>, and antonym se- lection ( <ref type="bibr" target="#b3">Chen et al., 2015)</ref>. Typically, word vectors are learned based on the distributional hypothesis <ref type="bibr" target="#b12">(Harris, 1954;</ref><ref type="bibr" target="#b22">Miller and Charles, 1991)</ref>, which assumes that words with a similar context tend to have a similar meaning. Under this hypothe- sis, various models, such as the skip-gram model ( <ref type="bibr" target="#b19">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b20">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b17">Levy and Goldberg, 2014)</ref> and GloVe model <ref type="bibr" target="#b25">(Pennington et al., 2014</ref>), have been proposed to lever- age the context of each word in large corpora to learn word embeddings. These methods can ef- ficiently estimate the co-occurrence statistics to model contextual distributions from very large text corpora and they have been demonstrated to be quite effective in a number of NLP tasks. How- ever, they still suffer from some major limitations. In particular, these corpus-based methods usu- ally fail to capture the precise meanings for many words. For example, some semantically related but dissimilar words may have similar contexts, such as synonyms and antonyms. As a result, these corpus-based methods may lead to some antony- mous word vectors being located much closer in the learned embedding space than many synony- mous words. Moreover, as word representations are mainly learned based on the co-occurrence in- formation, the learned word embeddings do not capture the accurate relationship between two se- mantically similar words if either one appears less frequently in the corpus.</p><p>To address these issues, some recent work has been proposed to incorporate prior lexical knowl- edge (WordNet, PPDB, etc.) or knowledge graph (Freebase, etc.) into word representations. Such knowledge enhanced word embedding methods have achieved considerable improvements on var- ious natural language processing tasks, like <ref type="bibr" target="#b33">(Yu and Dredze, 2014;</ref><ref type="bibr" target="#b32">Xu et al., 2014</ref>). These methods attempt to increase the se- mantic similarities between words belonging to one semantic category or to explicitly model the semantic relationships between different words. For example, <ref type="bibr" target="#b33">Yu and Dredze (2014)</ref> have proposed a new learning objective function to enhance word embeddings by combining neural models and a prior knowledge measure from semantic re- sources.  have recently proposed to leverage morphological, syntactic, and semantic knowledge to improve the learning of word em- beddings. Besides, a novel framework has been proposed in ( <ref type="bibr" target="#b32">Xu et al., 2014</ref>) to take advantage of both relational and categorical knowledge to learn high-quality word representations, where two reg- ularization functions are used to model the re- lational and categorical knowledge respectively. More recently, a retrofitting technique has been in- troduced in <ref type="bibr" target="#b7">(Faruqui et al., 2014</ref>) to improve se- mantic vectors by leveraging lexicon-derived rela- tional information in a post-processing stage.</p><p>In this paper, we propose a new and flexible method to incorporate semantic knowledge into the corpus-based learning of word embeddings. In our approach, we propose to represent seman- tic knowledge as many word ordinal ranking in- equalities. Furthermore, these inequalities are cast as semantic constraints in the optimization pro- cess to learn semantically sensible word embed- dings. The proposed method has several advan- tages. Firstly, many different types of seman- tic knowledge can all be represented as a num- ber of such ranking inequalities, such as synonym- antonym, hyponym-hypernym and etc. Secondly, these inequalities can be easily extracted from many existing knowledge resources, such as The- saurus, WordNet <ref type="bibr" target="#b23">(Miller, 1995)</ref> and knowledge graphs. Moreover, the ranking inequalities can also be manually generated by human annotation because ranking orders is much easier for human annotators than assigning specific scores. Next, we present a flexible learning framework to learn distributed word representation based on the ordi- nal semantic knowledge. By solving a constrained optimization problem using the efficient stochas- tic gradient descent algorithm, we can obtain se- mantic word embedding enhanced by the ordinal knowledge constraints. Experiments on four pop- ular natural language processing tasks, including word similarity, sentence completion, name en- tity recognition and synonym selection, have all demonstrated that the proposed method can learn good semantically sensible word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Representing Knowledge By Ranking</head><p>Many types of lexical semantic knowledge can be quantitatively represented by a large number of ranking inequalities such as:</p><formula xml:id="formula_0">similarity(w i , w j ) &gt; similarity(w i , w k ) (1)</formula><p>where w i , w j and w k denote any three words in vocabulary. For example, eq.(1) holds if w j is a synonym of w i and w k is an antonym of w i . In general, the similarity between a word and its syn- onymous word should be larger than the similar- ity between the word and its antonymous word. Moreover, a particular word should be more sim- ilar to the words belonging to the same semantic category as this word than other words belonging to a different category. Besides, eq.(1) holds if w i and w j have shorter distance in a semantic hierar- chy than w i and w k do in the same hierarchy <ref type="bibr" target="#b16">(Leacock and Chodorow, 1998;</ref><ref type="bibr" target="#b14">Jurafsky and Martin, 2000</ref>).</p><p>Equivalently, each of the above similarity in- equalities may be represented as the following constraint in the embedding space:</p><formula xml:id="formula_1">sim(w (1) i , w (1) j ) &gt; sim(w (1) i , w (1) k )<label>(2)</label></formula><p>where w</p><p>(1)</p><formula xml:id="formula_2">i , w<label>(1)</label></formula><p>j and w</p><p>(1) k denote the embedding vectors of the words, w i , w j and w k .</p><p>In this paper, we use the following three rules to gather the ordinal semantic knowledge from avail- able lexical knowledge resources, such as The- saurus and WordNet.</p><p>• Synonym Antonym Rule: Similarities be- tween a word and its synonymous words are always larger than similarities be- tween the word and its antonymous words. For example, the similarity between fool- ish and stupid is expected to be bigger than the similarity between foolish and clever, i.e., similarity(foolish, stupid) &gt; similarity(foolish, clever).</p><p>• Semantic Category Rule: Similarities of words that belong to the same semantic cat- egory would be larger than similarities of words that belong to different categories. This rule refers to the idea of Fisher lin- ear discriminant algorithm in <ref type="bibr" target="#b10">(Fisher, 1936</ref>  <ref type="figure">Figure 1</ref>: An example of hyponym and hypernym.</p><p>knowledge graphs. <ref type="figure">Figure 1</ref> shows a sim- ple example of the relationship between hy- ponyms and hypernyms. From there, it is reasonable to assume the following similar- ity inequality: similarity(Mallet, Plessor) &gt; similarity(Mallet, Hacksaw).</p><p>• Semantic Hierarchy Rule: Similarities be- tween words that have shorter distances in a semantic hierarchy should be larger than similarities of words that have longer dis- tances. In this work, the semantic hi- erarchy refers to the hypernym and hy- ponym structure in WordNet. In addition, we may generate many such se- mantically ranking similarity inequalities by hu- man annotation through crowdsourcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantic Word Embedding</head><p>In this section, we first briefly review the conven- tional skip-gram model ( <ref type="bibr" target="#b20">Mikolov et al., 2013b</ref>). Next, we study how to incorporate the ordinal sim- ilarity inequalities to learn semantic word embed- dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The skip-gram model</head><p>The skip-gram model is a recently proposed learn- ing framework ( <ref type="bibr" target="#b20">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>) to learn continuous word vectors from text corpora based on the aforementioned distribu- tional hypothesis, where each word in vocabulary (size of V ) is mapped to a continuous embedding space by looking up an embedding matrix W <ref type="bibr">(1)</ref> . And W (1) is learned by maximizing the predic- tion probability, calculated by another prediction matrix W (2) , of its neighbouring words within a context window.</p><p>Given a sequence of training data, denoted as w 1 , w 2 , w 3 , ..., w T with T words, the skip-gram model aims to maximize the following objective function:</p><formula xml:id="formula_3">Q = 1 T T t=1 −c≤j≤c,j =0 log p(w t+j |w t ) (3)</formula><p>where c is the size of context windows, w t de- notes the input central word and w t+j for its neigh- bouring word. The skip-gram model computes the above conditional probability p(w t+j |w t ) us- ing the following softmax function:</p><formula xml:id="formula_4">p(w t+j |w t ) = exp(w (2) t+j · w (1) t ) V k=1 exp(w (2) k · w (1) t ) (4)</formula><p>where w</p><p>(1) t and w <ref type="bibr">(2)</ref> k denotes row vectors in ma- trices W (1) and W (2) , corresponding to word w t and w k respectively.</p><p>The training process of the skip-gram model can be formulated as an optimization problem to maximize the above objective function Q. As in ( <ref type="bibr" target="#b20">Mikolov et al., 2013b)</ref>, this optimization problem is solved by the stochastic gradient descent (SGD) method and the learned embedding matrix W <ref type="bibr">(1)</ref> is used as the word embeddings for all words in vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Word Embedding (SWE) as Constrained Optimization</head><p>Here we consider how to combine the ordinal knowledge representation in section 2 and the skip-gram model in 3.1 to learn semantic word embeddings (SWE). As shown in section 2, each ranking inequal- ity involves a triplet, (i, j, k), of three words, {w i , w j , w k }. Assume the ordinal knowledge is represented by a large number of such inequalities, denoted as the inequality set S. For ∀(i, j, k) ∈ S, we have:</p><formula xml:id="formula_5">similarity(w i , w j ) &gt; similarity(w i , w k ) ⇔ sim(w (1) i , w (1) j ) &gt; sim(w (1) i , w (1) k ).</formula><p>For notational simplicity, we denote s ij = sim(w</p><formula xml:id="formula_6">(1) i , w (1) j ) hereafter.</formula><p>Next, we propose to use the following con- strained optimization problem to learn semantic word embeddings (SWE):  </p><formula xml:id="formula_7">{W (1) , W (2) } = arg max W (1) ,W (2) Q(W (1) , W (2) ) (5) s g w t-c w t-c+1 w t+c-1 w t+c w t W (1) W<label>(2</label></formula><formula xml:id="formula_8">s ij &gt; s ik ∀(i, j, k) ∈ S.<label>(6)</label></formula><p>In this work, we formulate the above con- strained optimization problem into an uncon- strained one by casting all the constraints as a penalty term in the objective function. The penalty term can be expressed as follows:</p><formula xml:id="formula_9">D = (i,j,k)∈S f (i, j, k)<label>(7)</label></formula><p>where the function f (·) is a normalization func- tion. It can be a sigmoid function like f (i, j, k) = σ(s ik − s ij ) with σ(x) = 1/(1 + exp(−x)). Al- ternatively, it may be a hinge loss function like f (i, j, k) = h(s ik −s ij ) where h(x) = max(δ 0 , x) with δ 0 denoting a parameter to control the de- cision margin. In this work, we adopt to use the hinge function to compute the penalty term in eq. <ref type="formula" target="#formula_9">(7)</ref> and δ 0 is set to be 0 for all experiments. Finally, the proposed semantic word embed- ding (SWE) model aims to maximize the follow- ing combined objective function:</p><formula xml:id="formula_10">Q = Q − β · D (8)</formula><p>where β is a control parameter to balance the con- tribution of the penalty term in the optimization process. It balances between the semantic infor- mation estimated from the corpus based on the distributional hypothesis and the semantic knowl- edge encoded in the ordinal ranking inequalities.</p><p>In <ref type="bibr" target="#b26">Rocktäschel et al. (2014)</ref>, a similar approach was proposed to capture knowledge constraint as extra terms in the objective function for optimiza- tion.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we show a diagram for the the over- all SWE learning framework to incorporate se- mantic knowledge into the basic skip-gram word embeddings. Comparing with the previous work in ( <ref type="bibr" target="#b32">Xu et al., 2014)</ref> and <ref type="bibr" target="#b7">(Faruqui et al., 2014</ref>), the proposed SWE framework is more general in terms of encoding the semantic knowledge for learning word embeddings. It is straightforward to show that the work in ( <ref type="bibr" target="#b32">Xu et al., 2014;</ref><ref type="bibr" target="#b35">Zweig, 2014;</ref><ref type="bibr" target="#b7">Faruqui et al., 2014</ref>) can be viewed as some special cases under our SWE learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization algorithm for SWE</head><p>In this work, the proposed semantic word em- beddings (SWE) are learned using the standard mini-batch stochastic gradient descent (SGD) al- gorithm. Furthermore, we adopt to use the cosine distance of the embedding vectors to compute the similarity between two words in the penalty term.</p><p>In the following, we show how to compute the derivatives of the penalty term for the SWE learn- ing.</p><formula xml:id="formula_11">∂D ∂w (1) t = (i,j,k)∈S ∂f (s ik − s ij ) ∂w (1) t = (i,j,k)∈S f · δ ik (t) ∂s ik ∂w (1) t − δ ij (t) ∂s ij ∂w (1) t<label>(9)</label></formula><p>where δ ik (t) and δ ij (t) are computed as</p><formula xml:id="formula_12">δ ik (t) = 1 t = i or t = k 0 otherwise<label>(10)</label></formula><p>and for the hinge loss function f (x), we have</p><formula xml:id="formula_13">f = 1 (s ik − s ij ) &gt; δ 0 0 (s ik − s ij ) ≤ δ 0<label>(11)</label></formula><p>and the derivatives of the cosine similarity mea-</p><formula xml:id="formula_14">sure, s ij = w (1) i ·w (1) j |w (1) i ||w (1) j |</formula><p>, with respect to a word vec-tor, i.e., ∂s ik ∂w (1) i , which can be derived as follows:</p><formula xml:id="formula_15">∂s ij ∂w (1) i = − s ij w (1) i |w (1) i | 2 + w (1) j |w (1) i ||w (1) j | .<label>(12)</label></formula><p>The learning rate used for the SWE learning is the same as that for the skip-gram model. In each mini-batch of SGD, we sample terms in the same way as the skip-gram model. As for the con- straints, we do not sample them but use all inequal- ities relevant to any words in a minibatch to update the model for the minibatch. Finally, by jointly op- timizing the two terms in the combined objective function, we may learn a new set of word vectors encoding with ordinal semantic knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report all experiments con- ducted to evaluate the effectiveness of the pro- posed semantic word embeddings (SWE). Here we compare the performance of the proposed SWE model with the conventional skip-gram baseline model on four popular natural language processing tasks, including word similarity mea- sure, sentence completion, name entity recogni- tion, and synonym selection. In the following, we first describe the experimental setup, training corpora, semantic knowledge databases. Next, we report the experimental results on these four NLP tasks. Note that the SWE training codes and scripts are made publicly available at http: //home.ustc.edu.cn/ ˜ quanliu/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training corpora</head><p>In this work, we use the popular Wikipedia cor- pus as our training data to learn word embeddings for experiments on the word similarity task and the TOEFL synonym selection task. Particularly, we utilize two Wikipedia corpora with different sizes. The first corpus with a smaller size is a data set including the first one billion characters from Wikipedia 1 , named as Wiki-Small in our ex- periments. The second corpus with a relatively large size is the latest Wikipedia dump 2 , named as Wiki-Large in our experiments. Both Wikipedia corpora have been pre-processed by removing all the HTML meta-data and hyper-links and replac- ing the digit numbers with English words using the perl script from the Matt Mahoney's page <ref type="bibr">3</ref> . After text normalization, the Wiki-Small corpus contains totally 130 million words, for which we create a lexicon of 225,909 distinct words appearing more than 5 times in the corpus. Similarly, the Wiki- Large corpus contains about 5 billion words, for which we create a lexicon of 228,069 words ap- pearing more than 200 times.</p><p>For the other two tasks, sentence completion and name entity recognition, we use the same training corpora from the previous state-of-the-art work for fair comparisons. The training corpus for the sentence completion is the Holmes text ( <ref type="bibr" target="#b34">Zweig and Burges, 2011;</ref><ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>). The training corpus for the name entity recognition task is the Reuters English newswire from RCV1 ( <ref type="bibr" target="#b30">Turian et al., 2010;</ref><ref type="bibr" target="#b18">Lewis et al., 2004</ref>). Refer to section 4.4 and section 4.5 for detailed descrip- tions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Semantic constraint collections</head><p>In this work, we use WordNet as the resource to collect ordinal semantic knowledge. WordNet is a large semantic lexicon database of English words <ref type="bibr" target="#b23">(Miller, 1995)</ref>, where nouns, verbs, adjectives and adverbs are grouped into sets of cognitive syn- onyms (usually called synsets). Each synset usu- ally expresses a distinct semantic concept. All synsets in WordNet are interlinked by means of conceptual-semantic and/or lexical relations such as synonyms and antonyms, hypernyms and hy- ponyms.</p><p>In our experiments, we use the version WordNet-3.1 for creating the corresponding se- mantic constraints. In detail, we follow the fol- lowing process to extract semantic similarity in- equalities from WordNet and Thesaurus:</p><p>1. Based on the Synonym Antonym Rule de- scribed in section 2, for each word in vo- cabulary, find its synset and use the syn- onym and antonym relations to find all re- lated synonymous and antonymous synsets. Note that the antonymous synset is selected as long as there exists an antonymous rela- tion between any word in this synset and any word in an synonymous synset. After find- ing the synonymous and antonymous synsets of the current word, the similarity inequali- ties could be generated according to the rank- ing rule. After processing all words, we have collected about 30,000 inequalities re- lated to the synonym and antonym relations. Furthermore, we extract additional 320,000 inequalities from an old English dictionary <ref type="bibr" target="#b8">(Fernald, 1896)</ref>. In total, we have about 345,000 inequalities related to the synonym and antonym relations. This set of inequali- ties is denoted as Synon-Anton constraints in our experiments.</p><p>2. Based on the Semantic Category Rule and Se- mantic Hierarchy Rule, we extract another in- equality set consisting of 75,000 inequalities from WordNet. We defined this collection as Hyper-Hypon constraints in our experiments.</p><p>In the following experiments, we just use all of these collected inequality constraints as is without further manually checking or cleaning-up. They may contain a very small percentage of errors or conflicts (due to multiple senses of a word).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training parameter setting</head><p>Here we describe the control parameters used to learn the baseline skip-gram model and the pro- posed SWE model. In our experiments, we use the open-source word2vec toolkit 4 to train the base- line skip-gram model, where the context window size is set to be 5. The initial learning rate is set as 0.025 and the learning rate is decreased lin- early during the SGD model training process. We use the popular negative sampling technique to speed up model training and set the negative sam- ple number as 5.</p><p>To train the proposed SWE model, we use the same configuration as the skip-gram model to maximize Q . For the penalty term in eq. <ref type="formula" target="#formula_9">(7)</ref>, we set δ 0 = 0 for the hinge loss function. The seman- tic similarity between words is computed by the cosine distance. The combination coefficient β in eq. <ref type="formula">(8)</ref> is usually set to be a number between 0.001 and 0.3 in our experiments.</p><p>In the following four NLP tasks, the dimension- ality of embedding vectors is different since we try to use the same settings from the state-of-the- art work for the comparison purpose. In the Word Similarity task and the TOEFL Synonym Selec- tion task, we followed the state of the art work  in ( <ref type="bibr" target="#b32">Xu et al., 2014</ref>), to set word embeddings to 300-dimension. Similarly, we refer to  to set the dimensionality of word vectors to 600 for the Sentence Completion task. And we set the dimensionality of word vectors to 50 for the NER task according to <ref type="bibr" target="#b30">(Turian et al., 2010;</ref><ref type="bibr" target="#b25">Pennington et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic inequality satisfied rates</head><p>Here we first examine the inequality satisfied rates of various word embeddings. The inequality sat- isfied rate is defined as how many percentage of semantic inequalities are satisfied based on the un- derlying word embedding vectors. In <ref type="figure" target="#fig_3">Figure 3</ref>, we show a typical curve of the inequality satis- fied rates as a function of β used in model train- ing. This figure is plotted based on the Wiki-Small corpus. Two semantic constraint sets Synon-Anton and Hyper-Hypon created in section 4.1.2 are em- ployed to learn semantic word embeddings.</p><p>In the framework of the proposed semantic word embedding method, we just need to tune one more parameter β, comparing with the skip- gram model. It shows that the baseline skip-gram (β = 0) can only satisfy about 50-60% of inequal- ities in the training set. As we choose a proper value for β, we may significantly improve the in- equality satisfied rate, up to 85-95%. Although we can get higher inequality satisfying rate on the training set by increasing beta continuously, how- ever, we do not suggest to use a big beta value because it would make the model overfitting. The major reason for this is that the constraints only cover a subset of words in vocabulary. Increasing the rate too much may screw up the entire word embeddings due to the sparsity of the constraints.</p><p>Meanwhile, we have found that the proposed SGD method is very efficient to handle a large number of inequalities in model training. When we use the total 345,000 inequalities, the SWE training is comparable with the baseline skip-gram model in terms of training speed. In the following, we continue to examine the SWE model on four popular natural language processing tasks, includ- ing word similarity, sentence completion, name entity recognition and the TOEFL synonym selec- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Task 1: Word Similarity Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Task description</head><p>Measuring word similarity is a traditional NLP task <ref type="bibr" target="#b27">(Rubenstein and Goodenough, 1965)</ref>. Here we compare several word embedding models on a popular word similarity task, namely WordSim- 353 ( <ref type="bibr" target="#b9">Finkelstein et al., 2001</ref>), which contains 353 English word pairs along with human-assigned similarity scores, which measure the relatedness of each word pair on a scale from 0 (totally unre- lated words) to 10 (very much related or identical words). The final similarity score for each pair is the average across 13 to 16 human judges. When evaluating word embeddings on this task, we mea- sure the performance by calculating the Spearman rank correlation between the human judgments and the similarity scores computed based on the learned word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experimental results</head><p>Here we compare the proposed SWE model with the baseline skip-gram model on the WordSim- 353 task. Both word embedding models are trained using the Wikipedia corpora. We set the di- mension of word embedding vectors to be 300. In <ref type="table">Table 1</ref>, we have shown all the Spearman rank cor- relation results. The baseline results on this task include PPMI ( <ref type="bibr" target="#b17">Levy and Goldberg, 2014</ref>), GloVe ( <ref type="bibr" target="#b25">Pennington et al., 2014)</ref>, and ESA-Wikipedia ( <ref type="bibr" target="#b11">Gabrilovich and Markovitch, 2007)</ref>.</p><p>From the results in <ref type="table">Table 1</ref>, we can see that the proposed SWE model can achieve consistent im- provements over the baseline skip-gram model, no matter which training corpus is used. These re- sults have demonstrated that, by incorporating se- mantic ordinal knowledge into the word vectors,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Embeddings Result</head><p>Others SPPMI 0.6870 GloVe <ref type="table">(6 billion)</ref> 0.6580 GloVe <ref type="table" target="#tab_3">(42 billion)</ref> 0 <ref type="table">Table 1</ref>: Spearman results on the WordSim-353 <ref type="table">Task.</ref> the proposed semantic word embedding frame- work can capture much better semantics for many words. The SWE model using the Wiki-Large corpus has achieved the state-of-the-art perfor- mance on this task, significantly outperforming other popular word embedding methods, such as skip-gram and GloVe. Moreover, we also find that the Synon-Anton constraint set is more relevant than Hyper-Hypon for the word similarity task.</p><note type="other">.7590 ESA-Wikipedia 0.7500 Skip-gram 0.6326 Wiki-Small SWE + Synon-Anton 0.6584 (0.13 billion) SWE + Hyper-Hypon 0.6407 SWE + Both 0.6442 Skip-gram 0.7085 Wiki-Large SWE + Synon-Anton 0.7274 (5 billion) SWE + Hyper-Hypon 0.7213 SWE + Both 0.7236</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Task 2: Sentence Completion Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Task description</head><p>The Microsoft sentence completion challenge has recently been introduced as a standard benchmark task for language modeling and other NLP tech- niques ( <ref type="bibr" target="#b34">Zweig and Burges, 2011</ref>). This task con- sists of 1040 sentences, each of which misses one word. The goal is to select a word that is the most coherent with the rest of the sentence, from a list of five candidates. Many NLP techniques have already been reported on this task, includ- ing N-gram model and LSA-based model pro- posed in ( <ref type="bibr" target="#b34">Zweig and Burges, 2011)</ref>, log-bilinear model <ref type="bibr" target="#b24">(Mnih and Teh, 2012)</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b21">(Mikolov, 2012)</ref>, the skip-gram model ( <ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>), a combination of the skip-gram and RNN model, and a knowledge enhanced word embedding model proposed by . The performance of all these techniques is listed in <ref type="table" target="#tab_3">Table 2</ref> for comparison.</p><p>In this work, we follow the the same proce- dure as in ( <ref type="bibr" target="#b19">Mikolov et al., 2013a</ref>) to examine the performance of our proposed semantic word em- beddings (SWE) on this task. We first train 600-dimension word embeddings based on a training corpus of 50M words provided by <ref type="bibr" target="#b34">(Zweig and Burges, 2011</ref>), with and without using the col- lected ordinal knowledge. Then, for each sen- tence in the test set, we use the learned word em- beddings to compute a sentence score for predict- ing all surrounding words based on each candidate word in the list. Finally, we use the computed sen- tence prediction scores to choose the most likely word from the given list to answer the question.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Experimental results</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we have shown the sentence comple- tion accuracy on this task for various word em- bedding models. We can see that the proposed SWE model has achieved considerable improve- ments over the baseline skip-gram model. Once again, this suggests that the semantic knowledge represented by the ordinal inequalities can signif- icantly improve the quality of the word embed- dings. Besides, the SWE model significantly out- performs the recent work in ( ), which considers syntactics and semantics of the sentence contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Task 3: Name Entity Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Task description</head><p>To further investigate the performance of seman- tic word embeddings, we have further conducted some experiments on the standard CoNLL03 name entity recognition (NER) task. The CoNLL03 NER dataset is drawn from the Reuters newswire. The training set contains 204K words (14K sentences, 946 documents), the test set contains 46K words (3.5K sentences, 231 doc- uments), and the development set contains 51K words (3.3K sentences, 216 documents). We have listed the state-of-the-art performance in <ref type="table" target="#tab_5">Table 3</ref> for this task <ref type="bibr" target="#b30">(Turian et al., 2010)</ref>.</p><p>To make a fair comparison, we have used the exactly same experimental configurations as in ( <ref type="bibr" target="#b30">Turian et al., 2010)</ref>, including the used training algorithm, the baseline discrete features and so on. Like the C&amp;W model, we use the same training text resource to learn word vectors, which contains one year of Reuters English newswire from RCV1, from August 1996 to August 1997, having about 810,000 news stories ( <ref type="bibr" target="#b18">Lewis et al., 2004</ref>). Mean- while, the dimension of word embeddings is set to 50 for all experiments on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Experimental results</head><p>In our experiments, we compare the proposed SWE model with the baseline skip-gram model for name entity recognition, measured by the standard F1 scores. We present the final NER F1 scores on the CoNLL03 NER task in <ref type="table" target="#tab_5">Table 3</ref>. The nota- tion "Gaz" stands for gazetteers that are added into the NER system as an auxiliary feature. For the SWE model, we experiment two configurations by adding gazetteers or not (denoted by "IsGaz" and "NoGaz" respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Dev  From the results shown in <ref type="table" target="#tab_5">Table 3</ref>, we could find the proposed semantic word embedding (SWE) model can consistently achieve 0.8% (or more) ab- solute improvements on the MUC7 task no mat-ter whether the gazetteers features are used or not. The proposed SWE model can also obtain 0.3% improvement in the CoNLL03 test set when no gazetteers is added into the NER system. How- ever, no significant improvement is observed in this test set for the proposed SWE model after we add the gazetteers feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Task 4: TOEFL Synonym Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Task description</head><p>The goal of a synonym selection task is to se- lect, from a list of candidate words, the semanti- cally closest word for each given target word. The dataset we use for this task is the standard TOEFL dataset <ref type="bibr" target="#b15">(Landauer and Dumais, 1997</ref>), which con- tains 80 questions. Each question consists of a tar- get word along with 4 candidate lexical substitutes for selection.</p><p>The evaluation criterion on this task is the synonym selection accuracy which indicates how many synonyms are correctly selected for all 80 questions. Similar to the configurations on the word similarity task, all the experiments on this task are conducted on the English Wikipedia cor- pora. In our experiments, we set all the vector di- mensions to 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Model Accuracy (%)</p><p>Wiki-Small  In <ref type="table" target="#tab_7">Table 4</ref>, we have shown the experimen- tal results for different word embedding models, learned from different Wikipedia corpora: Wiki- Small or Wiki-Large. We compare the proposed SWE with the baseline skip-gram model. From the experimental results in <ref type="table" target="#tab_7">Table 4</ref>, we can see that the proposed SWE model can achieve consistent improvements over the baseline skip-gram model on the TOEFL synonym selection task, about 5- 8% improvements on the selection accuracy. We find the similar performance differences between the SWE model trained with the Synon-Anton and Hyper-Hypon constraint set. The main reason would be that the synonym selection task is mainly related to lexical level similarity and less relevant to the hypernym-hyponym relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Word embedding models with good semantic rep- resentations are quite invaluable to many natu- ral language processing tasks. However, the cur- rent data-driven methods that learn word vectors from corpora based on the distributional hypoth- esis tend to suffer from some major limitations. In this paper, we propose a general and flexible framework to incorporate various types of seman- tic knowledge into the popular data-driven learn- ing procedure for word embeddings. Our main contributions are to represent semantic knowledge as a number of ordinal similarity inequalities as well as to formulate the entire learning process as a constrained optimization problem. Meanwhile, the optimization problem could be solved by effi- cient stochastic gradient descend algorithm. Ex- perimental results on four popular NLP tasks have all demonstrated that the propose semantic word embedding framework can significantly improve the quality of word representations.</p><p>As for the future work, we would incorpo- rate more types of knowledge, such as knowledge graphs and FrameNet, into the learning process for more powerful word representations. We also expect that some common sense related semantic knowledge may be generated as ordinal inequality constraints by human annotators for learning se- mantic word embeddings. At the end, we plan to apply the SWE word embedding models for more natural language processing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>From Fig- ure 1, this rule may suggest several inequal- ities like: similarity(Mallet, Hammer) &gt; similarity(Mallet, Tool).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed semantic word embedding (SWE) learning framework (The left part denotes the state-of-the-art skip-gram model; The right part represents the semantic constraints).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label></label><figDesc>https://code.google.com/p/word2vec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A curve of inequality satisfied rates (All models trained on the Wiki-Small corpus. HyperHypon and Synon-Anton stand for different semantic constraint sets employed for training semantic word embeddings).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>,w i ) &gt; sim(w j ,w k )</head><label></label><figDesc></figDesc><table>). 
A semantic category may be defined as a 
synset in WordNet, a hypernym in a se-
mantic hierarchy, or an entity category in w t-c+1 

w t+c-1 
w t+c 

w t 

W (1) 

W (2) 

sim(w t w i 
w j 
w k 

Share Embedding 

Semantic Knowledge Resources 

W (1) 

WordNet 

Human Labeling 
Knowledge 

Has 
Knowledge 
? 

Minimize 
Qsem 
Semantic 
Knowledge 

Input Word 

YES 

Maximize 
Qbase 

Word Sequence 

Get training samples 

Tool 

Hammer 
Saw 
Chisel 

Mallet 

w t-c 

Objective-1: 
Q base 

Plessor 
Jigsaw 
Hacksaw 

Hypernym 

Hyponyms 
Co-Hyponyms 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) sim (w i ,w j ) &gt; sim(w i ,w k )</head><label>sim</label><figDesc></figDesc><table>w i 
w j 
w k 

Share Embedding 

Semantic Knowledge Resou 

W (1) 

WordNet 

Human Labe 
Knowledge 
Bases 

Distributional 
Hypothesis 

Knowledge Constraints 

Joint 
Optimization 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Results on Sentence Completion Task.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : F1 scores on the CoNLL03 NER task.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 : The TOEFL synonym selection task.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://mattmahoney.net/dc/enwik9.zip 2 http://dumps.wikimedia.org/enwiki/latest/enwiki-latestpages-articles.xml.bz2</note>

			<note place="foot" n="3"> http://mattmahoney.net/dc/textdata.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Science and Technology Development of Anhui Province, China (Grants No. 2014z02006) and the Funda-mental Research Funds for the Central Universi-ties. Special thanks to Zhigang Chen, Ruiji Fu and the anonymous reviewers for their insightful com-ments as well as suggestions. The authors also want to thank Profs. Wu Guo and Li-Rong Dai for their wonderful help and supports during the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="132" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting word embedding for contrasting meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Deep learning and representation learning workshop</title>
		<meeting>the NIPS Deep learning and representation learning workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">English synonyms and antonyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernald</forename><surname>James Champlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1896" />
			<publisher>Funk &amp; Wagnalls Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipediabased explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations. In Parallel distributed processing: Explorations in the microstructure of cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="77" to="109" />
			<date type="published" when="1986" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Speech &amp; language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A solution to plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>David D Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Low-dimensional embeddings of logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Semantic Parsing</title>
		<meeting>the ACL 2014 Workshop on Semantic Parsing<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rcnet: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The microsoft research sentence completion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2011- 129</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Microsoft</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Explicit representation of antonymy in language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<idno>MSR-TR-2014-52</idno>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Microsoft</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
