<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continuous Profile Models in ASL Syntactic Facial Expression Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
							<email>hkacorri@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Rochester Institute of Technology B. Thomas Golisano College of Computing and Information Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University Human-Computer Interaction Institute</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh, 152 Lomb Memorial Drive Rochester</addrLine>
									<postCode>15213, 14623</postCode>
									<region>PA, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
							<email>matt.huenerfauth@rit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Rochester Institute of Technology B. Thomas Golisano College of Computing and Information Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University Human-Computer Interaction Institute</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh, 152 Lomb Memorial Drive Rochester</addrLine>
									<postCode>15213, 14623</postCode>
									<region>PA, NY</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Continuous Profile Models in ASL Syntactic Facial Expression Synthesis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2084" to="2093"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To create accessible content for deaf users, we investigate automatically synthesizing animations of American Sign Language (ASL), including grammatically important facial expressions and head movements. Based on recordings of humans performing various types of syntactic face and head movements (which include idiosyn-cratic variation), we evaluate the efficacy of Continuous Profile Models (CPMs) at identifying an essential &quot;latent trace&quot; of the performance, for use in producing ASL animations. A metric-based evaluation and a study with deaf users indicated that this approach was more effective than a prior method for producing animations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>While there is much written content online, many people who are deaf have difficulty reading text or may prefer sign language. For example, in the U.S., standardized testing indicates that a major- ity of deaf high school graduates (age 18+) have a fourth-grade reading level or below <ref type="bibr">(Traxler, 2000</ref>) (U.S. fourth-grade students are typically age 9). While it is possible to create video-recordings of a human performing American Sign Language (ASL) for use on websites, updating such material is expensive (i.e., re-recording). Thus, researchers investigate technology to automate the synthesis of animations of a signing virtual human, to make it more cost-effective for organizations to provide sign language content online that is easily updated and maintained. Animations can be automatically synthesized from a symbolic specification of the message authored by a human or perhaps by ma- chine translation, e.g. ( <ref type="bibr" target="#b1">Ebling and Glauert, 2013;</ref><ref type="bibr" target="#b2">Filhol et al., 2013;</ref><ref type="bibr">Stein et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">ASL Syntactic Facial Expressions</head><p>Facial expressions are essential in ASL, conveying emotion, semantic variations, and syntactic struc- ture. Prior research has verified that ASL ani- mations with missing or poor facial expressions are significantly less understandable for deaf users ( <ref type="bibr" target="#b11">Kacorri et al., 2013b;</ref><ref type="bibr" target="#b10">Kacorri et al., 2013a</ref>). While artists can produce indi- vidual animations with beautiful expressions, such work is time-consuming. For efficiently maintain- able online content, we need automatic synthesis of ASL from a sparse script representing the lexi- cal items and basic elements of the sentence.</p><p>Specifically, we are studying how to model and generate ASL animations that include syntactic facial expressions, conveying grammatical infor- mation during entire phrases and therefore con- strained by the timing of the manual signs in a phrase <ref type="bibr" target="#b0">(Baker-Shenk, 1983)</ref>. Generally speaking, in ASL, upper face movements (examined in this paper) convey syntactic information across entire phrases, with the mouth movements conveying lexical or adverbial information.</p><p>The meaning of a sequence of signs performed with the hands depends on the co-occuring fa- cial expression. (While we use the term "fa- cial expressions," these phenomena also include movements of the head.) For instance, the ASL sentence "BOB LIKE CHOCOLATE" (English: "Bob likes chocolate.") becomes a yes/no ques- tion (English: "Does Bob like chocolate?"), with the addition of a YesNo facial expression during the sentence. The addition of a Negative facial ex- pression during the verb phrase "LIKE CHOCO- LATE" changes the meaning of the sentence to "Bob doesn't like chocolate." (The lexical item NOT may optionally be used.) For interroga- tive questions, a WhQuestion facial expression must occur during the sentence, e.g., "BOB LIKE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2084</head><p>WHAT." The five types of ASL facial expressions investigated in this paper include:</p><p>• YesNo: The signer raises his eyebrows while tilting the head forward to indicate that the sentence is a polar question.</p><p>• WhQuestion: The signer furrows his eye- brows and tilts his head forward during a sen- tence to indicate an interrogative question, typically with a "WH" word such as what, who, where, when, how, which, etc.</p><p>• Rhetorical: The signer raises his eyebrows and tilts his head backward and to the side to indicate a rhetorical question.</p><p>• Topic: The signer raises his eyebrows and tilts his head backward during a clause-initial phrase that should be interpreted as a topic.</p><p>• Negative: The signer shakes his head left and right during the verb phrase to indicate negated meaning, often with the sign NOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Prior Work</head><p>A survey of recent work of several researchers on producing animations of sign language with fa- cial expressions appears in <ref type="bibr" target="#b9">(Kacorri, 2015)</ref>. There is recent interest in data-driven approaches using facial motion-capture of human performances to generate sign language animations: For example, ( <ref type="bibr">Schmidt et al., 2013</ref>) used clustering techniques to select facial expressions that co-occur with indi- vidual lexical items, and <ref type="bibr" target="#b3">(Gibet et al., 2011</ref>) stud- ied how to map facial motion-capture data to ani- mation controls.</p><p>In the most closely related prior work, we had investigated how to generate a face animation based on a set of video recordings of a human signer performing facial expressions ( <ref type="bibr">Kacorri et al., 2016)</ref>, with head and face movement data au- tomatically extracted from the video, and with in- dividual recordings labeled as each of the five syn- tactic types, as listed in section 1.1. We wanted to identify a single exemplar recording in our dataset, for each of the syntactic types, that could be used as the basis for generating the movements of vir- tual human character. (In a collection of record- ings of face and head movement, there will nat- urally be non-essential individual variation in the movements; thus, it may be desirable to select a recording that is maximally stereotypical of a set of recordings.) To do so, we made use of a variant of Dynamic Time Warping (DTW) as a distance metric to select the recording with minimal pair- wise normalized DTW distance from all of the ex- amples of each syntactic type. We had used this "centroid" recording as the basis for producing a novel animation of the face and head movements for a sign language sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this paper, we present a new methodology for generating face and head movements for sign lan- guage animations, given a set of human recordings of various syntactic types of facial expressions. Whereas we had previously selected a single ex- emplar recording of a human performance to serve as a basis for producing an animation ( <ref type="bibr">Kacorri et al., 2016)</ref>, in this work, we investigate how to con- struct a model that generalizes across the entire set of recordings, to produce an "average" of the face and head movements, which can serve as a basis for generating an animation. To enable compar- ison of our new methodology to our prior tech- nique, we make use of an identical training dataset as in ( <ref type="bibr">Kacorri et al., 2016)</ref> and an identical ani- mation rendering pipeline, described in <ref type="bibr" target="#b6">(Huenerfauth and Kacorri, 2015a</ref>). Briefly, the animation pipeline accepts a script of the hand location, hand orientation, and hand-shape information to pose and move the arms of the character over time, and it also accepts a file containing a stream of face movement information in MPEG4 Facial Anima- tion Parameters format (ISO/IEC, 1999) to pro- duce a virtual human animation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset and Feature Extraction</head><p>ASL is a low-resource language, and it does not have a writing system in common use. Therefore, ASL corpora are generally small in size and in limited supply; they are usually produced through manual annotation of video recordings. Thus, researchers generally work with relatively small datasets. In this work, we make use of two datasets that consist of video recordings of humans per- forming ASL with annotation labeling the times in the video when each of the five types of syntactic facial expressions listed in section 1.1 occur.</p><p>The training dataset used in this study was de- scribed in ( <ref type="bibr">Kacorri et al., 2016)</ref>, and consists of 199 examples of facial expressions performed by a female signer recorded at Boston University. While the Training dataset can naturally be par- titioned into five subsets, based on each of the five syntactic facial expression types, because adjacent Type Subgroup " A" (Num. of Videos)</p><p>Subgroup " B" (Num. of Videos)</p><p>YesNo Immediately pre- ceded by a facial expression with raised eyebrows, e.g. Topic. <ref type="formula">(9)</ref> Not immediately preceded by an eyebrow-raising expression. <ref type="formula">(10</ref>  facial expressions or phrase durations may affect the performance of ASL facial expressions, in this work, we sub-divide the dataset further, into ten sub-groups, as summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>The "gold-standard" dataset used in this study was shared with the research community by ; we use 10 ex- amples of ASL facial expressions (one for each sub-group listed in <ref type="table" target="#tab_1">Table 1</ref>) performed by a male signer who was recorded at the Linguistic and As- sistive Technologies laboratory.</p><p>To extract face and head movement information from the video, a face-tracker <ref type="bibr">(Visage, 2016)</ref> was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea- tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow. The first six values represent head location and orientation. The next six values represent vertical movement of the outer ("o "), middle ("m "), or inner ("i ") portion of the right ("r ") or left ("l ") eyebrows. The final values rep- resent horizontal movement of the eyebrows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Continuous Profile Models (CPM)</head><p>Continuous Profile Model (CPM) aligns a set of related time series data while accounting for changes in amplitude. This model has been previously evaluated on speech signals and on other biological time-series data ( <ref type="bibr">Listgarten et al., 2004</ref>). With the assumption that a noisy, stochas- tic process generates the observed time series data, the approach automatically infers the underlying noiseless representation of the data, the so-called "latent trace." <ref type="figure" target="#fig_4">Figure 6</ref> (on the last page of this paper) shows an example of multiple time series in unaligned and aligned space, with CPM identi- fying the the latent trace.</p><p>Given a set K of observed time series</p><formula xml:id="formula_0">x k = (x k 1 , x k 2 , ..., x k N ), CPM assumes there is a latent trace z = (z 1 , z 2 , ..., z M ).</formula><p>While not a require- ment of the model, the length of the time se- ries data is assumed to be the same (N ) and the length of the latent trace used in practice is M = (2+ε)N , where an ideal M would be large relative to N to allow precise mapping between observed data and an underlying point on the latent trace. Higher temporal resolution of the latent trace also accommodates flexible alignments by allowing an observational series to advance along the latent trace in small or large jumps <ref type="bibr">(Listgarten, 2007)</ref>.</p><p>Continuous Profile Models (CPMs) build on Hidden Markov Models (HMMs) <ref type="bibr">(Poritz, 1988)</ref> and share similarities with Profile HMMs which augment HMMs by two constrained-transition states: 'Insert' and 'Delete' (emitting no observa- tions). Similar to the Profile HMM, the CPM has strict left-to-right transition rules, constrained to only move forward along a sequence. <ref type="figure">Figure 1</ref> in- cludes a visualization we created, which illustrates the graphical model of a CPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Obtaining the CPM Latent Trace</head><p>We applied the CPM model to time align and co- herently integrate time series data from multiple ASL facial expression performances of a partic- ular type, e.g., Topic A as listed in section 2.1, with the goal of using the inferred 'latent traces' to drive ASL animations with facial expressions of that type. This section describes our work to train the CPM and to obtain the latent traces; im- plementation details appear in Appendix A.</p><p>The input time-series data for each CPM model is the face and head movement data extracted from ASL videos of one of the facial expression types, <ref type="figure">Figure 1</ref>: Depiction of a CPM for series x k , with hidden state variables π k i underlying each obser- vation x k i . The table illustrates the state-space: time-state/scale-state pairs mapped to the hidden variables, where time states belong to the integer set (1...M ) and scale states belong to an ordered set, here with 7 evenly spaced scales in logarith- mic space as in ( <ref type="bibr">Listgarten et al., 2004</ref>).</p><p>as shown in <ref type="table" target="#tab_3">Table 2</ref>. For each dataset, all the train- ing examples are stretched (resampled using cubic interpolation) to meet the length of the longest ex- ample in the set. The length of time series, N , corresponds to the duration in video frames of the longest example in the data set. The recordings in the training set have 14 dimensions, corresponding to the 14 facial features listed in Section 2.1. As discussed above, the latent trace has a time axis of length M , which is approximately double the tem- poral resolution of the original training examples.  To demonstrate our experiments, <ref type="figure" target="#fig_4">Figure 6</ref> il- lustrates one of the subcategories, Rhetorical B. (This figure appears at the end of the paper, due to its large size.) We illustrate the training set, before and after the alignment and amplitude nor- malization with the CPM, and the obtained latent trace for this subcategory. <ref type="figure" target="#fig_4">Figure 6a</ref> and <ref type="figure" target="#fig_4">Figure  6b</ref> illustrate each of the 8 training examples with a subplot extending from <ref type="bibr">[0, N ]</ref> in the x-axis, which is the observed time axis in video frames. Each of the 14 plots represents one of the head or face features. <ref type="figure" target="#fig_4">Figure 6c</ref> illustrates the learned latent trace with a subplot extending from <ref type="bibr">[0, M ]</ref> in the x-axis, which is the latent time axis. While the training set for this subcategory is very small and has high variability, upon visual inspection of <ref type="figure" target="#fig_4">Fig- ure 6</ref>, we can observe that the learned latent trace shares similarities with most of the time series in the training set without being identical to any of them.</p><p>We expect that during the Rhetorical facial ex- pression (Section 2.1), the signer's eyebrows will rise and the head will be tilted back and to the side. In the latent trace, the inner, middle, and outer por- tions of the left eyebrow rise <ref type="figure" target="#fig_4">(Figure 6c, plots 7, 9,  11)</ref>, and so do the inner, middle, and outer portions of the right eyebrow <ref type="figure" target="#fig_0">(Figure 6c, plots 8, 10, 12)</ref>. Note how the height of the lines in those plots rise, which indicates increased eyebrow height. For the Rhetorical facial expression, we would also ex- pect symmetry in the horizontal displacement of the eyebrows, and we see such mirroring in the latent-trace: In <ref type="figure" target="#fig_1">(Figure 6c, plots 13-14)</ref>, note the tendency for the line in plot 13 (left eyebrow) to increase in height as the line in plot 14 (right eye- brow) decreases in height, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>This section presents two forms of evaluation of the CPM latent trace model for ASL facial expres- sion synthesis. In Section 3.1, the CPM model will be compared to a "gold-standard" performance of each sub-category of ASL facial expression using a distance-metric-based evaluation, and in Section 3.2, the results of a user-study will be presented, in which ASL signers evaluated animations of ASL based upon the CPM model.</p><p>To provide a basis of comparison, in this sec- tion, we evaluate the CPM approach in compari- son to an alternative approach that we call 'Cen- troid', which we described in prior work in <ref type="bibr">(Ka-corri et al., 2016)</ref>, where we used a multivariate DTW to select one of the time series in the train- ing set as a representative performance of the fa- cial expression. The centroid examples are actual recordings of human ASL signers that are used to drive an animation. Appendix A lists the co- denames of the videos from the training dataset selected as centroids and the codenames of the videos used in the gold-standard dataset ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metric Evaluation</head><p>The gold-standard recordings of a male ASL signer were described in Section 2.1. In addition to the video recordings (which were processed to extract face and head movement data), we have an- notation of the timing of the facial expressions and the sequence of signs performed on the hands. To compare the quality of our CPM model and that of the Centroid approach, we used each method to produce a candidate sequence of face and head movements for the sentence performed by the hu- man in the gold-standard recording. Thus, the ex- tracted facial expressions from the human record- ing can serve as a gold standard for how the face and head should move. In this section, we com- pare: (a) the distance of the CPM latent trace from the gold standard to (b) the distance of the centroid form the gold standard. It is notable that these gold-standard recordings were previ- ously "unseen" during the creation of the CPM or Centroid models, that is, they were not used in the training data set during the creation of either model.</p><p>Since there was variability in the length of the latent trace, centroid, and gold-standard videos, for a fairer comparison, we first resampled these time series, using cubic interpolation, to match the duration (in milliseconds) of the gold-standard ASL sentence, and then we used multivariate DTW to estimate their distance, following the methodology of ( <ref type="bibr">Kacorri et al., 2016)</ref> and <ref type="bibr" target="#b9">(Kacorri and Huenerfauth, 2015)</ref>. In prior work <ref type="bibr" target="#b9">(Kacorri and Huenerfauth, 2015)</ref>, we had shown that a scoring algorithm based on DTW had moderate (yet significant) correlation with scores that partic- ipants assigned to ASL animation with facial ex- pressions. <ref type="figure" target="#fig_0">Figure 2</ref> shows an example of a DTW distance scoring between the gold standard and each of the latent trace and the centroid, for one face feature  (horizontal movement of the left eyebrow) during a Negative A facial expression. Given that the centroid and the training data for the latent trace are driven by recordings of a (female) signer and the gold standard is a different (male) signer, there are differences between these facial expressions due to idiosyncratic aspects of individual signers. Thus the metric evaluation in this section is chal- lenging because it is an inter-signer evaluation. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the overall calculated DTW distances, including a graph with the results bro- ken down per subcategory of ASL facial expres- sion. The results indicate that the CPM latent trace is closer to the gold standard than the centroid is. Note that the distance values are not zero since the latent trace and the centroid are being compared to a recording from a different signer on novel, previously unseen, ASL sentences. The results in these graphs suggest that the latent trace model out-performed the centroid approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">User Evaluation</head><p>To further assess our ASL synthesis approach, we conducted a user study where ASL signers watched short animations of ASL sentences with identical hand movements but differing in their face, head, and torso movements. There were three conditions in this between-subjects study: a) animations with a static neutral face through- out the animation (as a lower baseline), b) ani- mations with facial expressions driven by the cen- troid human recording, and c) animations with fa- cial expressions driven by the CPM latent trace based on multiple recordings of a human perform- ing that type of facial expression. <ref type="figure" target="#fig_2">Figure 4</ref> il- lustrates screenshots of each stimulus type for a YesNo A facial expression. The specific sentences used for this study were drawn from a standard test set of stimuli released to the research commu- nity by ) for eval- uating animations of sign language with facial ex- pressions.</p><p>All three types of stimuli (neutral, centroid and latent trace), shared identical animation-control scripts specifying the hand and arm movements; these scripts were hand-crafted by ASL signers in a pose-by-pose manner. For the neutral anima- tions, we did not specify any torso, head, nor face movements; rather, we left them in their neutral pose throughout the sentences. As for the cen- troid and latent trace animations, we applied the head and face movements (as specified by the cen- troid model or by the latent trace model) only to the portion of the animation where the facial ex- pression of interest occurs, leaving the head and face for the rest of the animation to a neutral pose. For instance, during a stimulus that contains a Wh- question, the face and head are animated only dur- ing the Wh-question, but they are left in a neutral pose for the rest of the stimulus (which may in- clude other sentences). The period of time when the facial expression occurred was time-aligned with the subset of words (the sequence of signs performed on the hands) for the appropriate syn- tactic domain; the phrase-beginning and phrase- ending was aligned with the performance of the facial expression. Thus, the difference in appear- ance between our animation stimuli was subtle: The only portion of the animations that differed between the three conditions (neutral, centroid, and latent-trace) was the face and the head move- ments during the span of time when the syntac- tic facial expression should occur (e.g., during the Wh-question).</p><p>We resampled the centroid and CPM time se- ries, using cubic interpolation, to match the dura- tion (in milliseconds) of the animation they would be applied to. To convert the centroid and latent trace time series into the input for the animation- generation system, we used the MPEG4-features- to-animation pipeline described in ( <ref type="bibr">Kacorri et al., 2016)</ref>. That platform is based upon the open- source EMBR animation system for producing hu- man animation <ref type="bibr" target="#b4">(Heloir and Kipp, 2009)</ref>; specif- ically, the facial expressions were represented as an EMBR PoseSequence with a pose defined ev- ery 133 milliseconds.</p><p>In prior work <ref type="bibr" target="#b7">(Huenerfauth and Kacorri, 2015b)</ref>, we investigated key methodological con- siderations in conducting a study to evaluate sign language animations with deaf users, including the use of appropriate baselines for comparison, appropriate presentation of questions and instruc- tions, demographic and technology experience factors influencing acceptance of signing avatars, and other factors that we have considered in the design of this current study. Our recent work ( <ref type="bibr" target="#b9">Kacorri et al., 2015</ref>) has established a set of de- mographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to eval- uate animation. Specifically, we screened for par- ticipants that identified themselves as "deaf/Deaf" or "hard-of-hearing," who had grown up using ASL at home or had attended an ASL-based school as a young child, such as a residential or daytime school.</p><p>Deaf researchers (all fluent ASL signers) re- cruited and collected data from participants, dur- ing meetings conducted in ASL. Initial advertise-ments were sent to local email distribution lists and Facebook groups. A total of 17 participants met the above criteria, where 14 participants self- identified as deaf/Deaf and 3 as hard-of-hearing. Of our participants in the study, 10 had attended a residential school for deaf students, and 7, a day- time school for deaf students. 14 participants had learned ASL prior to age 5, and the remaining 3 had been using ASL for over 7 years. There were 8 men and 9 women of ages 19-29 (average age 22.8). In prior work, we <ref type="bibr" target="#b9">(Kacorri et al., 2015)</ref> have advocated that participants in studies eval- uating sign language animation complete a two standardized surveys about their technology ex- perience (MediaSharing and AnimationAttitude) and that researchers report these values for partici- pants, to enable comparison across studies. In our study, participant scores for MediaSharing varied between 3 and 6, with a mean score of 4.3, and scores for AnimationAttitude varied from 2 to 6, with a mean score of 3.8.</p><p>At the beginning of the study, participants viewed a sample animation, to familiarize them with the experiment and the questions they would be asked about each animation. (This sample used a different stimulus than the other ten anima- tions shown in the study.) Next, they responded to a set of questions that measured their subjec- tive impression of each animation, using a 1-to-10 scalar response. Each question was conveyed us- ing ASL through an onscreen video, and the fol- lowing English question text was shown on the questionnaire: (a) Good ASL grammar? (10=Per- fect, 1=Bad); (b) Easy to understand? (10=Clear, 1=Confusing); (c) Natural? (10=Moves like per- son, 1=Like robot). These questions have been used in many prior experimental studies to evalu- ate animations of ASL, e.g. ( <ref type="bibr" target="#b9">Kacorri and Huenerfauth, 2015)</ref>, and were shared with research com- munity as a standard evaluation tool in ). To calculate a sin- gle score for each animation, the scalar response scores for the three questions were averaged. <ref type="figure" target="#fig_3">Figure 5</ref> shows distributions of subjective scores as boxplots with a 1.5 interquartile range (IQR). For comparison, means are denoted with a star and their values are labeled above each boxplot. When comparing the subjective scores that participants assigned to the animations in <ref type="figure" target="#fig_3">Fig- ure 5</ref>, we found a significant difference (Kruskal- Wallis test used since the data was not normally distributed) between the latent trace and centroid (p &lt; 0.005) and between the latent trace and neu- tral (p &lt; 0.05).</p><p>In summary, our CPM modeling approach for generating an animation out-performed an anima- tion produced from an actual recording of a sin- gle human performance (the "centroid" approach).</p><p>In prior methodological studies, we demonstrated that it is valid to use either videos of humans or animations (driven by a human performance) as the baseline for comparison in a study of ASL an- imation ( <ref type="bibr" target="#b10">Kacorri et al., 2013a)</ref>. As suggested by <ref type="figure" target="#fig_2">Figure 4</ref>, the differences in face and head move- ments between the Centroid and CPM conditions were subtle, yet fluent ASL signers rated the CPM animations higher in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>To facilitate the creation of ASL content that can easily be updated or maintained, we have investi- gated technologies for automating the synthesis of ASL animations from a sparse representation of the message. Specifically, this paper has focused on the synthesis of syntactic ASL facial expres- sions, which are essential to sentence meaning, using a data-driven methodology in which record- ings of human ASL signers are used as a basis for generating face and head movements for anima- tion. To avoid idiosyncratic aspects of a single performance, we have modeled a facial expres- sion based on the underlying trace of the move- ment trained on multiple recordings of different sentences where this type of facial expression oc- curs. We obtain the latent trace with Continuous Profile Model (CPM), a probabilistic generative model that relies on Hidden Markov Models. We assessed our modeling approach through compar- ison to an alternative centroid approach, where a single performance was selected as a representa- tive. Through both a metric evaluation and an experimental user study, we found that the facial expressions driven by our CPM models produce high-quality facial expressions that are more simi- lar to human performance of novel sentences.</p><p>While this work used the latent trace as the basis for animation, in future work, we also plan to ex- plore methods for sampling from the model to pro- duce variations in face and head movement. In ad- dition, to aid CPM convergence to a good local op- timum, in future work we will investigate dimen- sionality reduction approaches that are reversible such as Principal Component Analysis <ref type="bibr">(Pearson, 1901)</ref> and other pre-processing approaches similar to <ref type="bibr">(Listgarten, 2007)</ref>, where the training data set is coarsely pre-aligned and pre-scaled based on the center of mass of the time series. In addition we plan to further investigate how to fine-tune some of the hyper parameters of the CPM such as spline scaling, single global scaling factor, convergence tolerance, and initialization of the latent trace with a centroid. In subsequent work, we would like to explore alternatives for enhancing CPMs by incor- porating contextual features in the training data set such as timing of hand movements, and preceding, succeeding, and co-occurring facial expressions. <ref type="bibr">Hernisa Kacorri, Matt Huenerfauth, Sarah Ebling, Kasmira Patel, and Mackenzie Willard. 2015</ref>. Demo- graphic and experiential factors influencing accep- tance of sign language animation by deaf users. The stanford achievement test: National norming and perfor- mance standards for deaf and hard-of-hearing stu- dents. Journal of deaf studies and deaf education, 5 <ref type="formula">(4)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: Supplemental Material</head><p>In Section 2.3, we made use of a freely available CPM implementation available from http://www.cs.toronto.edu/∼jenn/CPM/ in MAT- LAB, Version 8. <ref type="bibr">5.0.197613 (R2015a)</ref>.</p><p>One parameter for regularizing the latent trace <ref type="bibr">(Listgarten, 2007</ref>) is a smoothing parameter (λ), with values being dataset-dependent. To select a good λ, we experimented with held-out data and found that λ = 4 and N umberOf Iterations = 3 resulted in a latent trace curve that captures the shape of the ASL features well. Other CPM pa- rameters were:</p><p>• U SE SP LIN E = 0: if set to 1, uses spline scaling rather than HMM scale states • oneScaleOnly = 0: no HMM scale states (only a single global scaling factor is applied to each time series.) • extraP ercent(ε) = 0.05: slack on the length of the latent trace M , where M = (2 + ε)N .</p><p>• learnStateT ransitions = 0: whether to learn the HMM state-transition probabilities • learnGlobalScaleF actor = 1: learn single global scale factor for each time series Section 3.1 described how the centroids were selected from among videos in the Boston Univer- sity dataset <ref type="bibr">(Neidle et al., 2014)</ref>, and the gold stan- dard videos were selected from among videos in a different dataset .   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DTW distances on the squeeze l brow feature (left eyebrow horizontal movement), during a Negative A facial expression: (left) between the CPM latent trace and gold standard and (right) between the centroid and gold standard. The timeline is given in milliseconds.</figDesc><graphic url="image-2.png" coords="5,307.28,62.81,218.26,101.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall normalized DTW distances for latent trace and centroid (left) and per each subcategory of ASL facial expression (right).</figDesc><graphic url="image-3.png" coords="5,307.28,275.11,218.26,138.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Screenshots of YesNo A stimuli of three types: a) neutral, b) centroid, and c) latent trace.</figDesc><graphic url="image-4.png" coords="6,72.00,62.81,218.27,117.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Subjective scores for centroid, latent trace, and neutral animations.</figDesc><graphic url="image-5.png" coords="7,307.28,62.81,218.27,151.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of CPM modeling for Rhetorical B: (a) training examples before CPM (each plot shows one of the 14 face features over time, with 8 colored lines in each plot showing each of the 8 training examples), (b) after CPM time-alignment and rescaling, and (c) the final latent trace based upon all 8 examples.</figDesc><graphic url="image-6.png" coords="10,72.00,143.72,453.55,471.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>)</head><label></label><figDesc></figDesc><table>WhQuestion Performed during 
a single word, 
namely the wh-
word (e.g., what, 
where, when). (4) 

Performed during 
a phrase consist-
ing of multiple 
words. (8) 

Rhetorical 
Performed during 
a single word, 
namely the wh-
word (e.g., what, 
where, when). (2) 

Performed during 
a phrase consist-
ing of multiple 
words. (8) 

Topic 
Performed during a 
single word. (29) 
Performed during 
a phrase consist-
ing of multiple 
words. (15) 
Negative 
Immediately pre-
ceded by a facial 
expression 
with 
raised eyebrows, 
e.g. Topic. (16) 

Not immediately 
preceded 
by 
eyebrow-raising 
expression. (25) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Ten subgroups of the training dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Training data and the obtained latent 
traces for each of the CPM models on ASL facial 
expression subcategories. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>: 337 -348.</head><label>337</label><figDesc></figDesc><table>Technologies Visage. 
2016. 
Face tracking. 
https://visagetechnologies.com/products-and-
services/visagesdk/facetrack. Accessed: 2016-03-
10. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 lists</head><label>3</label><figDesc>the code names of the selected videos, using the nomenclature of each dataset.</figDesc><table>Subcategory 
Centroid Codename 
Gold-Standard Codename 
YesNo A 
2011-12-01 0037-cam2-05 
Y4 
YesNo B 
2011-12-01 0037-cam2-09 
Y3 
WhQuestion A 
2011-12-01 0038-cam2-05 
W1 
WhQuestion B 
2011-12-01 0038-cam2-07 
W2 
Rhetorical A 
2011-12-01 0041-cam2-04 
R3 
Rhetorical B 
2011-12-01 0041-cam2-02 
R9 
Topic A 
2012-01-27 0050-cam2-05 
T4 
Topic B 
2012-01-27 0051-cam2-09 
T3 
Negative A 
2012-01-27 0051-cam2-03 
N2 
Negative B 
2012-01-27 0051-cam2-30 
N5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Codenames of videos selected as centoids 
and gold standards for comparison in section 3.1. </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the National Science Foundation under award num-ber 1065009 and 1506786. This material is also based upon work supported by the Science Fel-lowship and Dissertation Fellowship programs of The Graduate Center, CUNY. We are grateful for support and resources provided by Ali Raza Syed at The Graduate Center, CUNY, and by Carol Nei-dle at Boston University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A microanalysis of the nonmanual components of questions in american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Baker-Shenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting the full potential of jasigning to build an avatar signing train announcements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ebling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Glauert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Symposium on Sign Language Translation and Avatar Technology (SLTAT)</title>
		<meeting>the Third International Symposium on Sign Language Translation and Avatar Technology (SLTAT)<address><addrLine>Chicago, USA, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A rule triggering system for automatic text-to-sign translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Filhol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">N</forename><surname>Hadjadj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Testu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Universal Access in the Information Society</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The signcom system for data-driven animation of interactive virtual signers: Methodology and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvie</forename><surname>Gibet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><forename type="middle">Le</forename><surname>Naour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Embr-a realtime animation engine for interactive embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Heloir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Virtual Agents</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="393" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Release of experimental stimuli and questions for evaluating facial expressions in animations of american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on the Representation and Processing of Sign Languages: Beyond the Manual Channel</title>
		<meeting>the 6th Workshop on the Representation and Processing of Sign Languages: Beyond the Manual Channel<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>The 9th International Conference on Language Resources and Evaluation (LREC 2014)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augmenting embr virtual human animation system with mpeg-4 controls for producing asl facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Sign Language Translation and Avatar Technology</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Best practices for conducting evaluations of sign language animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Technology and Persons with Disabilities</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Information technology-Coding of audio-visual objects-Part 2: Visual. ISO 144962:1999, International Organization for Standardization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iso/Iec</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Geneva, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating a dynamic time warping based scoring algorithm for facial expressions in asl animations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effect of displaying human videos during an evaluation study of american sign language animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing (TACCESS)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating facial expressions in american sign language animations for accessible online information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Universal Access in Human-Computer Interaction. Design Methods, Tools, and Interaction Techniques for eInclusion</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring the perception of facial expressions in american sign language animations with eye tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Universal Access in Human-Computer Interaction. Design for All and Accessibility Practice</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="553" to="563" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
