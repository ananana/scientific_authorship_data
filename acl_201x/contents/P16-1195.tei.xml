<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Summarizing Source Code using a Neural Attention Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Summarizing Source Code using a Neural Attention Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2073" to="2083"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely data-driven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large margin .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Billions of lines of source code reside in online repositories ( <ref type="bibr">Dyer et al., 2013)</ref>, and high quality code is often coupled with natural language (NL) in the form of instructions, comments, and docu- mentation. Short summaries of the overall com- putation the code performs provide a particularly useful form of documentation for a range of appli- cations, such as code search or tutorials. However, such summaries are expensive to manually author.  As a result, this laborious process is only done for a small fraction of all code that is produced.</p><p>In this paper, we present the first completely data-driven approach for generating short high- level summaries of source code snippets in natu- ral language. We focus on C#, a general-purpose imperative language, and SQL, a declarative lan- guage for querying databases. <ref type="figure" target="#fig_1">Figure 1</ref> shows ex- ample code snippets with descriptions that sum- marize the overall function of the code, with the goal to generate high level descriptions, such as lookup a substring in a string. Generating such a summary is often challenging because the text can include complex, non-local aspects of the code (e.g., consider the phrase 'second largest' in Ex- ample 3 in <ref type="figure" target="#fig_1">Figure 1</ref>). In addition to being di- rectly useful for interpreting uncommented code, high-quality generation models can also be used for code retrieval, and in turn, for natural language programming by applying nearest neighbor tech- niques to a large corpus of automatically summa- rized code.</p><p>Natural language generation has traditionally been addressed as a pipeline of modules that de- cide 'what to say' (content selection) and 'how to say it' (realization) separately <ref type="bibr" target="#b26">(Reiter and Dale, 2000;</ref><ref type="bibr" target="#b38">Wong and Mooney, 2007;</ref><ref type="bibr">Chen et al., 2010;</ref><ref type="bibr" target="#b15">Lu and Ng, 2011)</ref>. Such approaches require super- vision at each stage and do not scale well to large domains. We instead propose an end-to-end neural network called CODE-NN that jointly performs content selection using an attention mechanism, and surface realization using Long Short Term Memory (LSTM) networks. The system generates a summary one word at a time, guided by an at- tention mechanism over embeddings of the source code, and by context from previously generated words provided by a LSTM network <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref>. The simplicity of the model allows it to be learned from the training data without the burden of feature engineering <ref type="bibr" target="#b3">(Angeli et al., 2010)</ref> or the use of an expensive approx- imate decoding algorithm ( <ref type="bibr" target="#b9">Konstas and Lapata, 2013)</ref>.</p><p>Our model is trained on a new dataset of code snippets with short descriptions, created using data gathered from Stackoverflow, 1 a popular pro- gramming help website. Since access is open and unrestricted, the content is inherently noisy (un- grammatical, non-parsable, lacking content), but as we will see, it still provides strong signal for learning. To reliably evaluate our model, we also collect a clean, human-annotated test set. <ref type="bibr">2</ref> We evaluate CODE-NN on two tasks: code summarization and code retrieval (Section 2). For summarization, we evaluate using automatic met- rics such as METEOR and BLEU-4, together with a human study for naturalness and informative- ness of the output. The results show that CODE- NN outperforms a number of strong baselines and, to the best of our knowledge, CODE-NN is the first approach that learns to generate summaries of source code from easily gathered online data. We further use CODE-NN for code retrieval for pro- gramming related questions on a recent C# bench- mark, and results show that CODE-NN improves the state of the art ( <ref type="bibr" target="#b2">Allamanis et al. (2015b)</ref>) for mean reciprocal rank (MRR) by a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>CODE-NN generates a NL summary of source code snippets (GEN task). We have also used CODE-NN on the inverse task to retrieve source code given a question in NL (RET task).</p><p>Formally, let U C be the set of all code snippets and U N be the set of all summaries in NL. For a training corpus with J code snippet and summary pairs (c j , n j ), 1 ≤ j ≤ J, c j ∈ U C , n j ∈ U N , we define the following two tasks:</p><p>GEN For a given code snippet c ∈ U C , the goal is to produce a NL sentence n * ∈ U N that max- imizes some scoring function s ∈ (U C × U N → R):</p><formula xml:id="formula_0">n * = argmax n s(c, n)<label>(1)</label></formula><p>RET We also use the scoring function s to re- trieve the highest scoring code snippet c * j from our training corpus, given a NL question n ∈ U N :</p><formula xml:id="formula_1">c * j = argmax c j s(c j , n), 1 ≤ j ≤ J<label>(2)</label></formula><p>In this work, s is computed using an LSTM neu- ral attention model, to be described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Although we focus on generating high-level sum- maries of source code snippets, there has been work on producing code descriptions at other lev- els of abstraction. Movshovitz-Attias and Co- hen (2013) study the task of predicting class-level comments by learning n-gram and topic models from open source Java projects and testing it us- ing a character-saving metric on existing com- ments. <ref type="bibr" target="#b1">Allamanis et al. (2015a)</ref> create models for suggesting method and class names by embed- ding them in a high dimensional continuous space. <ref type="bibr" target="#b29">Sridhara et al. (2010)</ref> present a pipeline that gener- ates summaries of Java methods by selecting rel- evant content and generating phrases using tem- plates to describe them. There is also work on improving program comprehension ( <ref type="bibr" target="#b4">Haiduc et al., 2010)</ref>, identifying cross-cutting source code con- cerns <ref type="bibr" target="#b25">(Rastkar et al., 2011)</ref>, and summarizing soft- ware bug reports ( <ref type="bibr" target="#b23">Rastkar et al., 2010)</ref>. To the best of our knowledge, we are the first to use learning techniques to construct completely new sentences from arbitrary code snippets.</p><p>Source code summarization is also related to generation from formal meaning representations. Wong and Mooney (2007) present a system that learns to generate sentences from lambda calculus expressions by inverting a semantic parser. <ref type="bibr" target="#b17">Mei et al. (2016)</ref>, <ref type="bibr" target="#b9">Konstas and Lapata (2013)</ref>, and An- geli et al. (2010) create learning algorithms for text generation from database records, again assuming data that pairs sentences with formal meaning rep- resentations. In contrast, we present algorithms for learning from easily gathered web data.</p><p>In the database community, Simitsis and Ioan- nidis (2009) recognize the need for SQL database systems to talk back to users. <ref type="bibr" target="#b10">Koutrika et al. (2010)</ref> built an interactive system (LOGOS) that translates SQL queries to text using NL templates and database schemas. Similarly there has been work on translating SPARQL queries to natural language using rules to create dependency trees for each section of the query, followed by a trans- formation step to make the output more natural <ref type="bibr" target="#b20">(Ngonga Ngomo et al., 2013</ref>). These approaches are not learning based, and require significant manual template-engineering efforts.</p><p>We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based ap- proaches have gained popularity for text gener- ation and have been used in machine transla- tion <ref type="bibr" target="#b32">(Sutskever et al., 2011</ref>), image and video de- scription ( <ref type="bibr" target="#b7">Karpathy and Li, 2015;</ref><ref type="bibr" target="#b35">Venugopalan et al., 2015;</ref><ref type="bibr">Devlin et al., 2015)</ref>, sentence summa- rization ( <ref type="bibr" target="#b27">Rush et al., 2015)</ref>, and Chinese poetry generation ( <ref type="bibr" target="#b40">Zhang and Lapata, 2014</ref>). Perhaps most closely related, <ref type="bibr" target="#b36">Wen et al. (2015)</ref> generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder seman- tically conditioned on the logical representation of speech acts, and a reranker to generate the fi- nal output. In contrast, we design an end-to-end attention-based model for source code.</p><p>For code retrieval, <ref type="bibr" target="#b2">Allamanis et al. (2015b)</ref> pro- posed a system that uses Stackoverflow data and web search logs to create models for retrieving C# code snippets given NL questions and vice versa. They construct distributional representa- tions of code structure and language and com- bine them using additive and multiplicative mod- els to score (code, language) pairs, an approach that could work well for retrieval but cannot be used for generation. We learn a neural generation model without using search logs and show that it can also be used to score code for retrieval, with much higher accuracy.</p><p>Synthesizing code from language is an alter- native to code retrieval and has been studied in both the Systems and NLP research com- munities. <ref type="bibr">Giordani and Moschitti (2012)</ref>, <ref type="bibr" target="#b13">Li and Jagadish (2014)</ref>, and Gulwani and Marron (2014) synthesize source code from NL queries for database and spreadsheet applications. Sim- ilarly, <ref type="bibr" target="#b12">Lei et al. (2013)</ref> interpret NL instruc- tions to machine-executable code, and <ref type="bibr" target="#b11">Kushman and Barzilay (2013)</ref> convert language to regu- lar expressions. Unlike most synthesis methods, CODE-NN is domain agnostic, as we demonstrate its applications on both C# and SQL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>We collected data from StackOverflow (SO), a popular website for posting programming-related questions. Anonymized versions of all the posts can be freely downloaded. <ref type="bibr">3</ref> Each post can have multiple tags. Using the C# tag for C# and the sql, database and oracle tags for SQL, we were able to collect 934,464 and 977,623 posts respectively. <ref type="bibr">4</ref> Each post comprises a short title, a detailed ques- tion, and one or more responses, of which one can be marked as accepted. We found that the text in the question and responses is domain-specific and verbose, mixed with details that are irrelevant for our tasks. Also, code snippets in responses that were not accepted were frequently incorrect or tangential to the question asked. Thus, we ex- tracted only the title from the post and use the code snippet from those accepted answers that contain exactly one code snippet (using &lt;code&gt; tags). We add the resulting (title, query) pairs to our corpus, resulting in a total of 145,841 pairs for C# and 41,340 pairs for SQL.</p><p>Cleaning We train a semi-supervised classifier to filter titles like 'Difficult C# if then logic' or 'How can I make this query easier to write?' that bear no relation to the corresponding code snippet.</p><p>To do so, we annotate 100 titles as being clean or not clean for each language and use them to boot- strap the algorithm. We then use the remaining titles in our training set as an unsupervised sig- nal, and obtain a classification accuracy of over 73% on a manually labeled test set for both lan- guages. For the final dataset, we retain 66,015 C# (title, query) pairs and 32,337 SQL pairs that are classified as clean, and use 80% of these datasets for training, 10% for validation and 10% for test- ing.</p><p>Parsing Given the informal nature of Stack- Overflow, the code snippets are approximate an- swers that are usually incomplete. For example, we observe that only 12% of the SQL queries parse without any syntactic errors (using zql 5 ). We therefore aim to perform a best-effort parse of the code snippet, using modified versions of an ANTLR parser for C# <ref type="bibr" target="#b22">(Parr, 2013)</ref> and python- sqlparse (Albrecht, 2015) for SQL. We strip out all comments and to avoid being context specific, we replace literals with tokens denoting their types. In addition, for SQL, we replace table and column names with numbered placeholder tokens while preserving any dependencies in the query. For example, the SQL query in <ref type="figure" target="#fig_1">Figure 1</ref> is repre- sented as SELECT MAX(col0) FROM tab0 WHERE col0 &lt; (SELECT MAX(col0) FROM tab0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Statistics</head><p>The structural complexity and size of the code snippets in our dataset makes our tasks challenging. More than 40% of our C# cor- pus comprises snippets with three or more state- ments and functions, and 20% contains loops and conditionals. Also, over a third of our SQL queries contain one or more subqueries and multiple ta- bles, columns and functions (like MIN, MAX, SUM). On average, our C# snippets are 38 tokens long and the queries in our corpus are 46 tokens long, while titles are 9-12 words long. <ref type="table" target="#tab_1">Table 2</ref> shows the complete data statistics.</p><p>Human Annotation For the GEN task, we use n-gram based metrics (see Section 6.1.2) of the summary generated by our model with respect to the actual title in our corpus. Titles can be short, and a given code snippet can be described in many different ways with little overlapping content be- tween them. For example, the descriptions for the second code snippet in <ref type="figure" target="#fig_1">Figure 1</ref>   tions, we extend our test set by asking human an- notators to provide two additional titles for 200 snippets chosen at random from the test set, mak- ing a total of three reference titles for each code snippet. To collect this data, annotators were shown only the code snippets and were asked to write a short summary after looking at a few ex- ample summaries. They were also asked to "think of a question that they could ask on a program- ming help website, to get the code snippet as a re- sponse." This encouraged them to briefly describe the key feature that the code is trying to demon- strate. We use half of this test set for model tuning (DEV, see Section 5) and the rest for evaluation (EVAL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The CODE-NN Model</head><p>Description We present an end-to-end genera- tion system that performs content selection and surface realization jointly. Our approach uses an attention-based neural network to model the con- ditional distribution of a NL summary n given a code snippet c. Specifically, we use an LSTM model that is guided by attention on the source code snippet to generate a summary one word at a time, as shown in <ref type="figure">Figure 2</ref>. <ref type="bibr">6</ref> Formally, we represent a NL summary n = n 1 , . . . , n l as a sequence of 1-hot vectors</p><formula xml:id="formula_2">LSTM LSTM LSTM . . n 1 E E n 1 n l−1 ∝ ∅ ∝ A + A + ∝ A + END h1 h 2 h l t 1 t 2 t l F ∝ α ⊙ h i t i ⊙ c = c 1 , c 2 , ..., c k n 2 F c c c h 1 ; m 1 h 2 ; m 2 h l−1 ; m l−1</formula><p>Figure 2: Generation of a title n = n 1 , . . . , END given code snippet c 1 , ..., c k . The attention cell computes a distributional representation t i of the code snippet based on the current LSTM hidden state h i . A combination of t i and h i is used to generate the next word, n i , which feeds back into the next LSTM cell. This is repeated until a fixed number of words or END is generated. ∝ blocks denote softmax operations. n 1 , . . . , n l ∈ {0, 1} |N | , where N is the vocabu- lary of the summaries. Our model computes the probability of n (scoring function s in Eq. 1) as a product of the conditional next-word probabilities</p><formula xml:id="formula_3">s(c, n) = l i=1</formula><p>p(n i |n 1 , . . . , n i−1 ) with,</p><formula xml:id="formula_4">p(n i |n 1 , . . . , n i−1 ) ∝ W tanh(W 1 h i + W 2 t i )</formula><p>where, W ∈ R |N |×H and W 1 , W 2 ∈ R H×H , H being the embedding dimensionality of the sum- maries. t i is the contribution from the attention model on the source code (see below). h i repre- sents the hidden state of the LSTM cell at the cur- rent time step and is computed based on the pre- viously generated word, the previous LSTM cell state m i−1 and the previous LSTM hidden state h i−1 as</p><formula xml:id="formula_5">m i ; h i = f (n i−1 E, m i−1 , h i−1 ; θ)</formula><p>where E ∈ R |N |×H is a word embedding matrix for the summaries. We compute f using the LSTM cell architecture used by <ref type="bibr" target="#b39">Zaremba et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention</head><p>The generation of each word is guided by a global attention model ( <ref type="bibr" target="#b16">Luong et al., 2015)</ref>, which computes a weighted sum of the em- beddings of the code snippet tokens based on the current LSTM state (see right part in <ref type="figure">Figure 2)</ref>. Formally, we represent c as a set of 1-hot vectors c 1 , . . . , c k ∈ {0, 1} |C| for each source code to- ken; C is the vocabulary of all tokens in our code snippets. Our attention model computes,</p><formula xml:id="formula_6">t i = k j=1 α i,j · c j F</formula><p>where F ∈ R |C|×H is a token embedding matrix and each α i,j is proportional to the dot product be- tween the current internal LSTM hidden state h i and the corresponding token embedding c j :</p><formula xml:id="formula_7">α i,j = exp(h i T c j F) k j=1 exp(h i T c j F)</formula><p>Training We perform supervised end-to-end training using backpropagation <ref type="bibr" target="#b37">(Werbos, 1990)</ref> to learn the parameters of the embedding matrices F and E, transformation matrices W, W 1 and W 2 , and parameters θ of the LSTM cell that computes f . We use multiple epochs of minibatch stochas- tic gradient descent and update all parameters to minimize the negative log likelihood (NLL) of our training set. To prevent over-fitting we make use of dropout layers ( <ref type="bibr" target="#b30">Srivastava et al., 2014</ref>) at the summary embeddings and the output softmax layer. Using pre-trained embeddings <ref type="bibr" target="#b18">(Mikolov et al., (2013)</ref>) for the summary embedding matrix or adding additional LSTM layers did not improve performance for the GEN task. Since the NLL training objective does not directly optimize for our evaluation metric (METEOR), we compute METEOR (see Section 6.1.2) on a small develop- ment set (DEV) after every epoch and save the in- termediate model that gives the maximum score, as the final model.</p><p>Decoding Given a trained model and an input code snippet c, finding the most optimal title en- tails generating the title n * that maximizes s(c, n) (see Eq. 1). We approximate n * by performing beam search on the space of all possible sum- maries using the model output.</p><p>Implementation Details We add special START and END tokens to our training sequences and replace all tokens and output words occurring with a frequency of less than 3 with an UNK token, making |C| = 31, 667 and |N | = 7, 470 for C# and |C| = 747 and |N | = 2, 506 for SQL. Our hyper-parameters are set based on performance on the validation set. We use a minibatch size of 100 and set the dimensionality of the LSTM hidden states, token embeddings, and summary embeddings (H) to 400. We initialize all model parameters uniformly between −0.35 and 0.35. We start with a learning rate of 0.5 and start decaying it by a factor of 0.8 after 60 epochs if accuracy on the validation set goes down, and terminate training when the learning rate goes below 0.001. We cap the parameter gradients to 5 and use a dropout rate of 0.5.</p><p>We use the Torch framework 7 to train our mod- els on GPUs. Training runs for about 80 epochs and takes approximately 7 hours. We compute METEOR score at every epoch on the develop- ment set (DEV) to choose the best final model, with the best results obtained between 60 and 70 epochs. For decoding, we set the beam size to 10, and the maximum summary length to 20 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">GEN Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Baselines</head><p>For the GEN task, we compare CODE-NN with a number of competitive systems, none of which had been previously applied to generate text from source code, and hence we adapt them slightly for this task, as explained below.</p><p>IR is an information retrieval baseline that out- puts the title associated with the code c j in the training set that is closest to the input code c in terms of token Levenshtein distance. In this case s from Eq. It uses an encoder-decoder architecture with an at- tention mechanism based on a fixed context win- dow of previously generated words. The decoder is a feed-forward neural language model that gen- erates the next word based on previous words in a context window of size k. In contrast, we de- code using an LSTM network that can model long range dependencies and our attention weights are tied to the LSTM hidden states. We set the em- bedding and hidden state dimensions and context window size by tuning on our validation set. We found this model to generate overly short titles like 'sql server 2008' when a length restriction was not imposed on the output text. Therefore, we fix the output length to be the average title length in the training set while decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Evaluation Metrics</head><p>We evaluate the GEN task using automatic met- rics, and also perform a human study.</p><p>Automatic Evaluation We report METEOR ( <ref type="bibr">Banerjee and Lavie, 2005</ref>) and sentence level BLEU-4 ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>) scores. ME- TEOR is recall-oriented and measures how well our model captures content from the references in our output. BLEU-4 measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. Since the gen- erated summaries are short and there are multi- ple alternate summaries for a given code snippet, higher order n-grams may not overlap. We remedy this problem by using +1 smoothing ( <ref type="bibr" target="#b14">Lin and Och, 2004</ref>). We compute these metrics on the tuning set DEV and the held-out evaluation set EVAL.</p><p>Human Evaluation Since automatic metrics do not always agree with the actual quality of the re- sults ( <ref type="bibr" target="#b31">Stent et al., 2005</ref>), we perform human eval- uation studies to measure the output of our sys- tem and baselines across two modalities, namely naturalness and informativeness. For the former, we asked 5 native English speakers to rate each ti- tle against grammaticality and fluency, on a scale between 1 and 5. For informativeness (i.e., the amount of content carried over from the input code to the NL summary, ignoring fluency of the text), we asked 5 human evaluators familiar with C# and SQL to evaluate the system output by rating the factual overlap of the summary with the reference titles, on a scale between 1 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">RET task</head><p>6.2.1 Model and Baselines CODE-NN As described in Section 2, for a given NL question n in the RET task, we rank all code snippets c j in our corpus by computing the scoring function s(c j , n), and return the query c * j that maximizes it (Eq. 2).</p><p>RET-IR is an information retrieval baseline that ranks the candidate code snippets using cosine similarity between the given NL question n and all summaries n j in the retrieval set, based on their vector representations using TF-IDF weights over unigrams. The scoring function s in Eq. 2 be- comes:</p><formula xml:id="formula_8">s(c j , n) = tf-idf(n j ) · tf-idf(n) tf-idf(n j )tf-idf(n) , 1 ≤ j ≤ J</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Evaluation Metrics</head><p>We assess ranking quality by computing the Mean Reciprocal Rank (MRR) of c * j . For every snippet c j in EVAL (and DEV), we use two of the three references (title and human annotation), namely n j,1 , n j,2 . We then build a retrieval set compris- ing (c j , n j,1 ) together with 49 random distractor pairs (c , n ), c = c j from the test set. Using n j,2 as the natural language question, we rank all 50 items in this retrieval set and use the rank of query c * j to compute MRR. We average MRR over all re- turned queries c * j in the test set, and repeat this ex- periment for several different random sets of dis- tractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Tasks from Allamanis et al. (2015b)</head><p>Allamanis et al. (2015b) take a retrieval approach to answer C# related natural language questions (L to C), similar to our RET task. In addition, they also use retrieval to summarize C# source code (C to L) and evaluate both tasks using the MRR met- ric. Although they also use data from Stackover- flow, their dataset preparation and cleaning meth- ods differs significantly from ours. For example, they filter out posts where the question has fewer than 2 votes, the answer has fewer than 3 votes, or the post has fewer than 1000 views. Additionally, they also filter code snippets that cannot be parsed by Roslyn (.NET compiler) or are longer than 300 characters. Thus, to directly compare with their model, we re-train our generation model on their dataset and use our model score for retrieval of both code and summaries.    <ref type="table" target="#tab_3">Table 3</ref> shows automatic evaluation metrics for our model and baselines. CODE-NN outperforms all the other methods in terms of METEOR and BLEU-4 score. We attribute this to its ability to perform better content selection, focusing on the more salient parts of the code by using its atten- tion mechanism jointly with its LSTM memory cells. The neural models have better performance on C# than SQL. This is in part because, unlike SQL, C# code contains informative intermediate variable names that are directly related to the ob- jective of the code. On the other hand, SQL is more challenging in that it only has a handful of keywords and functions, and summarization mod- els need to rely on other structural aspects of the code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">GEN Task</head><p>Informativeness and naturalness scores for each model from our human evaluation study are pre- sented in <ref type="table" target="#tab_4">Table 4</ref>. In general, CODE-NN performs well across both dimensions. Its superior perfor- mance in terms of informativeness further sup- ports our claim that it manages to select content more effectively. Although SUM-NN performs similar to CODE-NN on naturalness, its output lacks content and has very little variation (see Sec- tion 7.4), which also explains its surprisingly low</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C#</head><p>RET-IR 0.42 ± 0.02 (0.44 ± 0.01) CODE-NN 0.58 ± 0.01 (0.66 ± 0.02)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQL</head><p>RET-IR 0.28 ±0.01(0.4 ± 0.01) CODE-NN 0.44 ± 0.01 (0.54 ± 0.02) <ref type="table">Table 5</ref>: MRR for the RET task. Dev set results in parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model MRR</head><p>L to C Allamanis 0.182 ±0.009 CODE-NN 0.590 ± 0.044 C to L Allamanis 0.434 ±0.003 CODE-NN 0.461 ± 0.046 <ref type="table">Table 6</ref>: MRR values for the Language to Code (L to C) and the Code to Language (C to L) tasks using the C# dataset of <ref type="bibr" target="#b2">Allamanis et al. (2015b)</ref> score on informativeness. <ref type="table">Table 5</ref> shows the MRR on the RET task for CODE-NN and RET-IR, averaged over 20 runs for C# and SQL. CODE-NN outperforms the baseline by about 16% for C# and SQL. RET-IR can only output code snippets that are annotated with NL as potential matches. On the other hand, CODE- NN can rank even unannotated code snippets and nominate them as potential candidates. Hence, it can leverage vast amounts of such code available in online repositories like Github. To speed up re- trieval when using CODE-NN , it could be one of the later stages in a multi-stage retrieval system and candidates may also be ranked in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">RET Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison with Allamanis et al.</head><p>We train CODE-NN on their dataset and evaluate using the same MRR testing framework (see Ta- ble 6). Our model performs significantly better for the Language to Code task (L to C) and slightly better for the Code to Language task (C to L). The attention mechanism together with the LSTM net- work is able to generate better scores for (lan- guage, code) pairs. <ref type="figure">Figure 3</ref> shows the relative magnitudes of the at- tention weights (α i,j ) for example C# and SQL code snippets while generating their correspond- ing summaries. Darker regions represent stronger weights. CODE-NN automatically learns to do  Figure 3: Heatmap of attention weights α i,j for example C# (left) and SQL (right) code snippets. The model learns to align key summary words (like cell) with the corresponding tokens in the in- put (SelectedCells).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Qualitative Analysis</head><p>high-quality content selection by aligning key summary words with informative tokens in the code snippet. <ref type="table">Table 8</ref> shows examples of the output gener- ated by our model and baselines for code snippets in DEV. Most of the models produce meaningful output for simple code snippets (first example) but degrade on longer, compositional inputs. For ex- ample, the last SQL query listed in <ref type="table">Table 8</ref> in- cludes a subquery, where a complete description should include both summing and concatenation. CODE-NN describes the summation (but not con- catenation), while others return non-relevant de- scriptions.</p><p>Finally, we performed manual error analysis on 50 randomly selected examples from DEV (Ta- ble 7) for each language. Redundancy is a ma- jor source of error, i.e., generation of extraneous content-bearing phrases, along with missing con- tent, e.g., in the last example of <ref type="table">Table 8</ref> there is no reference to the concatenation operations present in the beginning of the query. Sometimes the out- put from our model can be out of context, in the sense that it does not match the input code. This often happens for low frequency tokens (7% of cases), for which CODE-NN realizes them with generic phrases. This also happens when there are very long range dependencies or compositional structures in the input, such as nested queries (13% of the cases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we presented CODE-NN , an end- to-end neural attention model using LSTMs to  <ref type="table">Table 7</ref>: Error analysis on 50 examples in DEV generate summaries of C# and SQL code by learning from noisy online programming websites. Our model outperforms competitive baselines and achieves state of the art performance on automatic metrics, namely METEOR and BLEU, as well as on a human evaluation study. We also used CODE-NN to answer programming questions by retrieving the most appropriate code snippets from a corpus, and beat previous baselines for this task in terms of MRR. We have published our C# and SQL datasets, the accompanying human annotated test sets, and our code for the tasks described in this paper.</p><p>In future work, we plan to develop better models for capturing the structure of the input, as well as extend the use of our system to other applications such as automatic documentation of source code.</p><p>Satanjeev Banerjee and Alon <ref type="bibr">Lavie. 2005</ref>. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evalu- ation measures for machine translation and/or sum- marization, volume 29, pages 65-72. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Get rendered width of string rounded up to the nearest integer b. Compute the actual textwidth inside a textblock 2. Source Code (C#): var input = " Hello "; var regEx = new Regex (" World " ); return ! regEx . IsMatch ( input ); Descriptions: a. Return if the input doesn't contain a particular word in it b. Lookup a substring in a string using regex 3. Source Code (SQL): SELECT Max ( marks ) FROM stud_records WHERE marks &lt; ( SELECT Max ( marks ) FROM stud_records ); Descriptions: a. Get the second largest value of a column b. Retrieve the next max record in a table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Code snippets in C# and SQL and their summaries in NL, from StackOverflow. Our goal is to automatically generate summaries from code snippets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 becomes, s(c, n j ) = −1 × lev(c j , c), 1 ≤ j ≤ J MOSES (Koehn et al., 2007) is a popular phrase-based machine translation system. We per- form generation by treating the tokenized code snippet as the source language, and the title as the target. We train a 3-gram language model using KenLM (Heafield, 2011) to use with MOSES, and perform MIRA-based tuning (Cherry and Foster, 2012) of hyper-parameters using DEV. SUM-NN is the neural attention-based abstrac- tive summarization model of Rush et al. (2015). 7 http://torch.ch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>David</head><label></label><figDesc>L Chen, Joohyun Kim, and Raymond J Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal of Ar- tificial Intelligence Research, pages 397-435. Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Pro- ceedings of the 2012 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427-436. Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Mar- garet Mitchell. 2015. Language models for image captioning: The quirks and what works. In Proceed- ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna- tional Joint Conference on Natural Language Pro- cessing (Volume 2: Short Papers), pages 100-105. Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. 2013. Boa: A language and in- frastructure for analyzing ultra-large-scale software repositories. In Proceedings of the 2013 Interna- tional Conference on Software Engineering, pages 422-431. Alessandra Giordani and Alessandro Moschitti. 2012. Translating questions to SQL queries with genera- tive parsers discriminatively reranked. In Proceed- ings of COLING 2012: Posters, pages 401-410. Sumit Gulwani and Mark Marron. 2014. Nlyze: Inter- active programming by natural language for spread- sheet data analysis and manipulation. In Proceed- ings of the 2014 ACM SIGMOD international con- ference on Management of data, pages 803-814.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average code and title lengths together 
with vocabulary sizes for C# and SQL after post-
processing. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance on EVAL for the GEN task. 
Performance on DEV is indicated in parentheses. 

Model 
Naturalness Informativeness 

C# 

IR 
3.42 
2.25 
MOSES 
1.41 
2.42 
SUM-NN 
4.61* 
1.99 
CODE-NN 
4.48 
2.83 

SQL 
IR 
3.21 
2.58 
MOSES 
2.80 
2.54 
SUM-NN 
4.44 
2.75 
CODE-NN 
4.54 
3.12 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Naturalness and Informativeness mea-
sures of model outputs. Stat. sig. between CODE-
NN and others is computed with a 2-tailed Stu-
dent's t-test; p &lt; 0.05 except for *. 

</table></figure>

			<note place="foot" n="1"> http://stackoverflow.com 2 Data and code are available at https://github.com/ sriniiyer/codenn.</note>

			<note place="foot" n="3"> http://archive.org/details/stackexchange 4 The data was downloaded in Dec 2014.</note>

			<note place="foot" n="6"> We experimented with other sequence (Sutskever et al., 2014) and tree based architectures (Tai et al., 2015) as well. None of these models significantly improved performance, however, this is an important area for future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Mike Lewis, Chloé Kiddon, Kenton Lee, Eunsol Choi and the anonymous reviewers for comments on an earlier version. We also thank Bill Howe, Dan Halperin and Mark Yatskar for helpful discussions and Miltiadis Allamanis for providing the dataset for the comparison study. This research was supported in part by the NSF (IIS-1252835), an Allen Distinguished Investiga-tor Award, and a gift from Amazon.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Identify the number in given string IR Convert string number to integer MOSES How to xIndex numbers in C#? <ref type="bibr">SUM</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Albrecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">python-sqlparse</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Suggesting accurate method and class names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2015 10th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="38" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bimodal modelling of source code and natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2123" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple domain-independent probabilistic approach Method Output C# code var x = &quot; FundList</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amount</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supporting program comprehension with source code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Haiduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jairo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrian</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering</title>
		<meeting>the 32nd ACM/IEEE International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="223" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A global model for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining structured queries in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Koutrika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alkis</forename><surname>Simitsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th International Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
	<note>Data Engineering (ICDE)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using semantic unification to generate regular expressions from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="826" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From natural language specifications to program input parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nalir: An interactive natural language interface for querying relational databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hosagrahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jagadish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2014 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="709" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Orange: a method for evaluating automatic evaluation metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">501</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A probabilistic forest-to-string model for language generation from typed lambda calculus expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1611" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Natural language models for predicting programming comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Movshovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sorry, i don&apos;t speak sparql: Translating sparql queries into natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on World Wide Web</title>
		<meeting>the 22Nd International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="977" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The definitive ANTLR 4 reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Parr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Pragmatic Bookshelf</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Summarizing software artifacts: a case study of bug reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Rastkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd</title>
		<meeting>the 32nd</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">ACM/IEEE International Conference on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating natural language summaries for crosscutting source code concerns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Rastkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander Wj</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Maintenance (ICSM), 2011 27th IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dbmss should talk back too</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alkis</forename><surname>Simitsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR 2009, Fourth Biennial Conference on Innovative Data Systems Research, Online Proceedings</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards automatically generating summary comments for java methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giriprasad</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divya</forename><surname>Muppaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM international conference on Automated software engineering</title>
		<meeting>the IEEE/ACM international conference on Automated software engineering</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluating evaluation methods for generation in the presence of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Singhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="341" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1494" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul J Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generation by inverting a semantic parser that uses statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to execute. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
