<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Discriminative Topic Model using Document Network Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Yang</surname></persName>
							<email>wwyang@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">Boyd</forename><surname>Graber@</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Resnik</forename><surname>Linguistics</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umiacs</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland College Park</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit3">University of Maryland College Park</orgName>
								<address>
									<region>MD, CO, MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Discriminative Topic Model using Document Network Structure</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="686" to="696"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Document collections often have links between documents-citations, hyperlinks, or revisions-and which links are added is often based on topical similarity. To model these intuitions, we introduce a new topic model for documents situated within a network structure, integrating latent blocks of documents with a max-margin learning criterion for link prediction using topic-and word-level features. Experiments on a scientific paper dataset and collection of webpages show that, by more robustly exploiting the rich link structure within a document network, our model improves link prediction, topic quality, and block distributions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Documents often appear within a network struc- ture: social media mentions, retweets, and fol- lower relationships; Web pages by hyperlinks; sci- entific papers by citations. Network structure in- teracts with the topics in the text, in that docu- ments linked in a network are more likely to have similar topic distributions. For instance, a cita- tion link between two papers suggests that they are about a similar field, and a mentioning link between two social media users often indicates common interests. Conversely, documents' sim- ilar topic distributions can suggest links between them. For example, topic model ( <ref type="bibr">Blei et al., 2003, LDA)</ref> and block detection papers <ref type="bibr" target="#b11">(Holland et al., 1983</ref>) are relevant to our research, so we cite them. Similarly, if a social media user A finds another user B with shared interests, then A is more likely to follow B.</p><p>Our approach is part of a natural progression of network modeling in which models integrate more information in more sophisticated ways. Some past methods only consider the network it- self ( <ref type="bibr" target="#b14">Kim and Leskovec, 2012;</ref><ref type="bibr" target="#b19">Liben-Nowell and Kleinberg, 2007)</ref>, which loses the rich information in text. In other cases, methods take both links and text into account ( <ref type="bibr" target="#b7">Chaturvedi et al., 2012</ref>), but they are modeled separately, not jointly, limiting the model's ability to capture interactions between the two. The relational topic model <ref type="bibr">(Chang and Blei, 2010, RTM)</ref> goes further, jointly modeling topics and links, but it considers only pairwise document relationships, failing to capture network structure at the level of groups or blocks of documents.</p><p>We propose a new joint model that makes fuller use of the rich link structure within a document network. Specifically, our model embeds the weighted stochastic block model <ref type="bibr">(Aicher et al., 2014, WSBM)</ref> to identify blocks in which docu- ments are densely connected. WSBM basically cat- egorizes each item in a network probabilistically as belonging to one of L blocks, by reviewing its connections with each block. Our model can be viewed as a principled probabilistic extension of <ref type="bibr" target="#b34">Yang et al. (2015)</ref>, who identify blocks in a doc- ument network deterministically as strongly con- nected components (SCC). Like them, we assign a distinct Dirichlet prior to each block to capture its topical commonalities. Jointly, a linear regression model with a discriminative, max-margin objec- tive function ( <ref type="bibr" target="#b36">Zhu et al., 2012;</ref><ref type="bibr" target="#b37">Zhu et al., 2014</ref>) is trained to reconstruct the links, taking into account the features of documents' topic and word distri- butions ( <ref type="bibr" target="#b26">Nguyen et al., 2013)</ref>, block assignments, and inter-block link rates.</p><p>We validate our approach on a scientific pa- per abstract dataset and collection of webpages, with citation links and hyperlinks respectively, to predict links among previously unseen documents and from those new documents to training docu- ments. Embedding the WSBM in a network/topic model leads to substantial improvements in link prediction over previous models; it also improves block detection and topic interpretability. The key advantage in embedding WSBM is its flexibility and robustness in the face of noisy links. Our re- sults also lend additional support for using max- margin learning for a "downstream" supervised topic model <ref type="bibr" target="#b22">(McAuliffe and Blei, 2008)</ref>, and that predictions from lexical as well as topic features improves performance <ref type="bibr" target="#b26">(Nguyen et al., 2013)</ref>.</p><p>The rest of this paper is organized as follows. Section 2 introduces two previous link-modeling methods, WSBM and RTM. Section 3 presents our methods to incorporate block priors in topic mod- eling and include various features in link predic- tion, as well as the aggregated discriminative topic model whose posterior inference is introduced in Section 4. In Section 5 we show how our model can improve link prediction and (often) improve topic coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dealing with Links</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weighted Stochastic Block Model</head><p>WSBM <ref type="bibr" target="#b0">(Aicher et al., 2014</ref>) is a generalized stochastic block model <ref type="bibr" target="#b11">(Holland et al., 1983;</ref><ref type="bibr">Wang and Wong, 1987, SBM)</ref> and predicts non- negative integer-weight links, instead of binary- weight links. A block is a collection of doc- uments which are densely connected with each other but sparsely connected with documents in other blocks. WSBM assumes that a document be- longs to exactly one block. A link connecting two documents in blocks l and l has a weight gen- erated from a Poisson distribution with parame- ters Ω l,l which has a Gamma prior with param- eters a and b, as <ref type="figure" target="#fig_0">Figure 1</ref> shows.</p><p>The whole generative process is:</p><p>1. For each pair of blocks (l, l ) ∈ {1, . . . , L} 2 (a) Draw inter-block link rate</p><formula xml:id="formula_0">Ω l,l ∼ Gamma(a, b) 2. Draw block distribution µ ∼ Dir(γ) 3. For each document d ∈ {1, . . . , D} (a) Draw block assignment y d ∼ Mult(µ)</formula><p>Figure 2: SCC can be distracted by spurious links connecting two groups, while WSBM maintains the distinction.</p><formula xml:id="formula_1">  K  ' d N d N d  ' d  ' ,d d B d z d w ' d z ' d w   Figure 3: A Two-document Segment of RTM 4. For each link (d, d ) ∈ {1, . . . , D} 2 (a) Draw link weight A d,d ∼ Poisson(Ωy d ,y d )</formula><p>WSBM is a probabilistic block detection algo- rithm and more robust than some deterministic al- gorithms like SCC, which is vulnerable to noisy links. For instance, we would intuitively say Fig- ure 2 has two blocks-as denoted by coloring- whether or not the dashed link exists. If the dashed link does not exist, both WSBM and SCC can iden- tify two blocks. However, if the dashed link does exist, SCC will return only one big block that con- tains all nodes, while WSBM still keeps the nodes in two reasonable blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relational Topic Model</head><p>RTM <ref type="bibr" target="#b6">(Chang and Blei, 2010</ref>) is a downstream model that generates documents and links simul- taneously ( <ref type="figure">Figure 3</ref>). Its generative process is:</p><formula xml:id="formula_2">1. For each topic k ∈ {1, . . . , K} (a) Draw word distribution φ k ∼ Dir(β) (b) Draw topic regression parameter η k ∼ N (0, ν 2 ) 2. For each document d ∈ {1, . . . , D} (a) Draw topic distribution θ d ∼ Dir(α) (b) For each token t d,n in document d i. Draw topic assignment z d,n ∼ Mult(θ d ) ii. Draw word w d,n ∼ Mult(φz d,n ) 3. For each explicit link (d, d ) (a) Draw link weight B d,d ∼ Ψ(· | z d , z d , η)</formula><p>In the inference process, the updating of topic assignments is guided by links so that linked doc- uments are more likely to have similar topic distri- butions. Meanwhile, the linear regression (whose <ref type="figure">Figure 4</ref>: Graphical Model of BP-LDA output is fed into link probability function Ψ) is updated to maximize the network likelihood using current topic assignments.</p><formula xml:id="formula_3">  '  K  L  d N D  z w y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discriminative Topic Model with Block</head><p>Prior and Various Features</p><p>Our model is able to identify blocks from the net- work with an embedded WSBM, extract topic pat- terns of each block as prior knowledge, and use all this information to reconstruct the links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LDA with Block Priors (BP-LDA)</head><p>As argued in the introduction, linked documents are likely to have similar topic distributions, which can be generalized to documents in the same block. Inspired by this intuition and the block assignment we obtain in the previous section, we want to extract some prior knowledge from these blocks. Thus we propose an LDA with block priors, hence BP-LDA, as shown in <ref type="figure">Figure 4</ref>, which has the following generative process:</p><formula xml:id="formula_4">1. For each topic k ∈ {1, . . . , K} (a) Draw word distribution φ k ∼ Dir(β) 2. For each block l ∈ {1, . . . , L} (a) Draw topic distribution π l ∼ Dir(α ) 3. For each document d ∈ {1, . . . , D} (a) Draw topic distribution θ d ∼ Dir(απy d ) (b) For each token t d,n in document d i. Draw topic assignment z d,n ∼ Mult(θ d ) ii. Draw word w d,n ∼ Mult(φz d,n )</formula><p>Unlike conventional LDA, which uses an un- informative topic prior, BP-LDA puts a Dirich- let prior π on each block to capture the block's topic distribution and use it as an informative prior when drawing each document's topic distribution. In other words, a document's topic distribution- i.e., what the document is about-is not just in- formed by the words present in the document but the broader context of its network neighborhood.</p><formula xml:id="formula_5">  K  ' d N d N d  ' d   L L  d w ' d w d z ' d z  d y ' d y   ' ,d d B Topical Feature</formula><p>Lexical Feature Block Feature <ref type="figure">Figure 5</ref>: A two-document segment of VF-RTM. Various features are denoted by grayscale. B d,d is observed, but we keep it in white background to avoid confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RTM with Various Features (VF-RTM)</head><p>Building on <ref type="bibr" target="#b6">Chang and Blei (2010)</ref>, we want to generate the links between documents based on various features, hence VF-RTM. In addition to topic distributions, VF-RTM also includes docu- ments' word distributions <ref type="bibr" target="#b26">(Nguyen et al., 2013</ref>) and the link rate of two documents' assigned blocks, with the intent that these additional fea- tures improve link generation. VF-RTM involves the relationship between a pair of documents, so it is difficult to show the whole model; therefore <ref type="figure">Figure 5</ref> illustrates with a two-document segment. The generative process is:</p><formula xml:id="formula_6">1. For each pair of blocks (l, l ) ∈ {1, . . . , L} 2 (a) Draw block regression parameter ρ l,l ∼ N (0, ν 2 ) 2. For each topic k ∈ {1, . . . , K} (a) Draw word distribution φ k ∼ Dir(β) (b) Draw topic regression parameter η k ∼ N (0, ν 2 ) 3. For each word v ∈ {1, . . . , V } (a) Draw lexical regression parameter τv ∼ N (0, ν 2 ) 4. For each document d ∈ {1, . . . , D} (a) Draw topic distribution θ d ∼ Dir(α) (b) For each token t d,n in document d i. Draw topic assignment z d,n ∼ Mult(θ d ) ii. Draw word w d,n ∼ Mult(φz d,n ) 5. For each explicit link (d, d ) (a) Draw link weight B d,d ∼ Ψ(· | y d , y d , Ω, z d , z d , w d , w d , η, τ , ρ)</formula><p>Links are generated by a link probability func- tion Ψ which takes the regression value R d,d of documents d and d as an argument. Assuming documents d and d belong to blocks l and l re- spectively,</p><formula xml:id="formula_7">R d,d is where z d is a K-length vector with each el- ement z d,k = 1 N d n 1 (z d,n = k); w d is a V -length vector with each element w d,v = 1 N d n 1 (w d,n = v);</formula><p>• denotes the Hadamard (element-wise) product; 1 η, τ , and ρ are the weight vectors and matrix for topic-based, lexical- based and rate-based predictions, respectively.</p><p>A common choice of Ψ is a sigmoid <ref type="bibr" target="#b6">(Chang and Blei, 2010)</ref>. However, we instead use hinge loss so that VF-RTM can use the max-margin prin- ciple, making more effective use of side informa- tion when inferring topic assignments ( <ref type="bibr" target="#b36">Zhu et al., 2012)</ref>. Using hinge loss, the probability that doc- uments d and d are linked is</p><formula xml:id="formula_8">Pr (B d,d ) = exp (−2 max(0, ζ d,d )) ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_9">ζ d,d = 1−B d,d R d,d</formula><p>. Positive and negative link weights are denoted by 1 and -1, respectively, in contrast to sigmoid loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregated Model</head><p>Finally, we put all the pieces together and propose LBH-RTM: RTM with lexical weights (L), block priors (B), and hinge loss (H). Its graphical model is given in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">For each pair of blocks</head><formula xml:id="formula_10">(l, l ) ∈ {1, . . . , L} 2 (a) Draw inter-block link rate Ω l,l ∼ Gamma(a, b) (b) Draw block regression parameter ρ l,l ∼ N (0, ν 2 ) 2. Draw block distribution µ ∼ Dir(γ) 3. For each block l ∈ {1, . . . , L} (a) Draw topic distribution π l ∼ Dir(α ) 4. For each topic k ∈ {1, . . . , K} (a) Draw word distribution φ k ∼ Dir(β) (b) Draw topic regression parameter η k ∼ N (0, ν 2 ) 5. For each word v ∈ {1, . . . , V } (a) Draw lexical regression parameter τv ∼ N (0, ν 2 ) 6. For each document d ∈ {1, . . . , D} (a) Draw block assignment y d ∼ Mult(µ) (b) Draw topic distribution θ d ∼ Dir(απy d ) (c) For each token t d,n in document d i. Draw topic assignment z d,n ∼ Mult(θ d ) ii. Draw word w d,n ∼ Mult(φz d,n ) 7. For each link (d, d ) ∈ {1, . . . , D} 2 (a) Draw link weight A d,d ∼ Poisson(Ωy d ,y d ) 8. For each explicit link (d, d ) (a) Draw link weight B d,d ∼ Ψ(· | y d , y d , Ω, z d , z d , w d , w d , η, τ , ρ)</formula><p>A and B are assumed independent in the model, but they can be derived from the same set of links in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sampling Process</head><p>1: Set λ = 1 and initialize topic assignments 2: for m = 1 to M do 3:</p><p>Optimize η, τ , and ρ using L-BFGS 4:</p><formula xml:id="formula_11">for d = 1 to D do 5:</formula><p>Draw block assignment y d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for each token n do end for 13: end for Link set A is primarily used to find blocks, so it treats all links deterministically. In other words, the links observed in the input are considered ex- plicit positive links, while the unobserved links are considered explicit negative links, in contrast to the implicit links in B.</p><p>In terms of link set B, while it adopts all explicit positive links from the input, it does not deny the existence of unobserved links, or implicit negative links. Thus B consists of only explicit positive links. However, to avoid overfitting, we sample some implicit links and add them to B as explicit negative links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Posterior Inference</head><p>Posterior inference (Algorithm 1) consists of the sampling of topic and block assignments and the optimization of weight vectors and matrix. <ref type="bibr">2</ref> We add an auxiliary variable λ for hinge loss (see Sec- tion 4.2), so the updating of λ is not necessary when using sigmoid loss.</p><p>The sampling procedure is an iterative process after initialization (Line 1). In each iteration, we first optimize the weight vectors and matrix (Line 3) before updating documents' block assign- ments (Line 5) and topic assignments (Line 7). When using hinge loss, the auxiliary variable λ for every explicit link needs to be updated (Line 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sampling Block Assignments</head><p>Block assignment sampling is done by Gibbs sam- pling, using the block assignments and links in A <ref type="figure">Figure 6</ref>: The graphical model of LBH-RTM for two documents, in which a weighted stochastic block model is embedded <ref type="figure">(γ, µ, y, a, b</ref>, Ω, and A). Each document's topic distribution has an informative prior π. The model predicts links between documents (B) based on topics (z), words (w), and inter- block link rates (Ω), using a max-margin objective.</p><formula xml:id="formula_12">   '   K  L   L L ' d N d N d  ' d  a b d y ' d y ' ,d d A ' ,d d B d z d w ' d z ' d w    </formula><p>excluding document d and its related links. <ref type="bibr">3</ref> The probability that d is assigned to block l is</p><formula xml:id="formula_13">Pr(y d = l | A −d , y −d , a, b, γ) ∝ N −d l + γ × l S −d e (l, l ) + b S −d w (l,l )+a S −d e (l, l ) + b + Se(d, l ) S −d w (l,l )+a+Sw (d,l ) Sw (d,l )−1 i=0 S −d w (l, l ) + a + i ,<label>(3)</label></formula><p>where N l is the number of documents assigned to block l; −d denotes that the count excludes doc- ument d; S w (d, l) and S w (l, l ) are the sums of link weights from document d to block l and from block l to block l , respectively:</p><formula xml:id="formula_14">Sw(d, l) = d :y d =l A d,d<label>(4)</label></formula><formula xml:id="formula_15">Sw(l, l ) = d:y d =l Sw(d, l ).<label>(5)</label></formula><p>S e (d, l) is the number of possible links from doc- ument d to l (i.e., assuming document d connects to every document in block l), which equals N l . The number of possible links from block l to l is S e (l, l ) (i.e., assuming every document in block l connects to every document in block l ):</p><formula xml:id="formula_16">Se(l, l ) = N l × N l l = l 1 2 N l (N l − 1) l = l .<label>(6)</label></formula><p>If we rearrange the terms of Equation 3 and put the terms which have S w (d, l ) together, the value 3 These equations deal with undirected edges, but they can be adapted for directed edges. See supplementary material. of S w (d, l ) increases (i.e., document d is more densely connected with documents in block l ), the probability of assigning d to block l decreases ex- ponentially. Thus if d is more densely connected with block l and sparsely connected with other blocks, it is more likely to be assigned to block l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sampling Topic Assignments</head><p>Following <ref type="bibr" target="#b28">Polson and Scott (2011)</ref>, by introducing an auxiliary variable λ d,d , the conditional prob- ability of assigning t d,n , the n-th token in docu- ment d, to topic k is</p><formula xml:id="formula_17">Pr(z d,n = k | z −d,n , w −d,n , w d,n = v, y d = l) ∝ N −d,n d,k + απ −d,n l,k N −d,n k,v + β N −d,n k,· + V β d exp − (ζ d,d + λ d,d ) 2 2λ d,d ,<label>(7)</label></formula><p>where N d,k is the number of tokens in document d that are assigned to topic k; N k,v denotes the count of word v assigned to topic k; Marginal counts are denoted by ·; −d,n denotes that the count ex- cludes t d,n ; d denotes all documents that have explicit links with document d. The block topic prior</p><formula xml:id="formula_18">π −d,n l,k</formula><p>is estimated based on the maximal path assumption <ref type="bibr" target="#b8">(Cowans, 2006;</ref><ref type="bibr" target="#b31">Wallach, 2008)</ref>:</p><formula xml:id="formula_19">π −d,n l,k = d :y d =l N −d,n d ,k + α d :y d =l N −d,n d ,· + Kα .<label>(8)</label></formula><p>the link prediction argument ζ d,d is</p><formula xml:id="formula_20">ζ d,d = 1 − B d,d η k N d,· N d ,k N d ,· + R −d,n d,d .<label>(9)</label></formula><p>690 where</p><formula xml:id="formula_21">R −d,n d,d = K k=1 η k N −d,n d,k N d,· N d ,k N d ,· + V v=1 τv N d,v N d,· N d ,v N d ,· + ρy d ,y d Ωy d ,y d .<label>(10)</label></formula><p>Looking at the first term of Equation 7, the probability of assigning t d,n to topic k depends not only on its own topic distribution, but also the topic distribution of the block it belongs to. The links also matter: Equation 9 gives us the intuition that a topic which could increase the likelihood of links is more likely to be selected, which forms an interaction between topics and the link graph- the links are guiding the topic sampling while up- dating topic assignments is maximizing the likeli- hood of the link graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Optimization</head><p>While topic assignments are updated iteratively, the weight vectors and matrix η, τ , and ρ are optimized in each global iteration over the whole corpus using L-BFGS ( <ref type="bibr" target="#b20">Liu and Nocedal, 1989)</ref>. It takes the likelihood of generating B using η, τ , ρ, and current topic and block assignments as the ob- jective function, and optimizes it using the par- tial derivatives with respect to every weight vec- tor/matrix element.</p><p>The log likelihood of B using hinge loss is</p><formula xml:id="formula_22">L(B) ∝ − d,d R 2 d,d − 2(1 + λ d,d )B d,d R d,d 2λ d,d − K k=1 η 2 k 2ν 2 − V v=1 τ 2 v 2ν 2 − L l=1 L l =1 ρ 2 l,l 2ν 2 . (11)</formula><p>We also need to update the auxiliary vari-</p><formula xml:id="formula_23">able λ d,d . Since the likelihood of λ d,d fol- lows a generalized inverse Gaussian distribution GIG λ d,d ; 1 2 , 1, ζ 2 d,d</formula><p>, we sample its recipro-</p><formula xml:id="formula_24">cal λ −1 d,d</formula><p>from an inverse Gaussian distribution as</p><formula xml:id="formula_25">Pr λ −1 d,d | z, w, η, τ , ρ = IG λ −1 d,d ; 1 |ζ d,d | , 1 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We evaluate using the two datasets. The first one is CORA dataset ( <ref type="bibr" target="#b23">McCallum et al., 2000</ref>). After re- moving stopwords and words that appear in fewer than ten documents, as well as documents with no  We treat all links as undirected. Both datasets are split into 5 folds, each further split into a devel- opment and test set with approximately the same size when used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Link Prediction Results</head><p>In this section, we evaluate LBH-RTM and its varia- tions on link prediction tasks using predictive link rank (PLR). A document's PLR is the average rank of the documents to which it has explicit positive links, among all documents, so lower PLR is better.</p><p>Following the experiment setup in <ref type="bibr" target="#b6">Chang and Blei (2010)</ref>, we train the models on the train- ing set and predict citation links within held-out documents as well as from held-out documents to training documents. We tune two important parameters-α and negative edge ratio (the ratio of the number of sampled negative links to the number of explicit positive links)-on the devel- opment set and apply the trained model which per- forms the best on the development set to the test set. <ref type="bibr">4</ref> The cross validation results are given in Ta- ble 1, where models are differently equipped with lexical weights (L), WSBM prior (B), SCC prior (C), hinge loss (H), and sigmoid loss (S). 5 Link pre- diction generally improves with incremental appli- cation of prior knowledge and more sophisticated learning techniques.</p><p>The embedded WSBM brings around 6.5% and 10.2% improvement over RTM in PLR on the CORA and WEBKB datasets, respectively. This indicates that the blocks identified by WSBM are reasonable and consistent with reality. The lexi- cal weights also help link prediction (LBS-RTM), though less for BS-RTM. This is understandable since word distributions are much sparser and do not make as significant a contribution as topic dis- tributions. Finally, hinge loss improves PLR sub- stantially (LBH-RTM), about 14.1% and 21.1% im- provement over RTM on the CORA and WEBKB datasets respectively, demonstrating the effective- ness of max-margin learning.</p><p>The only difference between LCH-RTM and LBH-RTM is the block detection algorithm. How- ever, their link prediction performance is poles apart-LCH-RTM even fails to outperform RTM. This implies that the quality of blocks identified by SCC is not as good as WSBM, which we also illustrate in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Illustrative Example</head><p>We illustrate our model's behavior qualitatively by looking at two abstracts, <ref type="bibr" target="#b15">Koplon and Sontag (1997)</ref> and <ref type="bibr" target="#b1">Albertini and Sontag (1992)</ref> from the CORA dataset, designated K and A for short.</p><p>Paper K studies the application of Fourier-type activation functions in fully recurrent neural net- works. Paper A shows that if two neural networks have equal behaviors as "black boxes", they must have the same number of neurons and the same weights (except sign reversals).</p><p>From the titles and abstracts, we can easily find that both of them are about neural networks (NN). They both contain words like neural, neuron, net- work, recurrent, activation, and nonlinear, which corresponds to the topic with words neural, net- work, train, learn, function, recurrent, etc. There is a citation between K and A. The ranking of this link improves as the model gets more sophisti- cated <ref type="table" target="#tab_3">(Table 2)</ref>, except LCH-RTM, which is con- sistent with our PLR results.</p><p>In <ref type="figure" target="#fig_2">Figure 7</ref>, we also show the proportions of topics that dominate the two documents accord- ing to the various models. There are multiple top- ics dominating K and A according to RTM <ref type="figure" target="#fig_2">(Fig- ure 7(a)</ref>). As the model gets more sophisticated, the NN topic proportion gets higher. Finally, only the NN topic dominates the two documents when LBH-RTM is applied <ref type="figure" target="#fig_2">(Figure 7(e)</ref>).</p><p>LCH-RTM gives the highest proportion to the NN topic <ref type="figure" target="#fig_2">(Figure 7(b)</ref>). However, the NN topic   <ref type="table">Table 3</ref>: Average Association Scores of Topics splits into two topics and the proportions are not assigned to the same topic, which greatly brings down the link prediction performance. The split- ting of the NN topic also happens in other mod- els <ref type="figure" target="#fig_2">(Figure 7</ref>(a) and 7(d)), but they assign propor- tions to the same topic(s). Further comparing with LBH-RTM, the blocks detected by SCC are not im- proving the modeling of topics and links-some documents that should be in two different blocks are assigned to the same one, as we will show in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Topic Quality Results</head><p>We use an automatic coherence detection method ( <ref type="bibr" target="#b17">Lau et al., 2014</ref>) to evaluate topic quality. Specifically, for each topic, we pick out the top n words and compute the average association score of each pair of words, based on the held-out documents in development and test sets. We choose n = 25 and use Fisher's exact test <ref type="bibr">(Upton, 1992, FET)</ref> and log likelihood ra- tio <ref type="bibr">(Moore, 2004, LLR)</ref> as the association mea- sures <ref type="table">(Table 3</ref>). The main advantage of these mea- sures is that they are robust even when the refer- ence corpus is not large.</p><p>Coherence improves with WSBM and max- margin learning, but drops a little when adding lexical weights except the FET score on the WE- BKB dataset, because lexical weights are intended to improve link prediction performance, not topic quality. Topic quality of LBH-RTM is also better than that of LCH-RTM, suggesting that WSBM ben- efits topic quality more than SCC.   <ref type="table">Table 4</ref>: Statistics of Blocks 1 (learning theory) and 2 (Bayes nets), which are merged in SCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Block Analysis</head><p>In this section, we illustrate the effectiveness of the embedded WSBM over SCC. <ref type="bibr">6</ref> As we have argued, WSBM is able to separate two internally densely-connected blocks even if there are few links connecting them, while SCC tends to merge them in this case. As an example, we focus on two blocks in the CORA dataset identified by WSBM, designated Blocks 1 and 2. Some statis- tics are given in <ref type="table">Table 4</ref>. The two blocks are very sparsely connected, but comparatively quite densely connected inside either block. The two blocks' topic distributions also reveal their differ- ences: abstracts in Block 1 mainly focus on learn- ing theory (learn, algorithm, bound, result, etc.) and MCMC (markov, chain, distribution, converge, etc.). Abstracts in Block 2, however, have higher <ref type="bibr">6</ref> We omit the comparison of WSBM with other models, be- cause this has been done by <ref type="bibr" target="#b0">Aicher et al. (2014)</ref>. In addition, WSBM is a probabilistic method while SCC is deterministic. They are not comparable quantitatively, so we compare them qualitatively.</p><p>weights on Bayesian networks (network, model, learn, bayesian, etc.) and Bayesian estimation (es- timate, bayesian, parameter, analysis, etc.), which differs from Block 1's emphasis. Because of the two inter-block links, SCC merges the two blocks into one, which makes the block topic distribution unclear and misleads the sampler. WSBM, on the other hand, keeps the two blocks separate, which generates a high-quality prior for the sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Topic models are widely used in information re- trieval ( <ref type="bibr" target="#b33">Wei and Croft, 2006</ref>), word sense dis- ambiguation <ref type="bibr" target="#b5">(Boyd-Graber et al., 2007)</ref>, dialogue segmentation ( <ref type="bibr" target="#b29">Purver et al., 2006</ref>), and collabora- tive filtering <ref type="bibr" target="#b21">(Marlin, 2003)</ref>.</p><p>Topic models can be extended in either up- stream or downstream way. Upstream models generate topics conditioned on supervisory in- formation <ref type="bibr" target="#b9">(Daumé III, 2009;</ref><ref type="bibr" target="#b24">Mimno and McCallum, 2012;</ref><ref type="bibr" target="#b18">Li and Perona, 2005</ref>). Down- stream models, on the contrary, generates topics and supervisory data simultaneously, which turns unsupervised topic models to (semi-)supervised ones. Supervisory data, like labels of documents and links between documents, can be generated from either a maximum likelihood estimation ap- proach <ref type="bibr" target="#b22">(McAuliffe and Blei, 2008;</ref><ref type="bibr" target="#b6">Chang and Blei, 2010;</ref><ref type="bibr" target="#b4">Boyd-Graber and Resnik, 2010</ref>) or a maximum entropy discrimination approach ( <ref type="bibr" target="#b36">Zhu et al., 2012;</ref><ref type="bibr" target="#b34">Yang et al., 2015)</ref>.</p><p>In block detection literature, stochastic block model ( <ref type="bibr" target="#b11">Holland et al., 1983;</ref><ref type="bibr">Wang and Wong, 1987, SBM)</ref> is one of the most basic generative models dealing with binary-weighted edges. SBM assumes that each node belongs to only one block and each link exists with a probability that de- pends on the block assignments of its connect- ing nodes. It has been generalized for degree- correction ( <ref type="bibr" target="#b13">Karrer and Newman, 2011</ref>), bipartite structure ( <ref type="bibr" target="#b16">Larremore et al., 2014)</ref>, and categorial values <ref type="bibr" target="#b10">(Guimerà and Sales-Pardo, 2013)</ref>, as well as nonnegative integer-weight network <ref type="bibr">(Aicher et al., 2014, WSBM)</ref>.</p><p>Our model combines both topic model and block detection in a unified framework. It takes text, links, and the interaction between text and links into account simultaneously, contrast to the methods that only consider graph structure <ref type="bibr" target="#b14">(Kim and Leskovec, 2012;</ref><ref type="bibr" target="#b19">Liben-Nowell and Kleinberg, 2007)</ref> or separate text and links <ref type="bibr" target="#b7">(Chaturvedi et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We introduce LBH-RTM, a discriminative topic model that jointly models topics and document links, detecting blocks in the document net- work probabilistically by embedding the weighted stochastic block model, rather via connected- components as in previous models. A separate Dirichlet prior for each block captures its topic preferences, serving as an informed prior when inferring documents' topic distributions. Max- margin learning learns to predict links from docu- ments' topic and word distributions and block as- signments.</p><p>Our model better captures the connections and content of paper abstracts, as measured by predic- tive link rank and topic quality. LBH-RTM yields topics with better coherence, though not all tech- niques contribute to the improvement. We sup- port our quantitative results with qualitative anal- ysis looking at a pair of example documents and at a pair of blocks, highlighting the robustness of embedded WSBM over blocks defined as SCC.</p><p>As next steps, we plan to explore model varia- tions to support a wider range of use cases. For example, although we have presented a version of the model defined using undirected binary weight edges in the experiment, it would be straightfor- ward to adapt to model both directed/undirected and binary/nonnegative real weight edges. We are also interested in modeling changing topics and vocabularies <ref type="bibr" target="#b2">(Blei and Lafferty, 2006;</ref><ref type="bibr" target="#b35">Zhai and Boyd-Graber, 2013</ref>). In the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not ex- ist but should, for example in discovering missed citations, marking social dynamics ( <ref type="bibr" target="#b27">Nguyen et al., 2014)</ref>, and identifying topically related content in multilingual networks of documents ( <ref type="bibr" target="#b12">Hu et al., 2014</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Weighted Stochastic Block Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>for each explicit link (d, d ) do 10: Draw λ −1 d,d (and then λ d,d ) 11: end for 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Topic proportions given by various models on our two illustrative documents (K and A, described in described in Section 5.2). As the model gets more sophisticated, the NN topic proportion gets higher and finally dominates the two documents when LBH-RTM is applied. Though LCH-RTM gives the highest proportion to the NN topic, it splits the NN topic into two and does not assign the proportions to the same one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Predictive Link Rank Results 

words or links, our vocabulary has 1,240 unique 
words. The corpus has 2,362 computer science pa-
per abstracts with 4,231 citation links. 
The second dataset is WEBKB. It is already pre-
processed and has 1,703 unique words in vocabu-
lary. The corpus has 877 web pages with 1,608 
hyperlinks. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>PLR of the citation link between example 
documents K and A (described in Section 5.2) 

Model 

FET 
LLR 

CORA WEBKB CORA WEBKB 

RTM 

0.1330 
0.1312 
3.001 
6.055 

LCH-RTM 

0.1418 
0.1678 
3.071 
6.577 

BS-RTM 

0.1415 
0.1950 
3.033 
6.418 

LBS-RTM 

0.1342 
0.1963 
2.984 
6.212 

LBH-RTM 

0.1453 
0.2628 
3.105 
6.669 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). As the model gets more sophisticated, the NN topic proportion gets higher and finally dominates the two documents when LBH-RTM is applied. Though LCH-RTM gives the highest proportion to the NN topic, it splits the NN topic into two and does not assign the proportions to the same one.</figDesc><table>Block 
1 
2 
#Nodes 
42 
84 
#Links in the Block 
55 142 
#Links across Blocks 
2 

</table></figure>

			<note place="foot">R d,d = η T (z d • z d ) + τ T (w d • w d ) + ρ l,l Ω l,l , (1)</note>

			<note place="foot" n="1"> As Chang and Blei (2010) point out, the Hadamard product is able to capture similarity between hidden topic representations of two documents.</note>

			<note place="foot" n="2"> More details about sampling procedures and equations in this section (including the sampling and optimization equations using sigmoid loss) are available in the supplementary material.</note>

			<note place="foot" n="4"> We also tune the number of blocks for embedded WSBM and set it to 35 (CORA) and 20 (WEBKB). The block topic priors are not applied on unseen documents, since we don&apos;t have available links. 5 The values of RTM are different from the result reported by Chang and Blei (2010), because we re-preprocessed the CORA dataset and used different parameters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research has been supported in part, under subcontract to Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113. Boyd-Graber is also supported by NSF grants IIS/1320538, IIS/1409287, and NCSE/1422492. Any opinions, findings, conclusions, or recommendations ex-pressed here are those of the authors and do not necessarily reflect the view of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning latent block structure in weighted networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Aicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><forename type="middle">Z</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">For neural networks, function determines form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Albertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Decision and Control</title>
		<meeting>IEEE Conference on Decision and Control</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Graber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A topic model for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical relational models for document networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A topical graph kernel for link prediction in labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Probabilistic Document Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Cowans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Markov random topic fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A network inference method for large-scale unsupervised identification of novel drug-drug interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Guimerà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Sales-Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">Blackmond</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Polylingual tree-based topic models for translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels and community structure in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent multi-group membership graph model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunghwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using Fourier-neural recurrent networks to fit sequential input/output data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renée</forename><surname>Koplon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficiently inferring community structure in bipartite networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">B</forename><surname>Larremore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><forename type="middle">Z</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Mathematical Programming</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling user rating profiles for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automating the construction of Internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topic models conditioned on arbitrary features with Dirichlet-multinomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On log-likelihood-ratios and the significance of rare events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lexical and hierarchical topic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling topic control to detect influence in conversations using nonparametric topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxin</forename><surname>Midberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data augmentation for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised topic modelling for multi-party spoken discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><forename type="middle">P</forename><surname>Körding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fisher&apos;s exact test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Upton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Structured Topic Models for Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuchung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Birds of a feather linked together: A discriminative topic model using link-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online latent Dirichlet allocation with infinite vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MedLDA: Maximum margin supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gibbs max-margin topic models with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
