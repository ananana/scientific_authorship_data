<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Rank-Based Similarity Metric for Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
							<email>esantus@mit.edu hongmin wang@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<country>MIT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California Santa Barbara</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuele</forename><surname>Chersoni</surname></persName>
							<email>emmanuelechersoni@gmail.com yue zhang@sutd.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Aix-Marseille University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">ISTD</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Rank-Based Similarity Metric for Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="volume">552</biblScope>
							<biblScope unit="page" from="552" to="557"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word Embeddings (WE) have recently imposed themselves as a standard for representing word meaning in NLP. Semantic similarity between word pairs has become the most common evaluation benchmark for these representations, with vector cosine being typically used as the only similarity metric. In this paper, we report experiments with a rank-based metric for WE, which performs comparably to vector cosine in similarity estimation and out-performs it in the recently-introduced and challenging task of outlier detection, thus suggesting that rank-based measures can improve clustering quality. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"All happy families resemble one another, but each unhappy family is unhappy in its own way." Anna Karenina, Leo Tolstoy Distributional Semantic Models (DSMs) have re- ceived an increasing attention in the NLP com- munity, as they constitute an efficient data-driven method for creating word representations and measuring their semantic similarity by computing their distance in the vector space <ref type="bibr" target="#b20">(Turney and Pantel, 2010)</ref>.</p><p>The most popular similarity metric in DSMs is the vector cosine. Compared to Euclidean dis- tances, vector cosine scores are normalized on each dimension and hence are robust to the scaling effect. On the other hand, one limitation of this metric is that it regards each dimension equally, without taking into account the fact that some di- mensions might be more relevant for characteriz-ing the semantic content of a word. Such a lim- itation led to the introduction of alternative met- rics based on feature ranking, which have been re- ported to outperform vector cosine in several sim- ilarity tasks ( <ref type="bibr">Santus et al., 2016a,b)</ref>.</p><p>Recently, the focus of the research on word rep- resentations has been shifting onto the so-called word embeddings (WE), which are dense vec- tors obtained by means of neural network train- ing that achieved significant improvements in sev- eral similarity-related tasks ( <ref type="bibr" target="#b13">Mikolov et al., 2013a;</ref>. Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their na- ture does not sensibly differ ( <ref type="bibr" target="#b12">Levy et al., 2015</ref>). Most research works involving WE still adopt vec- tor cosine for similarity estimation, yet little ex- perimentation has been done on alternative met- rics for comparing dense representations (excep- tions include <ref type="bibr" target="#b6">Camacho-Collados et al. (2015)</ref>).</p><p>Some attempts to directly transfer rank-based measures from traditional DSMs to WE have faced difficulties (see, for example, <ref type="bibr" target="#b11">Jebbara et al. (2018)</ref>). In this paper, we suggest a possible solution to this problem by adapting AP Syn, a rank-based similarity metric originally proposed for sparse vectors <ref type="bibr">(Santus et al., 2016b,a)</ref>, to low-dimensional word embeddings. This goal is achieved by removing the parameter N (the extent of the feature overlap to be taken into account) and adding a smoothing parameter that is proven to be constant under multiple settings, therefore making the measure unsupervised. Our experiments show performance improve- ments both in similarity estimation and in the more challenging outlier detection task <ref type="bibr">(CamachoCollados and Navigli, 2016)</ref>, which consists in cluster and outlier identification. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Similarity, Relatedness and Dissimilarity: Current Issues in the Evaluation of DSMs</head><p>A classical benchmark for DSMs is represented by the estimation of word similarity: evaluation datasets are built by asking human subjects to rate the degree of semantic similarity of word pairs, and the performance is assessed in terms of the correlation between the average scores assigned to the pairs by the subjects and the cosines of the cor- responding vectors (similary estimation task). Similarity as modeled by DSMs has been under debate, as its definition is underspecified. It in fact includes an ambiguity with the more generic no- tion of semantic relatedness, which is present also in many popular datasets (i.e. the concepts of cof- fee and cup are certainly related, but there is very little similarity about them), as opposed to 'gen- uine' semantic similarity (i.e. the relation holding between concepts such as coffee and tea) <ref type="bibr">(Agirre et al., 2009;</ref><ref type="bibr" target="#b10">Hill et al., 2015;</ref><ref type="bibr" target="#b8">Gladkova and Drozd, 2016)</ref>. Therefore, when testing a DSM, it is im- portant to pay attention to what type of seman- tic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agree- ment is relatively low in all datasets and the per- formance of several automated systems is already above the upper bound ( <ref type="bibr" target="#b2">Batchkarov et al., 2016)</ref>. As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs.</p><p>A recent proposal is the challenging outlier detection task <ref type="bibr" target="#b5">(Camacho-Collados and Navigli, 2016;</ref><ref type="bibr" target="#b3">Blair et al., 2016)</ref>, which consists in the recognition of cluster membership, as well as of a relative degree of semantic dissimilarity. The task is described as follows: given a group of words, identify the outlier, namely the word that does not belong to the group (i.e. the one that is less simi- lar to the others). On top of its potential applica- tions (e.g. ontology learning), detecting outliers in clusters is a goal that poses a more strict quality requirement on the distributional representations compared to tests based simply on pairwise com- parisons, as it is required that similar words group into semantically meaningful clusters. Clearly, the task involves the identification of discriminative semantic dimensions, which could set the clus- ter members apart from non-members. Outliers are not necessarily unrelated to the other words: rather, they have a lower degree of similarity with respect to some prominent property of the cluster (e.g. the case of Los Angeles Lakers as an out- lier in a cluster of football teams). In our view, a similarity metric has to exploit such discriminative dimensions to form cohesive clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Rank-Based Metric for Embeddings</head><p>We use cosine as a baseline and we test an adapta- tion of a rank-based measure to the dense features of the word embeddings.</p><p>Vector cosine computes the correlation between all the vector dimensions, independently of their relevance for a given word pair or for a seman- tic cluster, and this could be a limitation for dis- cerning different degrees of dissimilarity. The al- ternative rank-based measure is based on the hy- pothesis that similarity consists of sharing many relevant features, whereas dissimilarity can be de- scribed as either the non-sharing of relevant fea- tures or the sharing of non-relevant features <ref type="bibr" target="#b19">(Santus et al., 2014</ref><ref type="bibr" target="#b18">(Santus et al., , 2016b</ref>.</p><p>This hypothesis could turn out to be very help- ful for a task like the outlier detection, where prominent features might be the key to improve clustering quality: semantic dimensions that are shared by many of the cluster elements should be weighted more, as they are likely to be useful for setting the outliers apart. In fact, a cohesive clus- ter should be mostly characterized by the same 'salient' dimensions, and thus, basing word com- parisons on such dimensions should lead to more reliable estimates of cluster membership.</p><p>In our contribution, we propose to adapt AP Syn, a metric originally proposed by <ref type="bibr">Santus et al. (2016a,b)</ref>, to dense word embeddings representations. 3 AP Syn was shown to perform well on both synonymy detection and similarity estimation tasks, and it was recently adapted to achieve state-of-the-art results in thematic fit esti- mation ( <ref type="bibr" target="#b16">Santus et al., 2017</ref>). The original AP Syn formula is shown in equation 1.</p><formula xml:id="formula_0">AP Syn(wx, wy) = i=N i=0 1 AV G(rs x (fi), rs sy (fi))<label>(1)</label></formula><p>For every feature f i in the intersection between the top N features of two vectors w x and w y , we add the inverse of the average rank of such fea- ture, r sx (f x ) and r sy (f y ), in the two decreasingly value-sorted vectors s x and s y (in traditional vec- tors, often the parameter N ≥ 1000, but in WE N = |f |). AP Syn scores low if the features of the two vectors are inversely ranked and high if they are similarly ranked. AP Syn maps the average feature ranks to a non-linear function, emphasizing the contribution of top-ranked features. Its direct application to dense embeddings would shrink too much the con- tribution of lower ranks (see <ref type="figure" target="#fig_0">Figure 1)</ref>, with the score mostly affected by the top ∼ 25 features. While this is reasonable for the traditional vectors derived from co-occurrence counts, where thou- sands of smaller contributions can still affect the final score, dense vectors need a smoother curve. While preserving the idea of the non-linear weight allocation across the average feature ranks during the summation, we modify the original AP Syn formula by taking the exponential of the feature ranks to a power of a constant value ranging be- tween 0 and 1 (excluded), as shown in equation 2, such that now the number of ranks contributing to the final score is widen to all features (see the smoother curve of AP SynP ower in <ref type="figure" target="#fig_0">Figure 1</ref>). We name this variant AP SynP ower or, shortly, AP SynP .</p><formula xml:id="formula_1">AP SynP (wx, wy) = i=|f | i=0 1 AV G(rs x (fi) p , rs y (fi) p )<label>(2)</label></formula><p>The power p added to AP SynP formula is a train- able parameter. We trained it on the similarity subset of WordSim dataset, obtaining the optimal value of p = 0.1, which has been successfully used in all evaluations, under all settings (i.e. em- bedding types and training corpora). Such reg- ularity allows us to consider p = 0.1 as a con- stant, therefore dropping p. Since in WE we can drop also the parameter N by defining N = |f |, AP SynP can be not parametrized at all. context size of 10 and negative sampling). <ref type="bibr">4</ref> For comparison with Camacho-Collados and Navigli (2016) on outlier detection, we used the same training corpora: the UMBC ( <ref type="bibr" target="#b9">Han et al., 2013</ref>) and the English Wikipedia. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>As for the similarity estimation task, we eval- uate the Spearman correlation between system- generated scores and human judgments. We used three popular benchmark datasets: WordSim- 353 ( <ref type="bibr" target="#b7">Finkelstein et al., 2001</ref>), MEN ( <ref type="bibr" target="#b4">Bruni et al., 2014</ref>) and SimLex-999 ( <ref type="bibr" target="#b10">Hill et al., 2015)</ref>. It is im- portant to point out that SimLex-999 is the only one specifically built for targeting genuine seman- tic similarity, while the others tend to mix similar- ity and relatedness scores.</p><p>As for outlier detection, we evaluate our DSMs on the 8-8-8 dataset <ref type="bibr" target="#b5">(Camacho-Collados and Navigli, 2016</ref>). The dataset consists of eight clusters, each one with a different topic and consisting in turn of eight lexical items belonging to the clus- ter and eight outliers (with four degrees of relat- edness to the cluster members: C1, C2, C3, C4). In total, the dataset includes 64 sets of 8 words + 1 outlier for the evaluation. For each word w of a cluster W of n words, the authors defined a compactness score c(w) corresponding to the av- erage of all pairwise similarities of the words in W \ {w}. On the basis of the compactness score, they proposed two evaluation metrics: Outlier Po- sition (OP) and Outlier Detection (OD). Given a set W of n + 1 words, OP is the rank of the out- lier w n + 1 according to the compactness score. Ideally, the rank of the outlier should be n, mean-    <ref type="bibr">(2016)</ref>'s pairwise method, while PT-Cos refers to the prototype-based one. In bold, best scores per method; in bold and underlined, best scores per corpus-embedding combination.</p><p>ing that it has the lowest average similarity with the other cluster elements. The second metric, Outlier Detection (OD), is indeed defined as 1 iff OP (w n + 1) = n, 0 otherwise. Finally, the per- formance on a dataset composed of |D| sets of words was estimated in terms of Outlier Position Percentage (OP P , Eq. 3) and Accuracy (Eq. 4):</p><formula xml:id="formula_2">OP P = W ∈D OP (W ) |W |−1 D × 100 (3) Accuracy = W ∈D OD(W ) D × 100<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pairwise and Prototype Approaches to Outlier Detection</head><p>While for the similarity task scores are always cal- culated pairwise, for spotting the outlier two dif- ferent methods were tested: the pairwise compar- isons and the cluster prototype.</p><p>In the first case, we reimplemented the method of Camacho-Collados and Navigli (2016): (i) compute the average similarity score of each word with the other words in the cluster; (ii) pick as the outlier the word with the lowest average score. An alternative consists in creating a cluster prototype: (i) for a cluster of N words, we create N prototype vectors by excluding each time one of the words and averaging the vectors of the other ones; (ii) we pick as the outlier the word with the lowest similarity score with the prototype built out of the vectors of the other words in the cluster. <ref type="table" target="#tab_1">Table 1</ref> summarizes the correlations for the sim- ilarity task. AP SynP outperforms both vector cosine and AP Syn in all the datasets described in 4.2 when GloVe embeddings are used. The advantage is statistically significant over the co- sine on the MEN dataset (p &lt; 0.05) and over AP Syn on all datasets (p &lt; 0.01). 6 With Skip- Gram embeddings, AP SynP performs compara- bly to vector cosine for relatedness, dominant in WordSim and MEN, while retaining a significant advantage over AP Syn on the same datasets (p &lt; 0.05). It also performs slightly better than co- sine in SimLex-999, and this complies with previ- ous findings of <ref type="bibr" target="#b17">Santus et al. (2016a)</ref>, who showed that AP Syn performs better on genuine similar- ity datasets. Apparently, the top-ranked vector dimensions (those contributing more to APSyn scores) are more often shared by similar words, than by simply related ones. <ref type="table" target="#tab_2">Table 2</ref> shows the results for the outlier detec- tion task. The line CC-Cos contains the scores by Camacho-Collados and Navigli (2016) as a baseline. The models are divided into pair- wise comparison and cluster prototype (see Sec- tion 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>As it can be easily noticed by looking at the bold line, AP SynP outperforms the baselines in all settings for both Skip-Gram and GloVe, obtaining higher accuracies and OPPs. Not only AP SynP is better at identifying the outlier, but when it is not able to do so, its error is minimum (e.g. the outlier is eventually the second ranked candidate). The best accuracy (73.4 vs. SOA of 70.3) and the best OPP (94.9 vs. SOA of 93.8) are both obtained by AP SynP with the prototype approach, using the Skip-Gram trained on Wikipedia. We also tested the significance of the accuracy improve- ments with the χ 2 test but, also given the small size of the 8-8-8 dataset, the result was negative. Finally, we observe that the two approaches de- scribed in 4.3 do not lead to sensitively different results. The major factors of difference can be found instead in the embeddings (with Skip-Gram outperforming Glove) and in the training corpus (the smaller Wikipedia, 1.7B words, outperforms the bigger UMBC, 3B words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Error Analysis</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we report the 5 outliers that were most difficult to detect by AP SynP . Most of them are related to the German Car Manufactur- ers topic, which was ambiguous and populated by rare terms. All outliers in the Months and in the South American countries clusters (except for the two South-American cities Rio de Janeiro and Bo- gotá) are successfully identified under all experi- mental settings. Finally, the reader can notice that most errors belong to C1 and C2, which are the most challenging classes in the dataset, as these outliers are either very related or very similar to other cluster members.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced AP SynP , an adaptation of the rank-based similarity measure AP Syn (San- tus et al., 2016a,b) for word embeddings. This adaptation introduces a power parameter p, which is shown to be constant in multiple tasks (i.e. p = 0.1). The stability of this parameter, to- gether with the possibility of dropping the pa- rameter N of AP Syn when using WE by setting N = |f |, makes the measure unsupervised. We have tested it on the tasks of similarity estima- tion and outlier detection, obtaining similar or bet- ter performances than vector cosine and the orig- inal AP Syn. AP SynP performs more consis- tently on SimLex-999, showing a preference for genuine similarity, as already noticed by <ref type="bibr" target="#b17">Santus et al. (2016a)</ref>. We also introduced a new approach to the outlier detection task, based on a cluster prototype. The prototype method is competitive and computationally less expensive than pairwise comparisons. We leave to future work a systematic compari- son of AP SynP and other rank-based measures. Pilot tests have shown that other rank-based met- rics (e.g. Spearman's Rho) also outperform vector cosine in multiple settings and tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of weight per feature rank in AP Syn and AP SynP (p = 0.1) across feature ranks ranging from 1 to 300.</figDesc><graphic url="image-1.png" coords="3,331.37,62.81,170.08,105.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Similarity Estimation, Spearman Correlation by Setting. Embeddings trained on Wikipedia.</head><label>1</label><figDesc></figDesc><table>Skip-Gram 
GloVe 
UMBC 
Wiki 
UMBC 
Wiki 
OPP Acc. OPP Acc. OPP Acc. OPP Acc. 
CC − Cos 92.6 64.1 93.8 70.3 81.6 40.6 91.8 56.3 
Pairwise 
AP Syn 
93.0 67.2 94.0 68.8 78.7 40.6 89.3 53.1 
AP SynP 
94.0 68.8 94.5 73.4 81.8 42.2 92.8 61.0 
Prototype 
P T − Cos 93.4 65.6 93.8 68.8 80.3 40.6 90.6 54.7 
AP Syn 
92.6 70.3 91.0 62.5 81.6 40.6 88.7 54.7 
AP SynP 
94.0 70.3 94.9 73.4 82.2 43.8 92.0 60.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Outlier Detection, Performance by Setting. CC-Cos refers to Camacho-Collados and Navigli</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Outlier Detection: Top 10 common errors 
across settings and their difficulty class (i.e. C1, 
C2, C3 and C4). (GCM: German Car Manufac-
turers; AJC: Apostles of Jesus Christ; SSP: Solar 
System Planets; BC: Big Cats; ITC: IT Compa-
nies). 

</table></figure>

			<note place="foot" n="1"> Enrico Santus and Hongmin Wang equally contributed to this work, which was started while they were both affiliated to the Singapore University of Technology and Design.</note>

			<note place="foot" n="2"> Code and vectors used for the experiments are available at https://github.com/esantus/Outlier Detection.</note>

			<note place="foot" n="3"> The number of dimensions in word embeddings is in the scale of hundreds, and thus the dimensionality is way lower than in the original DSMs used by Santus and colleagues.</note>

			<note place="foot" n="4"> We also performed experiments with CBOW embeddings (Mikolov et al., 2013b), but results were irregular and inconsistent. We leave therefore their analysis to future work. 5 Dump of Nov. 2014, approx. 1.7 billion words.</note>

			<note place="foot" n="6"> p-values computed with Fisher&apos;s r-to-z transformation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>The authors thank the reviewers for the construc-tive reviews.</p><p>Enrico Santus' research is supported by the Singapore University of Technology and Design (SUTD) and by the Massachusetts Institute of Technology (MIT). Emmanuele Chersoni's research is supported by an A*MIDEX grant (nANR-11-IDEX-0001-02), funded by the French Government "Investisse-ments d'Avenir" program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<title level="m">Marius Pas¸caPas¸ca, and Aitor Soroa. 2009. A study on Similarity and Relatedness Using Distributional and Wordnet-based Approaches</title>
		<imprint/>
	</monogr>
	<note>Proceedings of NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t Count, Predict! A Systematic Comparison of Context-counting vs. Context-predicting Semantic Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Critique of Word Similarity as a Method for Evaluating Distributional Semantic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Batchkarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>ACL Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Merhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Barry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01547</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimodal Distributional Semantics. Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Find the Word that Does not Belong: A Framework for an Intrinsic Evaluation of Word Vector Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Collados</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>ACL Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NASARI: a Novel Approach to a Semantically-Aware Representation of Items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Placing Search in Context: The Concept Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intrinsic Evaluations of Word Embeddings: What Can We Do Better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>ACL Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lushan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Abhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of * SEM</title>
		<meeting>* SEM</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Extracting Common Sense Knowledge via Triple Ranking Using Supervised and Unsupervised Distributional Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soufian</forename><surname>Jebbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Semantic Web</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving Distributional Similarity with Lessons Learned from Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring Thematic Fit with Distributional Feature Overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuele</forename><surname>Chersoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Blache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Testing Apsyn against Vector Cosine on Similarity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuele</forename><surname>Chersoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Blache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACLIC</title>
		<meeting>PACLIC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin-Shing</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taking Antonymy Mask Off in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACLIC</title>
		<meeting>PACLIC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: Vector Space Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
