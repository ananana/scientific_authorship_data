<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hidden Softmax Sequence Model for Dialogue Structure Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xien</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tsinghua-iFlytek Joint Laboratory for Speech Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Lv</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tsinghua-iFlytek Joint Laboratory for Speech Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hidden Softmax Sequence Model for Dialogue Structure Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2063" to="2072"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new unsupervised learning model, hidden softmax sequence model (HSSM), based on Boltzmann machine for dialogue structure analysis. The model employs three types of units in the hidden layer to discovery dialogue latent structures: softmax units which represent latent states of utterances; binary units which represent latent topics specified by dialogues ; and a binary unit that represents the global general topic shared across the whole dialogue corpus. In addition, the model contains extra connections between adjacent hidden softmax units to formulate the dependency between latent states. Two different kinds of real world dialogue corpora, Twitter-Post and AirTicketBook-ing, are utilized for extensive comparing experiments, and the results illustrate that the proposed model outperforms sate-of-the-art popular approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue structure analysis is an important and fundamental task in the natural language process- ing domain. The technology provides essential clues for solving real-world problems, such as pro- ducing dialogue summaries ( <ref type="bibr" target="#b13">Murray et al., 2006</ref>; <ref type="bibr" target="#b12">Liu et al., 2010)</ref>, controlling conversational agents <ref type="bibr" target="#b21">(Wilks, 2006</ref>), and designing interactive dialogue systems <ref type="bibr" target="#b22">(Young, 2006;</ref><ref type="bibr" target="#b0">Allen et al., 2007)</ref> etc. The study of modeling dialogues always assumes that for each dialogue there exists an unique latent structure (namely dialogue structure), which con- sists of a series of latent states. <ref type="bibr">1</ref> Some past works mainly rely on supervised or semi-supervised learning, which always involve extensive human efforts to manually construct la- tent state inventory and to label training samples. <ref type="bibr" target="#b4">Cohen et al. (2004)</ref> developed an inventory of la- tent states specific to E-mail in an office domain by inspecting a large corpus of e-mail. <ref type="bibr" target="#b9">Jeong et al. (2009)</ref> employed semi-supervised learning to transfer latent states from labeled speech corpora to the Internet media and e-mail. Involving exten- sive human efforts constrains scaling the training sample size (which is essential to supervised learn- ing) and application domains.</p><p>In recent years, there has been some work on modeling dialogues with unsupervised learn- ing methods which operate only on unlabeled ob- served data. <ref type="bibr" target="#b5">Crook et al. (2009)</ref> employed Dirich- let process mixture clustering models to recog- nize latent states for each utterance in dialogues from a travel-planning domain, but they do not inspect dialogues' sequential structure. <ref type="bibr" target="#b3">Chotimongkol (2008)</ref> proposed a hidden Markov model (HMM) based dialogue analysis model to study structures of task-oriented conversations from in- domain dialogue corpus. More recently, <ref type="bibr" target="#b16">Ritter et al. (2010)</ref> extended the HMM based conversa- tion model by introducing additional word sources for topic learning process. <ref type="bibr" target="#b23">Zhai et al. (2014)</ref> assumed words in an utterance are emitted from topic models under HMM framework, and topics were shared across all latent states. All these dia- logue structure analysis models are directed gener- ative models, in which the HMMs, language mod- els and topic models are combined together.</p><p>In this study, we attempt to develop a Boltz- mann machine based undirected generative model for dialogue structure analysis. As for the document modeling using undirected gener- ative model, <ref type="bibr" target="#b6">Hinton and Salakhutdinov (2009)</ref> proposed a general framework, replicated soft-max model (RSM), for topic modeling based on restricted Boltzmann machine (RBM). The model focuses on the document-level topic anal- ysis, it cannot be applied for the structure analy- sis. We propose a hidden softmax sequence model (HSSM) for the dialogue modeling and structure analysis. HSSM is a two-layer special Boltzmann machine. The visible layer contains softmax units used to model words in a dialogue, which are the same with the visible layer in RSM <ref type="bibr" target="#b6">(Hinton and Salakhutdinov, 2009)</ref>. However, the hidden layer has completely different design. There are three kinds of hidden units: softmax hidden units, which is utilized for representing latent states of dia- logues; binary units used for representing dialogue specific topics; and a special binary unit used for representing the general topic of the dialogue cor- pus. Moreover, unlike RSM whose hidden binary units are conditionally independent when visible units are given, HSSM has extra connections uti- lized to formulate the dependency between adja- cent softmax units in the hidden layer. The con- nections are the latent states of two adjacent utter- ances. Therefore, HSSM can be considered as a special Boltzmann machine.</p><p>The remainder of this paper is organized as fol- lows. Section 2 introduces two real world dia- logue corpora utilized in our experiments. Section 3 describes the proposed hidden softmax sequence model. Experimental results and discussions are presented in Section 4. Finally, Section 5 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Set</head><p>Two different datasets are utilized to test the ef- fectiveness of our proposed model: a corpus of post conversations drawn from Twitter (Twitter- Post), and a corpus of task-oriented human-human dialogues in the airline ticket booking domain (AirTicketBooking).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Twitter-Post</head><p>Conversations in Twitter are carried out by re- plying or responding to specific posts with short 140-character messages. The post length restric- tion makes Twitter keep more chat-like interac- tions than blog posts. The style of writing used on Twitter is widely varied, highly ungrammatical, and often with spelling errors. For example, the terms "be4", "b4", and "bef4" are always appeared in the Twitter posts to represent the word "before".</p><p>Here, we totally collected about 900, 000 raw Twitter dialogue sessions. The majority of conver- sation sessions are very short; and the frequencies of conversation session lengths follow a power law relationship as described in ( <ref type="bibr" target="#b16">Ritter et al., 2010)</ref>. For simplicity , in the data preprocessing stage non-English sentences were dropped; and non- English characters, punctuation marks, and some non-meaning tokens (such as "&amp;") were also fil- tered from dialogues. We filtered short Twitter di- alogue sessions and randomly sampled 5,000 di- alogues (the numbers of utterances in dialogues rang from 5 to 25) to build the Twitter-Post dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AirTicketBooking</head><p>The AirTicketBooking corpus consists of a set of task-oriented human-human mandarin dialogues from an airline ticket booking service center. The manual transcripts of the speech dialogues are uti- lized in our experiments. In the dataset, there is always a relative clear structure underlying each dialogue. A dialogue often begins with a cus- tomer's request about airline ticket issues. And the service agent always firstly checks the client's personal information, such as name, phone num- ber and credit card numberm, etc. Then the agent starts to deal with the client's request. We totally collected 1,890 text-based dialogue sessions ob- taining about 40,000 conversation utterances with length ranging from 15 to 100. We design an undirected generative model based on Boltzmann machine. As we known, di- alogue structure analysis models are always based on an underlying assumption: each utterance in the dialogues is generated from one latent state, which has a causal effect on the words. For in- stance, an utterance in AirTicketBooking dataset, "Tomorrow afternoon, about 3 o'clock" corre-sponds to the latent state "Time Information". However, by carefully examining words in dia- logues we can observe that not all words are gener- ated from the latent states ( <ref type="bibr" target="#b16">Ritter et al., 2010;</ref><ref type="bibr" target="#b23">Zhai and Williams, 2014</ref>). There are some words rele- vant to a global or background topic shared across dialogues. For example, "about" and "that" be- long to a global (general English) topic. Some other words in a dialogue may be strongly re- lated to the dialogue specific topic. For exam- ple, "cake", "toast" and "pizza" may appear in a Twitter dialogue with respect to a specific topic, "food". From the perspective of generative model, we can also consider that words in a dialogue are generated by the mixture model of latent states, a global/background topic, and a dialogue specific topic. Therefore, there are three kinds of units in the hidden layer of our proposed model, which are displayed in <ref type="figure" target="#fig_0">Figure 1</ref>. h φ is a softmax unit, which indicates the latent state for a utterance. h ψ and h ξ represent the general topic, and the dia- logue specific topic, respectively. For the visible layer, we utilize the softmax units to model words in each utterance, which is the same with the ap- proach in RSM ( <ref type="bibr" target="#b6">Hinton and Salakhutdinov, 2009)</ref>. In Section 3.2, We propose a basic model based on Boltzmann machine to formulate each word in ut- terances of dialogues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dialogue Structure Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Design</head><p>A dialogue can be abstractly viewed as a se- quence of latent states in a certain reasonable or- der. Therefore, formulating the dependency be- tween latent states is another import issue for dia- logue structure analysis. In our model, we assume that each utterance's latent state is dependent on its two neighbours. So there exist connections be- tween each pair of adjacent hidden softmax units in the hidden layer. The details of the model will be presented in Section 3.3. Figure 2: Hidden Softmax Model. The bottom layer are softmax visible units and the top layer consists of three types of hidden units: softmax hidden units used for representing latent states, a binary stochastic hidden unit used for represent- ing the dialogue specific topic, and a special bi- nary stochastic hidden unit used for representing corpus general topic. Upper: The model for a di- alogue session containing three utterances. Con- nection lines in the same color related to a latent state represent the same weight matrix. Lower: A different interpretation of the Hidden Softmax Model, in which D r visible softmax units in the r th utterance are replaced by one single multino- mial unit which is sampled D r times. <ref type="table">Table 1</ref> summarizes important notations utilized in this paper. Before introducing the ultimate learning model for dialogue structure analysis, we firstly discuss a simplified version, Hidden Soft- max Model (HSM), which is based on Boltzmann machine and assumes that the latent variables are independent given visible units. HSM has a two- layer architecture as shown in <ref type="figure">Figure 2</ref>. The en- ergy of the state {V, h φ , h ψ , h ξ } is defined as fol- lows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HSM: Hidden Softmax Model</head><formula xml:id="formula_0">E(V, h φ , h ψ , h ξ ) = ¯ E φ (V, h φ ) + ¯ E ψ (V, h ψ ) + ¯ E ξ (V, h ξ ) + C(V),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">¯ E φ (V, h φ ), ¯ E ψ (V, h ψ ) and ¯ E ξ (V, h ξ )</formula><p>are sub-energy functions related to hidden variables h φ , h ψ , and h ξ , respectively. C(V) is the shared visible units bias term. Suppose K is the dictio- nary size, D r is the r th utterance size (i.e. the number of words in the r th utterance), and R is the number of utterances in the a dialogue.</p><p>For each utterance v r (r = 1, .., R) in the dia- logue session we have a hidden variable vector h φ r (with size of J ) as a latent state of the utterance, the sub-energy function ¯ E φ (V, h φ ) is defined by</p><formula xml:id="formula_2">¯ E φ (V, h φ ) = − R r=1 J j=1 Dr i=1 K k=1 h φ rj W φ rjik v rik − R r=1 J j=1 h φ rj a φ rj ,<label>(2)</label></formula><p>where v rik = 1 means the i th visible unit v ri in the r th utterance takes on k th value, h φ rj = 1 means the r th softmax hidden units takes on j th value, and a φ rj is the corresponding bias. W φ rjik is a symmetric interaction term between visible unit v ri that takes on k th value and hidden variable h φ r that takes on j th value.</p><p>The sub-energy function ¯ E ψ (V, h ψ ), related to the global general topic of the corpus, is defined by</p><formula xml:id="formula_3">¯ E ψ (V, h ψ ) = − R r=1 Dr i=1 K k=1 h ψ W ψ rik v rik − h ψ a ψ . (3)</formula><p>The sub-energy function ¯ E ξ (V, h ξ ) corresponds to the dialogue specific topic, and is defined by</p><formula xml:id="formula_4">¯ E ξ (V, h ξ ) = − R r=1 Dr i=1 K k=1 h ξ W ξ rik v rik − h ξ a ξ .<label>(4)</label></formula><p>W ψ rik in Eq. <ref type="formula">(3)</ref> and W ξ rik in Eq. (4) are two sym- metric interaction terms between visible units and the corresponding hidden units, which are similar to W φ rjik in <ref type="formula" target="#formula_2">(2)</ref>; a ψ and a ξ are the corresponding biases. C(V) is defined by</p><formula xml:id="formula_5">C(V) = − R r=1 Dr i=1 K k=1 v rik b rik ,<label>(5)</label></formula><p>where b rik is the corresponding bias.</p><p>The probability that the model assigns to a vis-</p><formula xml:id="formula_6">ible binary matrix V = {v 1 , v 2 , ..., v D } (where D = R r=1 D r is the dialogue session size) is P (V) = 1 Z h φ , h ψ ,h ξ exp(−E(V, h φ , h ψ , h ξ )) Z = V h φ , h ψ ,h ξ exp(−E(V, h φ , h ψ , h ξ ),<label>(6)</label></formula><p>where Z is known as the partition function or nor- malizing constant.</p><p>In our proposed model, for each word in the document we use a softmax unit to represent it. For the sake of simplicity, assume that the order of words in an utterance is ignored. Therefore, all of these softmax units can share the same set of weights that connect them to hidden units, thus the visible bias term C(V) and the sub-energy func-</p><formula xml:id="formula_7">tions ¯ E φ (V, h φ ), ¯ E ψ (V, h ψ ) and ¯ E ξ (V, h ξ ) in Eq.</formula><p>(1) can be redefined as follows:</p><formula xml:id="formula_8">¯ E φ (V, h φ ) = − R r=1 J j=1 K k=1 h φ rj W φ jkˆvjkˆ jkˆv rk − R r=1 (Dr J j=1 h φ rj a φ j )<label>(7)</label></formula><formula xml:id="formula_9">¯ E ψ (V, h ψ ) = − K k=1 h ψ W ψ k ˆ v k − Dh ψ a ψ (8) ¯ E ξ (V, h ξ ) = − K k=1 h ξ W ξ k ˆ v k − Dh ξ a ξ (9) C(V) = − K k=1ˆv k=1ˆ k=1ˆv k b k ,<label>(10)</label></formula><p>wherê v rk = Dr i=1 v rik denotes the count for the k th word in the r th utterance of the dialogue, ˆ v k = R r=1ˆvr=1ˆ r=1ˆv rk is the count for the k th word in whole dialogue session. D r and D (D = R r=1 D r ) are employed as the scaling parameters, which can make hidden units behave sensibly when dealing with dialogues of different lengths <ref type="bibr" target="#b6">(Hinton and Salakhutdinov, 2009)</ref>.</p><p>The conditional distributions are given by soft- max and logistic functions:</p><formula xml:id="formula_10">P (h φ rj = 1|V) = exp( K k=1 W φ jkˆvjkˆ jkˆv rk + Dra φ j ) J j =1 exp( K k=1 W φ j k ˆ v rk + Dra φ j )<label>(11)</label></formula><formula xml:id="formula_11">P (h ψ = 1|V) = σ( K k=1 W ψ k ˆ v k + Da ψ )<label>(12)</label></formula><formula xml:id="formula_12">P (h ξ = 1|V) = σ( K k=1 W ξ k ˆ v k + Da ξ )<label>(13)</label></formula><formula xml:id="formula_13">P (v rik = 1|h φ , h ψ , h ξ ) = exp( J j=1 h φ rj W φ jk + h ψ W ψ k + h ξ W ξ k + b k ) K k =1 exp( J j=1 h φ rj W φ jk + h ψ W ψ k + h ξ W ξ k + b k ) ,<label>(14)</label></formula><p>where σ(x) = 1/(1 + exp(−x)) is the logistic function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HSSM: Hidden Softmax Sequence Model</head><p>In this section, we consider the dependency be- tween the adjacent latent states of utterances, and extend the HSM to hidden softmax sequence model (HSSM), which is displayed in <ref type="figure">Figure 3</ref>.</p><p>We define the energy of the state {V, h φ , h ψ , h ξ } in HSSM as follows:</p><formula xml:id="formula_14">E(V, h φ , h ψ , h ξ ) = ¯ E φ (V, h φ ) + ¯ E ψ (V, h ψ ) + ¯ E ξ (V, h ξ ) + C(V) + ¯ EΦ(h φ , h φ ),<label>(15)</label></formula><p>where</p><formula xml:id="formula_15">C(V), ¯ E φ (V, h φ ), ¯ E ψ (V, h ψ ) and ¯ E ξ (V, h ξ )</formula><p>are the same with that in HSM. The last term ¯ E Φ (h φ , h φ ) is utilized to formulate the dependency between latent variables h φ , which is defined as follows:</p><formula xml:id="formula_16">¯ EΦ(h φ , h φ ) = − J q=1 h φ s F s q h φ 1q − J q=1 h φ Rq F e q h φ e − R−1 r=1 J j=1 J q=1 h φ rj Fjqh φ r+1,q ,<label>(16)</label></formula><p>where h φ s and h φ e are two constant scalar variables (h φ s ≡ 1, h φ e ≡ 1), which represent the virtual beginning state unit and ending state unit of a di- alogue. F s is a vector with size J, and its ele- ments measure the dependency between h φ s and the latent softmax units of the first utterance. F e also contains J elements, and in contrast to F s , F e represents the dependency measure between h φ e and the latent softmax units of the last utter- ance. F is a symmetric matrix for formulating de- pendency between each two adjacent hidden units pair (h φ r , h φ r+1 ), r = 1, ..., R − 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utterance 1 Utterance 2 Utterance 3</head><p>Figure 3: Hidden softmax sequence model. A con- nection between each pair of adjacent hidden soft- max units is added to formulate the dependency between the two corresponding latent states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Learning</head><p>Exact maximum likelihood learning in the pro- posed model is intractable. "Contrastive Diver- gence" <ref type="bibr" target="#b8">(Hinton, 2002</ref>) can be used for HSM's learning, however, it can not be utilized for HSSM, because the hidden-to-hidden interaction term, {F, F s , F e }, result in the intractability when ob- taining exact samples from the conditional distri- bution P (h φ rj = 1|V), r = [1, R], j ∈ [1, J]. We use the mean-field variational inference <ref type="bibr" target="#b7">(Hinton and Zemel, 1994;</ref><ref type="bibr" target="#b14">Neal and Hinton, 1998;</ref><ref type="bibr" target="#b10">Jordan et al., 1999</ref>) and a stochastic approxima- tion procedure (SAP) <ref type="bibr" target="#b19">(Tieleman, 2008)</ref> to esti- mate HSSM's parameters. The variational learn- ing is utilized to get the data-dependent expecta- tions, and SAP is utilized to estimate the model's expectation. The log-likelihood of the HSSM has the following variational lower bound: <ref type="formula" target="#formula_0">(17)</ref> Q(h) can be any distribution of h in theory. θ = {W φ , W ψ , W ξ , F, F s , F e } (the bias terms are omitted for clarity) are the model parameters. h = {h φ , h ψ , h ξ } represent all the hidden vari- ables. H(·) is the entropy functional. In varia- tional learning, we try to find parameters that min- imize the Kullback-Leibler divergences between Q(h) and the true posterior P (h|V; θ). A naive mean-field approach can be chosen to obtain a fully factorized distribution for Q(h):</p><formula xml:id="formula_17">log P (V; θ) ≥ h Q(h) log P (V, h; θ) + H(Q).</formula><formula xml:id="formula_18">Q(h) = R r=1 q(h φ ) q(h ψ ) q(h ξ ),<label>(18)</label></formula><p>where q(h φ rj = 1) = µ φ rj , q(h ψ = 1) = µ ψ , q(h ξ = 1) = µ ξ . µ = {µ φ , µ ψ , µ ξ } are the pa- rameters of Q(h). Then the lower bound on the log-probability log P (V; θ) has the form:</p><formula xml:id="formula_19">log P (V; θ) ≥ − ¯ E φ (V, µ φ ) − ¯ E ψ (V, µ ψ ) − ¯ E ξ (V, µ ξ ) − C(V) − ¯ EΦ(µ φ , µ φ ) − log Z,<label>(19)</label></formula><p>where</p><formula xml:id="formula_20">¯ E φ (V, µ φ ), ¯ E ψ (V, µ ψ ), ¯ E ξ (V, µ ξ )</formula><p>, and ¯ E Φ (µ φ , µ φ ) have the same forms, by replacing µ with h, as Eqs. <ref type="formula" target="#formula_8">(7)</ref>, (8), (9), and (16), respectively.</p><p>We can maximize this lower bound with respect to parameters µ for fixed θ, and obtain the mean- field fixed-point equations:</p><formula xml:id="formula_21">µ φ rj = exp( K k=1 W φ jkˆvjkˆ jkˆv rk + Dra φ j + D j prev + D j next − 1) J j =1 exp( K k=1 W φ j k ˆ v rk + Dra φ j + D j prev + D j next − 1) ,<label>(20)</label></formula><formula xml:id="formula_22">µ ψ = σ( K k=1 W ψ k ˆ v k + Da ψ )<label>(21)</label></formula><formula xml:id="formula_23">µ ξ = σ( K k=1 W ξ k ˆ v k + Da ξ ),<label>(22)</label></formula><p>where D j prev and D j next are two terms relevant to the derivative of the RHS of Eq. <ref type="formula" target="#formula_0">(19)</ref> with respect to µ φ rj , defined by</p><formula xml:id="formula_24">D j prev = F s j , r = 1 J q=1 µ φ r−1,q Fqj, r &gt; 1 D j next = J q=1 Fjqµ φ r+1,q , r &lt; R. F e j , r = R</formula><p>The updating of µ can be carried out iteratively until convergence. Then, (V, µ) can be considered as a special "state" of HSSM, thus the SAP can be applied to update the model's parameters, θ, for fixed (V, µ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussions</head><p>It's not easy to evaluate the performance of a dia- logue structure analysis model. In this study, we examined our model via qualitative visualization and quantitative analysis as done in ( <ref type="bibr" target="#b16">Ritter et al., 2010;</ref><ref type="bibr" target="#b23">Zhai and Williams, 2014</ref>). We implemented five conventional models to conduct an extensive comparing study on the two corpora: Twitter-Post and AirTicketBooking. Conventional models in- clude: LMHMM <ref type="bibr" target="#b3">(Chotimongkol, 2008)</ref>, LMH- MMS ( <ref type="bibr" target="#b16">Ritter et al., 2010)</ref>, TMHMM, TMHMMS, and TMHMMSS <ref type="bibr" target="#b23">(Zhai and Williams, 2014</ref>). In our experiments, for each corpus we randomly se- lect 80% dialogues for training, and use the rest 20% for testing. We select three different num- ber <ref type="bibr">(10, 20 and 30)</ref> of latent states to evaluate all the models. In TMHMM, TMHMMS and TMH- MMSS, the number of "topics" in the latent states and a dialogue is a hyper-parameter. We con- ducted a series of experiments with varying num- bers of topics, and the results illustrated that 20 is the best choice on the two corpora. So, for all the following experimental results of TMHMM, TMHMMS and TMHMMSS, the corresponding topic configurations are set to 20.</p><p>The number of estimation iterations for all the models on training sets is set to 10,000; and on held-out test sets, the numver of iterations for in- ference is set to 1000. In order to speed-up the learning of HSSM, datasets are divided into mini- batches, each has 15 dialogues. In addition, the learning rate and momentum are set to 0.1 and 0.9, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Qualitative Evaluation</head><p>Dialogues in Twitter-Post always begin with three latent states: broadcasting what they (Twitter users) are doing now ("Status"), broadcasting an interesting link or quote to their followers ("Ref- erence Broadcast"), or asking a question to their followers ("Question to Followers"). <ref type="bibr">2</ref> We find that structures discoverd by HSSM and LMHMMS with 10 latent states are most reasonable to inter- pret. For example, after the initiating state ("Sta- tus", "Reference Broadcast", or "Question to Fol- lowers"), it was often followed a "Reaction" to "Reference Broadcast" (or "Status"), or a "Com- ment" to "Status", or a "Question" to "Status" ( "Reference Broadcast", or "Question to Follow- ers"') etc. Compared with LMHMMS, besides ob- taining similar latent states, HSSM exhibits pow- erful ability in learning sequential dependency re- lationship between latent states. Take the follow- ing simple Twitter dialogue session as an example:</p><p>: rt i like katy perry lt lt we see tht lol LMHMMS labelled the second utterance ("lol gd morning ") and the third utterance ("lol good morning how u " ) into the same latent state, while HSSM treats them as two different latent states (Though they both have almost the same words). The result is reasonable: the first "gd morning" is a greeting, while the second "gd morning" is a re- sponse.</p><p>For AirTicketBooking dataset, the state- transition diagram generated with our model under the setting of 10 latent states is presented in <ref type="figure" target="#fig_2">Figure 4</ref>. And several utterance examples corresponding to the latent staes are also showed in <ref type="table" target="#tab_2">Table 2</ref>. In general, conversations begin with sever agent's short greeting, such as "Hi, very glad to be of service.", and then transit to checking the passenger's identity information or inquiring the passenger's air ticket demand; or it's directly interrupted by the passenger with booking demand which is always associated with place information. After that, conversations are carried out with other booking related issues, such as checking ticket price or flight time.</p><p>The flowchart produced by HSSM can be rea- sonably interpreted with knowledge of air ticket booking domain, and it most consistent with the agent's real workflow of the Ticket Booking Cor- poration 3 compared with other models. We notice that conventional models can not clearly distin- guish some relevant latent states from each other. For example, these baseline models always con- found the latent state "Price Info" with the latent state "Reservation", due to certain words assigned large weights in the two states, such as " (dis- count)", and " (credit card)" etc. Further- more, Only HSSM and LMHMMS have dialogue specific topics, and experimental results illustrate that HSSM can learn much better than LMHMMS which always mis-recognize corpus general words as belonging to dialogue specific topic (An exam- ple is presented in <ref type="table" target="#tab_3">Table 3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>For quantitative evaluation, we examine HSSM and traditional models with log likelihood and an ordering task on the held-out test set of Twitter- Post and AirTicketBooking. <ref type="bibr">3</ref> We hide the corporation's real name for privacy reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent States</head><note type="other">Utterance Examples Utterance Examples (Chinese) (English Translation) Start</note><p>Hello, very glad to be of service. Inquiry Do you want to make a flight reservation? <ref type="table">Place Info    I want to book an air ticket</ref>    Log Likelihood The likelihood metric mea- sures the probability of generating the test set us- ing a specified model. The likelihood of LMHMM and TMHMM can be directed computed with the forward algorithm. However, since likelihoods of LMHMMS, TMHMMS and TMHMMSS are in- tractable to compute due to the local dependen- cies with respect to certain latent variables, Chib- style estimating algorithms ( <ref type="bibr" target="#b20">Wallach et al., 2009)</ref> are employed in our experiments. For HSSM, the partition function is a key problem for calculating the likelihood, and it can be effectively estimated by Annealed Importance Sampling (AIS) <ref type="bibr" target="#b15">(Neal, 2001;</ref><ref type="bibr" target="#b17">Salakhutdinov and Murray, 2008)</ref>. <ref type="figure" target="#fig_3">Figure 5</ref> presents the likelihood of different models on the two held-out datasets. We can ob- serve that HSSM achieves better performance on likelihood than all the other models under different number of latent states. On Twitter-Post dataset our model slightly surpasses LMHMMS, and it performs much better than all traditional models on AirTicketBooking dataset.</p><p>Ordering Test Following previous work ( <ref type="bibr" target="#b2">Barzilay and Lee, 2004;</ref><ref type="bibr" target="#b16">Ritter et al., 2010;</ref><ref type="bibr" target="#b23">Zhai and Williams, 2014)</ref>, we utilize Kendall's τ ( <ref type="bibr" target="#b11">Kendall, 1938)</ref> as evaluation metric, which measures the similarity between any two se- quential data and ranges from −1 (indicating a reverse ordering) to +1 (indicating an identical J = 10 J = 20 J = 30 ordering). This is the basic idea: for each dialogue session with n utterances in the test set, we firstly generate all n! permutations of the utterances; then evaluate the probability of each permutation, and measure the similarity, i.e. Kendall's τ , between the max-probability permutation and the original order; finally, we average τ values for all dialogue sessions as the model's ordering test score. As pointed out by <ref type="bibr" target="#b23">Zhai et al. (2014)</ref>, it's however infeasible to enumerate all possible permutations of dialogue sessions when the number of utterances in large. In experiments, we employ the incrementally adding permutation strategy, as used by <ref type="bibr" target="#b23">Zhai et al. (2014)</ref>, to build up the permutation set. The results of ordering test are presented in <ref type="figure" target="#fig_4">Figure 6</ref>. We can see that HSSM exhibits better performance than all the other models. For the conventional models, it is interesting that LMHMMS, TMHMMS and TMHMMSS achieve worse performances than LMHMM and TMHMM. This is likely because the latter two models allow words to be emitted only from latent states <ref type="bibr" target="#b23">(Zhai and Williams, 2014)</ref>, while the former three models allow words to be generated from additional sources. This also implies HSSM's effectiveness of modeling distinct information uderlying dialogues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>The expermental results illustrate the effective- ness of the proposed undirected dialogue struc- ture analysis model based on Boltzmann machine.</p><p>The conducted experiments also demonstrate that undirected models have three main merits for text modeling, which are also demonstrated by <ref type="bibr" target="#b6">Hinton and Salakhutdinov (2009)</ref>, <ref type="bibr" target="#b18">Srivastava et al. (2013)</ref> through other tasks. Boltzmann machine based undirected models are able to generalize much bet- ter than traditional directed generative model; and model learning is more stable. Besides, an undi- rected model is more suitable for describing com- plex dependencies between different kinds of vari- ables.</p><p>We also notice that all the models can, to some degree, capture the sequential structure in the di- alogues, however, each model has a special char- acteristic which makes itself fit a certain kind of dataset better. HSSM and LMHMMS are more appropriate for modeling the open domain dataset, such as Twitter-Post used in this paper, and the task-oriented domain dataset with one relatively concentrated topic in the corpus and special in- formation for each dialogue, such as AirTicket- Booking. As we known, dialogue specific top- ics in HSSM or LMHMMS are used and trained only within corresponding dialogues. They are crucial for absorbing certain words that have im- portant meaning but do not belongs to latent states. In addition, for differet dataset, dialogue specific topics may have different effect to the model- ing. Take the Twitter-Post for an example, dia- logue specific topics formulate actual themes of dialogues, such as a pop song, a sport news. As for the AirTicketBooking dataset, dialogue specific topics always represent some special information, such as the personal information, including name, phone number, birthday, etc. In summary, each di- alogue specific topic reflects special information which is different from other dialogues. The three models, TMHMM, TMHMMS and TMHMMSS, which do not include dialogue spe- cific topics, should be utilized on the task-oriented domain dataset, in which each dialogue has little special or personnal information. For example, the three models perform well on the the BusTime and TechSupport datasets <ref type="bibr" target="#b23">(Zhai and Williams, 2014)</ref>, in which name entities are all replaced by different semantic types (e.g. phone numbers are replaced by "&lt;phone&gt;", E-mail addresses are replaced by "&lt;email&gt;", etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We develope an undirected generative model, HSSM, for dialogue structure analysis, and exam- ine the effectiveness of our model on two different datasets, Twitter posts occurred in open-domain and task-oriented dialogues from airline ticket booking domain. Qualitative evaluations and quantitative experimental results demonstrate that the proposed model achieves better performance than state-of-the-art approaches. Compared with traditional models, the proposed HSSM has more powerful ability of discovering structures of latent states and modeling different word sources, in- cluding latent states, dialogue specific topics and global general topic.</p><p>According to recent study ( <ref type="bibr" target="#b18">Srivastava et al., 2013)</ref>, a deep network model exhibits much ben- efits for latent variable learning. A dialogue may actually have a hierarchy structure of latent states, therefore the proposed model can be extended to a deep model to capture more complex structures. Another possible way to extend the model is to consider modeling long distance dependency be- tween latent states. This may further improve the model's performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hidden layer that consists of different types of latent variables</figDesc><graphic url="image-1.png" coords="2,315.61,533.70,201.41,71.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>:</head><label></label><figDesc>lol gd morning : lol gd morning how u : i'm gr8 n urself : i'm good gettin ready to head out : oh ok well ur day n up its cold out here ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Transitions between latent states on AirTicketBooking generated by our HSSM model under the setting of J = 10 latent states. Transition probability cut-off is 0.10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Negative log likelihood (smaller is better) on held-out datasets of Twitter-Post (upper) and AirTicketBooking (lower) under different number of latent states J.</figDesc><graphic url="image-56.png" coords="8,387.41,170.72,140.21,97.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average Kendall's τ measure (larger is better) on held-out datasets of Twitter-Post (upper) and AirTicketBooking (lower) under different number of latent states J.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Utterance examples of latent states dis-
covered by our model. 

Model 
Top Words 

HSSM 
, , , , , ... 
ten o'clock, Dong Li (name), Fuzhou (city), Xiamen 
(city), Shanghai Airlines, ... 

LMHMMS 
, , , , , ... 
have, ten o'clock, er, Dong Li (name), reserve, ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>One example of dialogue specific topic 
learned on the same dialogue session with HSSM 
and LMHMMS, respectively. 

</table></figure>

			<note place="foot" n="1"> Also called dialogue acts or speech acts in some past work. In this paper, for simplicity we will only use the term &quot;latent state&quot; to describe the sequential dialogue structure.</note>

			<note place="foot" n="2"> For simplicity and readability in consistent, we follow the same latent state names used in (Ritter et al., 2010)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to anonymous reviewers for their helpful comments and suggestions. We would like to thank Alan Ritter for kindly providing the raw Twitter dataset.</p><p>This work is supported in part by the Na-tional Natural Science Funds of China under Grant 61170197 and 61571266, and in part by the Elec-tronic Information Industry Development Fund under project "The R&amp;D and Industrialization on Information Retrieval System Based on Man-Machine Interaction with Natural Speech".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Plow: A collaborative task learning agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Galescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuckchul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Taysom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1514</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">AAAI Press</title>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Catching the drift: Probabilistic content models with applications to generation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of HLT-NAACL 2004</title>
		<meeting>HLT-NAACL 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning the structure of task-oriented conversations from the corpus of indomain dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananlada</forename><surname>Chotimongkol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, SRI International</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to classify email into&quot;speech acts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vitor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised classification of dialogue acts using a dirichlet process mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Granell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="341" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length, and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised speech act recognition in emails and forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1250" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dialogue-oriented review summary generation for spoken dialogue recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Seneff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating speaker and discourse features into speech summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A view of the em algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised modeling of twitter conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<title level="m">Modeling documents with deep boltzmann machines. UAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines using approximations to the likelihood gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1064" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Hanna M Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Artificial companions as a new kind of interface to the future internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using pomdps for dialog management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discovering latent structure in task-oriented dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
