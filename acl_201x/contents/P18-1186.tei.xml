<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zeroshot Multimodal Named Entity Disambiguation for Noisy Social Media Posts</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2000</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
							<email>seungwhm@cs.cmu.edu, lneves@snap.com, vitor carvalho@intuit.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Snap Research 3 Intuit</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Snap Research 3 Intuit</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Carvalho</surname></persName>
						</author>
						<title level="a" type="main">Zeroshot Multimodal Named Entity Disambiguation for Noisy Social Media Posts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2000" to="2008"/>
							<date type="published">July 15-20, 2018. 2018. 2000</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images. Social media posts bring significant challenges for disam-biguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations , 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training. To this end, we build a new dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base. We then build a deep zeroshot mul-timodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well. The proposed model significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online communications are increasingly becom- ing fast-paced and frequent, and hidden in these abundant user-generated social media posts are insights for understanding users and their pref- erences. However, these social media posts of- ten come in unstructured text or images, making massive-scale opinion mining extremely challeng- (a) Traditional NED (b) Multimodal NED <ref type="figure">Figure 1</ref>: Examples of (a) a traditional NED task, focused on disambiguating polysemous entities based on surrounding textual contexts, and (b) the proposed Multimodal NED task for short media posts, which leverages both visual and textual con- texts to disambiguate an entity. Note that mentions are often lexically inconsistent or incomplete, and thus a fixed candidates generation method (based on exact mention-entity statistics) is not viable.</p><p>ing. Named entity disambiguation (NED), the task of linking ambiguous entities from free-form text mention to specific entities in a pre-defined knowl- edge base (KB), is thus a critical step for extracting structured information which leads to its applica- tion for recommendations, advertisement, person- alized assistance, etc.</p><p>While many previous approaches on NED been successful for well-formed text in disam- biguating polysemous entities via context reso- lution, several additional challenges remain for disambiguating entities from extremely short and coarse text found in social media posts (e.g.</p><p>"juuustin " as opposed to "I love Justin Bieber/Justin Trudeau/etc."). In many of these cases it is simply impossible to disambiguate entities from text alone, due to enormous num- ber of surface forms arising from incomplete and inconsistent notations. In addition, social media posts often include mentions of newly emerging entities unseen in training sets, making traditional context-based entity linking often not viable.</p><p>However, as popular social media platforms are increasingly incorporating a mix of text and im- ages (e.g. Snapchat, Instargram, Pinterest, etc.), we can advance the disambiguation task to incor- porate additional visual context for understanding posts. For example, the mention of 'juuustin' is completely ambiguous in its textual form, but an accompanying snap image of a concert scene may help disambiguate or re-rank among several lex- ical candidates (e.g. Justin Bieber (a pop singer) versus Justin Trudeau (a politician) in <ref type="figure">Figure 1</ref>).</p><p>To this end, we introduce a new task called Mul- timodal Named Entity Disambiguation (MNED) that handles unique challenges for social media posts composed of extremely short text and im- ages, aimed at disambiguationg entities by lever- aging both textual and visual contexts.</p><p>We then propose a novel zeroshot MNED model, which obtains visual context vectors from images with a CNN ( <ref type="bibr" target="#b13">LeCun et al., 1989)</ref>, and com- bines with textual context extracted from a bidi- rectional LSTM ( <ref type="bibr" target="#b5">Dyer et al., 2015</ref>) (Section 2.2). In addition, we obtain embeddings representation of 1M entities from a knowledge graph, and train the MNED network to predict label embeddings of entities in the same space as corresponding knowl- edge graph embeddings (Section 2.4). This ap- proach effectively allows for zeroshot prediction of unseen entities, which is critical for scarce-label scenario due to extensive human annotation efforts required. Lastly, we develop a lexical embeddings model that determines lexical similarity between a mention and potential entities, to aid in prediction of a correct entity (Section 2.3). Section 2.5 details the model combining the components above. Note that our method takes different perspec- tives from the previous work on NED ( <ref type="bibr" target="#b10">He et al., 2013;</ref><ref type="bibr" target="#b24">Yamada et al., 2016;</ref><ref type="bibr" target="#b7">Eshel et al., 2017</ref>) in the following important ways. First, while most of the previous methods generate fixed "candidates" for disambiguation given a mention from mention- entity pair statistics (thus disambiguation is lim- ited for entities with exact surface form matches), we do not fixate candidate generation, due to in- tractable variety of surface forms for each named entity and unforeseen mentions of emerging en- tities. Instead, we have a lexical model incorpo- rated into the discriminative score function that serves as soft normalization of various surface forms. Second, we extract auxiliary visual con- texts for detected entities from user-generated im- ages accompanied with textual posts, which is cru- cial because captions in our dataset are substan- tially shorter than text documents in most other NED datasets. To the best of our knowledge, our work is the first in using visual contexts for the named entity disambiguation task. See Section 4 for the detailed literature review.</p><p>Our contributions are as follows: for the new MNED task we introduce, we propose a deep zeroshot multimodal network with (1) a CNN- LSTM hybrid module that extracts contexts from both image and text, (2) a zeroshot learning layer which via embeddings projection allows for entity linking with 1M knowledge graph entities even for entities unseen from captions in training set, and (3) a lexical language model called Deep Lev- enshtein to compute lexical similarities between mentions and entities, relaxing the need for fixed candidates generation. We show that the proposed approaches successfully disambiguate incomplete mentions as well as polysemous entities, outper- forming the state-of-the-art models on our newly crawled SnapCaptionsKB dataset, composed of 12K image-caption pairs with named entities an- notated and linked with an external KB. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the proposed model, which maps each multimodal social media post data to one of the corresopnding entities in the KB. Given a multimodal input that contains a men- tion of an ambiguous entity, we first extract tex- tual and visual features contexts with RCNNs and Bi-LSTMs, respectively (Section 2.2). We also obtain lexical character-level representation of a mention to compare with lexical representation of KB entities, using a proposed model called Deep Levenshtein (Section 2.3). We then get high- dimensional label embeddings of KB entities con- structed from a knowledge graph, where similar entities are mapped as neighbors in the same space (Section 2.4). Finally, we aggregate all the contex- tual information extracted from surrounding text, image, and lexical notation of a mention, and pre- dict the best matching KB entity based on knowl- edge graph label representation and lexical nota- tion of KB entity candidates (Section 2.5). The main architecture of our Multimodal NED network. We extract contextual informa- tion from an image, surrounding words, and lex- ical embeddings of a mention. The modality at- tention module determines weights for modalities, the weighted projections of which produce label embeddings in the same space as knowledge-base (KB) entity embeddings. We predict a final candi- date by ranking based on similarities with KB en- tity knowledge graph embeddings as well as with lexical embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><formula xml:id="formula_0">Let X = {x (i) } N i=1</formula><p>a set of N input social me- dia posts samples for disambiguation, with cor- responding ground truth named entities Y = {y (i) } N i=1 for y ∈ Y KB , where Y KB is a set of entities in KB. Each input sample is composed of three modalities:</p><formula xml:id="formula_1">x = {x w ; x v ; x c }, where x w = {x w,t } Lw t=1</formula><p>is a sequence of words with length L w surrounding a mention in a post, x v is an image associated with a post (Section 2.2), and x c = {x c,t } Lc t=1 is a sequence of characters comprising a mention (Section 2.3), respectively. We denote high-dimensinal feature extractor func- tions for each modality as:</p><formula xml:id="formula_2">w(x w ), c(x c ), v(x v ).</formula><p>We represent each output label in two modali- ties: y = {y KB ; y c }, where y KB is a knowl- edge base label embeddings representation (Sec- tion 2.4), and and y c is a character embeddings representation of KB entities (Section 2.3: Deep Levenshtein).</p><p>We formulate our zeroshot multimodal NED task as follows:</p><formula xml:id="formula_3">y = argmax y ∈Y KB sim f x→y (x), y</formula><p>where f x→y is a function with learnable parame- ters that project multimodal input samples (x) into the same space as label representations (y), and sim(·) produces a similarity score between predic- tion and ground truth KB entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual and Visual Contexts Features</head><p>Textual features: we represent textual context of surrounding words of a mention with a Bi- LSTM language model ( <ref type="bibr" target="#b5">Dyer et al., 2015</ref>) with distributed word semantics embeddings. We use the following implementation for the LSTM.</p><formula xml:id="formula_4">i t = σ(W xi h t−1 + W ci c t−1 ) c t = (1 − i t ) c t−1 + i t tanh(W xc x w,t + W hc h t−1 ) o t = σ(W xo x w,t + W ho h t−1 + W co c t ) h t = o t tanh(c t ) w(x w ) = [ − − → h Lw ; ← − − h Lw ] (1)</formula><p>where h t is an LSTM hidden layer output at de- coding step t, and w(x w ) is an output textual rep- resentation of bi-directional LSTM concatenating left and right context at the last decoding step t = L w . Biase terms for gates are omitted for simplicity of formulation. For the Bi-LSTM sentence encoder, we use pre- trained word embeddings obtained from an un- supervised language model aimed at learning co- occurrence statistics of words from a large external corpus. Word embeddings are thus represented as distributional semantics of words. In our experi- ments, we use pre-trained embeddings from Stan- ford GloVE model ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>).</p><p>Visaul features: we take the final activation of a modified version of the recurrent convolutional network model called Inception (GoogLeNet) ( <ref type="bibr" target="#b21">Szegedy et al., 2015</ref>) trained on the ImageNet dataset ( <ref type="bibr" target="#b18">Russakovsky et al., 2015</ref>) to classify mul- tiple objects in the scene. The final layer repre- sentation (v(x v )) thus encodes discriminative in- formation describing what objects are shown in an image, providing cues for disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lexical Embeddings: Deep Levenshtein</head><p>While traditional NED tasks assume perfect lex- ical match between mentions and their corresop- nding entities, in our task it is important to ac- count for various surface forms of mentions (nick- names, mis-spellings, inconsistent notations, etc.) corresponding to each entity. Towards this goal, we train a separate deep neural network to com- pute approximate Levenshtein distance which we call Deep Levenshtein <ref type="figure" target="#fig_1">(Figure 3)</ref>, composed of a shared bi-directional character LSTM, shared character embedding matrix, fully connected lay- ers, and a dot product merge operation layer. The optimization is as follows:</p><formula xml:id="formula_5">min c 1 2 c(x c ) · c(x c ) c(x c )c(x c ) + 1 − sim(x c , x c ) 2<label>(2)</label></formula><p>where c(</p><formula xml:id="formula_6">x c ) = [ − −− → h c,Lc ; ← −− − h c,Lc ]</formula><p>where c(·) is a bi-directional LSTM output vec- tor for a character sequence defined similar as in Eq.1, sim(·) is an output of the Deep Levenshtein network, producing a normalized similarity score with a range [0,1] based on Levenshtein edit dis- tance, and (x c , x c ) is any pair of two strings. We generate millions of these pairs as training data by artificially corrupting seed strings by varying de- grees (addition, deletion, replacement).</p><p>Once trained, it can produce a purely lexical embedding of a string without semantic allusion (via c(·)), and predict lexical similarity between two strings based on their distance in the embed- ding space. On an intuitive level, this component effectively bypasses normalization steps, and in- stead incorporates lexical similarities between in- put mentions and output KB entities into the over- all optimization of the disambiguation network.</p><p>We use by-product c(·) network to extract lex- ical embedings of mentions and KB entities, and freeze c in training of the disambiguation network. We observe that this approach significantly out- performs alternative ways to obtain character em- beddings (e.g. having a character Bi-LSTM as a part of the disambiguation network training, which unnecessarily learns semantic allusions that are prone to errors when notations are inconsistent.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Label Embeddings from Knowledge Graph</head><p>Due to the overwhelming variety of (newly trend- ing) entities mentioned over social media posts, at . Once high-level mapping from contextual information to label embeddings is learned, the knowledge- graph based zeroshot approach can improve the entity linking performance given ambiguous en- tities unseen in training data. In brief formula- tion, the model for obtaining embeddings from a knowledge graph (composed of subject-relation- object (s, r, o) triplets) is as follows:</p><formula xml:id="formula_7">P (I r (s, o) = 1|e, e r , θ) = score θ e(s), e r (r), e(o)<label>(3)</label></formula><p>where I r is an indicator function of a known re- lation r for two entities (s,o) (1: valid relation, 0: unknown relation), e is a function that extracts em- beddings for entities, e r extracts embeddings for relations, and score θ (·) is a deep neural network that produces a likelihood of a valid triplet.</p><p>In our experiments, we use the 1M subset of the Freebase knowledge graph ( <ref type="bibr" target="#b0">Bast et al., 2014</ref>) to obtain label embeddings with the Holographic KB implementation by <ref type="bibr" target="#b15">(Nickel et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Deep Zeroshot MNED Network (DZMNED)</head><p>Using the contextual information extracted from surrounding text and an accompanying image (Section 2.2) and lexical embeddings of a mention (Section 2.3), we build a Deep Zeroshot MNED network (DZMNED) which predicts a correspond- ing KB entity based on its knowledge graph em- beddings (Section 2.4) and lexical similarity (Sec- tion 2.3) with the following objective:</p><formula xml:id="formula_8">min W L KB (x, y KB ;W w ,W v ,W f ) + L c (x c , y c ; W c )</formula><p>where</p><formula xml:id="formula_9">L KB (·)= 1 N N i=1˜y i=1˜ i=1˜y =y (i) KB max[0, ˜ y · y (i) KB −f (x (i) ) · (y (i) KB − ˜ y) ] L c (·) = 1 N N i=1˜y i=1˜ i=1˜y =y (i) c max[0, ˜ y · y (i) c −c(x (i) c ) · (y (i) c − ˜ y) ] R(W): regularization</formula><p>where L KB (·) is the supervised hinge rank loss for knowledge graph embeddings prediction, L c (·) is the loss for lexical mapping between mentions and KB entities, x is a weighted average of three modalities x = {x w ; x v ; x c } via the modality at- tention module. f (·) is a transformation function with stacked layers that projects weighted input to the KB embeddings space, ˜ y refers to the embed- dings of negative samples randomly sampled from KB entities except the ground truth label of the in- stance, W = {W f ,W c ,W w ,W v } are the learn- able parameters for f , c, w, and v respectively, and R(W) is a weight decay regularization term.</p><p>Similarly to ( <ref type="bibr" target="#b14">Moon et al., 2018)</ref>, we formulate the modality attention module for our MNED network as follows, which selectively attenuates or amplifies modalities: </p><formula xml:id="formula_10">[a w ; a c ; a v ] = σ W m · [xw; xc; x v ] + b m<label>(4)</label></formula><formula xml:id="formula_11">α m x m<label>(5)</label></formula><p>where α = [α w ; α c ; α v ] ∈ R 3 is an attention vec- tor, and x is a final context vector that maximizes information gain.</p><p>Intuitively, the model is trained to produce a higher dot product similarity between the pro- jected embeddings with its correct label than with an incorrect negative label in both the knowledge graph label embeddings and the lexical embed- dings spaces, where the margin is defined as the similarity between a ground truth sample and a negative sample.</p><p>At test time, the following label-producing nearest neighbor (1-NN) classifier is used for the target task (we cache all the label embeddings to avoid repetitive projections):</p><formula xml:id="formula_12">1-NN(x) = argmax (y KB ,yc)∈Y KB f (x) · y KB +g(x c ) · y c<label>(6)</label></formula><p>In summary, the model produces (1) projec- tion of input modalities (mention, surrounding text, image) into the knowledge graph embed- dings space, and (2) lexical embeddings represen- tation of mention, which then calculates a com- bined score of contextual (knowledge graph) and string similarities with each entity in Y KB .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Evaluation</head><p>Task: Given a caption and an accompanying im- age (if available), the goal is to disambiguate and link a target mention in a caption to a correspond- ing entity from the knowledge base (1M subset of the Freebase knowledge graph <ref type="figure" target="#fig_0">(Bast et al., 2014)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Our SnapCaptionsKB dataset is composed of 12K user-generated image and textual caption pairs where named entities in captions and their links to KB entities are manually labeled by ex- pert human annotators. These captions are col- lected exclusively from snaps submitted to pub- lic and crowd-sourced stories (aka Live Stories or Our Stories). Examples of such stories are "New York Story" or "Thanksgiving Story", which are aggregated collections of snaps for various pub- lic venues, events, etc. Our data do not con- tain raw images, and we only provide textual cap- tions and obfuscated visual descriptor features ex- tracted from the pre-trained InceptionNet. We split the dataset randomly into train (70%), val- idation (15%), and test sets (15%). The cap- tions data have average length of 29.5 characters (5.57 words) with vocabulary size 16,553, where 6,803 are considered unknown tokens from Stan- ford GloVE embeddings ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>).</p><p>Named entities annotated in the dataset include many of new and emerging entities found in vari- ous surface forms. To the best of our knowledge, our SnapCaptionsKB is the only dataset that con- tains image-caption pairs with human-annotated named entities and their links to KB entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We report performance of the following state-of- the-art NED models as baselines, with several can- didate generation methods and variations of our proposed approach to examine contributions of each component (W: word, C: char, V: visual).</p><p>Candidates generation: Note that our zeroshot approach allows for entity disambiguation with- out a fixed candidates generation process. In fact, we observe that the conventional method for fixed candidates generation harms the performance for noisy social media posts with many emerging enti- ties. This is because the difficulty of entity linking at test time rises not only from multiple entities (e) linking to a single mention (m), but also from each entity found in multiple surface forms of mentions (often unseen at train time). To show the efficacy of our approach that does not require candidates generation, we compare with the following candi- dates generation methods:</p><p>• m→e hash list: This method retrieves KB en- tity (e) candidates per mention (m) based on exact (m, e) pair occurrence statistics from a training corpora. This is the most predom- inantly used candidates generation method ( <ref type="bibr" target="#b10">He et al., 2013;</ref><ref type="bibr" target="#b24">Yamada et al., 2016;</ref><ref type="bibr" target="#b7">Eshel et al., 2017)</ref>. Note that this approach is es- pecially vulnerable at test time to noisy men- tions or emerging entities with no or a few matching candidate entities from training set.</p><p>• k-NN: We also consider using lexical neigh- bors of mentions from KB entities as can- didates. This approach can be seen as soft normalization to relax the issue of having to match a variety of surface forms of a men- tion to KB entities. We use our Deep Leven- shtein (Section 2.3) to compute lexical em- beddings of KB entities and mentions, and retrieves Euclidean neighbors (and their pol- ysemous entities) as candidates.</p><p>NED models: We choose as baselines the fol- lowing state-of-the-art NED models for noisy text, as well as several configurations of our proposed approach to examine contributions of each compo- nent (W: word, C: char, V: visual).</p><p>• sDA-NED (W only) <ref type="bibr" target="#b10">(He et al., 2013)</ref>: uses a deep neural network with stacked denoising autoencoders (sDA) to encode bag-of-words representation of textual contexts and to di- rectly compare mentions and entities.</p><p>• ARNN (W only) ( <ref type="bibr" target="#b7">Eshel et al., 2017)</ref>: uses an Attention RNN model that computes similar- ity between word and entity embeddings to disambiguate among fixed candidates.</p><p>• Deep Zeroshot (W only): uses the deep ze- roshot architecture similar to <ref type="figure" target="#fig_0">Figure 2</ref>, but uses word contexts (caption) only.</p><p>• (proposed) DZMNED + Deep Levenshtein + InceptionNet with modality attention (W+C+V): is the proposed approach as de- scribed in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>• (proposed) DZMNED + Deep Levenshtein + InceptionNet w/o modality attention (W+C+V): concatenates all the modality vectors instead.</p><p>• (proposed) DZMNED + Deep Levenshtein (W+C): only uses textual context.</p><p>• (proposed) DZMNED + Deep Levenshtein w/o modality attention (W+C): does not use the modality attention module, and instead concatenates word and lexical embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Parameters: We tune the parameters of each model with the following search space (bold in- dicate the choice for our final model): character embeddings dimension: {25, 50, 100, 150, 200, 300}, word embeddings size: {25, 50, 100, 150, 200, 300}, knowledge graph embeddings size: {100, 200, 300}, LSTM hidden states: {50, 100, 150, 200, 300}, and x dimension: {25, 50, 100, 150, 200, 300}. We optimize the parameters with Adagrad (Duchi et al., 2011) with batch size 10, learning rate 0.01, epsilon 10 −8 , and decay 0.1.</p><p>Main Results: <ref type="table">Table 1</ref> shows the Top-1, 3, 5, 10, and 50 candidates retrieval accuracy results on the Snap Captions dataset. We see that the proposed approach significantly outperforms the baselines which use fixed candidates generation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modalities</head><p>Model Candidates Generation Accuracy (%)</p><p>Top-1 Top-3 Top-5 Top-10 Top-50  method. Note that m → e hash list-based meth- ods, which retrieve as candidates the KB entities that appear in the training set of captions only, has upper performance limit at 66.9%, showing the limitance of fixed candidates generation method for unseen entities in social media posts. k-NN methods which retrieve lexical neighbors of men- tion (in an attempt to perform soft normalization on mentions) also do not perform well. Our pro- posed zeroshot approaches, however, do not fixate candidate generation, and instead compares com- bined contextual and lexical similarities among all 1M KB entities, achieving higher upper per- formance limit (Top-50 retrieval accuracy reaches 88.1%). This result indicates that the proposed zeroshot model is capable of predicting for un- seen entities as well. The lexical sub-model can also be interpreted as functioning as soft neural mapping of mention to potential candidates, rather than heuristic matching to fixed candidates. In addition, when visual context is available (W+C+V), the performance generally improves over the textual models (W+C), showing that vi- sual information can provide additional contexts for disambiguation. The modality attention mod- ule also adds performance gain by re-weighting the modalities based on their informativeness.</p><p>Error Analysis: <ref type="table" target="#tab_3">Table 3</ref> shows example cases where incorporation of visual contexts affects dis- ambiguation of mentions in textual captions. For example, polysemous entities such as 'Jordan' in the caption "Taking the new Jordan for a walk" or 'CID' as in "LETS GO CID" are hard to dis- ambiguate due to the limited textual contexts pro- vided, while visual information (e.g. visual tags 'footwear' for Jordan, 'DJ' for CID) provides sim- ilarities to each mention's distributional semantics from other training examples. Mentions unseen at train time ('STEPHHHH', 'murica') often re- sort to lexical neighbors by (W+C), whereas vi- sual contexts can help disambiguate better. A few cases where visual contexts are not helpful include visual tags that are not related to mentions, or do not complement already ambiguous contexts.</p><p>Sensitivity to KB Embeddings Quality: The proposed approach relies its prediction on entity matching in the KB embeddings space, and hence the quality of KB embeddings is crucial for suc- cessful disambiguation. To characterize this as- pect, we provide <ref type="table" target="#tab_1">Table 2</ref> which shows MNED per- formance with varying quality of embeddings as follows: KB embeddings learned from 1M knowl- edge graph entities (same as in the main experi- ments), from 10K subset of entities (less triplets to train with in Eq.3, hence lower quality), and random embeddings (poorest) -while all the other parameters are kept the same. It can be seen that the performance notably drops with lower quality of KB embeddings. When KB embeddings are re- placed by random embeddings, the network effec- tively prevents the contextual zeroshot matching to KB entities and relies only on lexical similarities, achieving the poorest performance.   , and that named entitiy disambigua- tion specifically for short and noisy social media posts are rarely discussed. Note also that most of the previous literature assume the availability of "candidates" or web links for disambiguation via mention-entity pair counts from training set, which is vulnerable to inconsistent surface forms of entities predominant in social media posts. Our model improves upon the state-of-the-art NED models in three very critical ways: (1) in- corporation of visual contexts, (2) addition of the zeroshot learning layer, which allows for disam- biguation of unseen entities during training, and (3) addition of the lexical model that computes lexical similarity entities to correctly recognize in- consistent surface forms of entities.</p><p>Multimodal learning studies learning of a joint model that leverages contextual information from multiple modalities in parallel. Some of the rele- vant multimodal learning task to our MNED sys- tem include the multimodal named entity recog- nition task <ref type="bibr" target="#b14">(Moon et al., 2018)</ref>, which leverages both text and image to classify each token in a sentence to named entity or not. In their work, they employ an entity LSTM that takes as in- put each modality, and a softmax layer that out- puts an entity label at each decoding step. Con- trast to their work, our MNED addresses unique challenges characterized by zeroshot ranking of 1M knowledge-base entities (vs. categorical en- tity types prediction), incorporation of an external knowledge graph, lexical embeddings, etc. An- other is the multimodal machine translation task <ref type="bibr" target="#b6">(Elliott et al., 2015;</ref><ref type="bibr" target="#b19">Specia et al., 2016)</ref>, which takes as input text in source language as well as an accompanying image to output a translated text in target language. These models usually employ a sequence-to-sequence architecture (e.g. target lan- guage decoder takes as input both encoded source language and images) often with traditional atten- tion modules widely used in other image caption- ing systems ( <ref type="bibr" target="#b23">Xu et al., 2015;</ref><ref type="bibr" target="#b20">Sukhbaatar et al., 2015)</ref>. To the best of our knowledge, our approach is the first multimodal learning work at incorporat- ing visual contexts for the NED task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduce a new task called Multimodal Named Entity Disambiguation (MNED), which is applied on short user-generated social media posts that are composed of text and accompanying im- ages. Our proposed MNED model improves upon the state-of-the-art models by 1) extracting visual contexts complementary to textual contexts, 2) by leveraging lexical embeddings into entity match- ing which accounts for various surface forms of entities, removing the need for fixed candidates generation process, and 3) by performing entity matching in the distributed knowledge graph em- beddings space, allowing for matching of unseen mentions and entities by context resolutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The main architecture of our Multimodal NED network. We extract contextual information from an image, surrounding words, and lexical embeddings of a mention. The modality attention module determines weights for modalities, the weighted projections of which produce label embeddings in the same space as knowledge-base (KB) entity embeddings. We predict a final candidate by ranking based on similarities with KB entity knowledge graph embeddings as well as with lexical embeddings.</figDesc><graphic url="image-4.png" coords="3,72.00,63.80,218.26,299.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Deep Levenshtein, which predicts approximate Levenshtein scores between two strings. As a byproduct of this model, the shared Bi-LSTM can produce lexical embeddings purely based on lexical property of character sequences. test phases we frequently encounter new named entities that are unseen in the training data. In order to address this issue, we propose a zeroshot learning approach (Frome et al., 2013) by inducing embeddings obtained from knowledge graphs on KB entities. Knowledge graph label embeddings are learned from known relations among entities within a graph (e.g. 'IS-A', 'LOCATED-AT', etc.), the resulting embeddings of which can group similar entities closer in the same space (e.g. 'pop stars' are in a small cluster, 'people' and 'organizations' clusters are far apart, etc.) (Bordes et al., 2013; Wang et al., 2014; Nickel et al., 2016). Once high-level mapping from contextual information to label embeddings is learned, the knowledgegraph based zeroshot approach can improve the entity linking performance given ambiguous entities unseen in training data. In brief formulation, the model for obtaining embeddings from a knowledge graph (composed of subject-relationobject (s, r, o) triplets) is as follows:</figDesc><graphic url="image-5.png" coords="4,329.10,63.80,174.62,148.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Caption</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>MNED performance (Top-1, 5, 10 accu-
racies) on SnapCaptionsKB with varying qualities 
of KB embeddings. Model: DZMNED (W+C+V) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Error analysis: when do images help NED? Ground-truth (GT) and predictions of our model 
with vision input (W+C+V) and the one without (W+C) for the underlined mention are shown. For 
interpretability, visual tags (label output of InceptionNet) are presented instead of actual feature vectors. 

4 Related Work 

NED task: Most of the previous NED mod-
els leverage local textual information (He et al., 
2013; Eshel et al., 2017) and/or document-wise 
global contexts (Hoffart et al., 2011; Chisholm and 
Hachey, 2015; Pershina et al., 2015; Globerson 
et al., 2016), in addition to other auxiliary con-
texts or priors for disambiguating a mention. Note 
that most of the NED datasets (e.g. TAC KBP 
(Ji et al., 2010), ACE (Bentivogli et al., 2010), 
CoNLL-YAGO (Hoffart et al., 2011), etc.) are 
extracted from standardized documents with web 
links such as Wikipedia (with relatively ample tex-
tual contexts)</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Easy access to the freebase dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Baurle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Buchhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>Haussmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extending english ace 2005 corpus annotation with ground-truth links to wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Giuliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on The Peoples Web Meets NLP: Collaboratively Constructed Semantic Resources</title>
		<meeting>the 2nd Workshop on The Peoples Web Meets NLP: Collaboratively Constructed Semantic Resources</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entity disambiguation with web links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-language image description with neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<idno>abs/1510.04709</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Named entity disambiguation for noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Eshel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuda</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collective entity resolution with multi-focal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ringaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="621" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning entity representation for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the tac 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><forename type="middle">Ellis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Text Analysis Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multimodal named entity recognition for short social media posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Carvalho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<editor>Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A shared task on multimodal machine translation and crosslingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going deeper with convolutions. CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
