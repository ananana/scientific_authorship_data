<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Physics Department</orgName>
								<orgName type="institution">Universidade Federal do Rio Grande do Sul Porto Alegre</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
							<email>{atsalle,avillavicencio}@inf.ufrgs.br, idiart@if.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="419" to="424"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed word representations, or word em- beddings, have been successfully used in many NLP applications <ref type="bibr" target="#b26">(Turian et al., 2010;</ref><ref type="bibr" target="#b7">Collobert et al., 2011;</ref>. Tradition- ally, word representations have been obtained us- ing count-based methods ( <ref type="bibr" target="#b0">Baroni et al., 2014)</ref>, where the co-occurrence matrix is derived directly from corpus counts <ref type="bibr" target="#b15">(Lin, 1998</ref>) or using associ- ation measures like Point-wise Mutual Informa- tion (PMI) <ref type="bibr" target="#b6">(Church and Hanks, 1990)</ref> and Posi- tive PMI (PPMI) ( <ref type="bibr" target="#b3">Bullinaria and Levy, 2007;</ref>.</p><p>Techniques for generating lower-rank represen- tations have also been employed, such as PPMI- SVD ( <ref type="bibr" target="#b13">Levy et al., 2015</ref>) and GloVe ( <ref type="bibr" target="#b22">Pennington et al., 2014</ref>), both achieving state-of-the-art per- formance on a variety of tasks.</p><p>Alternatively, vector-space models can be gen- erated with predictive methods, which gener- ally outperform the count-based methods ( <ref type="bibr" target="#b0">Baroni et al., 2014</ref>), the most notable of which is Skip- gram with Negative Sampling <ref type="bibr">(SGNS, Mikolov et al. (2013b)</ref>), which uses a neural network to generate embeddings. It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences ( .</p><p>In this paper, we present Lexical Vectors (LexVec), a method for factorizing PPMI matri- ces that combines characteristics of all these meth- ods. On the one hand, it uses SGNS window sampling, negative sampling, and stochastic gra- dient descent (SGD) to minimize a loss function that weights frequent co-occurrences heavily but also takes into account negative co-occurrence. However, since PPMI generally outperforms PMI on semantic similarity tasks <ref type="bibr" target="#b3">(Bullinaria and Levy, 2007)</ref>, rather than implicitly factorize a shifted PMI matrix (like SGNS), LexVec explicitly fac- torizes the PPMI matrix.</p><p>This paper is organized as follows: First, we de- scribe PPMI-SVD, GloVe, and SGNS ( §2) before introducing the proposed method, LexVec ( §3), and evaluating it on word similarity and analogy tasks ( §4). We conclude with an analysis of results and discussion of future work.</p><p>We provide source code for the model at https://github.com/alexandres/ lexvec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PPMI-SVD</head><p>Given a word w and a symmetric window of win context words to the left and win to the right, the co-occurrence matrix of elements M wc is defined as the number of times a target word w and the context word c co-occurred in the corpus within the window. The PMI matrix is defined as</p><formula xml:id="formula_0">P M I wc = log M wc M * * M w * M * c<label>(1)</label></formula><p>where '*' represents the summation of the cor- responding index. As this matrix is unbounded in the inferior limit, in most applications it is replaced by its positive definite version, PPMI, where negative values are set to zero. The per- formance of the PPMI matrix on word similarity tasks can be further improved by using context- distribution smoothing ( <ref type="bibr" target="#b13">Levy et al., 2015)</ref> and sub- sampling the corpus ( <ref type="bibr" target="#b18">Mikolov et al., 2013b</ref>). As word embeddings with lower dimensionality may improve efficiency and generalization ( <ref type="bibr" target="#b13">Levy et al., 2015)</ref>, the improved PPMI * matrix can be factor- ized as a product of two lower rank matrices.</p><formula xml:id="formula_1">P P M I * wc W w ˜ W c<label>(2)</label></formula><p>where W w and˜Wand˜ and˜W c are d-dimensional row vectors corresponding to vector embeddings for the target and context words. Using the truncated SVD of size d yields the factorization U ΣT with the low- est possible L 2 error <ref type="bibr" target="#b8">(Eckert and Young, 1936)</ref>. <ref type="bibr" target="#b13">Levy et al. (2015)</ref> recommend using W = U Σ p as the word representations, as suggested by <ref type="bibr" target="#b4">Bullinaria and Levy (2012)</ref>, who borrowed the idea of weighting singular values from the work of Caron <ref type="formula" target="#formula_0">(2001)</ref> on Latent Semantic Analysis. Although the optimal value of p is highly task- dependent <ref type="bibr" target="#b21">( ¨ Osterlund et al., 2015)</ref>, we set p = 0.5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experi- ments ( <ref type="bibr" target="#b13">Levy et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GloVe</head><p>GloVe ( <ref type="bibr" target="#b22">Pennington et al., 2014</ref>) factors the loga- rithm of the co-occurrence matrixˆMmatrixˆ matrixˆM that consid- ers the position of the context words in the win- dow. The loss function for factorization is</p><formula xml:id="formula_2">L GloV e wc = 1 2 f ( ˆ M wc )(W w ˜ W c +b w + ˜ b c −logˆMlogˆ logˆM wc ) 2 (3)</formula><p>where b w and˜band˜and˜b c are bias terms, and f is a weight- ing function defined as</p><formula xml:id="formula_3">f (x) = (x/x max ) β if x &lt; x max 1 otherwise (4)</formula><p>W and˜Wand˜ and˜W are obtained by iterating over all non- zero (w, c) cells in the co-occurrence matrix and minimizing eq. <ref type="formula">(3)</ref> through SGD. The weighting function (in eq. <ref type="formula">(3)</ref>) penalizes more heavily reconstruction error of frequent co- occurrences, improving on PPMI-SVD's L 2 loss, which weights all reconstruction errors equally. However, as it does not penalize reconstruction er- rors for pairs with zero counts in the co-occurrence matrix, no effort is made to scatter the vectors for these pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Skip-gram with Negative Sampling</head><p>(SGNS) SGNS ( <ref type="bibr" target="#b18">Mikolov et al., 2013b</ref>) trains a neural net- work to predict the probability of observing a con- text word c given a target word w, sliding a sym- metric window over a subsampled training corpus with the window size being sampled uniformly from the range <ref type="bibr">[1, win]</ref>. Each observed (w, c) pair is combined with k randomly sampled noise pairs (w, w i ) and used to calculate the loss function</p><formula xml:id="formula_4">L SGN S wc = log σ(W w ˜ W c )+ k i=1 E w i ∼Pn(w) log σ(−W w ˜ W w i )<label>(5)</label></formula><p>where P n (w) is the distribution from which noise words w i are sampled. <ref type="bibr">1</ref> We refer to this routine which SGNS uses for selecting (w, c) pairs by sliding a context window over the corpus for loss calculation and SGD as window sampling. SGNS is implicitly performing the weighted factorization of a shifted PMI matrix ( ). Window sampling ensures the factorization weights frequent co-occurrences heavily, but also takes into account negative co- occurrences, thanks to negative sampling.</p><formula xml:id="formula_5">L LexV ec wc = 1 2 (Ww˜WcWw˜ Ww˜Wc − P P M I * wc ) 2<label>(6)</label></formula><formula xml:id="formula_6">L LexV ec w = 1 2 k i=1 E w i ∼Pn(w) (Ww˜WwWw˜ Ww˜Ww i − P P M I * ww i ) 2<label>(7)</label></formula><p>We minimize eqs. <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_6">(7)</ref>  where #(w) is the number of times w is observed in the subsampled corpus.</p><p>If a pair (w, c) co-occurs frequently, #(w, c) will weigh heavily in both eqs. <ref type="formula">(8)</ref> and <ref type="formula">(9)</ref>, giving the desired weighting for frequent co-occurrences. The noise term, on the other hand, has corrections proportional to #(w) and #(w i ), for each pair (w, w i ). It produces corrections in pairs that due to frequency should be in the corpus but are not observed, therefore accounting automatically for negative co-occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Materials</head><p>All models were trained on a dump of Wikipedia from June 2015, split into sentences, with punc- tuation removed, numbers converted to words, and lower-cased. Words with less than 100 counts were removed, resulting in a vocabulary of 302,203 words. All models generate embeddings of 300 dimensions.</p><p>The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of α = 3/4 suggested in ( <ref type="bibr" target="#b13">Levy et al., 2015)</ref> and an unweighted window of size 2. A dirty sub- sampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10 −5 (Mikolov et al., 2013b). <ref type="bibr">2</ref> Additionally, SGNS uses 5 negative samples ( <ref type="bibr" target="#b18">Mikolov et al., 2013b</ref>), a window of size 10 ( <ref type="bibr" target="#b13">Levy et al., 2015)</ref>, for 5 iterations with initial learning rate set to the default 0.025. GloVe is run with a window of size 10, x max = 100, β = 3/4, for 50 iterations and initial learning rate of 0.05 ( <ref type="bibr" target="#b22">Pennington et al., 2014</ref>).</p><p>In LexVec two window sampling alternatives are compared: W S P P M I , which keeps the same fixed size win = 2 as used to create the P P M I * matrix; or W S SGN S , which adopts identical SGNS settings (win = 10 with size randomiza- tion). We run LexVec for 5 iterations over the training corpus.</p><p>All methods generate both word and context matrices (W and˜Wand˜ and˜W ): W is used for SGNS, PPMI- SVD and W + ˜ W for GloVe (following <ref type="bibr" target="#b13">Levy et al. (2015)</ref>, and W and W + ˜ W for LexVec. For evaluation, we use standard word simi- larity and analogy tasks ( <ref type="bibr" target="#b18">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b22">Pennington et al., 2014;</ref><ref type="bibr" target="#b13">Levy et al., 2015)</ref>. We examine, in particular, if LexVec weighted PPMI * factorization outperforms SVD, GloVe (weighted factorization of logˆMlogˆ logˆM ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini- batch approaches.</p><p>Word similarity tasks are: 3 WS-353 Similar- ity (WSim) and Relatedness (WRel) ( <ref type="bibr" target="#b9">Finkelstein et al., 2001</ref>), MEN ( <ref type="bibr" target="#b1">Bruni et al., 2012</ref>), MTurk (Radinsky et al., 2011), RW ( <ref type="bibr" target="#b16">Luong et al., 2013)</ref>, SimLex-999 ( <ref type="bibr" target="#b10">Hill et al., 2015)</ref>, MC <ref type="bibr" target="#b20">(Miller and Charles, 1991)</ref>, RG <ref type="bibr" target="#b24">(Rubenstein and Goodenough, 1965)</ref>, and SCWS <ref type="figure">(Huang et al., 2012)</ref>, calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>) and MSR syntactic analogy dataset ( <ref type="bibr" target="#b19">Mikolov et al., 2013c</ref>), using 3CosAdd and 3CosM ul ( ).   <ref type="table">Table 2</ref>: Results on word analogy tasks, given as percent accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><note type="other">WSim WRel MEN MTurk RW SimLex-999 MC RG SCWS PPMI-</note><note type="other">LexVec + MB + W SSGNS + (W + ˜ W ) .768 .675 .755 .654 .448 .312 .824 .827 .626 LexVec + St. + W SSGNS + (W + ˜ W ) .775 .673 .762 .654 .468 .339 .838 .848 .628 LexVec + MB + W SSGNS + W .745 .640 .734 .645 .447 .311 .814 .802 .624 LexVec + St. + W SSGNS + W .740 .628 .728 .640 .459 .339 .821 .818 .638</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Results for word similarity and for the analogy tasks are in tables 1 and 2, respectively. Com- pared with PPMI-SVD, LexVec performs better in all tasks. As they factorize the same P P M I * matrix, it is the loss weighting from window sam- pling that is an improvement over L 2 loss. As ex- pected, due to PPMI, LexVec performs better than SGNS in several word similarity tasks, but in ad- dition it also does so on the semantic analogy task, nearly approaching GloVe. LexVec generally out- performs GloVe on word similarity tasks, possibly due to the factorization of the PPMI matrix and to window sampling's weighting of negative co- occurrences.</p><p>We believe LexVec fares well on semantic analogies because its vector-space does a good job of preserving semantics, as evidenced by its per- formance on word similarity tasks. We believe the poor syntactic performance is a result of the PPMI measure. PPMI-SVD also struggled with syntac- tic analogies more than any other task. <ref type="bibr" target="#b13">Levy et al. (2015)</ref> obtained similar results, and suggest that using positional contexts as done by  might help in recovering syntactic analo- gies.</p><p>In terms of configurations, WS SGN S performed marginally better than WS P P M I . We hypothe- size it is simply because of the additional com- putation. While W and (W + ˜ W ) are roughly equivalent on word similarity tasks, W is bet- ter for analogies. This is inline with results for PPMI-SVD and SGNS models ( <ref type="bibr" target="#b13">Levy et al., 2015)</ref>. Both mini-batch and stochastic approaches result in similar scores for all tasks. For the same pa- rameter k of negative samples, the mini-batch ap- proach uses 2 * win W S P P M I times more negative samples than stochastic when using W S P P M I , and win W S SGN S times more samples when us- ing W S SGN S . Therefore, the stochastic approach is more computationally efficient while delivering similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we introduced LexVec, a method for low-rank, weighted factorization of the PPMI matrix that generates distributed word represen- tations, favoring low reconstruction error on fre- quent co-occurrences, whilst accounting for neg- ative co-occurrences as well. This is in con- trast with PPMI-SVD, which does no weight- ing, and GloVe, which only considers positive co-occurrences. Finally, its PPMI factorization seems to better capture semantics when compared to the shifted PMI factorization of SGNS. As a result, it outperforms PPMI-SVD and SGNS in a variety of word similarity and semantic analogy tasks, and generally outperforms GloVe on similarity tasks.</p><p>Future work will examine the use of positional contexts for improving performance on syntactic analogy tasks. Moreover, we will explore further the hyper-parameter space to find globally optimal values for LexVec, and will experiment with the factorization of other matrices for developing al- ternative word representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>using two alternative approaches:</head><label></label><figDesc>Mini-Batch (MB): This variant executes gradient descent in exactly the same way as SGNS. Every time a pair (w, c) is observed by window sampling and pairs (w, w 1...k ) drawn by negative sampling, W w , ˜ W c , and˜Wand˜ and˜W w 1...k are updated by gradient de- scent on the sum of eq.(6) and eq.(7). The global loss for this approach is L LexV ec = (w,c) #(w, c) (L LexV ec wc + L LexV ec w ) (8) where #(w, c) is the number of times (w, c) is ob- served in the subsampled corpus. Stochastic (St): Every context window is ex- tended with k negative samples w 1...k . Iterative gradient descent of eq. (6) is then run on pairs (w, c j ), for j = 1, .., 2 * win and (w, c i ), j = 1, .., k for each window. The global loss for this approach is L LexV ec = (w,c) #(w, c)L LexV ec wc + w #(w)L LexV ec w (9)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Spearman rank correlation on word similarity tasks. 

Method 
GSem 
3CosAdd / 3CosMul 
GSyn 
3CosAdd / 3CosMul 
MSR 
3CosAdd / 3CosMul 
PPMI-SVD 
.460 / .498 
.445 / .455 
.303 / .313 
GloVe 
.818 / .813 
.630 / .626 
.539 / .547 
SGNS 
.773 / .777 
.642 / .644 
.481 / .505 
LexVec + MB + W SP P M I + (W + ˜ 
W ) 
.775 / .792 
.520 / .539 
.371 / .413 
LexVec + St + W SP P M I + (W + ˜ 
W ) 
.794 / .807 
.543 / .555 
.378 / .408 
LexVec + MB + W SP P M I + W 
.800 / .805 
.584 / .597 
.421 / .457 
LexVec + St. + W SP P M I + W 
.787 / .782 
.597 / .613 
.445 / .475 
LexVec + MB + W SSGNS + (W + ˜ 
W ) 
.762 / .785 
.520 / .534 
.349 / .386 
LexVec + St. + W SSGNS + (W + ˜ 
W ) 
.792 / .809 
.536 / .553 
.362 / .396 
LexVec + MB + W SSGNS + W 
.798 / .807 
.573 / .580 
.399 / .435 
LexVec + St. + W SSGNS + W 
.779 / .778 
.600 / .614 
.434 / .463 

</table></figure>

			<note place="foot" n="3"> LexVec LexVec is based on the idea of factorizing the PPMI matrix using a reconstruction loss function that does not weight all errors equally, unlike SVD, but instead penalizes errors of frequent co-occurrences more heavily, while still treating negative co-occurrences, unlike GloVe. Moreover, given that using PPMI results in better performance than PMI on semantic tasks, we propose keeping the SGNS weighting scheme by using window sampling and negative sampling, but explicitly factorizing the PPMI matrix rather than implicitly factorizing the shifted PMI matrix. The LexVec loss function has two terms 1 Following Mikolov et al. (2013b) it is the unigram distribution raised to the 3/4 power.</note>

			<note place="foot" n="2"> Words with unigram relative frequency f &gt; t are discarded from the training corpus with probability pw = 1 − t/f. 3 http://www.cs.cmu.edu/ mfaruqui/suite.html</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th</title>
		<meeting>the 50th</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word cooccurrence statistics: A computational study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word cooccurrence statistics: stop-lists, stemming, and svd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph P</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="890" to="907" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Experiments with lsa scoring: Optimal rank and basis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM Computational Information Retrieval Workshop</title>
		<meeting>the SIAM Computational Information Retrieval Workshop</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="157" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramatgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of the 36th and 17th</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorization of latent variables in distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvid¨osterlundarvid¨</forename><surname>Arvid¨osterlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David¨odlingdavid¨</forename><surname>David¨odling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="227" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A word at a time: computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
