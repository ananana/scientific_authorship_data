<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Movshovitz-Attias</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Machine Learning Department</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Machine Learning Department</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KB-LDA: Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1449" to="1459"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many existing knowledge bases (KBs), including Freebase, Yago, and NELL, rely on a fixed ontology, given as an input to the system, which defines the data to be cataloged in the KB, i.e., a hierarchy of categories and relations between them. The system then extracts facts that match the predefined ontology. We propose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identifies facts from the corpus that match the learned structure. Our approach combines mixed membership stochastic block models and topic models to infer a structure by jointly mod-eling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software domain , and evaluate the accuracy of the various components of the learned ontology.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge base (KB) construction methods can be broadly categorized along several dimensions. One dimension is ontology-guided construction, where the list of categories and relations that de- fine the schema of the KB are explicit, versus open IE methods, where they are not. Another dimen- sion is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchi- cal, in that they contain mainly concept types, su- pertypes and instances, while other KBs contain many types of relationships between concepts. Hi- erarchical knowledge can be learned by methods including distributional clustering ( <ref type="bibr" target="#b23">Pereira et al., 1993)</ref>, as well as Hearst patterns <ref type="bibr" target="#b13">(Hearst, 1992)</ref> and similar techniques ( <ref type="bibr" target="#b25">Snow et al., 2006</ref>). Re- verb <ref type="bibr" target="#b10">(Fader et al., 2011</ref>) and <ref type="bibr">TextRunner (Yates et al., 2007</ref>) are open methods for learning multi- relation KBs. Finally, NELL <ref type="bibr" target="#b4">(Carlson et al., 2010;</ref><ref type="bibr" target="#b16">Mitchell et al., 2015)</ref>, FreeBase (Google, 2011) and <ref type="bibr">Yago (Suchanek et al., 2007;</ref><ref type="bibr" target="#b15">Hoffart et al., 2013</ref>) are ontology-guided methods for extracting KBs containing both hierarchies and relations.</p><p>One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized in a meaningful hierarchical structure, often pro- viding seeds, i.e., representative examples for each category, and finally, defining inter-category rela- tions. This process is often done manually <ref type="bibr" target="#b4">(Carlson et al., 2010</ref>) leading to a rigid set of categories. Redesigning a new ontology for a specialized do- main represents an additional challenge as it re- quires extensive knowledge of the domain.</p><p>In this paper, we propose an unsupervised model that learns a latent hierarchical structure of categories from an input corpus, learns latent semantic relations between categories, and also identifies facts from the corpus that match the learned structure. In other words, the model learns both the schema for a KB, and a set of facts that are related to that schema, thus combining the processes of KB population and ontology con- struction. The intent is to build systems that ex- tract facts which can be interpreted relative to a meaningful ontology without requiring the effort of manual ontology construction.</p><p>The input to the learning method is a cor- pus of documents, plus two sets of resources ex- tracted from the same corpus: a set of hypernym- hyponym pairs (e.g., "animal", "horse") extracted using Hearst patterns, and a set of subject-verb- object triples (e.g., "horse", "eats", "hay") ex- tracted from parsed sentences. These resources are analogous to the output of open IE systems for hierarchies and relations, and as we demonstrate, our method can be used to highlight domain- specific data from open IE repositories.</p><p>Our approach combines mixed membership stochastic block models and topic models to in- fer a structure by jointly modeling text docu- ments, and links that indicate hierarchy and rela- tion among the entities mentioned in the text. Joint modeling allows information on topics of nouns (referred to as instances) and verbs (referred to as relations) to be shared between text documents and an ontological structure, resulting in a set of compelling topics. This model offers a complete solution for KB construction based on an input corpus, and we therefore name it KB-LDA.</p><p>We additionally propose a method for recover- ing meaningful names for concepts in the learned hierarchy. These are equivalent to category names in other KBs, however, following our method we extract from the data a set of potential alterna- tive concepts describing each category, including probabilities for their strength of association.</p><p>To show the effectiveness of our method, we ap- ply the model to a dataset of Web based documents from the software domain, and learn a software KB. This is an example of a specialized domain in which, to our knowledge, no broad-coverage on- tology exists. We evaluate the model on the in- duced categories, relations, and facts, and we com- pare the proposed categories with an independent set of human-provided labels for documents. Fi- nally, we use KB-LDA to retrieve domain-specific relations from an open IE resource. We provide the learned software KB as supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KB-LDA</head><p>Modeling latent sets of entities from observed in- teractions among them is a well researched task, often encountered in social network analysis for the purpose of identifying specialized communi- ties in the network. Mixed Membership Stochas- tic Blockmodels ( <ref type="bibr" target="#b0">Airoldi et al., 2009;</ref><ref type="bibr" target="#b22">Parkkinen et al., 2009</ref>) model entities as graph nodes with pairwise relations drawn from latent blocks with mixed membership. A related approach is taken by topic models such as LDA (Latent Dirichlet Allocation; ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref>), which model doc- uments as generated by a mixture of latent topics, and words in the documents as generated by topic- specific word distributions. The KB-LDA model combines the two approaches. It models links be- πO -multinomial over ontology topic pairs, with Dirichlet prior αO πR -multinomial over relation topic tuples, with Dirichlet prior αR θ d -topic multinomial for document d, with Dirichlet prior αD σ k -multinomial over instances for topic k, with Dirichlet prior γI δ k -multinomial over relations for topic k , with Dirichlet prior γR CIi = Ci, Ii -i-th ontological assignment pair SVOj = Sj, Oj, Vj -j-th relation assignment tuple</p><formula xml:id="formula_0">z CI i = zC i , zI i -topic pair chosen for example Ci, Ii z SV O j = zS j , zO j , zV j -topic tuple chosen for example Sj, Oj, Vj z D E 1 , z D E 2</formula><p>-topic chosen for instance entity E1, or relation entity E2, respectively, in a document n I z,i -number of times instance i is observed under topic</p><formula xml:id="formula_1">z (in either z D , z CI or z SV O ) n R z,r -number of times relation r is observed under topic z (in either z D or z SV O ) n O</formula><p>zc,z i -count of ontological pairs assigned the topic pair zc, zi (in z CI ) n R zs,zo,zv -count of relation tuples assigned the topic tuple zs, zo, zv (in z SV O ) <ref type="table">Table 1</ref>: KB-LDA notation.</p><p>tween tuples of two or three entities using stochas- tic block models, and these are additionally influ- enced by latent topic assignments of the entities in a document corpus.</p><p>In the KB-LDA model, shown as a plate dia- gram in <ref type="figure">Figure 1</ref> with notation in <ref type="table">Table 1</ref>, informa- tion is shared between three components, through common latent topics over noun and verb entities. The Ontology component (upper right) models hierarchical links between Concept-Instance (CI) entity pairs. The Relations component (left) mod- els links between Subject-Verb-Object (SVO) en- tity triples, where the subject and object are nouns and the verb represents a relation between them. Finally, the Documents component (lower left) is a link-LDA model ( <ref type="bibr" target="#b9">Erosheva et al., 2004</ref>) of text documents containing a combination of noun and verb entity types. In this formulation, distribu- tions over noun and verb entities that are related according to hierarchical or relational constraints, are linked with a text model via shared parameters.</p><p>In more detail, the Documents component pro- vides the context in which noun and verb entities are being used in text. It is modeled as an exten- sion of LDA, viewing documents as sets of "bags of words", where in this case, each bag contains either noun or verb entities. Each entity type has a topic-wise multinomial distribution over the set of entities in the vocabulary of that type.</p><formula xml:id="formula_2">S j O j V j z Sj z Oj z Vj α R π R N R Relations γ I σ k K γ R δ k K α O π O z Ci z Ii C i I i N O Ontology α D θ d z E l1 z E l2 E l1 E l2 N d,I N d,R N D Documents Figure 1: Plate Diagram of KB-LDA.</formula><p>The Ontology component is a generative model representing hierarchal links between pairs of nouns. The examples for this component are ex- tracted using a small collection of Hearst patterns indicating concept-instance or concept-concept links, including, 'X such as Y', and 'X including Y'. For example, the sentence "websites such as StackOverflow" indicates that Stackoverflow is a type of website, leading to the extracted noun pair websites, StackOverflow. We refer to the exam- ples extracted using these hierarchical patterns as concept-instance pairs, and to the individual enti- ties as instances.</p><p>The pairs have an underlying block structure derived from a sparse block model <ref type="bibr" target="#b22">(Parkkinen et al., 2009</ref>). They are generated by topic specific instance distributions conditioned on topic pair edges, which are defined by the multinomial π O over the Cartesian product of the noun topic set with itself. The individual instances, therefore, have a mixed membership in topics. Note that we allow for a concept and instance to be drawn from different noun topics, defined by σ. For ex- ample, we may learn a topic highlighting concept tokens like 'websites', 'platforms', 'applications'. Another topic can highlight instances shared by these concepts, such as, 'stackoverflow', 'google', and 'facebook'. Finally, the observation that the former topic frequently contains concepts of in- stances from the latter topic, is encoded in the multinomial distribution π O . From this we infer that the former topic should be placed higher in the induced hierarchy.</p><p>Similarly, the Relations component represents relational links between a noun subject, a verb and a noun object. The examples for this component Let K be the number of target latent topics. 1. Generate topics: For topic k ∈ 1, . . . , K, sample:</p><formula xml:id="formula_3">• σ k ∼ Dirichlet(γ I ), the per-topic instance distribution • δ k ∼ Dirichlet(γ R ), the per-topic relation distribution 2. Generate ontology: Sample π O ∼ Dirichlet(α O ), the instance topic pair distribution. • For each concept-instance pair CI i , i ∈ 1, . . . , N O : -Sample topic pair z CI i ∼ Multinomial(π O ) -Sample instances C i ∼ Multinomial(σ z C i ), I i ∼ Multinomial(σ z I i ), then CI i = C i , I i 3. Generate relations: Sample π R ∼ Dirichlet(α R ), the relation topic tuple distribution. • For each tuple SVO j , j ∈ 1, . . . , N R : -Sample topic tuple z SV O j ∼ Multinomial(π R ) -Sample instances, S j ∼ Multinomial(σ z S j ), O j ∼ Multinomial(σ z O j )</formula><p>, and sample a relation</p><formula xml:id="formula_4">V j ∼ Multinomial(δ z V j ) 4. Generate documents: For document d ∈ 1, . . . , D: • Sample θ d ∼ Dirichlet(α D ), the topic mixing distri- bution for document d.</formula><p>• For every noun entity (E l1 ) and verb entity <ref type="bibr">(E l2</ref>  <ref type="table">Table 2</ref>: KB-LDA generative process.</p><formula xml:id="formula_5">), l1 ∈ 1, . . . , N d,I , l2 ∈ 1, . . . , N d,R : -Sample topics z E l1 , z E l2 ∼ Multinomial(θ d ) -Sample entities E l1 ∼ Multinomial(σ z E l1 ) and E l2 ∼ Multinomial(δ z E l2 )</formula><p>are extracted from SVO patterns found in the doc- ument corpus, following <ref type="bibr" target="#b28">Talukdar et al. (2012)</ref>. An extracted example looks like: websites, ex- ecute, javascript. Subject and object topics are drawn from the noun topics (σ), while the verb topics is drawn from the verb topics, defined by δ. The multinomial π R encodes the interaction of noun and verb topics based on the extracted rela- tional links, and it is defined over the Cartesian product of the noun topic set with itself and with the verb topic set. The generative process of KB-LDA is de- scribed in <ref type="table">Table 2</ref>. Given the hyperparameters (α O , α R , α D , γ I , γ R ), the joint distribution over CI pairs, SVO tuples, documents, topics and topic assignments is given by</p><formula xml:id="formula_6">p(π O , π R , σ, δ, CI, z CI , SVO, z SV O , θ, E, z D | α O , α R , α D , γ I , γ R ) = K k=1 Dir(σ k |γ I ) × K k =1 Dir(δ k |γ R )× (1) Dir(π O |α O ) N O i=1 π z C i ,z I i O σ C i z C i σ I i z I i × Dir(π R |α R ) N R j=1 π z S j ,z O j ,z V j R σ S j z S j σ O j z O j δ V j z V j × N D d=1 Dir(θ d |α D ) N d,I l1=1 θ z D E l1 d σ E l1 z D E l1 N d,R l2=1 θ z D E l2 d δ E l2 z D E l2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inference in KB-LDA</head><p>Exact inference is intractable in the KB-LDA model. We use a collapsed Gibbs sampler <ref type="bibr" target="#b12">(Griffiths and Steyvers, 2004</ref>) to perform approximate inference in order to query the topic distributions and assignments. It samples a latent topic pair for a CI pair in the corpus conditioned on the assign- ments to all other CI pairs, SVO tuples, and docu- ment entities, using the following expression, after collapsing π O :</p><formula xml:id="formula_7">ˆ p(z CI i |CI i , z CI ¬i , z SV O , z D , CI ¬i , α O , γ I ) (2) ∝ n O¬i z CI i + α O × (n I¬i z C i ,C i + γ I )(n I¬i z I i ,I i + γ I ) ( C n I¬i z C i ,C + T I γ I )( I n I¬i z I i ,I + T I γ I )</formula><p>where counts of observations from the training set are noted by n (see <ref type="table">Table 1</ref>), and T I is the number of instance entities (size of noun vocabulary). We similarly sample topics for each SVO tuple conditioned on the assignments to all other tuples, CI pairs and document entities, using the follow- ing expression, after collapsing π R :</p><formula xml:id="formula_8">ˆ p(z SV O j |SVOj, z SV O ¬j , z CI , z D , SVO¬j, αR, γI , γR)<label>(3)</label></formula><formula xml:id="formula_9">∝ n R¬j z SV O j + αR × (n I¬j z S j ,S j + γI )(n I¬j z O j ,O j + γI )(n R¬j z V j ,V j + γR) ( I n I¬j z S i ,I +TI γI )( I n I¬j z O i ,I +TI γI )( V n R¬j z V j ,V +TRγR)</formula><p>We sample a latent topic for an entity mention in a document from the text corpus conditioned on the assignments to all other entity mentions af- ter collapsing θ d . The following expression shows topic sampling for a noun entity in a document:</p><formula xml:id="formula_10">ˆ p(z E l1 |E, CI, SVO, z D , z CI , z SV O , α D , γ I ) (4) ∝ (n ¬l1 d,z + α D ) n I¬l1 z E l1 ,E l1 + γ I E l1 n I¬l1 z E l1 ,E l1 + T I γ I</formula><p>The per-topic multinomial parameters and topic distributions of CI pairs, SVO tuples and docu- ments can be recovered with MLE estimates using their observation counts:</p><formula xml:id="formula_11">ˆ σ I k = n I k,I + γ I I n I k,I + T I γ I , ˆ δ R k = n R k,R + γ R R n R k,R + T R γ R ˆ θ z d = n z,d + α D z n z ,d + Kα D ˆ π z C ,z I O = n O z C ,z I + α O z C ,z I n O z C ,z I + K 2 · α O ˆ π z S ,z O ,z V R = n R z S ,z O ,z V + α R z S ,z O ,z V n R z S ,z O ,z V + K 3 · α R</formula><p>Using the KB-LDA model we can describe the latent topic hierarchy underlying the input cor- pus. We consider the multinomial of the Ontology component, π O , as an adjacency matrix describ- ing a network where the nodes are instance topics and edges indicate a hypernym-to-hyponym rela- tion. By extracting the maximum spanning tree over this adjacency matrix, we recover a hierarchy over the input data. We recover relations among instance topics by extracting from the Relations multinomial, π R , the set of most probable tuples of a subject topic, verb topic, object topic.</p><p>Our model is implemented using a fast, parallel approximation of collapsed Gibbs sampling, fol- lowing <ref type="bibr" target="#b20">Newman et al. (2009)</ref>. In each sampling iteration, topics are sampled locally on a subset of the training examples. At the end of each iteration, data from worker threads is joined and model pa- rameters are updated with complete information. In the next iteration, thread-local sampling starts with complete topic assignment information from the previous iteration. In each thread, the process can be viewed as a reordering of the input exam- ples, where the examples sampled in that thread are viewed first. It has been shown that parallel ap- proaches considerably speed up iterative inference methods such as collapsed Gibbs sampling, result- ing in test data log probabilities indistinguishable from those obtained using serial methods <ref type="bibr" target="#b24">(Porteous et al., 2008;</ref><ref type="bibr" target="#b20">Newman et al., 2009)</ref></p><note type="other">. A paral- lel approach is especially important when training the KB-LDA model due to the large dimensions of the multinomials of the Ontology and Relations components (K 2 and K 3 , respectively for a model with K topics). We train KB-LDA over 2000 iter- ations, more than what has traditionally been used for collapsed Gibbs samplers.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data-driven discovery of topic concepts</head><p>The KB-LDA model described above clusters noun entities into sets of instance topics, and re- covers a latent hierarchical structure among these topics. Each instance topic can be described by a multinomial distribution of the underlying nouns. It is often more intuitive, however, to refer to a topic containing a set of high probability nouns by a name, or category, just as traditional ontologies describe hierarchies over categories.</p><p>Our model is trained over nouns that originate from concept-instance example pairs (used to train the Ontology component). We describe a method for selecting a category name for a topic, based on concepts that best represent high probability nouns of the topic in the concept-instance examples.</p><p>We calculate the probability that a concept noun c describes the set of instances I that have been assigned the topic z using</p><formula xml:id="formula_12">p(c, z|I) ∝ p(I|c, z) * p(c, z)<label>(5)</label></formula><formula xml:id="formula_13">= p(I|c, z) * p(z|c) * p(c) Let rep(c, z) = i:C i =c n I z,I i</formula><p>describe how well concept c represents topic z according to the as- signments of instances with concept c to the topic. Then,</p><formula xml:id="formula_14">p(z|c) = rep(c, z) z rep(c, z )<label>(6)</label></formula><p>The concept prior, p(c), is based on the relative weight of instances with concept c in the concept- instance example set, and is an indicator of the generality of a concept:</p><formula xml:id="formula_15">p(c) = i:C i =c w c,I i c i:C i =c w c ,I i<label>(7)</label></formula><p>where w C,I is the number of occurrences of concept-instance pair C, I in the corpus.</p><p>Finally, p(I|c, z) measures how specific are the topic instances to the concept c,</p><formula xml:id="formula_16">p(I|c, z) = i:I i ∈I,C i =c w c,I i i:C i =c w c,I i Z<label>(8)</label></formula><p>where I is the set of training instances assigned with topic z, and Z is a normalizer over all con- cepts and topics. Following this method we extract concepts that have a high probability p(c, z|I) with respect to a topic z. These can be thought of as equivalent to the single, fixed, category name provided by tra- ditional KB ontologies; however, here we extract from the data a set of potential alternative noun phrases describing each topic, including a proba- bility for the strength of this association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>We evaluate the KB-LDA model on a corpus of 5.5M documents from the software domain; thereby we are using the model to construct a soft- ware domain knowledge base. Our evaluation ex- plores the following questions:</p><p>• Can KB-LDA learn categories, relations, a hierarchy and topic concepts with high pre- cision? • How well do KB-LDA topics correspond with human-provided document labels? • Is KB-LDA useful in extracting facts from existing open IE resources?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We use data from the Q&amp;A website StackOver- flow 1 where users ask and answer technical ques- tions about software development, tools, algo- rithms, etc'. We extracted 562K concept-instance example pairs from the data, and kept the 17K ex- amples appearing at least twice. Noun phrases in these examples make up our Instance Dictio- nary. Out of 6.8M SVO examples found in the data we keep 37K in which the subject and ob- ject are in the Instance Dictionary, and the exam- ple appears at least twice in the corpus. The verbs in these SVOs make up our Relation Dictionary. Finally, we consider as documents the 5.5M ques- tions from StackOverflow with all their answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluating the learned KB precision</head><p>In this section we evaluate the direct output of a model trained with 50 topics: the extracted in-  stance topics, topic hierarchy, relations among topics and extracted topic concepts. In each of the experiments below, we extract facts based on one of the learned components and evaluate each fact based on annotations from human judges: two experts and three non-expert users, collected us- ing Mechanical Turk, that were pre-tested on a basic familiarity with concepts from the software domain, such as programming languages, version control systems, and databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Precision of Instance Topics</head><p>We measure the coherence of instance topics us- ing an approach called word intrusion ). We extract the top 30 instance tokens of a topic ranked by the instance topic multinomial σ. We present to workers tokens 1-5,6-10,. . . ,26- 30, where each 5 tokens are randomly ordered and augmented with an extra token that is ranked low for the topic, (the intruder). We ask workers to select all tokens that do not belong in the group (and at least one). We define the topic Match Pre- cision as the fraction of questions for which the reviewer identified the correct intruder (out of 6 questions per topic), and the topic Group Precision as the fraction of correct tokens (those not selected as not belonging in the group). Thus Match Pre- cision measures how well labelers understand the topic, and Group Precision measures what fraction of words appeared relevant to the topic. <ref type="figure" target="#fig_0">Figure 2</ref> shows the average Match and Group precision over the top tokens of all 50 topics learned with the model, as evaluated by expert and non-expert workers. Both groups find the intruder token in over 75% of questions. In the more subtle task of validating each topic token (Group preci- sion) we see a greater variance among the two la- beler groups. This highlights the difficulty of eval- uating domain specific facts with non-expert users. <ref type="table">Table 3</ref> displays the top 20 instance topics learned with KB-LDA, ranked by expert Group precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Precision of Topic Concepts</head><p>We assess the precision of the top 5 concept names proposed for instance topics, following the method presented in Section 2.2. Top concepts for a sub- set of topics are shown in <ref type="table">Table 3</ref>. For each topic, we present to the user a hypernym-hyponym pat- tern of the topic based on the top concepts and top instances of the topic. As an example, if the top 5 instances of a topic are ie, firefox, chrome, buttons, safari and the top 5 concepts for this topic are web browsers, web browser, browser, ie, chrome, the pattern presented to workers is</p><p>• Workers were asked to match at least 3 instances to a proposed concept name. In addition, the same assessment was applied for each topic using ran- domly sampled concepts. We present in <ref type="table">Table 4</ref> the number and precision of patterns based on ex- tracted concepts (Concepts) and random concepts (Random), that were labeled by 1, 2 or 3 workers, as well as the average results among experts. We achieve nearly 90% precision according to expert labeling, however we do not observe large agree- ment among non-expert labelers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Precision of Relations</head><p>To assess the precision of the relations learned in the KB-LDA model, we extract the top 100 rela- tions learned according to their probability in the relation multinomial π R . Relation patterns were presented to workers as sets of the top subject- verb-object tokens of the respective topics in the relation. An example relation is • Object words: [function, method, class, object, query] and workers are asked to state whether the pat- tern indicates a valid relation or not, by check- ing whether a reasonable number of combinations of subject-verb-object triples extracted from each of the relation groups can produce valid relations. <ref type="table">Top 10 Topic Tokens   table, key  table, query, database, sql, column, data, tables, mysql, index, columns  properties, css</ref> image, code, images, problem, point, color, data, size, screen, points credentials, user information name, images, id, number, text, password, address, strings, files, string page, content page, html, code, file, image, javascript, browser, http, jquery, js orm tools, orm tool tomcat, hibernate, server, boost, apache, spring, mongodb, framework, nhibernate, png clients, apps app, application, http, android, device, phone, code, api, iphone, google applications, systems devices, systems, applications, services, platforms, tools, sites, apps, system, service systems, platforms google, windows, linux, facebook, git, ant, database, gmail, android, so limits, limit memory, time, thread, code, threads, process, file, program, data, object <ref type="table">data, table  query, table, data, list, example, number, results, search, database, rows  type, value</ref> code, function, value, type, pointer, array, memory, compiler, example, string table, request data, information, types, properties, details, fields, values, content, resources, attributes dependencies, jar file libraries, library, framework, frameworks, formats, format, database, databases, tools, server type, object value, focus, place, property, method, reference, interface, effect, pointer, data kinds, code languages, language, features, objects, functions, methods, code, operations, structures, types element, elements button, form, link, item, file, mouse, image, value, option, row javascript libraries, javascript framework jquery, mysql, http, json, xml, library, html, sqlite, asp, php process, operating system server, client, connection, data, http, socket, message, request, port, service folder, files file, files, directory, folder, path, code, name, resources, project, folders value, array array, list, value, values, number, string, code, elements, loop, object <ref type="table">Table 3</ref>: Top 20 instance topics learned with KB-LDA. For each topic we show the top 2 concepts recovered for the topic, and top 10 tokens. In italics are words marked as out-of-topic by expert labelers.  <ref type="table">Table 4</ref>: Precision of topic concepts, relations, and subsumptions. For items extracted from the model (KB-LDA), and randomly (Random), we show the number of items marked as correct, and precision in parentheses (p), as labeled by 1, 2, or 3 non-expert workers, and the average precision by experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top 2 Topic Concepts</head><p>We present in <ref type="table">Table 4</ref> the number and precision of patterns based on the top 100 relations (Relations) and 100 random relations (Random), that were la- beled by 1, 2 or 3 workers, and the average results among experts. We achieve 80% precision accord- ing to experts, and only 18% on random relations. We observe similar agreement among expert and non-expert workers as in the concept evaluation experiment, however we note that random rela- tions prove more confusing for non-experts and more of them are (falsely) labeled as correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Precision of Hierarchy</head><p>We assess the precision of subsumption relations making up the ontology hierarchy. These are ex- tracted using the maximum spanning tree over the graph represented by the Ontology component, π O (see Section 2.1 for details), resulting in 49 sub- sumption relations. We compare their quality to that of 49 randomly sampled subsumption rela- tions. Subsumptions are presented to the worker using is a patterns, similar to the ones described above for concept evaluation, however in this case, the concept tokens are the top tokens of the hyper- nym topic. An example subsumption relation is The results shown in <ref type="table">Table 4</ref> indicate a low pre- cision among the extracted subsumption relations. This might be explained by the fact that at the final training iteration (2K) of the model, the perplexity of the Ontology component was still improving, while the perplexity of the other model compo- nents seemed closer to convergence. It is possible that the low precision observed here indicates that more training iterations are needed to achieve an accurate ontology using KB-LDA.  <ref type="table">Table 5</ref>: Top tags associated with sample topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overlap of KB-LDA topics with human-provided labels</head><p>We evaluated how well topics from KB-LDA cor- respond to document labels provided by humans, over a randomly sampled set of 40K documents from our corpus. In StackOverflow, questions (which we consider as documents) can be labeled with predefined tags. Here, we estimate the over- lap with the most frequently used tags. First, for topic k, we aggregate tags from documents where k = argmax k θ k d , where θ d is the document topic distribution. <ref type="table">Table 5</ref> shows examples of the top tags associated with sample topics, indicating a good correlation between top topic words and the underlying concepts.</p><p>Next, for each tested document d ∈ D, let W d be the top 30 words of the most probable topic in θ d , and T d the set of human provided document tags. We consider the following metrics:</p><formula xml:id="formula_17">Docs-Overlap = D d 1 {∃t∈T d :t∈W d } |D|</formula><p>measures the ratio of documents for which at least one tag overlaps with a top topic word. The aver- age ratio of overlapping tags per document is</p><formula xml:id="formula_18">Tag-Overlap = 1 |D| D d |t : t ∈ T d ∧ t ∈ W d | |T d |</formula><p>As a baseline, we measure similar overlap metrics using the 30 most frequent instance tokens in the document corpus. The results in <ref type="table" target="#tab_6">Table 6</ref> indicate an overlap of nearly half of the 20, 50, 100, and 500 most frequent tags with top topic tokens -sig- nificantly higher than the overlap with frequent to- ken. Our evaluation is based on the subset of tags found in the instance dictionary of KB-LDA.    <ref type="table" target="#tab_7">Table 7</ref> shows the top and bottom 10 triples according to this ranking, which suggests that the triples ranked higher by KB-LDA are more relevant to the software domain.</p><p>We compare the ranking based on KB-LDA to a ranking using a confidence score for the triple as assigned by ReVerb. We manually labeled 500 of the triples according to their relevance to the software domain, and measured the precision and recall of the two rankings at any cutoff thresh- old. <ref type="figure" target="#fig_2">Figure 3</ref> shows precision-recall curves for the two rankings, demonstrating that the ranking using probabilities based on KB-LDA leads to a more accurate detection of domain-relevant triples (with AUC of 0.67 for KB-LDA versus 0.57 for ReVerb).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>KB-LDA is an extension to LDA and link-LDA ( <ref type="bibr" target="#b3">Blei et al., 2003;</ref><ref type="bibr" target="#b9">Erosheva et al., 2004</ref>), model- ing documents as a mixed membership over en- tity types with additional annotated metadata, such as links ( <ref type="bibr" target="#b19">Nallapati et al., 2008;</ref>. It is a generalization of Block-LDA ( <ref type="bibr" target="#b2">Balasubramanyan and Cohen, 2011</ref>), however, KB- LDA models two link components, and the input links have a meaningful semantic correspondence to a KB structure (hierarchical and relational). In a related approach, <ref type="bibr" target="#b8">Dalvi et al. (2012)</ref> cluster web table concepts to non-probabilistically create hier- archies with assigned concept names.</p><p>Our work is related to latent tensor representa- tion of KBs, aimed at enhancing the ontological structure of existing KBs with relational data in the form of tensor structures. <ref type="bibr" target="#b21">Nickel et al. (2012)</ref> fac- torized the ontology of Yago 2 for relational learn- ing. A related approach was using Neural Tensor Networks to extract new facts from an existing KB ( ). In con- trast, in KB-LDA, relational data is learned jointly with the model through the Relations component.</p><p>Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion <ref type="bibr" target="#b14">(Hindle et al., 2012;</ref><ref type="bibr" target="#b17">Movshovitz-Attias and Cohen, 2013)</ref>, learning coding conventions ( <ref type="bibr" target="#b1">Allamanis et al., 2014)</ref>, and extracting actionable tasks from software doc- umentation ( <ref type="bibr" target="#b29">Treude et al., 2014</ref>). In related work, specific semantic relations, coordinate re- lations, have been extracted for a restricted class of software entities, ones that refer to Java classes <ref type="bibr" target="#b18">(Movshovitz-Attias and Cohen, 2015)</ref>. KB-LDA extends previous work by reasoning over a large variety of semantic relations among general soft- ware entities, as found in a document corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a model that jointly learns a latent ontological structure of a corpus augmented by re- lations, and identifies facts matching the learned structure. The quality of the produced structure was demonstrated through a series of real-world evaluations employing human judges, which mea- sured the semantic coherence of instance topics, relations, topic concepts, and hierarchy. We fur- ther validated the semantic meaning of topic con- cepts, by their correspondence to an independent source of human-provided document tags. The ex- perimental evaluation validates the usefulness of the proposed model for corpus exploration.</p><p>The results highlight the benefits of generaliz- ing pattern-based facts (hypernym-hyponym pairs and subject-verb-object tuples), using text docu- ments in a topic model framework. This modular approach offers opportunities to further improve an induced KB structure by posing additional con- straints on corpus entities in the form of additional components to the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average Match (top) and Group (bottom) precision of top tokens of 50 topics learned with KB-LDA, according to expert (dark blue) and non-expert (light blue, stripes) labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision-recall curves of rankers of open IE triples by software relevance, based on KB-LDA probabilities (blue), and ReVerb confidence (red). A star is pointing the highest F1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Topic string , character, characters, text, line Tags regex, string, python, php, ruby Topic element, div, css, elements, http Tags css, html, jquery, html5, javascript Topic table, query, database, sql, column Tags sql, mysql, database, performance, php</head><label>string</label><figDesc></figDesc><table>Topic jquery, mysql, http, json, xml 
Tags jquery, json, javascript, ruby, string 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Docs and Tag overlap of human-provided 
tags with KB-LDA topics, and frequent tokens. 

Top 10 ranked triples: server, not found, error, 
user, can access, file, method, not found, error, 
user, can change, password, page, not found, error, 
user, can upload, videos, compiler, will generate, 
error, users, can upload, files, users, can upload, 
files, object, not found, error 

Bottom 10 ranked triples: france, will visit, 
germany, utilities, may include, heat, iran, has had, 
russia, russia, can stop, germany, macs, do not 
support, windows media player, cell phones, do not 
make, phone calls, houses, have made, equipment, 
guests, will find, restaurants, guests, can request, 
bbq, inspectors, do not make, appointments 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Top and bottom ReVerb software triples 
ranked with KB-LDA. 

3.4 Extracting facts from an open IE 
resource 

We use KB-LDA to extract domain specific triples 
from an existing open IE KB, the 15M relations 
extracted using ReVerb (Fader et al., 2011) from 
ClueWeb09. By extracting the relations in which 
the subject, verb and object noun phrases are in-
cluded in the KB-LDA dictionary, we are left with 
under 5K triples, indicating the low coverage of 
software related triples using open domain extrac-
tion, in comparison with the 37K triples extracted 
from StackOverflow and given as an input to KB-
LDA. 
Due to word polysemy, many of the 5K 
extracted triples are themselves not specific 
to the domain. This suggests a hybrid ap-
proach in which KB-LDA is used to rank 
open IE triples for relevance to a domain. We 
ranked the 5K open triples by the probability 
of the triple given a trained KB-LDA model: 
p(s, v, o) = 
K 

ks 

K 

kv 

K 
ko π 

ks,kv,ko 
R 

σ s 
ks σ o 
ko δ v 
kv . 
</table></figure>

			<note place="foot" n="1"> Data source: https://archive.org/details/stackexchange</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank Premkumar Devanbu and Kathryn Rivard Mazaitis for helpful discus-sions, and the anonymous reviewers for their in-sightful comments. This work was funded by NSF under grant CCF-1414030.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.4182</idno>
		<title level="m">Learning natural coding conventions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Block-lda: Jointly modeling entity-annotated text and entity-entity links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramnath</forename><surname>Balasubramanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th SIAM International Conference on Data Mining</title>
		<meeting>the 7th SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fourth Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational topic models for document networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3618</idno>
		<title level="m">Learning new facts from knowledge bases with neural tensor networks and semantic word vectors</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Websets: Extracting sets of entities from the web using unsupervised information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Bhavana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM international conference on Web search and data mining</title>
		<meeting>the fifth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixed-membership models of scientific publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Erosheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Freebase data dumps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
		<title level="m">Finding scientific topics. Proc. of the</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics. ACL</title>
		<meeting>the 14th conference on Computational linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
	<note>Software Engineering (ICSE)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Yago2: a spatially and temporally enhanced knowledge base from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Never-ending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saparov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greaves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Natural language models for predicting programming comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Movshovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Grounded discovery of coordinate term relationships between software entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Movshovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00277</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint latent topic models for text and citations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ramesh M Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="542" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed algorithms for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1801" to="1828" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factorizing yago: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A block model suitable for sparse graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juuso</forename><surname>Parkkinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Sinkkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gyenge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Mining and Learning with Graphs</title>
		<meeting>the 7th International Workshop on Mining and Learning with Graphs<address><addrLine>Leuven</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributional clustering of english words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual meeting on Association for Computational Linguistics</title>
		<meeting>the 31st annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast collapsed gibbs sampling for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="569" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Acquiring temporal constraints between relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="992" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extracting development tasks to navigate software documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Treude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barthélémy</forename><surname>Robillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dagenais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Textrunner: open information extraction on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</title>
		<meeting>Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
