<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inner Attention based Recurrent Neural Networks for Answer Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingning</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inner Attention based Recurrent Neural Networks for Answer Selection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1288" to="1297"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Attention based recurrent neural networks have shown advantages in representing natural language sentences (Hermann et al., 2015; Rocktäschel et al., 2015; Tan et al., 2015). Based on recurrent neural networks (RNN), external attention information was added to hidden representations to get an attentive sentence representation. Despite the improvement over non-attentive models, the attention mechanism under RNN is not well studied. In this work, we analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively. Then we present three new RNN models that add attention information before RNN hidden representation , which shows advantage in representing sentence and achieves new state-of-art results in answer selection task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Answer selection (AS) is a crucial subtask of the open domain question answering (QA) problem. Given a question, the goal is to choose the an- swer from a set of pre-selected sentences <ref type="bibr" target="#b3">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b25">Yao et al., 2013)</ref>. Traditional AS models are based on lexical features such as parsing tree edit distance. Neural networks based models are proposed to represent the meaning of a sentence in a vector space and then compare the question and answer candidates in this hidden space ( <ref type="bibr" target="#b21">Wang and Nyberg, 2015;</ref><ref type="bibr" target="#b1">Feng et al., 2015)</ref>, which have shown great success in AS. However, these models represent the question and sentence separately, which may ignore the information sub- ject to the question when representing the answer. For example, given a candidate answer:</p><p>Michael Jordan abruptly retired from Chicago Bulls before the beginning of the 1993-94 NBA season to pursue a career in baseball. For a question: When did Michael Jordan retired from NBA? we should focus on the be- ginning of the 1993-94 in the sentence; how- ever, when we were asked: Which sports does Michael Jordan participates after his retire- ment from NBA? we should pay more attention to pursue a career in baseball.</p><p>Recent years, attention based models are pro- posed in light of this purpose and have shown great success in many NLP tasks such as ma- chine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref>), question answering ( <ref type="bibr" target="#b17">Sukhbaatar et al., 2015</ref>) and recognizing textual entailments <ref type="bibr" target="#b12">(Rocktäschel et al., 2015)</ref>. When building the rep- resentation of a sentence, some attention informa- tion is added to the hidden state. For example, in attention based recurrent neural networks mod- els ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) each time-step hidden representation is weighted by attention. Inspired by the attention mechanism, some attention-based RNN answer selection models have been proposed <ref type="bibr" target="#b19">(Tan et al., 2015</ref>) in which the attention when com- puting answer representation is from question rep- resentation.</p><p>However, in the RNN architecture, at each time step a word is added and the hidden state is up- dated recurrently, so those hidden states near the end of the sentence are expected to capture more information <ref type="bibr">1</ref> . Consequently, after adding the at- tention information to the time sequence hidden representations, the near-the-end hidden variables will be more attended due to their comparatively abundant semantic accumulation, which may re- sult in a biased attentive weight towards the later coming words in RNN.</p><p>In this work, we analyze this attention bias problem qualitatively and quantitatively, and then propose three new models to solve this prob- lem. Different from previous attention based RNN models in which attention information is added af- ter RNN computation, we add the attention be- fore computing the sentence representation. Con- cretely, the first one uses the question attention to adjust word representation (i.e. word embedding) in the answer directly, and then we use RNN to model the attentive word sequence. However, this model attends a sentence word by word which may ignore the relation between words. For example, if we were asked: what is his favorite food? one answer candidate is: He likes hot dog best. hot or dog may be not relate to the question by itself, but they are informative as a whole in the context. So we propose the second model in which every word representation in answer is impacted by not only question attention but also the context repre- sentation of the word (i.e. the last hidden state). In our last model, inspired by previous work on adding gate into inner activation of RNN to con- trol the long and short term information flow, we embed the attention to the inner activation gate of RNN to influence the computation of RNN hid- den representation. In addition, inspired by recent work called Occam's Gate in which the activation of input units are penalized to be as less as pos- sible, we add regulation to the summation of the attention weights to impose sparsity. Overall, in this work we make three contribu- tions: (1) We analyze the attention bias problem in traditional attention based RNN models. (2) We propose three inner attention based RNN models and achieve new state-of-the-art results in answer selection. (3) We use Occam's Razor to regulate the attention weights which shows advantage in long sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years, many deep learning framework has been developed to model the text in a vector space, and then use the embedded representations in this space for machine learning tasks. There are many neural networks architectures for this represen- tation such as convolutional neural networks( <ref type="bibr" target="#b29">Yin et al., 2015)</ref>, recursive neural networks <ref type="bibr" target="#b15">(Socher et al., 2013</ref>) and recurrent neural networks <ref type="bibr" target="#b8">(Mikolov et al., 2011</ref>). In this work we propose Inner Attention based RNN (IARNN) for answer selec- tion, and there are two main works which we are related to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention based Models</head><p>Many recent works show that attention techniques can improve the performance of machine learning models ( <ref type="bibr" target="#b9">Mnih et al., 2014;</ref><ref type="bibr" target="#b29">Zheng et al., 2015)</ref>. In attention based models, one representation is built with attention (or supervision) from other repre- sentation. <ref type="bibr">Weston et al (2014)</ref> propose a neural networks based model called Memory Networks which uses an external memory to store the knowl- edge and the memory are read and written on the fly with respect to the attention, and these attentive memory are combined for inference. Since then, many variants have been proposed to solve ques- tion answering problems ( <ref type="bibr" target="#b17">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b7">Kumar et al., 2015)</ref>.  and many other researchers ( <ref type="bibr" target="#b19">Tan et al., 2015;</ref><ref type="bibr" target="#b12">Rocktäschel et al., 2015</ref>) try to introduce the attention mechanism into the LSTM-RNN architecture. RNN models the input sequence word-by-word and updates its hidden variable recurrently. Compared with CNN, RNN is more capable of exploiting long-distance sequential information. In attention based RNN models, after computing each time step hidden representation, attention information is added to weight each hidden representation, then the hid- den states are combined with respect to that weight to obtain the sentence (or document) representa- tion. Commonly there are two ways to get atten- tion from source sentence, either by the whole sen- tence representation (which they call attentive) or word by word attention (called impatient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Selection</head><p>Answer selection is a sub-task of QA and many other tasks such as machine comprehension. Given a question and a set of candidate sentences, one should choose the best sentence from a can- didate sentence set that can answer the question. Previous works usually stuck in employing fea- ture engineering, linguistic tools, or external re- sources. For example, <ref type="bibr" target="#b26">Yih et al. (2013)</ref> use se- mantic features from WordNet to enhance lexical features. <ref type="bibr">Wang and Manning (2007)</ref> try to com- pare the question and answer sentence by their syntactical matching in parse trees. <ref type="bibr" target="#b3">Heilman and Smith (Heilman and Smith, 2010</ref>) try to fulfill the matching using minimal edit sequences between their dependency parse trees. <ref type="bibr" target="#b14">Severyn and Moschitti (2013)</ref> automate the extraction of discrimi- native tree-edit features over parsing trees.</p><p>While these methods show effectiveness, they might suffer from the availability of additional re- sources and errors of many NLP tools such as dependency parsing. Recently there are many works use deep learning architecture to represent the question and answer in a same hidden space, and then the task can be converted into a classi- fication or learning-to-rank problem <ref type="bibr" target="#b1">(Feng et al., 2015;</ref><ref type="bibr" target="#b21">Wang and Nyberg, 2015)</ref>. With the develop- ment of attention mechanism, Tan et.al <ref type="bibr">(2015)</ref> pro- pose an attention-based RNN models which intro- duce question attention to answer representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Traditional Attention based RNN Models and Their Deficiency</head><p>The attention-based models introduce the atten- tion information into the representation process.</p><p>In answer selection, given a question Q = {q 1 , q 2 , q 3 , ..., q n } where q i is i-th word, n is the question length, we can compute its representation in RNN architecture as follows:</p><formula xml:id="formula_0">X = D[q 1 , q 2 , ..., q n ] h t = σ(W ih x t + W hh h t−1 + b h ) y t = σ(W ho h t + b o ) (1)</formula><p>where D is an embedding matrix that projects word to its embedding space in R d ; W ih , W hh , W ho are weight matrices and b h , b o are bias vec- tors; σ is active function such as tanh. Usually we can ignore the output variables and use the hidden variables. After recurrent process, the last hidden variable h n or all hidden states average 1 n n t=1 h t is adopted as the question representation r q .</p><p>When modeling the candidate answer sentence with length m:S = {s 1 , s 2 , s 3 , ..., s m } in attention based RNN model, instead of using the last hidden state or average hidden states, we use attentive hidden states that are weighted by r q :</p><formula xml:id="formula_1">H a = [h a (1), h a (2), ..., h a (m)] s t ∝ f attention (r q , h a (t)) ˜ h a (t) = h a (t)s t r a = m t=1˜h t=1˜ t=1˜h a (t) (2)</formula><p>where h a (t) is hidden state of the answer at time t. In many previous work ( <ref type="bibr" target="#b12">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b19">Tan et al., 2015</ref>), the at- tention function f attention was computed as:</p><formula xml:id="formula_2">m(t) = tanh(W hm h a (t) + W qm r q ) f attention (r q , h a (t)) = exp(w T ms m(t))<label>(3)</label></formula><p>W hm and W qm are attentive weight matrices and w ms is attentive weight vector. So we can ex- pect that the candidate answer sentence represen- tation r a may be represented in a question-guided way: when its hidden state h a (t) is irrelevant to the question (determined by attention weight s t ), it will take less part in the final representation; but when this hidden state is relavent to the question, it will contribute more in representing r a . We call this type of attention based RNN model OARNN which stands for Outer Attention based RNN mod- els because this kind of model adds attention in- formation outside the RNN hidden representation computing process. An illustration of traditional attention-based RNN model is in <ref type="figure" target="#fig_0">Figure 1</ref>. However, we know in the RNN architecture, the input words are processed in time sequence and the hidden states are updated recurrently, so the current hidden state h t is supposed to contain all the information up to time t, when we add ques- tion attention information, aiming at finding the useful part of the sentence, these near-the-end hid- den states are prone to be selected because they contains much more information about the whole sentence. In other word, if the question pays atten- tion to the hidden states at time t , then it should also pay attention to those hidden states after t (i.e {h t |t &gt; t}) as they contain the information at least as much as h t , but in answer selection for a specific candidate answer, the useful parts to answer the question may be located anywhere in a sentence, so the attention should also dis- tribute uniformly around the sentence. Traditional attention-based RNN models under attention after representation mechanism may cause the attention to bias towards the later coming hidden states. We will analyze this attention bias problem quantita-tively in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inner Attention based Recurrent Neural Networks</head><p>In order to solve the attention bias problem, we propose an intuition:</p><p>Attention before representation</p><p>Instead of adding attention information after en- coding the answer by RNN, we add attention be- fore computing the RNN hidden representations. Based on this intuition, we propose three inner at- tention based RNN models detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IARNN-WORD</head><p>As attention mechanism aims at finding useful part of a sentence, the first model applies the above intuition directly. Instead of using the original answer words to the RNN model, we weight the words representation according to question atten- tion as follows:</p><formula xml:id="formula_3">α t = σ(r T q M qi x t ) ˜ x t = α t * x t (4)</formula><p>where M qi is an attention matrix to transform a question representaion into the word embedding space. Then we use the dot value to determine the question attention strength, σ is sigmoid function to normalize the weight α t between 0 and 1. The above attention process can be understood as sentence distillation where the input words are distilled (or filtered) by question attention. Then, we can represent the whole sentence based on this distilled input using traditional RNN model. In this work, we use GRU instead of LSTM as build- ing block for RNN because it has shown advan- tages in many tasks and has comparatively less parameter( <ref type="bibr" target="#b6">Jozefowicz et al., 2015)</ref> which is for- mulated as follows:</p><formula xml:id="formula_4">z t = σ(W xz˜xxz˜xz˜x t + W hz h t−1 ) f t = σ(W xf˜xxf˜xf˜x t + W hf h t−1 ) ˜ h t = tanh(W xh˜xxh˜xh˜x t + W hh (f t h t−1 )) h t = (1 − z t ) h t−1 + z t ˜ h t<label>(5)</label></formula><p>where W xz , W hz , W xf , W hh , W xh are weight matrices and stands for element-wise multipli- cation. Finally, we get candidate answer represen- tation by average pooling all the hidden state h t . we call this model IARNN-WORD as the atten- tion is paid to the original input words. This model is shown in <ref type="figure">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IARNN-CONTEXT</head><p>IABRNN-WORD attend input word embedding directly. However, the answer sentence may con- sist of consecutive words that are related to the question, and a word may be irrelevant to ques- tion by itself but relevant in the context of answer sentence.</p><p>So the above word by word attention mech- anism may not capture the relationship between multiple words. In order to import contextual in- formation into attention process, we modify the at- tention weights in Equation 4 with additional con- text information:</p><formula xml:id="formula_5">w C (t) = M hc h t−1 + M qc r q α t C = σ(w T C (t)x t ) ˜ x t = α t C * x t<label>(6)</label></formula><p>where we use h t−1 as context, M hc and M qc are attention weight matrices, w C (t) is the attention representation which consists of both question and word context information. This additional con- text attention endows our model to capture rele- vant part in longer text span. We show this model in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">IARNN-GATE</head><p>Inspired by the previous work of LSTM (Hochre- iter and Schmidhuber, 1997) on solving the gra- dient exploding problem in RNN and recent work on building distributed word representation with topic information( <ref type="bibr" target="#b2">Ghosh et al., 2016)</ref>, instead of adding attention information to the original input, we can apply attention deeper to the GRU inner activation (i.e z t and f t ). Because these inner ac- tivation units control the flow of the information within the hidden stage and enables information to pass long distance in a sentence, we add atten- tion information to these active gates to influence the hidden representation as follows:</p><formula xml:id="formula_6">z t = σ(W xz x t + W hz h t−1 +M qz r q ) f t = σ(W xf x t + W hf h t−1 +M qf r q ) ˜ h t = tanh(W xh x t + W hh (f t h t−1 )) h t = (1 − z t ) h t−1 + z t ˜ h t<label>(7)</label></formula><p>where M qz and M hz are attention weight matrices. In this way, the update and forget units in GRU can focus on not only long and short term memory but also the attention information from the question. The architecture is shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">IARNN-OCCAM</head><p>In answer selection, the answer sentence may only contain small number of words that are re- lated to the question. In IARNN-WORD and IARNN-CONTEXT, we calculate each word at- tention weight without considering total weights. Similar with Raiman(2015) who adds regulation to the input gate, we punish the summation of the attention weights to enforce sparsity. This is an application of Occam's Razor: Among the whole words set, we choose those with fewest number that can represent the sentence. However, assign- ing a pre-defined hyper-parameter for this regula- tion 2 is not an ideal way because it punishes all question attention weights with same strength. For different questions there may be different number of snippets in candidate answer that are required. For example, when the question type is When or Who, answer sentence may only contains a little relavant words so we should impose more sparsity on the summation of the attention. But when the 2 For example, in many machine learning problem the original objective sometimes followed with a L1 or L2 regulation with hyper-parameter λ1 or λ2 to control the tradeoff between the original objective J and the sparsity criterion: question type is Why or How, there may be much more words on the sentence that are relevant to the question so we should set the regulation value small accordingly. In this work, this attention reg- ulation is added as follows: for the specific ques- tion Q i and its representation r i q , we use a vector w qp to project it into scalar value n i p , and then we add it into the original objective J i as follows:</p><formula xml:id="formula_7">J * = J + (λ1|λ2)<label>(L1|L2norm</label></formula><formula xml:id="formula_8">n i p = max{w T qp r i q , λ q } J * i = J i + n i p mc t=1 α i t<label>(8)</label></formula><p>where α i t is attention weights in Equation 4 and Equation 6. λ q is a small positive hyper-parameter. It needs to mention that we do not regulate IARNN-GATE because the attention has been em- bedded to gate activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantify Traditional Attention based Model Bias Problem</head><p>In order to quantify the outer attention based RNN model's attention bias problem in Section 3, we build an outer attention based model similar with <ref type="bibr" target="#b19">Tan (2015)</ref>. First of all, for the question we build its representation by averaging its hidden states in LSTM, then we build the candidate answer sen- tence representation in an attentive way introduced in Section 3. Next we use the cosine similarity to compare question and answer representation simi- larity. Finally, we adopt max-margin hinge loss as objective:   where a + is ground truth answer candidate and a − stands for negative one, the scalar M is a pre- defined margin. When training result saturates af- ter 50 epoches, we get the attention weight distri- bution (i.e. s q in Equation 2). The experiment is conducted on two answer selection datasets: Wik- iQA ( <ref type="bibr" target="#b24">Yang et al., 2015</ref>) and TrecQA ( <ref type="bibr" target="#b22">Wang et al., 2007)</ref>. The normalized attention weights is re- ported in <ref type="figure" target="#fig_2">Figure 5</ref>. However, the above model use only forward LSTM to build hidden state representation, the at- tention bias problem may attribute to the biased answer distribution: the useful part of the an- swer to the question sometimes may located at the end of the sentence. So we try OARNN in bidi- rectional architecture, where the forward LSTM and backward LSTM are concatenated for hidden representation, The bidirectional attention based LSTM attention distribution is shown in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><formula xml:id="formula_9">L = max{0,M − cosine(r q , r a + ) + cosine(r q , r a − )}<label>(9)</label></formula><p>Analysis: As is shown in <ref type="figure" target="#fig_2">Figure 5</ref> and 6, for one-directional OARNN, as we move from begin- ning to the end in a sentence, the question atten- tion gains continuously; when we use bidirectional OARNN, the hidden representations near two ends of a sentence get more attention. This is consistent with our assumption that for a hidden representa- tion in RNN, the closer to the end of a sentence, the more attention it should drawn from question. But the relevant part may be located anywhere in a answer. As a result, when the sample size is large enough 3 , the attention weight should be unformly distributed. The traditional attention after repre- sentation style RNN may suffer from the biased attention problem. Our IARNN models are free from this problem and distribute nearly uniform (orange line) in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">IARNN evaluation</head><p>Common Setup: We use the off-the-shelf 100- dimension word embeddings from word2vec 4 , and initiate all weights and attention matrices by fixing their largest singular values to 1 ( <ref type="bibr" target="#b10">Pascanu et al., 2013)</ref>. IARNN-OCCAM base regulation hyper- parameter λ q is set to 0.05, we add L 2 penalty with a coefficient of 10 −5 . Dropout ( <ref type="bibr" target="#b16">Srivastava et al., 2014</ref>) is further applied to every parameters with probability 30%. We use Adadelta <ref type="bibr" target="#b28">(Zeiler, 2012)</ref> with ρ = 0.90 to update parameters.</p><p>We choose three datasets for evaluation: Insur- anceQA, WikiQA and TREC-QA. These datasets contain questions from different domains. <ref type="table" target="#tab_2">Table 1</ref> presents some statistics about these datasets. We adopt a max-margin hinge loss as training objec- tive. The results are reported in terms of MAP and MRR in WikiQA and TREC-QA and accuracy in InsuranceQA.</p><p>We use bidirectional GRU for all models. We share the GRU parameter between question and answer which has shown significant improvement on performance and convergency rate ( <ref type="bibr" target="#b19">Tan et al., 2015;</ref><ref type="bibr" target="#b1">Feng et al., 2015)</ref>.</p><p>There are two common baseline systems for above three datasets:</p><p>• GRU: A non-attentive GRU-RNN that mod- els the question and answer separately.</p><p>• OARNN: Outer attention-based RNN mod- els (OARNN) with GRU which is detailed in Section 5.1.</p><p>WikiQA <ref type="formula">(</ref>  <ref type="table">Table 2</ref>: Performances on WikiQA dataset in which all answers are collected from Wikipedia. In addition to the original (ques- tion,positive,negative) triplets, we randomly select a bunch of negative answer candidates from answer sentence pool and finally we get a relatively abundant 50,298 triplets. We use cosine similarity to compare the question and candidate answer sentence. The hidden variable's length is set to 165 and batch size is set to 1. We use sigmoid as GRU inner active function, we keep word embedding fixed during training. Margin M was set to 0.15 which is tuned in the development set. We adopt three additional baseline systems applied to WikiQA: <ref type="formula">(</ref> The result is shown in <ref type="table">Table 2</ref>.</p><p>InsuranceQA ( <ref type="bibr" target="#b1">Feng et al., 2015</ref>) is a domain specific answer selection dataset in which all ques- tions is related to insurance. Its vocabulary size is comparatively small <ref type="bibr">(22,</ref><ref type="bibr">353)</ref>, we set the batch size to 16 and the hidden variable size to 145, hinge loss margin M is adjusted to 0.12 by evalu- ation behavior. Word embeddings are also learned during training. We adopt the Geometric mean of Euclidean and Sigmoid Dot (GESD) proposed in <ref type="bibr" target="#b1">(Feng et al., 2015)</ref> to measure the similarity be-    <ref type="bibr" target="#b20">Wang and Ittycheriah, 2015)</ref> propose a ques- tion similarity model to extract features from word alignment between two questions which is suitable to FAQ based QA. It needs to mention that the sys- tem marked with † are learned on TREC-QA orig- inal full training data.</p><p>tween two representations:</p><formula xml:id="formula_10">GESD(x, y) = 1 1 + ||x − y|| × 1 1 + exp(−γ(xy T + c)) (10)</formula><p>which shows advantage over cosine similarity in experiments.</p><p>We report accuracy instead of MAP/MRR be- cause one question only has one right answers in InsuranceQA. The result is shown in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>TREC-QA was created by <ref type="bibr" target="#b22">Wang et al.(2007)</ref> based on Text REtrieval Conference (TREC) QA track (8-13) data. The size of hidden variable was set to 80, M was set to 0.1. This dataset is com- paratively small so we set word embedding vector Q: how old was monica lewinsky during the affair ? <ref type="bibr">Monica Samille Lewinsky ( born July 23 , 1973</ref> ) is an American woman with whom United States President Bill Clinton admitted to having had an``an`` improper relationship '' while she worked at the White House in 1995 and 1996 .</p><p>Monica Samille Lewinsky ( born July <ref type="bibr">23 , 1973</ref> ) is an American woman with whom United States President Bill Clinton admitted to having had an``an`` improper relationship '' while she worked at the White House in 1995 and 1996 . size to 50 and update it during training. It needs to mention that we do not use the original TREC-QA training data but the smaller one which has been edited by human. The result is shown in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Result and Analysis</head><p>We can see from the result tables that the atten- tion based RNN models achieve better results than the non-attention RNN models (GRU). OARNN and IARNN beat the non-attentive GRU in ev- ery datasets by a large margin, which proves the importance of attention mechanism in represent- ing answer sentence in AS. For the non-attentive models, the fixed width of the hidden vectors is a bottleneck for interactive information flow, so the informative part of the question could only propagate through the similarity score which is blurred for the answer representation to be prop- erly learned. But in attention based models, the question attention information is introduced to in- fluence the answer sentence representation explic- itly, in this way we can improve sentence repre- sentation for the specific target (or topic ( <ref type="bibr" target="#b2">Ghosh et al., 2016)</ref>).</p><p>The inner attention RNN models outperform outer attention model in three datasets, this is corresponds to our intuition that the bias atten- tion problem in OARNN may cause a biasd sen- tence representation. An example of the attention heatmap is shown in Figure7. To answer the ques- tion, we should focus on "born July <ref type="bibr">23 , 1973"</ref> which is located at the beginning of the sentence. But in OARNN, the attention is biases towards the last few last words in the answer. In IARNN- CONTEXT, the attention is paid to the relevant part and thus results in a more relevant representa- tion.</p><p>The attention with context information could also improves the result, we can see that IARNN- CONTEXT and IARNN-GATE outperform IARNN-WORD in three experiments. IARNN- WORD may ignore the importance of some words because it attends answer word by word, for example in Figure8, the specific word self or focusing may not be related to the question by itself, but their combination and the previous word relativistic is very informative for answering the question. In IARNN-CONTEXT we add attention information dynamically in RNN process, thus it could capture the relationship between word and its context.</p><p>In general, we can see from table3-5 that the IARNN-GATE outperforms IARNN-CONTEXT and IARNN-WORD. In IARNN-WORD and IARNN-CONTEXT, the attention is added to im- pact each word representation, but the recur- rent process of updating RNN hidden state rep- resentations are not influenced. IARNN-GATE embeds the attention into RNN inner activa- tion, the attentive activation gate are more ca- pable of controlling the attention information in RNN. This enlights an important future work: we could add attention information as an individ- ual activation gate, and use this additional gate to control attention information flow in RNN. The regulation of the attention weights (Oc- cam's attention) could also improve the represen- tation. We also conduct an experiment on Wik- iQA (training process) to measure the Occam's at- tention regulation on different type of questions. We use rules to classify question into 6 types (i.e. who,why,how,when,where,what), and each of them has the same number of samples to avoid data imbalance. We report the Occam'm regula- tion (n i p in Equation.8) in <ref type="figure" target="#fig_6">Figure 9</ref>. As we can see from the radar graph, who and where are regulized severely compared with other types of question, this is correspond to their comparetively less infor- mation in the answer candidate to answer the ques- tion. This emphasize that different types question should impose different amount of regulation on its candidate answers. The experiment result on three AS datasets shows that the improvement of Occam's attention is significant in WikiQA and in- suranceQA. Because most of the sentence are rel- atively long in these two datasets, and the longer the sentence, the more noise it may contain, so we should punish the summation of the attention weights to remove some irrelevant parts. Our question-specific Occam's attention punishes the summation of attention and thus achieves a bet- ter result for both IARNN-WORD and IARNN- CONTEXT.</p><note type="other">who why how when where what 0.131 0.065 0.052 0.103 0.118 0.089</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this work we present some variants of tradi- tional attention-based RNN models with GRU.</p><p>The key idea is attention before representation. We analyze the deficiency of traditional outer attention-based RNN models qualitatively and quantitatively. We propose three models where at- tention is embedded into representation process. Occam's Razor is further implemented to this at- tention for better representation. Our results on answer selection demonstrate that the inner atten- tion outperforms the outer attention in RNN. Our models can be further extended to other NLP tasks such as recognizing textual entailments where at- tention mechanism is important for sentence rep- resentation. In the future we plan to apply our inner-attention intuition to other neural networks such as CNN or multi-layer perceptron.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Traditional attention based RNN answer selection model. Dark blue rectangles represent hidden virable, ⊗ means gate opperation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: IARNN-WORD architecture. r q is question representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: One directional OARNN attention distribution, the horizontal axis is position of word in a sentence that has been normalized from 1 to 10000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Bi-directional OARNN attention distribution, the horizontal axis is the postion of the word in a sentence that has been normalized from 1 to 10000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1) A bigram CNN models with average pooling(Yang et al., 2015). (2) An attention-based CNN model which uses an interactive attention matrix for both question and answer(Yin et al., 2015) 5 (3) An attention based CNN models which builds the attention matrix after sentence representation(Santos et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example demonstrates the advantage of IARNN in capturing the informed part of a sentence compared with OARNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The Occam's attention regulation on different types of question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : The statistics of three answer selection datasets. For the TREC-QA, we use the cleaned dataset that has been edit by human. For WikiQA and TREC-QA we remove all the questions that has no right or wrong answers.</head><label>1</label><figDesc></figDesc><table>Yang et al., 2015) is a recently 
released open-domain question answering </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experiment result in InsuranceQA, (Feng 
et al., 2015) is a CNN architecture without atten-
tion mechanism. 

System 
MAP 
MRR 
(Wang and Nyberg, 2015)  † 
0.7134 
0.7913 
(Wang and Ittycheriah, 2015)  † 0.7460 
0.8200 
(Santos et al., 2016)  † 
0.7530 
0.8511 
GRU 
0.6487 
0.6991 
OARNN 
0.6887 
0.7491 
IARNN-word 
0.7098 
0.7757 
IARNN-Occam(word) 
0.7162 
0.7916 
IARNN-context 
0.7232 
0.8069 
IARNN-Occam(context) 
0.7272 
0.8191 
IARNN-Gate 
0.7369 
0.8208 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Result of different systems in Trec-
QA.(</table></figure>

			<note place="foot" n="1"> so in many previous RNN-based model use the last hidden variable as the whole sentence representation</note>

			<note place="foot" n="3"> 10000 for WikiQA and 5000 for TrecQA in experiment. 4 https://code.google.com/archive/p/word2vec/</note>

			<note place="foot" n="5"> In their experiment some extra linguistic features was also added for better performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by the Natural Science Foundation of China (No.61533018), the National High Technology Development 863 Program of China (No.2015AA015405) and the National Nat-ural Science Foundation of China (No.61272332). And this research work was also supported by Google through focused research awards program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Applying deep learning to answer selection: A study and an open task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Michael R Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06291</idno>
		<title level="m">Contextual lstm (clstm) models for large scale nlp tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tree edit models for recognizing textual entailments, paraphrases, and answers to questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1011" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HonzaČernock`yHonzaˇHonzaČernock`HonzaČernock`y, and Sanjeev Khudanpur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>Extensions of recurrent neural network language model</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08251</idno>
		<title level="m">Occam&apos;s gates</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03609</idno>
		<title level="m">Attentive pooling networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="458" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lstmbased deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02628</idno>
		<title level="m">Faqbased question answering via word alignment</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A long shortterm memory model for answer sentence selection in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What is the jeopardy model? a quasisynchronous grammar for qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Answer extraction as sequence tagging with tree edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A neural autoregressive approach to attention-based recognition. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jin</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="67" to="79" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
