<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Putting Sarcasm Detection into Context: The Effects of Class Imbalance and Manual Labelling on Supervised Machine Classification of Twitter Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Abercrombie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Putting Sarcasm Detection into Context: The Effects of Class Imbalance and Manual Labelling on Supervised Machine Classification of Twitter Conversations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="107" to="113"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sarcasm can radically alter or invert a phrase&apos;s meaning. Sarcasm detection can therefore help improve natural language processing (NLP) tasks. The majority of prior research has modeled sarcasm detection as classification, with two important limitations: 1. Balanced datasets, when sarcasm is actually rather rare. 2. Using Twitter users&apos; self-declarations in the form of hashtags to label data, when sarcasm can take many forms. To address these issues, we create an unbalanced corpus of manually annotated Twitter conversations. We compare human and machine ability to recognize sarcasm on this data under varying amounts of context. Our results indicate that both class imbalance and labelling method affect performance, and should both be considered when designing automatic sarcasm detection systems. We conclude that for progress to be made in real-world sarcasm detection, we will require a new class labelling scheme that is able to access the &apos;common ground&apos; held between conversational parties.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sarcasm, or verbal irony, is prevalent both in spo- ken and written communication, and can radically alter or invert a phrase's meaning. Automatic sar- casm detection can therefore help improve natural language processing (NLP) tasks, such as senti- ment analysis, where failure to take ironic intent into account has been recognised as a major cause of errors.</p><p>However, automatic sarcasm detection is a non- trivial problem, and research into this subject is in its infancy. The majority of prior research has treated sarcasm detection as a classification task, with two important limitations: 1. It focuses on balanced datasets, when sarcasm is actually rather rare. 2. In order to obtain labelled data for su- pervised learning, many studies relied on Twitter users' supposed self-declarations of sarcasm in the form of hashtags such as #sarcasm, but sarcasm can take many forms.</p><p>Although reporting impressive results for sar- casm detection, even state-of-the-art systems fail to address the above issues. Research suggesting that verbal irony occurs in less than a fifth of con- versations ( <ref type="bibr" target="#b8">Gibbs, 2000</ref>) implies that, rather than using balanced datasets, a more realistic approach may be to view sarcasm recognition as a prob- lem of anomaly detection, in which positive ex- amples are scarce. While convenient, obtaining labelled data from hashtags has been found to in- troduce both noise, in the form of incorrectly la- belled examples, and bias to the datasets used - analysis suggests that only certain forms of sar- casm are likely to be tagged in this way ( , and predominantly by certain types of Twitter users <ref type="bibr" target="#b0">(Bamman and Smith, 2015)</ref>.</p><p>To address these issues, we create a novel corpus of manually annotated Twitter conversa- tions and, using the feature classes of <ref type="bibr" target="#b0">Bamman and Smith (2015)</ref>, perform sarcasm classifica- tion experiments on both balanced and unbalanced datasets. We also compare model performance to a dataset of conversations automatically retrieved using hashtags.</p><p>Our contributions In this paper, we present a novel corpus of manually annotated two-part Twit- ter conversations for use in supervised classifi- cation of sarcastic and non-sarcastic text. We compare human vs. machine learning classifica- tion performance under varying amounts of con- textual information, and evaluate machine perfor-mance on balanced and unbalanced, and manually labelled and automatically retrieved datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>Most prior research into sarcasm detection has been conducted on Twitter. To make comparisons with other research, and because use of sarcasm seems to be prevalent on Twitter, we too make use of Twitter data for this study.</p><p>However, the collection of data using explicit markers of sarcasm (hashtags) has been shown to introduce bias to the datasets used in prior re- search ( <ref type="bibr" target="#b9">González-Ibánez et al., 2011;</ref><ref type="bibr" target="#b15">Maynard and Greenwood, 2014;</ref><ref type="bibr" target="#b0">Bamman and Smith, 2015)</ref>. We therefore create a novel hand-annotated corpus of contextualised sarcastic and non-sarcastic Twitter conversations. For com- parison, we also create an automatically collected dataset using hashtags.</p><p>Corpus creation The data set is taken from a Twitter corpus of 64 million tweets gathered in 2013. Matching tweet reply IDs to the status IDs of other tweets, and filtering by language, pro- duces 650,212 two-line English Twitter 'conver- sations.' We manually annotate these, finding 448 positive examples, to which we add 1,792 neg- atively labelled examples in which sarcasm was found not to be present. The resulting corpus con- tains 2,240 conversations in total. A second cor- pus, which is automatically retrieved using hash- tags, is created, producing 448 Twitter conversa- tion where the second tweet contains #sarcasm, and 1,792 without this feature. Following previous work, we remove usernames and web addresses. For the second corpus, we also remove the term #sarcasm. We collect up to 3,200 historical tweets written by each user ID in the datasets.</p><p>Annotation We annotated the conversations manually with full access to the text of the con- versations and user profile information and tweet history of the users. Following prior work ( <ref type="bibr" target="#b11">Kreuz and Caucci, 2007)</ref>, and because people have been found to conflate many forms of verbal irony un- der the term sarcasm <ref type="bibr" target="#b7">(Gibbs, 1986)</ref>, positive labels were not assigned according to any fixed criteria or definition, but according to our intuitive un- derstanding of whether or not examples contained verbal irony 1</p><p>Procedure We asked two participants to rate the reply tweet of each conversation as either sarcas- tic or non-sarcastic. Again, following <ref type="bibr" target="#b11">Kreuz and Caucci (2007)</ref>, raters were not provided with a definition of sarcasm, but were asked to judge the tweets based on their intuitive understanding of the term.</p><p>Inter-rater agreement We use inter-rater agree- ment measures to assess both the difficulty of the sarcasm recognition task under different condi- tions and the reliability of the participants. We report both raw percentage agreement and -as in previous work on sarcasm annotation <ref type="bibr" target="#b18">(Swanson et al., 2014</ref>) -Krippendorffs α, which takes into ac- count expected chance disagreement.</p><p>Contrary to expectations, annotators are not more likely to agree if given access to more infor- mation. Agreement is highest for the tweet only condition (% = 70.49, α = 0.35). Krippendorffs α scores for tweet + audience (0.08) and tweet + original + author + audience (0.18) are very low, while tweet + audience produces a negative score (-0.10) which indicates that agreement is be- low chance levels.</p><p>Rater reliability Agreement scores are gener- ally low. Only two pairs obtain 'good' agree- ment scores. <ref type="bibr">2</ref> The majority (20 pairs) receive a score between 0.0 and 0.67, while eight of the pairs achieve negative scores, indicating less than chance expected agreement. Two possible expla- nations for low rater agreement are (1) that sar- casm recognition is a difficult task for humans ( <ref type="bibr" target="#b11">Kreuz and Caucci, 2007;</ref><ref type="bibr" target="#b9">González-Ibánez et al.,</ref> ing e.g., <ref type="bibr" target="#b9">, González-Ibánez et al. (2011</ref>. <ref type="bibr">2</ref> Krippendorff (2012) considers 0.67 to be lowest accept- able agreement score. 2011), especially without access to the surround- ing context <ref type="bibr" target="#b5">(Filatova, 2012;</ref><ref type="bibr" target="#b21">Wallace et al., 2014)</ref>, and (2) that people undertaking such tasks re- motely online are often guilty of 'spamming,' or providing careless or random responses <ref type="bibr" target="#b10">(Hovy et al., 2013)</ref>.</p><p>To mitigate the effects of unreliable raters and get an upper bound for human performance, we use two measures: (1) discard the results of the worst performing rater in each pair (in terms of F1) and use the vote of the higher scoring raters. (2) identify the least trustworthy raters and down- weight their votes using scores from an item- response model, MACE <ref type="bibr" target="#b10">(Hovy et al., 2013)</ref>.</p><p>The first requires access to the original anno- tated labels, the latter can be done with or without access to the gold standard. We compare both F1 and Area Under the Curve (AUC) scores of both raters in each pair, the better performing rater only, and the MACE most competent rater in each pair over all conditions.</p><p>For both measures, MACE competent rater scores (F1: 0.547; AUC: 0.731) are marginally higher than the mean of both raters (F1: 0.523; AUC: 0.729), while the best rater scores (F1: 0.641; AUC: 0.817) are highest of all, as might be expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Machine classification experiments</head><p>To compare human to machine performance, we fit binary classification models on both balanced and unbalanced splits of the two datasets.</p><p>Experimental setup We evaluate performance using a standard logistic regression model with 2 regularization, evaluated via five-fold cross- validation.</p><p>Features For the five conditions, we use the fol- lowing feature classes, as named and described by <ref type="bibr" target="#b0">Bamman and Smith (2015)</ref> Normalisation We convert all features to binary or numeric values and normalize them to the range between zero and one.</p><p>Procedure Following <ref type="bibr" target="#b0">Bamman and Smith (2015)</ref>, we evaluate classification performance on the above feature sets in the following combina- tions: tweet features only, tweet + author features, tweet + audience features, tweet + environment features, and tweet + author + audience + environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Accuracy is commonly reported in classifica- tion tasks, but unsuitable for unbalanced datasets ( <ref type="bibr">López et al., 2013</ref>), so we report two other met- rics frequently used with uneven class distribu- tions: F1 score, and Area Under the ROC Curve (AUC), which reflects the relationship between the true positive rate (TPR) and false positive rate (FPR). Unlike accuracy, these measures penalize predicting only the majority class. AUC is consid- ered to be more resistant to the skew of unbalanced data than F1 <ref type="bibr" target="#b4">(Fawcett, 2004)</ref>.</p><p>Comparison with baselines <ref type="figure" target="#fig_0">Figure 1</ref> compares random performance, human raters, <ref type="bibr">3</ref> and the clas- sifier's AUC scores. The scores of both the human raters and the machine classifier surpass random performance in all conditions, with the classifier attaining the lowest score of 0.615 on tweet + au- thor features. Machine classification is not, how- ever, able to match human performance. But there are parallels between human and machine perfor- mance: the classifier achieves its highest score us- ing tweet + environment features (human: 0.802; machine: 0.630). Interestingly, both humans and the classifier appear to suffer from an 'information saturation' effect, obtaining lowest scores when trained on a combination of all the possible fea- tures.</p><p>Machine classification performance across con- ditions We have two data-related factors that affect performance, namely (1) label prevalence (i.e., balanced vs. unbalanced splits), and (2) the labelling scheme (manual vs. automatically in- duced from #sarcasm). <ref type="figure" target="#fig_1">Figure 2</ref> shows the ef- fects on F1 and AUC for each combination of these two factors under all five conditions. Class label balance AUC scores are largely unaffected by change in label balance. We see broadly similar results on both balanced and unbalanced data splits across all the feature classes on both corpora. The small changes in performance that do occur can be at- tributed to the increase in size of the unbalanced datasets, which have more negative training exam- ples compared to the balanced sets.</p><p>However, for both corpora, and across all fea- ture classes, F1 scores suffer large drops on the unbalanced data compared to results on the bal- anced datasets. These results indicate that F1, known to be biased to the negative class and to ignore the effect of true negatives <ref type="bibr" target="#b16">(Powers, 2015)</ref>, may not be a suitable metric for this task, as it is very sensitive to the changes in class balance of the datasets. Nevertheless, even when measured with AUC score, in the majority of feature config- urations classifier performance drops on the unbal- anced datasets. Results therefore suggest that class balance (and dataset size) should be taken into ac- count when designing sarcasm detection systems. Labelling scheme Overall, higher scores are achieved with the auto- matically collected corpus. All feature combina- tions obtain higher F1 and AUC scores on this data using the balanced split, as do tweet + auth and tweet + aud on the unbalanced data. This points to greater homogeneity in the data in the automat- ically collected corpus. This may be because it is often certain types of users, such as those who do not know their audience personally, who feel the need to label their sarcastic statements with hash- tags ( <ref type="bibr" target="#b0">Bamman and Smith, 2015)</ref>. Manually anno- tated data includes instances of sarcasm which the author has not deemed necessary to explicitly label as sarcastic. This may lead to greater variation in the features of the positive examples in the manu- ally annotated data, and hence lower classification scores.</p><p>The only feature category in which F1 and AUC scores for the manually annotated data are higher than those for the automatically collected data are on the unbalanced split for tweet features (F1: +0.012, AUC: +0.08) and tweet + env (F1: +0.037, AUC: +0.015), while tweet + auth + aud + env produces a higher F1 score (+0.275), but a slightly lower AUC score (-0.023). These fig- ures point to the fact that for the manually an- notated data, performance is best when linguistic features from both tweets in the conversations are included. Indeed, on both balanced and unbal- anced data splits of the manually annotated data, better results are generally produced using these textual features than using features related to the writers of those texts. It would therefore seem that the annotation process has introduced some biases to the data. This process, in which sarcasm, or the ambiguous possibility of sarcasm, is first recognised in the dialogues and then confirmed by scrutiny of users Twitter pages, heavily favours textual features. Twitter conversations automati- cally selected using hashtags on the other hand, are likely to be highly ambiguous once those hashtags are removed and, as discussed above, more likely to be predictable from information in the conversa- tional participants' profile metadata than from lin- guistic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Research in both cognitive psychology <ref type="bibr" target="#b20">(Utsumi, 2000;</ref><ref type="bibr" target="#b6">Gibbs and Colston, 2007)</ref> and NLP <ref type="bibr" target="#b5">(Filatova, 2012</ref>) has suggested that it may not be possible to produce an overarching definition of sarcasm. <ref type="bibr" target="#b12">Kreuz (1996)</ref> noted that use of sar- casm often depends on the 'common ground' peo- ple share. Work on human sarcasm recognition ( <ref type="bibr" target="#b11">Kreuz and Caucci, 2007)</ref> and automatic sarcasm detection ( <ref type="bibr" target="#b0">Bamman and Smith, 2015)</ref> has relied on people's intuitive understanding of the term 'sar- casm' for rating and data labelling purposes.</p><p>Following the insights of <ref type="bibr" target="#b11">Kreuz and Caucci (2007)</ref>, <ref type="bibr" target="#b1">Carvalho et al. (2009)</ref>, <ref type="bibr" target="#b9">González-Ibánez et al. (2011)</ref> and , among others, used textual cues for automatic sarcasm detection. Addressing the wider context in which tweets are written, <ref type="bibr" target="#b17">Rajadesingan et al. (2015)</ref> mapped infor- mation from the posting history of Twitter users to research on why, when, and how sarcasm tends to be used. They also tested their model on both balanced and unbalanced datasets. <ref type="bibr" target="#b0">Bamman and Smith (2015)</ref> showed that a variety of contextual features can improve classification performance over use of textual features alone. However, like <ref type="bibr" target="#b9">González-Ibánez et al. (2011)</ref> and <ref type="bibr" target="#b15">Maynard and Greenwood (2014)</ref>, they concluded that the use of hashtags for data labelling introduced biases to their dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We evaluated the performance of human raters and a machine learning algorithm on sarcasm detection under different information conditions. We find that humans generally benefit from con- text more than machines, but that machine per- formance is even more affected by the label- ing scheme (automatically induced vs. hand- annotated) and the prevalence of the target class. Our results indicate that sarcasm detection is far from solved, and that any results on the task need to be viewed in the light of the two factors outlined here.</p><p>In automatic sarcasm detection, use of unbal- anced datasets led to large drops in F1 scores, due to this metric not taking into account true nega- tives. As the ratio of TNs is necessarily large for effective sarcasm detection on data in which posi- tive examples are rare, AUC seems a more appro- priate performance metric.</p><p>Although more robust to class imbalance, AUC scores also varied between the balanced and un- balanced datasets. This indicates that label class balance and dataset size should be taken into ac- count when designing sarcasm detection systems.</p><p>Previous work suggests that the automatic se- lection of positive examples using user-written hashtags biases the data towards (1) particularly ambiguous forms of sarcasm, and (2) 'celebrity' Twitter users who are anxious not to be misun- derstood. Our labelling method avoids these pit- falls, as well as eliminating noise in the form of tweets that use sarcastic hashtags but are not in fact ironic. However, in using the labels of an out- side observer to the conversations, we may be in-troducing other forms of bias. It seems that a gold standard sarcasm corpus would require labelling by the annotators who are party to the 'common ground' shared by the participants in the conversa- tions. It would also need to include those instances that they would not normally publicly mark as be- ing sarcastic with hashtags.</p><p>Future work will focus on improving the qual- ity and size of labelled corpora available for this task. It will also explore the use of features from the wider conversational context beyond the two- sentence dialogues examined here, and investigate the effects of data labelling method and class bal- ance on media other than Twitter.</p><p>In Proceedings of the Annual Meeting of the Asso- ciation for Computational Linguistics (ACL), pages 512-516.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>113</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: AUC scores of random performance, the most competent human raters, and machine classification on an unbalanced split of the manually annotated data.</figDesc><graphic url="image-1.png" coords="4,72.96,62.81,216.35,152.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effects of labeling method (top vs. bottom row) and label prevalance (left vs. right column) on F1 and AUC scores.</figDesc><graphic url="image-2.png" coords="5,109.04,62.81,379.48,309.36" type="bitmap" /></figure>

			<note place="foot" n="1"> This was also necessary because prior sarcasm detection studies relied on self-annotation of sarcasm by Twitter users applying their own judgements of sarcastic mean3 Human performance baseline study This study was undertaken with the participation of 60 native English speaking volunteers. We randomly selected 300 Twitter conversations from the corpus and assigned them each one of five conditions: tweet only-the text of the reply tweet from the conversation, tweet + author-including access to the Twitter profile of the author, tweet + audience-including the profile of the writer of the original tweet in the conversation, tweet + environment-the texts of both tweets, and tweet + author + audience + environment-access to all the above information. Each participant rated 10 conversations.</note>

			<note place="foot" n="3"> Using the MACE most competent rater scores, which we judge to be the fairest comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the participants who vol-unteered their time and effort to make this study possible. We also thank the anonymous reviewers for their invaluable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextualized sarcasm detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International AAAI Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Clues for detecting irony in user-generated contents: oh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Sarmento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugénio De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">s so easy;-)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!!</forename><surname>It</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion</title>
		<meeting>the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="53" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised recognition of sarcastic sentences in twitter and amazon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Roc graphs: Notes and practical considerations for researchers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Irony and sarcasm: Corpus generation and analysis using crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Filatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="392" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The future of irony studies. Irony in Language and Thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Colston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="339" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the psycholinguistics of sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond W Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Irony in talk among friends. Metaphor and symbol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond W Gibbs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="5" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying sarcasm in twitter: a closer look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>González-Ibánez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning whom to trust with mace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1120" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lexical influences on the perception of sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina</forename><forename type="middle">M</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on computational approaches to Figurative Language</title>
		<meeting>the Workshop on computational approaches to Figurative Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The use of verbal irony: Cues and constraints. Metaphor: Implications and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kreuz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="23" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Content analysis: An introduction to its methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Sage</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Palade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page" from="113" to="141" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greenwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M W</forename><surname>Powers</surname></persName>
		</author>
		<title level="m">What the F-measure doesn&apos;t measure: Features, Flaws, Fallacies and Fixes</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sarcasm detection on twitter: A behavioral modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Rajadesingan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Getting reliable annotations for sarcasm in online dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn A</forename><surname>Corcoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4250" to="4257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Verbal irony as implicit display of ironic environment: Distinguishing ironic utterances from nonirony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Utsumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1777" to="1806" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Humans require context to infer ironic intent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura Kertz Do Kook</forename><surname>Byron C Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>so computers probably do, too</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
