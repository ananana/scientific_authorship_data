<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Contrastive Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><forename type="middle">Joey</forename><surname>Bose</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Borealis AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Borealis AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
							<email>{yanshuai.cao}@borealisai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Borealis AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Contrastive Estimation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1021" to="1032"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sam-pler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embed-dings and observe both faster convergence and improved results on multiple metrics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many models learn by contrasting losses on ob- served positive examples with those on some fic- titious negative examples, trying to decrease some score on positive ones while increasing it on neg- ative ones. There are multiple reasons why such contrastive learning approach is needed. Com- putational tractability is one. For instance, in- stead of using softmax to predict a word for learn- ing word embeddings, noise contrastive estima- tion (NCE) <ref type="bibr" target="#b8">(Dyer, 2014;</ref><ref type="bibr" target="#b29">Mnih and Teh, 2012)</ref> can be used in skip-gram or CBOW word em- bedding models ( <ref type="bibr" target="#b18">Gutmann and Hyvärinen, 2012;</ref><ref type="bibr" target="#b26">Mikolov et al., 2013;</ref><ref type="bibr" target="#b28">Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b42">Vaswani et al., 2013)</ref>. Another reason is * authors contributed equally † Work done while author was an intern at Borealis AI modeling need, as certain assumptions are best ex- pressed as some score or energy in margin based or un-normalized probability models <ref type="bibr" target="#b36">(Smith and Eisner, 2005</ref>). For example, modeling entity re- lations as translations or variants thereof in a vec- tor space naturally leads to a distance-based score to be minimized for observed entity-relation-entity triplets ( <ref type="bibr" target="#b3">Bordes et al., 2013)</ref>. Given a scoring function, the gradient of the model's parameters on observed positive examples can be readily computed, but the negative phase requires a design decision on how to sample data. In noise contrastive estimation for word embed- dings, a negative example is formed by replacing a component of a positive pair by randomly select- ing a sampled word from the vocabulary, resulting in a fictitious word-context pair which would be unlikely to actually exist in the dataset. This nega- tive sampling by corruption approach is also used in learning knowledge graph embeddings ( <ref type="bibr" target="#b3">Bordes et al., 2013;</ref><ref type="bibr" target="#b22">Lin et al., 2015;</ref><ref type="bibr" target="#b20">Ji et al., 2015;</ref><ref type="bibr" target="#b45">Wang et al., 2014;</ref><ref type="bibr" target="#b38">Trouillon et al., 2016;</ref><ref type="bibr" target="#b47">Yang et al., 2014;</ref><ref type="bibr" target="#b7">Dettmers et al., 2017)</ref>, order embeddings ( <ref type="bibr" target="#b43">Vendrov et al., 2016)</ref>, caption generation <ref type="bibr" target="#b6">(Dai and Lin, 2017)</ref>, etc.</p><p>Typically the corruption distribution is the same for all inputs like in skip-gram or CBOW NCE, rather than being a conditional distribution that takes into account information about the input sample under consideration. Furthermore, the cor- ruption process usually only encodes a human prior as to what constitutes a hard negative sam- ple, rather than being learned from data. For these two reasons, the simple fixed corruption process often yields only easy negative examples. Easy negatives are sub-optimal for learning discrimina- tive representation as they do not force the model to find critical characteristics of observed positive data, which has been independently discovered in applications outside NLP previously ( <ref type="bibr" target="#b35">Shrivastava et al., 2016)</ref>. Even if hard negatives are occasion- ally reached, the infrequency means slow conver- gence. Designing a more sophisticated corruption process could be fruitful, but requires costly trial- and-error by a human expert.</p><p>In this work, we propose to augment the sim- ple corruption noise process in various embedding models with an adversarially learned conditional distribution, forming a mixture negative sampler that adapts to the underlying data and the em- bedding model training progress. The resulting method is referred to as adversarial contrastive es- timation (ACE). The adaptive conditional model engages in a minimax game with the primary em- bedding model, much like in Generative Adversar- ial Networks (GANs) ( <ref type="bibr" target="#b11">Goodfellow et al., 2014a)</ref>, where a discriminator net (D), tries to distinguish samples produced by a generator (G) from real data ( <ref type="bibr" target="#b12">Goodfellow et al., 2014b</ref>). In ACE, the main model learns to distinguish between a real posi- tive example and a negative sample selected by the mixture of a fixed NCE sampler and an adver- sarial generator. The main model and the genera- tor takes alternating turns to update their parame- ters. In fact, our method can be viewed as a con- ditional <ref type="bibr">GAN (Mirza and Osindero, 2014</ref>) on dis- crete inputs, with a mixture generator consisting of a learned and a fixed distribution, with additional techniques introduced to achieve stable and con- vergent training of embedding models.</p><p>In our proposed ACE approach, the conditional sampler finds harder negatives than NCE, while being able to gracefully fall back to NCE when- ever the generator cannot find hard negatives. We demonstrate the efficacy and generality of the pro- posed method on three different learning tasks, word embeddings ( <ref type="bibr" target="#b26">Mikolov et al., 2013)</ref>, order embeddings ( <ref type="bibr" target="#b43">Vendrov et al., 2016</ref>) and knowledge graph embeddings ( <ref type="bibr" target="#b20">Ji et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background: contrastive learning</head><p>In the most general form, our method applies to supervised learning problems with a contrastive objective of the following form:</p><formula xml:id="formula_0">L(ω) = E p(x + ,y + ,y − ) l ω (x + , y + , y − ) (1)</formula><p>where l ω (x + , y + , y − ) captures both the model with parameters ω and the loss that scores a positive tuple (x + , y + ) against a negative one (x + , y − ). E p(x + ,y + ,y − ) (.) denotes expectation with respect to some joint distribution over pos- itive and negative samples. Furthermore, by the law of total expectation, and the fact that given x + , the negative sampling is not depen- dent on the positive label, i.e. p(y + , y − |x + ) = p(y + |x + )p(y − |x + ), Eq. 1 can be re-written as</p><formula xml:id="formula_1">E p(x + ) [E p(y + |x + )p(y − |x + ) l ω (x + , y + , y − )] (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separable loss</head><p>In the case where the loss decomposes into a sum of scores on positive and negative tuples such as</p><formula xml:id="formula_2">l ω (x + , y + , y − ) = s ω (x + , y + )−˜ s ω (x + , y − ), then Expression. 2 becomes E p + (x) [E p + (y|x) s ω (x, y) − E p − (y|x) ˜ s ω (x, y)]<label>(3</label></formula><p>) where we moved the + and − to p for notational brevity. Learning by stochastic gradient descent aims to adjust ω to pushing down s ω (x, y) on samples from p + while pushing up˜sup˜ up˜s ω (x, y) on samples from p − . Note that for generality, the scoring function for negative samples, denoted by˜s by˜ by˜s ω , could be slightly different from s ω . For in- stance, ˜ s could contain a margin as in the case of Order Embeddings in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non separable loss</head><p>Eq. 1 is the general form that we would like to consider because for certain problems, the loss function cannot be separated into sums of terms containing only positive (x + , y + ) and terms with negatives (x + , y − ). An example of such a non- separable loss is the triplet ranking loss ( <ref type="bibr" target="#b33">Schroff et al., 2015)</ref>:</p><formula xml:id="formula_3">l ω = max(0, η + s ω (x + , y + ) − s ω (x + , y − ))</formula><p>, which does not decompose due to the rectification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise contrastive estimation</head><p>The typical NCE approach in tasks such as word embeddings ( <ref type="bibr" target="#b26">Mikolov et al., 2013)</ref>, order embed- dings ( <ref type="bibr" target="#b43">Vendrov et al., 2016)</ref>, and knowledge graph embeddings can be viewed as a special case of Eq. 2 by taking p(y − |x + ) to be some unconditional p nce (y).</p><p>This leads to efficient computation during train- ing, however, p nce (y) sacrifices the sampling effi- ciency of learning as the negatives produced using a fixed distribution are not tailored toward x + , and as a result are not necessarily hard negative exam- ples. Thus, the model is not forced to discover discriminative representation of observed positive data. As training progresses, more and more neg- ative examples are correctly learned, the probabil- ity of drawing a hard negative example diminishes further, causing slow convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial mixture noise</head><p>To remedy the above mentioned problem of a fixed unconditional negative sampler, we propose to augment it into a mixture one, λp nce (y) + (1 − λ)g θ (y|x), where g θ is a conditional distribution with a learnable parameter θ and λ is a hyper- parameter. The objective in Expression. 2 can then be written as (conditioned on x for notational brevity):</p><formula xml:id="formula_4">L(ω, θ; x) = λ E p(y + |x)pnce(y − ) l ω (x, y + , y − ) + (1 − λ) E p(y + |x)g θ (y − |x) l ω (x, y + , y − )<label>(4)</label></formula><p>We learn (ω, θ) in a GAN-style minimax game:</p><formula xml:id="formula_5">min ω max θ V (ω, θ) = min ω max θ E p + (x) L(ω, θ; x)<label>(5)</label></formula><p>The embedding model behind l ω (x, y + , y − ) is similar to the discriminator in (conditional) GAN (or critic in <ref type="bibr">Wasserstein (Arjovsky et al., 2017)</ref> or Energy-based GAN ( <ref type="bibr" target="#b49">Zhao et al., 2016)</ref>, while g θ (y|x) acts as the generator. Henceforth, we will use the term discriminator (D) and embedding model interchangeably, and refer to g θ as the gen- erator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning the generator</head><p>There is one important distinction to typical GAN: g θ (y|x) defines a categorical distribution over pos- sible y values, and samples are drawn accordingly; in contrast to typical GAN over continuous data space such as images, where samples are gener- ated by an implicit generative model that warps noise vectors into data points. Due to the discrete sampling step, g θ cannot learn by receiving gradi- ent through the discriminator. One possible solu- tion is to use the Gumbel-softmax reparametriza- tion trick ( <ref type="bibr" target="#b19">Jang et al., 2016;</ref><ref type="bibr" target="#b25">Maddison et al., 2016)</ref>, which gives a differentiable approximation. However, this differentiability comes at the cost of drawing N Gumbel samples per each categorical sample, where N is the number of categories. For word embeddings, N is the vocabulary size, and for knowledge graph embeddings, N is the num- ber of entities, both leading to infeasible computa- tional requirements.</p><p>Instead, we use the REINFORCE <ref type="bibr" target="#b46">(Williams, 1992)</ref> gradient estimator for θ L(θ, x):</p><formula xml:id="formula_6">(1−λ) E −l ω (x, y + , y − ) θ log(g θ (y − |x))<label>(6)</label></formula><p>where the expectation E is with respect to</p><formula xml:id="formula_7">p(y + , y − |x) = p(y + |x)g θ (y − |x)</formula><p>, and the dis- criminator loss l ω (x, y + , y − ) acts as the reward. With a separable loss, the (conditional) value function of the minimax game is:</p><formula xml:id="formula_8">L(ω, θ; x) = E p + (y|x) s ω (x, y) − E pnce(y) ˜ s ω (x, y) − E g θ (y|x) ˜ s ω (x, y) (7)</formula><p>and only the last term depends on the generator parameter ω. Hence, with a separable loss, the re- ward is −˜ s(x + , y − ). This reduction does not hap- pen with a non-separable loss, and we have to use l ω (x, y + , y − ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Entropy and training stability</head><p>GAN training can suffer from instability and de- generacy where the generator probability mass collapses to a few modes or points. Much work has been done to stabilize GAN training in the continuous case ( <ref type="bibr" target="#b16">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b16">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b5">Cao et al., 2018)</ref>. In ACE, if the generator g θ probability mass collapses to a few candidates, then after the discriminator success- fully learns about these negatives, g θ cannot adapt to select new hard negatives, because the REIN- FORCE gradient estimator Eq. 6 relies on g θ being able to explore other candidates during sampling. Therefore, if the g θ probability mass collapses, in- stead of leading to oscillation as in typical GAN, the min-max game in ACE reaches an equilibrium where the discriminator wins and g θ can no longer adapt, then ACE falls back to NCE since the nega- tive sampler has another mixture component from NCE.</p><p>This behavior of gracefully falling back to NCE is more desirable than the alternative of stalled training if p − (y|x) does not have a simple p nce mixture component. However, we would still like to avoid such collapse, as the adversarial samples provide greater learning signals than NCE sam- ples. To this end, we propose to use a regularizer to encourage the categorical distribution g θ (y|x) to have high entropy. In order to make the the reg- ularizer interpretable and its hyperparameters easy to tune, we design the following form:</p><formula xml:id="formula_9">R ent (x) = min(0, c − H(g θ (y|x)))<label>(8)</label></formula><p>where H(g θ (y|x)) is the entropy of the categorical distribution g θ (y|x), and c = log(k) is the entropy of a uniform distribution over k choices, and k is a hyper-parameter. Intuitively, R ent expresses the prior that the generator should spread its mass over more than k choices for each x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Handling false negatives</head><p>During negative sampling, p − (y|x) could actually produce y that forms a positive pair that exists in the training set, i.e., a false negative. This possi- bility exists in NCE already, but since p nce is not adaptive, the probability of sampling a false nega- tive is low. Hence in NCE, the score on this false negative (true observation) pair is pushed up less in the negative term than in the positive term. However, with the adaptive sampler, g ω (y|x), false negatives become a much more severe issue. g ω (y|x) can learn to concentrate its mass on a few false negatives, significantly canceling the learn- ing of those observations in the positive phase. The entropy regularization reduces this problem as it forces the generator to spread its mass, hence re- ducing the chance of a false negative.</p><p>To further alleviate this problem, whenever computationally feasible, we apply an additional two-step technique. First, we maintain a hash map of the training data in memory, and use it to effi- ciently detect if a negative sample (x + , y − ) is an actual observation. If so, its contribution to the loss is given a zero weight in ω learning step. Sec- ond, to upate θ in the generator learning step, the reward for false negative samples are replaced by a large penalty, so that the REINFORCE gradient update would steer g θ away from those samples. The second step is needed to prevent null compu- tation where g θ learns to sample false negatives which are subsequently ignored by the discrimi- nator update for ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Variance Reduction</head><p>The basic REINFORCE gradient estimator is poised with high variance, so in practice one of- ten needs to apply variance reduction techniques. The most basic form of variance reduction is to subtract a baseline from the reward. As long as the baseline is not a function of actions (i.e., sam- ples y − being drawn), the REINFORCE gradi- ent estimator remains unbiased. More advanced gradient estimators exist that also reduce vari- ance ( <ref type="bibr" target="#b14">Grathwohl et al., 2017;</ref><ref type="bibr" target="#b41">Tucker et al., 2017;</ref><ref type="bibr" target="#b23">Liu et al., 2018)</ref>, but for simplicity we use the self-critical baseline method ( <ref type="bibr" target="#b31">Rennie et al., 2016)</ref>, where the baseline is b(x) = l ω (y + , y , x), or b(x) = −˜ s ω (y , x) in the separable loss case, and y = argmax i g θ (y i |x). In other words, the base- line is the reward of the most likely sample accord- ing to the generator.  <ref type="figure">|x)</ref> is not zero, meaning that for it to be effective in helping ex- ploration, the generator cannot be collapsed at the first place. Hence, in practice, this term is only used to further help on top of the entropy regular- ization, but it does not replace it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Improving exploration in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Smith and <ref type="bibr" target="#b36">Eisner (2005)</ref> proposed contrastive es- timation as a way for unsupervised learning of log-linear models by taking implicit evidence from user-defined neighborhoods around observed dat- apoints. <ref type="bibr" target="#b17">Gutmann and Hyvärinen (2010)</ref> intro- duced NCE as an alternative to the hierarchical softmax. In the works of Mnih and Teh (2012) and <ref type="bibr" target="#b28">Mnih and Kavukcuoglu (2013)</ref>, NCE is applied to log-bilinear models and <ref type="bibr" target="#b42">Vaswani et al. (2013)</ref> ap- plied NCE to neural probabilistic language models ( <ref type="bibr" target="#b48">Yoshua et al., 2003</ref>). Compared to these previous NCE methods that rely on simple fixed sampling heuristics, ACE uses an adaptive sampler that pro- duces harder negatives.</p><p>In the domain of max-margin estimation for structured prediction <ref type="bibr" target="#b37">(Taskar et al., 2005</ref>), loss augmented MAP inference plays the role of find- ing hard negatives (the hardest). However, this in- ference is only tractable in a limited class of mod- els such structured SVM ( <ref type="bibr" target="#b39">Tsochantaridis et al., 2005</ref>). Compared to those models that use exact maximization to find the hardest negative config- uration each time, the generator in ACE can be viewed as learning an approximate amortized in- ference network. Concurrently to this work, <ref type="bibr" target="#b40">Tu and Gimpel (2018)</ref> proposes a very similar frame- work, using a learned inference network for Struc- tured prediction energy networks (SPEN) <ref type="bibr" target="#b1">(Belanger and McCallum, 2016)</ref>.</p><p>Concurrent with our work, there have been other interests in applying the GAN to NLP prob- lems <ref type="bibr" target="#b9">(Fedus et al., 2018;</ref><ref type="bibr" target="#b44">Wang et al., 2018;</ref><ref type="bibr" target="#b4">Cai and Wang, 2017)</ref>. Knowledge graph models natu- rally lend to a GAN setup, and has been the sub- ject of study in <ref type="bibr" target="#b44">Wang et al. (2018)</ref> and <ref type="bibr" target="#b4">Cai and Wang (2017)</ref>. These two concurrent works are most closely related to one of the three tasks on which we study ACE in this work. Besides a more general formulation that applies to problems be- yond those considered in <ref type="bibr" target="#b44">Wang et al. (2018)</ref> and <ref type="bibr" target="#b4">Cai and Wang (2017)</ref>, the techniques introduced in our work on handling false negatives and en- tropy regularization lead to improved experimen- tal results as shown in Sec. 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application of ACE on three tasks 4.1 Word Embeddings</head><p>Word embeddings learn a vector representation of words from co-occurrences in a text corpus. NCE casts this learning problem as a binary classifica- tion where the model tries to distinguish positive word and context pairs, from negative noise sam- ples composed of word and false context pairs. The NCE objective in Skip-gram ( <ref type="bibr" target="#b26">Mikolov et al., 2013</ref>) for word embeddings is a separable loss of the form:</p><formula xml:id="formula_10">L = − wt∈V [log p(y = 1|w t , w + c ) + K c=1 log p(y = 0|w t , w − c )]<label>(9)</label></formula><p>Here, w + c is sampled from the set of true con- texts and w − c ∼ Q is sampled k times from a fixed noise distribution. <ref type="bibr" target="#b26">Mikolov et al. (2013)</ref> in- troduced a further simplification of NCE, called "Negative Sampling" <ref type="bibr" target="#b8">(Dyer, 2014)</ref>. With respect to our ACE framework, the difference between NCE and Negative Sampling is inconsequential, so we continue the discussion using NCE. A draw- back of this sampling scheme is that it favors more common words as context. Another issue is that the negative context words are sampled in the same way, rather than tailored toward the ac- tual target word. To apply ACE to this problem we first define the value function for the minimax game, V (D, G), as follows:</p><formula xml:id="formula_11">V (D, G) = E p + (wc) [log D(w c , w t )] − E pnce(wc) [− log(1 − D(w c , w t ))] − E g θ (wc|wt) [− log(1 − D(w c , w t ))]<label>(10)</label></formula><p>with D = p(y = 1|w t , w c ) and G = g θ (w c |w t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>For our experiments, we train all our models on a single pass of the May 2017 dump of the En- glish Wikipedia with lowercased unigrams. The vocabulary size is restricted to the top 150k most frequent words when training from scratch while for finetuning we use the same vocabulary as <ref type="bibr" target="#b30">Pennington et al. (2014)</ref>, which is 400k of the most frequent words. We use 5 NCE samples for each positive sample and 1 adversarial sample in a win- dow size of 10 and the same positive subsampling scheme proposed by <ref type="bibr" target="#b26">Mikolov et al. (2013)</ref>. Learn- ing for both <ref type="bibr">G and D uses Adam (Kingma and Ba, 2014</ref>) optimizer with its default parameters. Our conditional discriminator is modeled using the Skip-Gram architecture, which is a two layer neural network with a linear mapping between the layers. The generator network consists of an em- bedding layer followed by two small hidden lay- ers, followed by an output softmax layer. The first layer of the generator shares its weights with the second embedding layer in the discriminator net- work, which we find really speeds up convergence as the generator does not have to relearn its own set of embeddings. The difference between the dis- criminator and generator is that a sigmoid nonlin- earity is used after the second layer in the discrim- inator, while in the generator, a softmax layer is used to define a categorical distribution over nega- tive word candidates. We find that controlling the generator entropy is critical for finetuning exper- iments as otherwise the generator collapses to its favorite negative sample. The word embeddings are taken to be the first dense matrix in the dis- criminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Order Embeddings Hypernym Prediction</head><p>As introduced in <ref type="bibr" target="#b43">Vendrov et al. (2016)</ref>, ordered representations over hierarchy can be learned by order embeddings. An example task for such or- dered representation is hypernym prediction. A hypernym pair is a pair of concepts where the first concept is a specialization or an instance of the second.</p><p>For completeness, we briefly describe order em- beddings, then analyze ACE on the hypernym pre- diction task. In order embeddings, each entity is represented by a vector in R N , the score for a positive ordered pair of entities (x, y) is defined by s ω (x, y) = ||max(0, y − x)|| 2 and, score for a negative ordered pair (x + , y − ) is defined by˜s by˜ by˜s ω (x + , y − ) = max{0, η − s(x + , y − )}, where is η is the margin. Let f (u) be the embedding function which takes an entity as input and outputs en em- bedding vector. We define P as a set of positive pairs and N as negative pairs, the separable loss function for order embedding task is defined by:</p><formula xml:id="formula_12">L = (u,v)∈P s ω (f (u), f (v)))+ (u,v)∈N˜s ∈N˜ ∈N˜s(f (u), f (v))<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Our generator for this task is just a linear fully con- nected softmax layer, taking an embedding vector from discriminator as input and outputting a cate- gorical distribution over the entity set. For the dis- criminator, we inherit all model setting from <ref type="bibr" target="#b43">Vendrov et al. (2016)</ref>: we use 50 dimensions hidden state and bash size 1000, a learning rate of 0.01 and the Adam optimizer. For the generator, we use a batch size of 1000, a learning rate 0.01 and the Adam optimizer. We apply weight decay with rate 0.1 and entropy loss regularization as described in Sec. 2.4. We handle false negative as described in Sec. 2.5. After cross validation, variance reduc- tion and leveraging NCE samples does not greatly affect the order embedding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Knowledge Graph Embeddings</head><p>Knowledge graphs contain entity and relation data of the form (head entity, relation, tail entity), and the goal is to learn from observed positive entity relations and predict missing links (a.k.a. link prediction). There have been many works on knowledge graph embeddings, e.g. TransE <ref type="bibr" target="#b3">(Bordes et al., 2013</ref>), TransR ( <ref type="bibr" target="#b22">Lin et al., 2015</ref>), TransH ( <ref type="bibr" target="#b45">Wang et al., 2014</ref>), <ref type="bibr">TransD (Ji et al., 2015)</ref>, Com- plex ( <ref type="bibr" target="#b38">Trouillon et al., 2016)</ref>, <ref type="bibr">DistMult (Yang et al., 2014</ref>) and <ref type="bibr">ConvE (Dettmers et al., 2017</ref>). Many of them use a contrastive learning objective. Here we take TransD as an example, and modify its noise contrastive learning to ACE, and demonstrate sig- nificant improvement in sample efficiency and link prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Let a positive entity-relation-entity triplet be de- noted by ξ + = (h + , r + , t + ), and a negative triplet could either have its head or tail be a negative sam- ple, i.e. ξ − = (h − , r + , t + ) or ξ − = (h + , r + , t − ). In either case, the general formulation in Sec. 2.1 still applies. The non-separable loss function takes on the form:</p><formula xml:id="formula_13">l = max(0, η + s ω (ξ + ) − s ω (ξ − ))<label>(12)</label></formula><p>The scoring rule is:</p><formula xml:id="formula_14">s = h ⊥ + r − t ⊥ (13)</formula><p>where r is the embedding vector for r, and h ⊥ is projection of the embedding of h onto the space of r by h ⊥ = h + r p h p h, where r p and h p are projection parameters of the model. t ⊥ is defined in a similar way through parameters t, t p and r p .</p><p>The form of the generator g θ (t − |r + , h + ) is cho- sen to be f θ (h ⊥ , h ⊥ + r), where f θ is a feedfor- ward neural net that concatenates its two input ar- guments, then propagates through two hidden lay- ers, followed by a final softmax output layer. As a function of (r + , h + ), g θ shares parameter with the discriminator, as the inputs to f θ are the embed- ding vectors. During generator learning, only θ is updated and the TransD model embedding param- eters are frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate ACE with experiments on word embeddings, order embeddings, and knowledge graph embeddings tasks. In short, whenever the original learning objective is contrastive (all tasks except Glove fine-tuning) our results con- sistently show that ACE improves over NCE. In some cases, we include additional comparisons to the state-of-art results on the task to put the sig- nificance of such improvements in context: the generic ACE can often make a reasonable base- line competitive with SOTA methods that are op- timized for the task.</p><p>For word embeddings, we evaluate models trained from scratch as well as fine-tuned Glove models ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>) on word similar- ity tasks that consist of computing the similarity   between word pairs where the ground truth is an average of human scores. We choose the Rare word dataset ( <ref type="bibr" target="#b24">Luong et al., 2013)</ref> and WordSim- 353 ( <ref type="bibr" target="#b10">Finkelstein et al., 2001</ref>) by virtue of our hy- pothesis that ACE learns better representations for both rare and frequent words. We also qualita- tively evaluate ACE word embeddings by inspect- ing the nearest neighbors of selected words.</p><p>For the hypernym prediction task, following <ref type="bibr" target="#b43">Vendrov et al. (2016)</ref>, hypernym pairs are created from the WordNet hierarchy's transitive closure. We use the released random development split and test split from <ref type="bibr" target="#b43">Vendrov et al. (2016)</ref>, which both contain 4000 edges.</p><p>For knowledge graph embeddings, we use TransD ( <ref type="bibr" target="#b20">Ji et al., 2015</ref>) as our base model, and perform ablation study to analyze the behavior of ACE with various add-on features, and confirm that entropy regularization is crucial for good per- formance in ACE. We also obtain link prediction results that are competitive or superior to the state- of-arts on the WN18 dataset ( <ref type="bibr" target="#b2">Bordes et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Word Embeddings from scratch</head><p>In this experiment, we empirically observe that training word embeddings using ACE converges significantly faster than NCE after one epoch. As shown in <ref type="figure">Fig. 3</ref> both ACE (a mixture of p nce and g θ ) and just g θ (denoted by ADV) significantly outperforms the NCE baseline, with an absolute improvement of 73.1% and 58.5% respectively on RW score. We note similar results on WordSim- 353 dataset where ACE and ADV outperforms NCE by 40.4% and 45.7%. We also evaluate our model qualitatively by inspecting the nearest neighbors of selected words in <ref type="table">Table.</ref> 1. We first present the five nearest neighbors to each word to show that both NCE and ACE models learn sen- sible embeddings. We then show that ACE em- beddings have much better semantic relevance in a larger neighborhood (nearest neighbor 45-50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Finetuning Word Embeddings</head><p>We take off-the-shelf pre-trained Glove embed- dings which were trained using 6 billion tokens ( <ref type="bibr" target="#b30">Pennington et al., 2014</ref>) and fine-tune them us- ing our algorithm. It is interesting to note that the original Glove objective does not fit into the con- trastive learning framework, but nonetheless we find that they benefit from ACE. In fact, we ob- serve that training such that 75% of the words ap- pear as positive contexts is sufficient to beat the largest dimensionality pre-trained Glove model on word similarity tasks. We evaluate our perfor- mance on the Rare Word and WordSim353 data. As can be seen from our results in <ref type="table" target="#tab_3">Table 2</ref>, ACE on RW is not always better and for the 100d and 300d Glove embeddings is marginally worse. How- ever, on WordSim353 ACE does considerably bet- ter across the board to the point where 50d Glove embeddings outperform the 300d baseline Glove model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hypernym Prediction</head><p>As shown in <ref type="table" target="#tab_4">Table 3</ref>, with ACE training, our method achieves a 1.5% improvement on accu- <ref type="table">Queen  King  Computer  Man  Woman  Skip-Gram NCE Top 5  princess  prince  computers  woman  girl  king  queen  computing  boy  man  empress  kings  software  girl  prostitute  pxqueen  emperor  microcomputer  stranger  person  monarch monarch  mainframe  person</ref>     <ref type="figure" target="#fig_0">Fig. 1</ref>, we report loss curve on randomly sampled pairs. We stress that in the ACE model, we train random pairs and generator generated pairs jointly, as shown in <ref type="figure">Fig.  2</ref>, hard negatives help the order embedding model converges faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study and Improving TransD</head><p>To analyze different aspects of ACE, we perform an ablation study on the knowledge graph em- bedding task. As described in Sec. 4.3, the base Method Accuracy (%) order-embeddings 90.6 order-embeddings + Our ACE 92.0  <ref type="figure" target="#fig_3">Fig. 5</ref> shows validation per- formance as training progresses. All variants of ACE converges to better results than base NCE. Among ACE variants, all methods that include en- tropy regularization significantly outperform with- out entropy regularization. Without the self crit- ical baseline variance reduction, learning could progress faster at the beginning but the final per- formance suffers slightly. The best performance is obtained without the additional off-policy learning of the generator. <ref type="table" target="#tab_5">Table. 4</ref> shows the final test results on WN18 link prediction task. It is interesting to note that ACE improves MRR score more significantly than hit@10. As MRR is a lot more sensitive to the top rankings, i.e., how the correct configuration ranks among the competitive alternatives, this is consis- tent with the fact that ACE samples hard negatives and forces the base model to learn a more discrim- inative representation of the positive examples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hard Negative Analysis</head><p>To better understand the effect of the adversarial samples proposed by the generator we plot the dis- criminator loss on both p nce and g θ samples. In this context, a harder sample means a higher loss assigned by the discriminator. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that discriminator loss for the word embedding task on g θ samples are always higher than on p nce sam- ples, confirming that the generator is indeed sam- pling harder negatives. For Hypernym Prediction task, <ref type="figure">Fig.2</ref> shows dis- criminator loss on negative pairs sampled from NCE and ACE respectively. The higher the loss the harder the negative pair is. As indicated in the left plot, loss on the ACE negative terms collapses faster than on the NCE negatives. After adding entropy regularization and weight decay, the gen- erator works as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>When the generator softmax is large, the current implementation of ACE training is computation- ally expensive. Although ACE converges faster per iteration, it may converge more slowly on wall-clock time depending on the cost of the soft- max. However, embeddings are typically used as pre-trained building blocks for subsequent tasks. Thus, their learning is usually the pre-computation step for the more complex downstream models and spending more time is justified, especially with GPU acceleration. We believe that the com- putational cost could potentially be reduced via some existing techniques such as the "augment and reduce" variational inference of ( <ref type="bibr" target="#b32">Ruiz et al., 2018)</ref>, adaptive softmax ( <ref type="bibr" target="#b15">Grave et al., 2016)</ref>, or the "sparsely-gated" softmax of <ref type="bibr" target="#b34">Shazeer et al. (2017)</ref>, but leave that to future work. Another limitation is on the theoretical front. As noted in <ref type="bibr" target="#b13">Goodfellow (2014)</ref>, GAN learning does not implement maximum likelihood estima- tion (MLE), while NCE has MLE as an asymp- totic limit. To the best of our knowledge, more distant connections between GAN and MLE train- ing are not known, and tools for analyzing the equilibrium of a min-max game where players are parametrized by deep neural nets are currently not available to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose Adversarial Contrastive Estimation as a general technique for improving supervised learning problems that learn by con- trasting observed and fictitious samples. Specifi- cally, we use a generator network in a conditional GAN like setting to propose hard negative exam- ples for our discriminator model. We find that a mixture distribution of randomly sampling neg- ative examples along with an adaptive negative sampler leads to improved performances on a va- riety of embedding tasks. We validate our hypoth- esis that hard negative examples are critical to op- timal learning and can be proposed via our ACE framework. Finally, we find that controlling the entropy of the generator through a regularization term and properly handling false negatives is cru- cial for successful training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Order embedding Accuracy plot. Right: Order embedding discriminator Loss plot on NCE sampled negative pairs and positive pairs.</figDesc><graphic url="image-5.png" coords="7,72.00,174.84,147.40,98.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: loss curve on NCE negative pairs and ACE negative pairs. Left: without entropy and weight decay. Right: with entropy and weight decay</figDesc><graphic url="image-6.png" coords="7,219.40,174.84,147.40,98.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training from scratch losses on the Discriminator</figDesc><graphic url="image-7.png" coords="7,368.61,174.24,149.68,99.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation study: measuring validation Mean Reciprocal Rank (MRR) on WN18 dataset as training progresses.</figDesc><graphic url="image-8.png" coords="9,72.00,62.81,218.25,160.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Top 5 Nearest Neighbors of Words followed by Neighbors 45-50 for different Models. 

RW 
WS353 
Skipgram Only NCE baseline 
18.90 31.35 

Skipgram + Only ADV 
29.96 58.05 

Skipgram + ACE 
32.71 55.00 

Glove-50 (Recomputed based on(Pennington et al., 2014)) 
34.02 49.51 

Glove-100 (Recomputed based on(Pennington et al., 2014)) 
36.64 52.76 

Glove-300 (Recomputed based on(Pennington et al., 2014)) 
41.18 60.12 

Glove-50 + ACE 
35.60 60.46 

Glove-100 + ACE 
36.51 63.29 

Glove-300 + ACE 
40.57 66.50 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Spearman score (ρ  *  100) on RW and 
WS353 Datasets. We trained a skipgram model 
from scratch under various settings for only 1 
epoch on wikipedia. For finetuned models we re-
computed the scores based on the publicly avail-
able 6B tokens Glove models and we finetuned un-
til roughly 75% of the vocabulary was seen. 

racy over Vendrov et al. (2016) without tunning 
any of the discriminator's hyperparameters. We 
further report training curve in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Order Embedding Performance 

model (discriminator) we apply ACE to is TransD 
(Ji et al., 2015). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>WN18 experiments: the first portion of 
the table contains results where the base model is 
TransD, the last separated line is the COMPLEX 
embedding model (Trouillon et al., 2016), which 
achieves the SOTA on this dataset. Among all 
TransD based models (the best results in this group 
is underlined), ACE improves over basic NCE and 
another GAN based approach KBGAN. The gap 
on MRR is likely due to the difference between 
TransD and COMPLEX models. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. 2017. Wasserstein GAN</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="983" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A semantic matching energy function for learning with multi-relational data. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Kbgan: Adversarial learning for knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04071</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving GAN training via binarized representation entropy (BRE) regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><forename type="middle">Weiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kry Yik-Chau</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruitong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="898" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01476</idno>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8251</idno>
		<title level="m">Notes on noise contrastive estimation and negative sampling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MaskGAN: Better text generation via filling in the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew M</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07736</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On distinguishability criteria for estimating generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6515</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Backpropagation through the void: Optimizing control variates for black-box gradient estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dami</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00123</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04309</idno>
		<title level="m">Efficient softmax approximation for GPUs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action-dependent control variates for policy optimization via stein identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6426</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Steven J Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04220</idno>
		<title level="m">Augment and reduce: Stochastic inference for large categorical distributions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning structured prediction models: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning approximate inference networks for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2627" to="2636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Incorporating GAN for negative sampling in knowledge representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangyin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the TwentyEighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ducharme</forename><surname>Rejean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jauvin</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
