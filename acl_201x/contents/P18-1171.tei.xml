<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence-to-sequence Models for Cache Transition Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
							<email>satta@dei.unipd.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Padua</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence-to-sequence Models for Cache Transition Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1842" to="1852"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neu-ral AMR parsing, we feed feature embed-dings from the transition state to provide relevant local information for each de-coder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser out-performs other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract Meaning Representation (AMR) ( <ref type="bibr" target="#b4">Banarescu et al., 2013</ref>) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations be- tween the concepts. AMR has been used in vari- ous applications such as text summarization ( <ref type="bibr" target="#b18">Liu et al., 2015</ref>), sentence compression ( <ref type="bibr" target="#b30">Takase et al., 2016)</ref>, and event extraction ( <ref type="bibr" target="#b15">Huang et al., 2016)</ref>. <ref type="bibr">1</ref> The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map nat- ural language strings to AMR semantic graphs. Different parsers have been developed to tackle this problem ( <ref type="bibr" target="#b13">Flanigan et al., 2014;</ref><ref type="bibr">Wang et al., 2015b,a;</ref><ref type="bibr" target="#b25">Peng et al., 2015;</ref><ref type="bibr" target="#b1">Artzi et al., 2015;</ref><ref type="bibr" target="#b28">Pust et al., 2015;</ref><ref type="bibr" target="#b22">van Noord and Bos, 2017)</ref>. On the other hand, due to the limited amount of la- beled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing. <ref type="bibr" target="#b26">Peng et al. (2017)</ref> propose a linearization approach that encodes la- beled graphs as sequences. To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vo- cabulary size for the neural models. <ref type="bibr" target="#b17">Konstas et al. (2017)</ref> use self-training on a huge amount of un- labeled text to lower the out-of-vocabulary rate. However, the final performance still falls behind the best-performing models.</p><p>The best performing AMR parsers model graph structures directly. One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system of , which is currently the top performing system. This raises the question of whether the advantages of neural and transition- based system can be combined, as for example with the syntactic parser of <ref type="bibr" target="#b11">Dyer et al. (2015)</ref>, who use stack LSTMs to capture action history in- formation in the transition state of the transition system. <ref type="bibr" target="#b3">Ballesteros and Al-Onaizan (2017)</ref> ap- ply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions.</p><p>Instead of linearizing the target AMR graph to a sequence structure, <ref type="bibr" target="#b5">Buys and Blunsom (2017)</ref> propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deter- ministic oracle algorithm on the training sentence, AMR graph pairs. They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer.  propose a special transition framework called a cache transition system to gen- erate the set of semantic graphs. They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the tradi- tional stack and buffer.  apply the cache transition system to AMR parsing and design refined action phases, each modeled with a separate feedforward neural network, to deal with some practical implementation issues.</p><p>In this paper, we propose a sequence-to-action- sequence approach for AMR parsing with cache transition systems. We want to take advantage of the sequence-to-sequence model to encode whole- sentence context information and the history ac- tion sequence, while using the transition system to constrain the possible output. The transition system can also provide better local context in- formation than the linearized graph representation, which is important for neural AMR parsing given the limited amount of data.</p><p>More specifically, we use bi-LSTM to encode two levels of input information for AMR pars- ing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories.</p><p>We also want to make better use of the complex transition system to address the data sparsity is- sue for neural AMR parsing. We extend the hard attention model of <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref>, which deals with the nearly-monotonic alignment in the morphological inflection task, to the more general scenario of transition systems where the input buffer is processed from left-to-right. When we process the buffer in this ordered manner, the sequence of target transition actions are also strictly aligned left-to-right according to the in- put order. On the decoder side, we augment the prediction of output action with embedding fea- tures from the current transition state. Our exper- iments show that encoding information from the transition state significantly improves sequence- to-sequence models for AMR parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cache Transition Parser</head><p>We adopt the transition system of , which has been shown to have good cov- erage of the graphs found in AMR.</p><p>A cache transition parser consists of a stack, a cache, and an input buffer. The stack is a sequence σ of (integer, concept) pairs, as explained below, with the topmost element always at the rightmost position. The buffer is a sequence of ordered con- cepts β containing a suffix of the input concept se- quence, with the first element to be read as a newly introduced concept/vertex of the graph. (We use the terms concept and vertex interchangeably in this paper.) Finally, the cache is a sequence of concepts η = [v 1 , . . . , v m ]. The element at the leftmost position is called the first element of the cache, and the element at the rightmost position is called the last element.</p><p>Operationally, the functioning of the parser can be described in terms of configurations and transi- tions. A configuration of our parser has the form:</p><formula xml:id="formula_0">C = (σ, η, β, G p )</formula><p>where σ, η and β are as described above, and G p is the partial graph that has been built so far. The initial configuration of the parser is ([], <ref type="bibr">[$, . . . , $]</ref>, [c 1 , . . . , c n ], ∅), meaning that the stack and the partial graph are initially empty, and the cache is filled with m occurrences of the special symbol $. The buffer is initialized with all the graph vertices constrained by the order of the input sentence. The final configuration is ([], <ref type="bibr">[$, . . . , $]</ref>, [], G), where the stack and the cache are as in the initial configuration and the buffer is empty. The constructed graph is the target AMR graph.</p><formula xml:id="formula_1">stack cache buffer edges actions taken [] [$, $, $] [Per, want-01, go-01] ∅ - [1, $] [$, $, Per] [want-01, go-01] ∅ Shift; PushIndex(1) [1, $] [$, $, Per] [want-01, go-01] ∅ Arc(1, -, NULL); Arc(2, -, NULL) [1, $, 1, $] [$, Per, want-01] [go-01] ∅ Shift; PushIndex(1) [1, $, 1, $] [$, Per, want-01] [go-01] E 1 Arc(1, -, NULL); Arc(2, L, ARG0) [1, $, 1, $, 1, $] [Per, want-01, go-01] [] E 1 Shift; PushIndex(1) [1, $, 1, $, 1, $] [Per, want-01, go-01] [] E 2 Arc(1, L, ARG0); Arc(2, R, ARG1) [1, $, 1, $] [$, Per, want-01 ] [] E 2 Pop [1, $] [$, $, Per] [] E 2 Pop [] [$, $, $] [] E 2 Pop</formula><p>Figure 2: Example run of the cache transition system constructing the graph for the sentence "John wants to go" with cache size of 3. The left four columns show the parser configura- tions after taking the actions shown in the last column.</p><formula xml:id="formula_2">E 1 = {(Per, want-01, L-ARG0)}, E 2 = {(Per, want-01, L-ARG0), (Per, go-01, L-ARG0), (want-01, go-01, R-ARG1)}.</formula><p>In the first step, which is called concept iden- tification, we map the input sentence w 1:n = w 1 , . . . , w n to a sequence of concepts c 1:n = c 1 , . . . , c n . We decouple the problem of concept identification from the transition system and ini- tialize the buffer with a recognized concept se- quence from another classifier, which we will in- troduce later. As the sequence-to-sequence model uses all possible output actions as the target vo- cabulary, this can significantly reduce the target vocabulary size. The transitions of the parser are specified as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Arc(i, d, l) builds an arc with direction d and</head><p>label l between the rightmost concept and the i-th concept in the cache. The label l is NULL if no arc is made and we use the action NOARC in this case. Otherwise we decom- pose the arc decision into two actions ARC and d-l. We consider all arc decisions be- tween the rightmost cache concept and each of the other concepts in the cache. We can consider this phase as first making a binary decision whether there is an arc, and then pre- dicting the label in case there is one, between each concept pair.</p><p>Given the sentence "John wants to go" and the recognized concept sequence "Per want-01 go-01" (person name category Per for "John"), our cache transition parser can construct the AMR graph shown in <ref type="figure" target="#fig_0">Figure 1</ref> using the run shown in <ref type="figure">Figure 2</ref> with cache size of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Oracle Extraction Algorithm</head><p>We use the following oracle algorithm <ref type="bibr" target="#b21">(Nivre, 2008)</ref> to derive the sequence of actions that leads to the gold AMR graph for a cache transition parser with cache size m. The correctness of the oracle is shown by . Let E G be the set of edges of the gold graph G. We maintain the set of vertices that is not yet shifted into the cache as S, which is initialized with all vertices in G. The vertices are ordered according to their aligned position in the word se- quence and the unaligned vertices are listed ac- cording to their order in the depth-first traversal of the graph. The oracle algorithm can look into E G to decide which transition to take next, or else to decide that it should fail. This decision is based on the mutually exclusive rules listed below.</p><p>1. ShiftOrPop phase: the oracle chooses transi- tion Pop, in case there is no edge (v m , v) in E G such that vertex v is in S, or chooses tran- sition Shift and proceeds to the next phase.</p><p>2. PushIndex phase: in this phase, the oracle first chooses a position i (as explained below) in the cache to place the candidate concept and removes the vertex at this position and places its index, vertex pair onto the stack. The oracle chooses transition PushIndex(i) and proceeds to the next phase.</p><p>3. ArcBinary, ArcLabel phases: between the rightmost cache concept and each concept in the cache, we make a binary decision about whether there is an arc between them. If there is an arc, the oracle chooses its direction and label. After arc decisions to m−1 cache con- cepts are made, we jump to the next step.</p><p>4. If the stack and buffer are both empty, and the cache is in the initial state, the oracle fin- ishes with success, otherwise we proceed to the first step.</p><p>We use the equation below to choose the cache concept to take out in the step PushIndex(i). For j ∈ [|β|], we write β j to denote the j-th vertex in β. We choose a vertex v i * in η such that: In words, v i * is the concept from the cache whose closest neighbor in the buffer β is furthest forward in β. We move out of the cache vertex v i * and push it onto the stack, for later processing.</p><formula xml:id="formula_3">i * = argmax i∈[m] min {j | (v i , β j ) ∈ E G }</formula><p>For each training example (x 1:n , g), the tran- sition system generates the output AMR graph g from the input sequence x 1:n through an oracle se- quence a 1:q ∈ Σ * a , where Σ a is the union of all possible actions. We model the probability of the output with the action sequence:</p><formula xml:id="formula_4">P (a 1:q |x 1:n ) = q t=1 P (a t |a 1 , . . . , a t−1 , x 1:n ; θ)</formula><p>which we estimate using a sequence-to-sequence model, as we will describe in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Soft vs Hard Attention for</head><p>Sequence-to-action-sequence</p><p>Shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our sequence-to-sequence model takes a word sequence w 1:n and its mapped concept sequence c 1:n as the input, and the action sequence a 1:q as the output. It uses two BiLSTM encoders, each encoding an input sequence. As the two encoders have the same structure, we only introduce the encoder for the word sequence in de- tail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BiLSTM Encoder</head><p>Given an input word sequence w 1:n , we use a bidi- rectional LSTM to encode it. At each step j, the current hidden states ← − h w j and − → h w j are generated from the previous hidden states ← − h w j+1 and − → h w j−1 , and the representation vector x j of the current in- put word w j :</p><formula xml:id="formula_5">← − h w j = LSTM( ← − h w j+1 , x j ) − → h w j = LSTM( − → h w j−1 , x j )</formula><p>The representation vector x j is the concatenation of the embeddings of its word, lemma, and POS tag, respectively. Then the hidden states of both directions are concatenated as the final hidden state for word w j :</p><formula xml:id="formula_6">h w j = [ ← − h w j ; − → h w j ]</formula><p>Similarly, for the concept sequence, the final hidden state for concept c j is:</p><formula xml:id="formula_7">h c j = [ ← − h c j ; − → h c j ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LSTM Decoder with Soft Attention</head><p>We use an attention-based LSTM decoder (Bah- danau et al., 2014) with two attention memories H w and H c , where H w is the concatenation of the state vectors of all input words, and H c for input concepts correspondingly:</p><formula xml:id="formula_8">H w = [h w 1 ; h w 2 ; . . . ; h w n ]<label>(1)</label></formula><formula xml:id="formula_9">H c = [h c 1 ; h c 2 ; . . . ; h c n ]<label>(2)</label></formula><p>The decoder yields an action sequence a 1 , a 2 , . . . , a q as the output by calculating a se- quence of hidden states s 1 , s 2 . . . , s q recurrently. While generating the t-th output action, the decoder considers three factors: (1) the previous hidden state of the LSTM model s t−1 ; (2) the embedding of the previous generated action e t−1 ; and (3) the previous context vectors for words µ w t−1 and concepts µ c t−1 , which are calculated using H w and H c , respectively. When t = 1, we initialize µ 0 as a zero vector, and set e 0 to the embedding of the start token "s". The hidden state s 0 is initialized as:</p><formula xml:id="formula_10">s 0 = W d [ ← − h w 1 ; − → h w n ; ← − h c 1 ; − → h c n ] + b d ,</formula><p>where W d and b d are model parameters. For each time-step t, the decoder feeds the con- catenation of the embedding of previous action e t−1 and the previous context vectors for words µ w t−1 and concepts µ c t−1 into the LSTM model to update its hidden state.</p><formula xml:id="formula_11">s t = LSTM(s t−1 , [e t−1 ; µ w t−1 ; µ c t−1 ])<label>(3)</label></formula><p>Then the attention probabilities for the word se- quence and the concept sequence are calculated similarly. Take the word sequence as an example, α w t,i on h w i ∈ H w for time-step t is calculated as:</p><formula xml:id="formula_12">t,i = v T c tanh(W h h w i + W s s t + b c ) α w t,i = exp( t,i ) N j=1 exp( t,j ) W h , W</formula><note type="other">s , v c and b c are model parameters. The new context vector µ</note><formula xml:id="formula_13">w t = n i=1 α w t,i h w i .</formula><p>The calcula- tion of µ c t follows the same procedure, but with a different set of model parameters.</p><p>The output probability distribution over all ac- tions at the current state is calculated by:</p><formula xml:id="formula_14">P Σa = softmax(V a [s t ; µ w t ; µ c t ] + b a ),<label>(4)</label></formula><p>where V a and b a are learnable parameters, and the number of rows in V a represents the number of all actions. The symbol Σ a is the set of all actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Monotonic Hard Attention for Transition Systems</head><p>When we process each buffer input, the next few transition actions are closely related to this input position. The buffer maintains the order informa- tion of the input sequence and is processed strictly left-to-right, which essentially encodes a mono- tone alignment between the transition action se- quence and the input sequence.</p><p>As we have generated a concept sequence from the input word sequence, we maintain two hard attention pointers, l w and l c , to model monotonic attention to word and concept sequences respec- tively. The update to the decoder state now relies on a single position of each input sequence in con- trast to Equation 3:</p><formula xml:id="formula_15">s t = LSTM(s t−1 , [e t−1 ; h w lw ; h c lc ])<label>(5)</label></formula><p>Control Mechanism. Both pointers are initial- ized as 0 and advanced to the next position deter- ministically. We move the concept attention focus l c to the next position after arc decisions to all the other m − 1 cache concepts are made. We move the word attention focus l w to its aligned position in case the new concept is aligned, otherwise we don't move the word focus. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, after we have made arc decisions from concept want-01 to the other cache concepts, we move the concept focus to the next concept go-01. As this concept is aligned, we move the word focus to its aligned position go in the word sequence and skip the unaligned word to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Transition State Features for Decoder</head><p>Another difference of our model with <ref type="bibr" target="#b5">Buys and Blunsom (2017)</ref> is that we extract features from the current transition state configuration C t :</p><formula xml:id="formula_16">e f (C t ) = [e f 1 (C t ); e f 2 (C t ); · · · ; e f l (C t )]</formula><p>where l is the number of features extracted from C t and e f k (C t ) (k = 1, . . . , l) represents the em- bedding for the k-th feature, which is learned dur- ing training. These feature embeddings are con- catenated as e f (C t ), and fed as additional input to the decoder. For the soft attention decoder:</p><formula xml:id="formula_17">s t = LSTM(s t−1 , [e t−1 ; µ w t−1 ; µ c t−1 ; e f (C t )])</formula><p>and for the hard attention decoder:</p><formula xml:id="formula_18">s t = LSTM(s t−1 , [e t−1 ; h w lw ; h c lc ; e f (C t )])</formula><p>We use the following features in our experiments:</p><p>1. Phase type: indicator features showing which phase the next transition is.</p><p>2. ShiftOrPop features: token features 3 for the rightmost cache concept and the leftmost buffer concept. Number of dependencies to words on the right, and the top three depen- dency labels for them.</p><p>3. ArcBinary or ArcLabel features: token fea- tures for the rightmost concept and the cur- rent cache concept it makes arc decisions to. Word, concept and dependency distance be- tween the two concepts. The labels for the two most recent outgoing arcs for these two concepts and their first incoming arc and the number of incoming arcs. Dependency label between the two positions if there is a depen- dency arc between them.</p><p>4. PushIndex features: token features for the leftmost buffer concept and all the concepts in the cache.</p><p>The phase type features are deterministic from the last action output. For example, if the last action output is Shift, the current phase type would be PushIndex. We only extract corresponding fea- tures for this phase and fill all the other feature types with -NULL-as placeholders. The features for other phases are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AMR Parsing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Decoding</head><p>We train our models using the cross-entropy loss, over each oracle action sequence a * 1 , . . . , a * q :</p><formula xml:id="formula_19">L = − q t=1 log P (a * t |a * 1 , . . . , a * t−1 , X; θ),<label>(6)</label></formula><p>where X represents the input word and concept sequences, and θ is the model parameters. Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2014</ref>) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best performance on the dev set is se- lected to evaluate on the test set. Dropout with rate 0.3 is used during training. Beam search with a beam size of 10 is used for decoding. Both train- ing and decoding use a Tesla K20X GPU. Hidden state sizes for both encoder and decoder are set to 100. The word embeddings are ini- tialized from Glove pretrained word embeddings ( <ref type="bibr" target="#b27">Pennington et al., 2014</ref>) on Common Crawl, and are not updated during training. The embeddings for POS tags and features are randomly initialized, with the sizes of 20 and 50, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preprocessing and Postprocessing</head><p>As the AMR data is very sparse, we collapse some subgraphs or spans into categories based on the alignment. We define some special cate- gories such as named entities (NE), dates (DATE), single rooted subgraphs involving multiple con- cepts (MULT) 4 , numbers (NUMBER) and phrases (PHRASE). The phrases are extracted based on the multiple-to-one alignment in the training data. One example phrase is more than which aligns to a single concept more-than. We first collapse spans and subgraphs into these categories based on the alignment from the JAMR aligner ( <ref type="bibr" target="#b13">Flanigan et al., 2014)</ref>, which greedily aligns a span of words to AMR subgraphs using a set of heuristics. This cat- egorization procedure enables the parser to capture mappings from continuous spans on the sentence side to connected subgraphs on the AMR side.</p><p>We use the semi-Markov model from <ref type="bibr" target="#b12">Flanigan et al. (2016)</ref> as the concept identifier, which jointly segments the sentence into a sequence of spans and maps each span to a subgraph. During decod- ing, our output has categories, and we need to map  <ref type="table">Table 1</ref>: Performance breakdown of each transi- tion phase.</p><p>each category to the corresponding AMR concept or subgraph. We save a table Q which shows the original subgraph each category is collapsed from, and map each category to its original subgraph representation. We also use heuristic rules to gen- erate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on mean- ing representation parsing <ref type="bibr" target="#b20">(May, 2016</ref>  ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>We categorize the training data using the auto- matic alignment and dump a template for date en- tities and frequent phrases from the multiple to one alignment. We also generate an alignment table from tokens or phrases to their candidate target- side subgraphs. For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and ex- tract date entities by matching spans with the date template. We further categorize the dataset with the categories we have defined. After categoriza- tion, we use Stanford CoreNLP ( ) to get the POS tags and dependencies of the categorized dataset. We run the oracle algorithm separately for training and dev data (with align- ment) to get the statistics of individual phases. We use a cache size of 5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Individual Phase Accuracy We first evaluate the prediction accuracy of individual phases on the dev oracle data assuming gold prediction history. The four transition phases ShiftOrPop, PushIndex, ArcBinary, and ArcLabel account for 25%, 12.5%, 50.1%, and 12.4% of the total transition actions respectively. <ref type="table">Table 1</ref> shows the phase-wise ac- curacy of our sequence-to-sequence model.  use a separate feedforward network to predict each phase independently. We use the same alignment from the SemEval dataset as in  to avoid differences resulting from the aligner. Soft+feats shows the perfor- mance of our sequence-to-sequence model with soft attention and transition state features, while Hard+feats is using hard attention. We can see that the hard attention model outperforms the soft attention model in all phases, which shows that the single-pointer attention finds more relevant in- formation than the soft attention on the relatively small dataset. The sequence-to-sequence mod- els perform better than the feedforward model of  on ShiftOrPop and ArcBinary, which shows that the whole-sentence context in- formation is important for the prediction of these two phases. On the other hand, the sequence-to- sequence models perform worse than the feedfor- ward models on PushIndex and ArcLabel. One possible reason is that the model tries to optimize the overall accuracy, while these two phases ac- count for fewer than 25% of the total transition actions and might be less attended to during the update. <ref type="table" target="#tab_2">Table 2</ref> shows the impact of different components for the sequence-to-sequence model. We can see that the transition state features play a very important role for predicting the correct transition action. This is because different transition phases have very different prediction behaviors and need different types of local information for the prediction. Rely- ing on the sequence-to-sequence model alone does not perform well in disambiguating these choices, while the transition state can enforce direct con- straints. We can also see that while the hard at- tention only attends to one position of the input, it performs slightly better than the soft attention model, while the time complexity is lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Different Components</head><p>Impact of Different Cache Sizes The cache size of the transition system can be optimized as a trade-off between coverage of AMR graphs and the prediction accuracy. While larger cache size increases the coverage of AMR graphs, it com- plicates the prediction procedure with more cache decisions to make. From <ref type="table">Table 3</ref> we can see that System P R F Soft 0.55 0.51 0.53 Soft+feats 0.69 0.63 0.66 Hard+feats 0.70 0.64 0.67  <ref type="table">Table 3</ref>: Impact of cache size for the sequence- to-sequence model, hard attention (dev). the hard attention model performs best with cache size 5. The soft attention model also achieves best performance with the same cache size.</p><p>Comparison with other Parsers <ref type="table" target="#tab_3">Table 4</ref> shows the comparison with other AMR parsers. The first three systems are some competitive neural models. We can see that our parser significantly outper- forms the sequence-to-action-sequence model of <ref type="bibr" target="#b5">Buys and Blunsom (2017)</ref>. <ref type="bibr" target="#b17">Konstas et al. (2017)</ref> use a linearization approach that linearizes the AMR graph to a sequence structure and use self- training on 20M unlabeled Gigaword sentences. Our model achieves better results without using additional unlabeled data, which shows that rele- vant information from the transition system is very useful for the prediction. Our model also outper- forms the stack-LSTM model by <ref type="bibr" target="#b3">Ballesteros and Al-Onaizan (2017)</ref>, while their model is evaluated on the previous release of LDC2014T12.   We also show the performance of some of the best-performing models. While our hard attention achieves slightly lower performance in compari- son with <ref type="bibr" target="#b32">Wang et al. (2015a)</ref> and , it is worth noting that their approaches of using WordNet, semantic role labels and word cluster features are complimentary to ours. The alignment from the aligner and the concept iden- tification identifier also play an important role for improving the performance.  propose to improve AMR parsing by improving the alignment and concept identification, which can also be combined with our system to im- prove the performance of a sequence-to-sequence model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dealing with Reentrancy</head><p>Reentrancy is an im- portant characteristic of AMR, and we evaluate the Smatch score only on the reentrant edges fol- lowing <ref type="bibr" target="#b9">Damonte et al. (2017)</ref>. From <ref type="table" target="#tab_4">Table 5</ref> we can see that our hard attention model significantly outperforms the feedforward model of  in predicting reentrancies. This is because predicting reentrancy is directly related to the Ar- cBinary phase of the cache transition system since it decides to make multiple arc decisions to the same vertex, and we can see from <ref type="table">Table 1</ref> that the hard attention model has significantly better prediction accuracy in this phase. We also com- pare the reentrancy results of our transition system with two other systems, <ref type="bibr" target="#b9">Damonte et al. (2017)</ref> and JAMR, where these statistics are available. From <ref type="table" target="#tab_4">Table 5</ref>, we can see that our cache transition sys- tem slightly outperforms these two systems in pre- dicting reentrancies. <ref type="figure" target="#fig_5">Figure 5</ref> shows a reentrancy example where JAMR and the feedforward network of  do not predict well, while our system pre- dicts the correct output. JAMR fails to predict the reentrancy arc from desire-01 to i, and connects the wrong arc from "live-01" to "-" instead of from "desire-01". The feedforward model of   Our hard attention output:</p><p>Sentence: I have no desire to live in any city .</p><p>Cache arc decisions creating the reentrancy (cache size of 5): and live-01 to i. This error is because their feed- forward ArcBinary classifier does not model long- term dependency and usually prefers making arcs between words that are close and not if they are distant. Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc deci- sions shown in <ref type="figure" target="#fig_5">Figure 5</ref>. When desire-01 and live- 01 are shifted into the cache respectively, the tran- sition system makes a left-going arc from each of them to the same concept i, thus creating the reen- trancy as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented a sequence-to- action-sequence approach for cache transition sys- tems and applied it to AMR parsing. To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and im- proving the performance of sequence-to-sequence models. We also show that the monotonic hard at- tention model can be generalized to the transition- based framework and outperforms the soft atten- tion model when limited data is available. While we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks ( <ref type="bibr" target="#b23">Oepen et al., 2015;</ref><ref type="bibr" target="#b10">Du et al., 2015;</ref><ref type="bibr" target="#b34">Zhang et al., 2016;</ref><ref type="bibr" target="#b7">Cao et al., 2017</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of AMR graph representing the meaning of: "John wants to go"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sequence-to-sequence model with soft attention, encoding a word sequence and concept sequence separately by two BiLSTM encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sequence-to-sequence model with monotonic hard attention. Different colors show the changes of hard attention focus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>fails to predict the two arcs from desire-01</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example showing how our system predicts the correct reentrancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Impact of various components for the 
sequence-to-sequence model (dev). 

Cache Size 
P 
R 
F 
4 
0.69 0.63 0.66 
5 
0.70 0.64 0.67 
6 
0.69 0.64 0.66 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison to other AMR parsers. 
*Model has been trained on the previous release 
of the corpus (LDC2014T12). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Reentrancy statistics. 

</table></figure>

			<note place="foot" n="1">. Pop pops a pair (i, v) from the stack, where the integer i records the position in the cache that it originally came from. We place concept v in position i in the cache, shifting the remainder of the cache one position to the right, and discarding the last element in the cache. 2. Shift signals that we will start processing the next input concept, which will become a new vertex in the output graph. 3. PushIndex(i) shifts the next input concept out of the buffer and moves it into the last position of the cache. We also take out the concept v i appearing at position i in the cache and push it onto the stack σ, along with the integer i recording its original position in the cache. 2 2 Our transition design is different from Peng et al. (2018) in two ways: the PushIndex phase is initiated before making all the arc decisions; the newly introduced concept is placed at the last cache position instead of the leftmost buffer position, which essentially increases the cache size by 1.</note>

			<note place="foot" n="3"> Concept, concept category at the specified position in concept sequence. And the word, lemma, POS tag at the aligned input position.</note>

			<note place="foot" n="4"> For example, verbalization of &quot;teacher&quot; as &quot;(person :ARG0-of teach-01)&quot;, or &quot;minister&quot; as &quot;(person :ARG0-of (have-org-role-91 :ARG2 minister))&quot;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Morphological inflection generation with hard monotonic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG semantic parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AMR parsing using stack-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1269" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust incremental neural semantic graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1215" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing to 1-endpoint-crossing, pagenumber-2 graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th</title>
		<meeting>the 55th</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2110" to="2120" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An incremental parser for abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Peking: Building semantic dependency graphs with a hybrid parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="927" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CMU at SemEval-2016 task 8: Graph-based AMR parsing with infinite ramp loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1202" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cache transition systems for graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="118" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Liberal event extraction and event schema induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="258" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural AMR: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 8: Meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page" from="1063" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural semantic parsing by character-based translation: Experiments with abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09980</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semeval 2015 task 18: Broad-coverage semantic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="915" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AMR parsing with cache transition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the National Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A synchronous hyperedge replacement grammar based approach for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15)</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Addressing the data sparsity issue in neural AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the ACL (EACL-17)</title>
		<meeting>the European Chapter of the ACL (EACL-17)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parsing English into abstract meaning representation using syntaxbased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1143" to="1154" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural headline generation on abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Getting the most out of AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1257" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting transition-based AMR parsing with refined actions and auxiliary analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15)</title>
		<meeting>the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15)<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transition-based parsing for deep dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="389" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
