<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Robust Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
							<email>chengyong3001@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Beijing Advanced Innovation Center for Language Resources</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Robust Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1756" to="1766"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1756</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adver-sarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) models have advanced the state of the art by building a sin- gle neural network that can better learn represen- tations ( <ref type="bibr" target="#b7">Cho et al., 2014;</ref><ref type="bibr">Sutskever et al., 2014</ref>). The neural network consists of two components: an encoder network that encodes the input sen- tence into a sequence of distributed representa- tions, based on which a decoder network generates the translation with an attention model ( <ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Luong et al., 2015)</ref>. A variety of NMT models derived from this encoder-decoder frame- work have further improved the performance of machine translation systems ( <ref type="bibr" target="#b8">Gehring et al., 2017;</ref><ref type="bibr">Vaswani et al., 2017)</ref>. NMT is capable of general- izing better to unseen text by exploiting word simi- larities in embeddings and capturing long-distance reordering by conditioning on larger contexts in a continuous way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>tamen bupa kunnan zuochu weiqi AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>They are not afraid of difficulties to make Go AI. Input tamen buwei kunnan zuochu weiqi AI. Output They are not afraid to make Go AI. <ref type="table">Table 1</ref>: The non-robustness problem of neural machine translation. Replacing a Chinese word with its synonym (i.e., "bupa" → "buwei") leads to significant erroneous changes in the English trans- lation. Both "bupa" and "buwei" can be translated to the English phrase "be not afraid of."</p><p>However, studies reveal that very small changes to the input can fool state-of-the-art neural net- works with high probability ( <ref type="bibr" target="#b10">Goodfellow et al., 2015;</ref><ref type="bibr">Szegedy et al., 2014</ref>). <ref type="bibr" target="#b3">Belinkov and Bisk (2018)</ref> confirm this finding by pointing out that NMT models are very brittle and easily falter when presented with noisy input. In NMT, due to the introduction of RNN and attention, each contextual word can influence the model predic- tion in a global context, which is analogous to the "butterfly effect." As shown in <ref type="table">Table 1</ref>, although we only replace a source word with its synonym, the generated translation has been completely dis- torted. We investigate severe variations of trans- lations caused by small input perturbations by re- placing one word in each sentence of a test set with its synonym. We observe that 69.74% of transla- tions have changed and the BLEU score is only 79.01 between the translations of the original in- puts and the translations of the perturbed inputs, suggesting that NMT models are very sensitive to small perturbations in the input. The vulnerabil- ity and instability of NMT models limit their ap- plicability to a broader range of tasks, which re- quire robust performance on noisy inputs. For ex- ample, simultaneous translation systems use auto-matic speech recognition (ASR) to transcribe in- put speech into a sequence of hypothesized words, which are subsequently fed to a translation sys- tem. In this pipeline, ASR errors are presented as sentences with noisy perturbations (the same pro- nunciation but incorrect words), which is a signif- icant challenge for current NMT models. More- over, instability makes NMT models sensitive to misspellings and typos in text translation.</p><p>In this paper, we address this challenge with adversarial stability training for neural machine translation. The basic idea is to improve the ro- bustness of two important components in NMT: the encoder and decoder. To this end, we pro- pose two approaches to constructing noisy inputs with small perturbations to make NMT models re- sist them. As important intermediate representa- tions encoded by the encoder, they directly deter- mine the accuracy of final translations. We intro- duce adversarial learning to make behaviors of the encoder consistent for both an input and its per- turbed counterpart. To improve the stability of the decoder, our method jointly maximizes the likeli- hoods of original and perturbed data. Adversarial stability training has the following advantages:</p><p>1. Improving both the robustness and transla- tion performance: Our adversarial stability training is capable of not only improving the robustness of NMT models but also achiev- ing better translation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Applicable to arbitrary noisy perturbations:</head><p>In this paper, we propose two approaches to constructing noisy perturbations for inputs. However, our training framework can be eas- ily extended to arbitrary noisy perturbations. Especially, we can design task-specific per- turbation methods.</p><p>3. Transparent to network architectures: Our adversarial stability training does not depend on specific NMT architectures. It can be ap- plied to arbitrary NMT systems.</p><p>Experiments on Chinese-English, English- French and English-German translation tasks show that adversarial stability training achieves significant improvements across different lan- guages pairs. Our NMT system outperforms the state-of-the-art RNN-based NMT system (GNMT) ( <ref type="bibr" target="#b5">Wu et al., 2016)</ref> and obtains compara- ble performance with the CNN-based NMT sys- tem ( <ref type="bibr" target="#b8">Gehring et al., 2017)</ref>. Related experimen- tal analyses validate that our training approach can improve the robustness of NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>NMT is an end-to-end framework which directly optimizes the translation probability of a target sentence y = y 1 , ..., y N given its corresponding source sentence x = x 1 , ..., x M :</p><formula xml:id="formula_0">P (y|x; θ) = N n=1 P (y n |y &lt;n , x; θ)<label>(1)</label></formula><p>where θ is a set of model parameters and y &lt;n is a partial translation. P (y|x; θ) is defined on a holis- tic neural network which mainly includes two core components: an encoder encodes a source sen- tence x into a sequence of hidden representations H x = H 1 , ..., H M , and a decoder generates the n-th target word based on the sequence of hidden representations:</p><formula xml:id="formula_1">P (y n |y &lt;n , x; θ) ∝ exp{g(y n−1 , s n , H x ; θ)} (2)</formula><p>where s n is the n-th hidden state on target side. Thus the model parameters of NMT include the parameter sets of the encoder θ enc and the decoder θ dec : θ = {θ enc , θ dec }. The standard training ob- jective is to minimize the negative log-likelihood of the training corpus</p><formula xml:id="formula_2">S = {{x (s) , y (s) } |S| s=1 : ˆ θ = argmin θ L(x, y; θ) = argmin θ x,y∈S − log P (y|x; θ)<label>(3)</label></formula><p>Due to the vulnerability and instability of deep neural networks, NMT models usually suffer from a drawback: small perturbations in the input can dramatically deteriorate its translation results. Be- linkov and Bisk (2018) point out that character- based NMT models are very brittle and easily fal- ter when presented with noisy input. We find that word-based and subword-based NMT mod- els also confront with this shortcoming, as shown in <ref type="table">Table 1</ref>. We argue that the distributed repre- sentations should fulfill the stability expectation, which is the underlying concept of the proposed approach. Recent work has shown that adversar- ially trained models can be made robust to such perturbations ( <ref type="bibr">Zheng et al., 2016;</ref><ref type="bibr" target="#b21">Madry et al., 2018)</ref>. Inspired by this, in this work, we im- prove the robustness of encoder representations against noisy perturbations with adversarial learn- ing ( <ref type="bibr" target="#b9">Goodfellow et al., 2014</ref>).</p><formula xml:id="formula_3">x' x +perturbations Encoder H x H x'</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Discriminator</head><p>Linv(x, x') L true (x, y) Lnoisy(x', y) <ref type="figure">Figure 1</ref>: The architecture of NMT with adversar- ial stability training. The dark solid arrow lines represent the forward-pass information flow for the input sentence x, while the red dashed arrow lines for the noisy input sentence x , which is transformed from x by adding small perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The goal of this work is to propose a general ap- proach to make NMT models learned to be more robust to input perturbations. Our basic idea is to maintain the consistency of behaviors through the NMT model for the source sentence x and its perturbed counterpart x . As aforementioned, the NMT model contains two procedures for project- ing a source sentence x to its target sentence y: the encoder is responsible for encoding x as a se- quence of representations H x , while the decoder outputs y with H x as input. We aim at learning the perturbation-invariant encoder and decoder. <ref type="figure">Figure 1</ref> illustrates the architecture of our ap- proach. Given a source sentence x, we construct a set of perturbed sentences N (x), in which each sentence x is constructed by adding small per- turbations to x. We require that x is a subtle variation from x and they have similar semantics. Given the input pair (x, x ), we have two expecta- tions: (1) the encoded representation H x should be close to H x ; and <ref type="formula">(2)</ref> given H x , the decoder is able to generate the robust output y. To this end, we introduce two additional objectives to improve the robustness of the encoder and decoder:</p><p>• L inv (x, x ) to encourage the encoder to out- put similar intermediate representations H x and H x for x and x to achieve an invariant encoder, which benefits outputting the same translations. We cast this objective in the ad- versarial learning framework.</p><p>• L noisy (x , y) to guide the decoder to generate output y given the noisy input x , which is modeled as − log P (y|x ). It can also be de- fined as KL divergence between P (y|x) and P (y|x ) that indicates using P (y|x) to teach P (y|x ).</p><p>As seen, the two introduced objectives aim to im- prove the robustness of the NMT model which can be free of high variances in target outputs caused by small perturbations in inputs. It is also natural to introduce the original training objective L(x, y) on x and y, which can guarantee good transla- tion performance while keeping the stability of the NMT model. Formally, given a training corpus S, the adver- sarial stability training objective is</p><formula xml:id="formula_4">J (θ) = x,y∈S L true (x, y; θ enc , θ dec ) +α x ∈N (x) L inv (x, x ; θ enc , θ dis ) +β x ∈N (x) L noisy (x , y; θ enc , θ dec )<label>(4)</label></formula><p>where L true (x, y) and L noisy (x , y) are calculated using Equation 3, and L inv (x, x ) is the adversar- ial loss to be described in Section 3.3. α and β control the balance between the original transla- tion task and the stability of the NMT model. θ = {θ enc , θ dec , θ dis } are trainable parameters of the encoder, decoder, and the newly introduced dis- criminator used in adversarial learning. As seen, the parameters of encoder θ enc and decoder θ dec are trained to minimize both the translation loss L true (x, y) and the stability losses (L noisy (x , y) and</p><formula xml:id="formula_5">L inv (x, x )).</formula><p>Since L noisy (x , y) evaluates the translation loss on the perturbed neighbour x and its corre- sponding target sentence y, it means that we aug- ment the training data by adding perturbed neigh- bours, which can potentially improve the transla- tion performance. In this way, our approach not only makes the output of NMT models more ro- bust, but also improves the performance on the original translation task.</p><p>In the following sections, we will first describe how to construct perturbed inputs with different strategies to fulfill different goals (Section 3.2), followed by the proposed adversarial learning mechanism for the perturbation-invariant encoder (Section 3.3). We conclude this section with the training strategy (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing Perturbed Inputs</head><p>At each training step, we need to generate a per- turbed neighbour set N (x) for each source sen- tence x for adversarial stability training. In this paper, we propose two strategies to construct the perturbed inputs at multiple levels of representa- tions.</p><p>The first approach generates perturbed neigh- bours at the lexical level. Given an input sentence x, we randomly sample some word positions to be modified. Then we replace words at these posi- tions with other words in the vocabulary according to the following distribution:</p><formula xml:id="formula_6">P (x|x i ) = exp {cos (E[x i ], E[x])} x∈Vx\x i exp {cos (E[x i ], E[x])}<label>(5)</label></formula><p>where E[x i ] is the word embedding for word x i , V x \x i is the source vocabulary set excluding the word x i , and cos (E[x i ], E <ref type="bibr">[x]</ref>) measures the simi- larity between word x i and x. Thus we can change the word to another word with similar semantics. One potential problem of the above strategy is that it is hard to enumerate all possible positions and possible types to generate perturbed neigh- bours. Therefore, we propose a more general ap- proach to modifying the sentence at the feature level. Given a sentence, we can obtain the word embedding for each word. We add the Gaussian noise to a word embedding to simulate possible types of perturbations. That is</p><formula xml:id="formula_7">E[x i ] = E[x i ] + , ∼ N(0, σ 2 I)<label>(6)</label></formula><p>where the vector is sampled from a Gaussian dis- tribution with variance σ 2 . σ is a hyper-parameter. We simply introduce Gaussian noise to all of word embeddings in x.</p><p>The proposed scheme is a general framework where one can freely define the strategies to con- struct perturbed inputs. We just present two pos- sible examples here. The first strategy is poten- tially useful when the training data contains noisy words, while the latter is a more general strategy to improve the robustness of common NMT mod- els. In practice, one can design specific strategies for particular tasks. For example, we can replace correct words with their homonyms (same pronun- ciation but different meanings) to improve NMT models for simultaneous translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial Learning for the Perturbation-invariant Encoder</head><p>The goal of the perturbation-invariant encoder is to make the representations produced by the en- coder indistinguishable when fed with a correct sentence x and its perturbed counterpart x , which is directly beneficial to the output robustness of the decoder. We cast the problem in the adversar- ial learning framework ( <ref type="bibr" target="#b9">Goodfellow et al., 2014</ref>).</p><p>The encoder serves as the generator G, which de- fines the policy that generates a sequence of hid- den representations H x given an input sentence x.</p><p>We introduce an additional discriminator D to dis- tinguish the representation of perturbed input H x from that of the original input H x . The goal of the generator G (i.e., encoder) is to produce sim- ilar representations for x and x which could fool the discriminator, while the discriminator D tries to correctly distinguish the two representations. Formally, the adversarial learning objective is</p><formula xml:id="formula_8">L inv (x, x ; θ enc , θ dis ) = E x∼S [− log D(G(x))] + E x ∼N (x) − log(1 − D(G(x )))<label>(7)</label></formula><p>The discriminator outputs a classification score given an input representation, and tries to max- imize D(G(x)) to 1 and minimize D(G(x )) to 0. The objective encourages the encoder to output similar representations for x and x , so that the discriminator fails to distinguish them. The training procedure can be regarded as a min-max two-player game. The encoder parame- ters θ enc are trained to maximize the loss function to fool the discriminator. The discriminator pa- rameters θ dis are optimized to minimize this loss for improving the discriminating ability. For ef- ficiency, we update both the encoder and the dis- criminator simultaneously at each iteration, rather than the periodical training strategy that is com- monly used in adversarial learning. <ref type="bibr" target="#b17">Lamb et al. (2016)</ref> also propose a similar idea to use Professor Forcing to make the behaviors of RNNs be indis- tinguishable when training and sampling the net- works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>As shown in <ref type="figure">Figure 1</ref>, our training objective in- cludes three sets of model parameters for three modules. We use mini-batch stochastic gradient descent to optimize our model. In the forward pass, besides a mini-batch of x and y, we also construct a mini-batch consisting of the perturbed neighbour</p><note type="other">x and y. We propagate the informa- tion to calculate these three loss functions accord- ing to arrows. Then, gradients are collected to up- date three sets of model parameters. Except for the gradients of L inv with respect to θ enc are mul- tiplying by −1, other gradients are normally back- propagated. Note that we update θ inv and θ enc si- multaneously for training efficiency.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluated our adversarial stability training on translation tasks of several language pairs, and re- ported the 4-gram BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>) score as calculated by the multi-bleu.perl script. Chinese-English We used the LDC corpus con- sisting of 1.25M sentence pairs with 27.9M Chi- nese words and 34.5M English words respectively. We selected the best model using the NIST 2006 set as the validation set (hyper-parameter opti- mization and model selection). <ref type="bibr">The NIST 2002</ref><ref type="bibr">, 2003</ref><ref type="bibr">, 2004</ref><ref type="bibr">, 2005</ref>, and 2008 datasets are used as test sets. English-German We used the WMT 14 corpus containing 4.5M sentence pairs with 118M En- glish words and 111M German words. The vali- dation set is newstest2013, and the test set is new- stest2014. English-French We used the IWSLT corpus which contains 0.22M sentence pairs with 4.03M English words and 4.12M French words. The IWLST corpus is very dissimilar from the NIST and WMT corpora. As they are collected from TED talks and inclined to spoken language, we want to verify our approaches on the non- normative text. The IWSLT 14 test set is taken as the validation set and 15 test set is used as the test set.</p><p>For English-German and English-French, we tokenize both English, German and French words using tokenize.perl script. We follow <ref type="bibr" target="#b27">Sennrich et al. (2016b)</ref> to split words into sub- word units. The numbers of merge operations in byte pair encoding (BPE) are set to 30K, 40K and 30K respectively for Chinese-English, English-German, and English-French. We re- port the case-sensitive tokenized BLEU score for English-German and English-French and the case- insensitive tokenized BLEU score for Chinese- English.</p><p>Our baseline system is an in-house NMT sys- tem. Following <ref type="bibr" target="#b2">Bahdanau et al. (2015)</ref>, we im- plement an RNN-based NMT in which both the encoder and decoder are two-layer RNNs with residual connections between layers ( <ref type="bibr" target="#b12">He et al., 2016b</ref>). The gating mechanism of RNNs is gated recurrent unit (GRUs) ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>). We apply layer normalization ( <ref type="bibr">Ba et al., 2016</ref>) and dropout ( <ref type="bibr" target="#b13">Hinton et al., 2012</ref>) to the hidden states of GRUs. Dropout is also added to the source and target word embeddings. We share the same ma- trix between the target word embeedings and the pre-softmax linear transformation ( <ref type="bibr">Vaswani et al., 2017)</ref>. We update the set of model parameters us- ing Adam SGD ( <ref type="bibr" target="#b16">Kingma and Ba, 2015)</ref>. Its learn- ing rate is initially set to 0.05 and varies according to the formula in <ref type="bibr">Vaswani et al. (2017)</ref>.</p><p>Our adversarial stability training initializes the model based on the parameters trained by maxi- mum likelihood estimation (MLE). We denote ad- versarial stability training based on lexical-level perturbations and feature-level perturbations re- spectively as AST lexical and AST feature . We only sample one perturbed neighbour x ∈ N (x) for training efficiency. For the discriminator used in L inv , we adopt the CNN discriminator proposed by <ref type="bibr" target="#b15">Kim (2014)</ref> to address the variable-length prob- lem of the sequence generated by the encoder. In the CNN discriminator, the filter windows are set to 3, 4, 5 and rectified linear units are applied af- ter convolution operations. We tune the hyper- parameters on the validation set through a grid search. We find that both the optimal values of α and β are set to 1.0. The standard variance in Gaussian noise used in the formula (6) is set to 0.01. The number of words that are replaced in the sentence x during lexical-level perturbations is taken as max(0.2|x|, 1) in which |x| is the length of x. The default beam size for decoding is 10.    Chinese-English NIST datasets trained on RNN- based NMT. <ref type="bibr">Shen et al. (2016)</ref> propose minimum risk training (MRT) for NMT, which directly op- timizes model parameters with respect to BLEU scores. <ref type="bibr">Wang et al. (2017)</ref> address the issue of severe gradient diffusion with linear associative units (LAU). Their system is deep with an encoder of 4 layers and a decoder of 4 layers. <ref type="bibr">Zhang et al. (2018)</ref> propose to exploit both left-to-right and right-to-left decoding strategies for NMT to cap- ture bidirectional dependencies. Compared with them, our NMT system trained by MLE outper- forms their best models by around 3 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">NIST Chinese-English Translation</head><p>We hope that the strong baseline systems used in this work make the evaluation convincing.</p><p>We find that introducing adversarial stability training into NMT can bring substantial improve- ments over previous work (up to +3.16 BLEU points over <ref type="bibr">Shen et al. (2016)</ref>, up to +3.51 BLEU points over <ref type="bibr">Wang et al. (2017)</ref> and up to +2.74 BLEU points over <ref type="bibr">Zhang et al. (2018)</ref>) and our system trained with MLE across all the datasets. Compared with our baseline system, AST lexical achieves +1.75 BLEU improvement on average. AST feature performs better, which can obtain +2.59 BLEU points on average and up to +3.34 BLEU points on NIST08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">WMT 14 English-German Translation</head><p>In <ref type="table" target="#tab_2">Table 3</ref>, we list existing NMT systems as com- parisons. All these systems use the same WMT 14 English-German corpus. Except that <ref type="bibr">Shen et al. (2016)</ref> and <ref type="bibr" target="#b5">Wu et al. (2016)</ref> respectively adopt MRT and reinforcement learning (RL), other sys- tems all use MLE as training criterion. All the sys- tems except for <ref type="bibr">Shen et al. (2016)</ref> are deep NMT models with no less than four layers. Google's neural machine translation (GNMT) ( <ref type="bibr" target="#b5">Wu et al., 2016</ref>) represents a strong RNN-based NMT sys- tem. Compared with other RNN-based NMT sys- tems except for GNMT, our baseline system with two layers can achieve better performance than theirs.</p><p>When training our NMT system with AST leixcal , significant improvement (+1.  <ref type="table">Table 5</ref>: Translation results of synthetic perturbations on the validation set in Chinese-English translation. "1 Op." denotes that we conduct one operation (swap, replacement or deletion) on the original sentence.</p><note type="other">11 Synthetic Type Training 0 Op. 1 Op. 2 Op. 3 Op. 4 Op. 5 Op</note><p>Source zhongguo dianzi yinhang yewu guanli xingui jiangyu sanyue yiri qi shixing Reference china's new management rules for e-banking operations to take effect on march 1 MLE china's electronic bank rules to be implemented on march 1</p><p>AST lexical new rules for business administration of china 's electronic banking industry will come into effect on march 1 .</p><p>AST feature new rules for business management of china 's electronic banking industry to come into effect on march 1 Perturbed Source zhongfang dianzi yinhang yewu guanli xingui jiangyu sanyue yiri qi shixing MLE china to implement new regulations on business management AST lexical the new regulations for the business administrations of the chinese electronics bank will come into effect on march 1 .</p><p>AST feature new rules for business management of china's electronic banking industry to come into effect on march 1 <ref type="table">Table 6</ref>: Example translations of a source sentence and its perturbed counterpart by replacing a Chinese word "zhongguo" with its synonym "zhongfang."</p><p>BLEU points) can be observed. AST feature can obtain slightly better performance. Our NMT system outperforms the state-of-the-art RNN-based NMT system, GNMT, with +0.66 BLEU point and performs comparably with <ref type="bibr" target="#b8">Gehring et al. (2017)</ref> which is based on CNN with 15 layers. Given that our approach can be applied to any NMT systems, we expect that the adversarial stability training mechanism can further improve performance upon the advanced NMT architectures. We leave this for future work. <ref type="table" target="#tab_3">Table 4</ref> shows the results on IWSLT English- French Translation. Compared with our strong baseline system trained by MLE, we observe that our models consistently improve translation per- formance in all datasets. AST feature can achieve significant improvements on the tst2015 although AST lexical obtains comparable results. These demonstrate that our approach maintains good per- formance on the non-normative text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">IWSLT English-French Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Synthetic Perturbed Data</head><p>In order to investigate the ability of our training approaches to deal with perturbations, we experi- ment with three types of synthetic perturbations:</p><p>• Swap: We randomly choose N positions from a sentence and then swap the chosen words with their right neighbours.</p><p>• Replacement: We randomly replace sam- pled words in the sentence with other words.</p><p>• Deletion: We randomly delete N words from each sentence in the dataset.</p><p>As shown in <ref type="table">Table 5</ref>, we can find that our train- ing approaches, AST lexical and AST feature , consis- tently outperform MLE against perturbations on all the numbers of operations. This means that our <ref type="table">Table 7</ref>: Ablation study of adversarial stabil- ity training AST lexical on Chinese-English trans- lation. " √ " means the loss function is included in the training objective while "×" means it is not.</p><formula xml:id="formula_9">L true L noisy L adv BLEU √ × × 41.38 √ × √ 41.91 × √ × 42.20 √ √ × 42.93 √ √ √ 43.57</formula><p>approaches have the capability of resisting pertur- bations. Along with the number of operations in- creasing, the performance on MLE drops quickly. Although the performance of our approaches also drops, we can see that our approaches consistently surpass MLE. In AST lexical , with 0 operation, the difference is +2. <ref type="bibr">19 (43.57 Vs. 41.38</ref>) for all syn- thetic types, but the differences are enlarged to +3.20, +9.39, and +3.12 respectively for the three types with 5 operations.</p><p>In the Swap and Deletion types, AST lexical and AST feature perform comparably after more than four operations. Interestingly, AST lexical per- forms significantly better than both of MLE and AST feature after more than one operation in the Replacement type. This is because AST lexical trains the model specifically on perturbation data that is constructed by replacing words, which agrees with the Replacement Type. Overall, AST lexical performs better than AST feature against perturbations after multiple operations. We spec- ulate that the perturbation method for AST lexical and synthetic type are both discrete and they keep more consistent. <ref type="table">Table 6</ref> shows example transla- tions of a Chinese sentence and its perturbed coun- terpart.</p><p>These findings indicate that we can construct specific perturbations for a particular task. For example, in simultaneous translation, an auto- matic speech recognition system usually generates wrong words with the same pronunciation of cor- rect words, which dramatically affects the quality of machine translation system. Therefore, we can design specific perturbations aiming for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Ablation Study</head><p>Our training objective function Eq. <ref type="formula" target="#formula_4">(4)</ref>   study on the Chinese-English translation to under- stand the importance of these loss functions by choosing AST lexical as an example. As <ref type="table">Table 7</ref> shows, if we remove L adv , the translation perfor- mance decreases by 0.64 BLEU point. However, when L noisy is excluded from the training objec- tive function, it results in a significant drop of 1.66 BLEU point. Surprisingly, only using L noisy is able to lead to an increase of 0.88 BLEU point. <ref type="figure">Figure 2</ref> shows the changes of BLEU scores over iterations respectively for AST lexical and AST feature . They behave nearly consistently. Ini- tialized by the model trained by MLE, their per- formance drops rapidly. Then it starts to go up quickly. Compared with the starting point, the maximal dropping points reach up to about 7.0 BLEU points. Basically, the curves present the state of oscillation. We think that introducing random perturbations and adversarial learning can make the training not very stable like MLE. <ref type="figure">Figure 3</ref> shows the learning curves of three loss functions, L true , L inv and L noisy . We can find that their costs of loss functions decrease not steadily. Similar to the <ref type="figure">Figure 2</ref>, there still exist oscilla- tions in the learning curves although they do not change much sharply. We find that L inv converges to around 0.68 after about 100K iterations, which indicates that discriminator outputs probability 0.5 for both positive and negative samples and it can- not distinguish them. Thus the behaviors of the encoder for x and its perturbed neighbour x per- form nearly consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">BLEU Scores over Iterations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Learning Curves of Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is inspired by two lines of research: (1) adversarial learning and (2) data augmentation.</p><p>Adversarial Learning Generative Adversarial Network (GAN) ( <ref type="bibr" target="#b9">Goodfellow et al., 2014</ref>) and its related derivative have been widely applied in computer vision ( <ref type="bibr" target="#b24">Radford et al., 2015;</ref><ref type="bibr" target="#b25">Salimans et al., 2016</ref>) and natural language process- ing ( <ref type="bibr">Yang et al., 2018)</ref>. Previous work has constructed adversarial examples to at- tack trained networks and make networks resist them, which has proved to improve the robust- ness of networks ( <ref type="bibr" target="#b10">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b22">Miyato et al., 2016;</ref><ref type="bibr">Zheng et al., 2016)</ref>. <ref type="bibr" target="#b3">Belinkov and Bisk (2018)</ref> introduce adversarial examples to training data for character-based NMT models. In contrast to theirs, adversarial stability training aims to stabilize both the encoder and decoder in NMT models. We adopt adversarial learning to learn the perturbation-invariant encoder.</p><p>Data Augmentation Data augmentation has the capability to improve the robustness of NMT mod- els. In NMT, there is a number of work that aug- ments the training data with monolingual corpora ( <ref type="bibr" target="#b26">Sennrich et al., 2016a;</ref><ref type="bibr" target="#b5">Cheng et al., 2016;</ref><ref type="bibr" target="#b11">He et al., 2016a;</ref><ref type="bibr">Zhang and Zong, 2016)</ref>. They all leverage complex models such as inverse NMT models to generate translation equivalents for monolingual corpora. Then they augment the par- allel corpora with these pseudo corpora to improve NMT models. Some authors have recently en- deavored to achieve zero-shot NMT through trans- ferring knowledge from bilingual corpora of other language pairs ( <ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr">Zheng et al., 2017;</ref> or monolingual corpora ( <ref type="bibr" target="#b18">Lample et al., 2018;</ref><ref type="bibr" target="#b0">Artetxe et al., 2018)</ref>. Our work significantly differs from these work. We do not resort to any complicated models to generate perturbed data and do not depend on extra mono- lingual or bilingual corpora. The way we exploit is more convenient and easy to implement. We focus more on improving the robustness of NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed adversarial stability training to improve the robustness of NMT models. The ba- sic idea is to train both the encoder and decoder robust to input perturbations by enabling them to behave similarly for the original input and its per- turbed counterpart. We propose two approaches to construct perturbed data to adversarially train the encoder and stabilize the decoder. Experi- ments on Chinese-English, English-German and English-French translation tasks show that the pro- posed approach can improve both the robustness and translation performance. As our training framework is not limited to spe- cific perturbation types, it is interesting to evalu- ate our approach in natural noise existing in prac- tical applications, such as homonym in the simul- taneous translation system. It is also necessary to further validate our approach on more advanced NMT architectures, such as CNN-based NMT <ref type="bibr" target="#b8">(Gehring et al., 2017)</ref> and <ref type="bibr">Transformer (Vaswani et al., 2017</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: BLEU scores of AST lexical over iterations on Chinese-English validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 shows</head><label>2</label><figDesc>the results on Chinese-English translation. Our strong baseline system signifi- cantly outperforms previously reported results on</figDesc><table>System 
Training 
MT06 MT02 MT03 MT04 MT05 MT08 
Shen et al. (2016) 
MRT 
37.34 40.36 40.93 41.37 38.81 29.23 
Wang et al. (2017) MLE 
37.29 -
39.35 41.15 38.07 -
Zhang et al. (2018) MLE 
38.38 -
40.02 42.32 38.84 -

this work 

MLE 
41.38 43.52 41.50 43.64 41.58 31.60 
AST lexical 43.57 44.82 42.95 45.05 43.45 34.85 
AST feature 44.44 46.10 44.07 45.61 44.06 34.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Case-insensitive BLEU scores on Chinese-English translation.</head><label>2</label><figDesc></figDesc><table>System 
Architecture 
Training 
BLEU 
Shen et al. (2016) 
Gated RNN with 1 layer 
MRT 
20.45 
Luong et al. (2015) 
LSTM with 4 layers 
MLE 
20.90 
Kalchbrenner et al. (2017) ByteNet with 30 layers 
MLE 
23.75 
Wang et al. (2017) 
DeepLAU with 4 layers 
MLE 
23.80 
Wu et al. (2016) 
LSTM with 8 layers 
RL 
24.60 
Gehring et al. (2017) 
CNN with 15 layers 
MLE 
25.16 
Vaswani et al. (2017) 
Self-attention with 6 layers MLE 
28.40 

this work 
Gated RNN with 2 layers 

MLE 
24.06 
AST lexical 25.17 
AST feature 25.26 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Case-sensitive BLEU scores on WMT 14 English-German translation.</head><label>3</label><figDesc></figDesc><table>Training 
tst2014 tst2015 
MLE 
36.92 
36.90 
AST lexical 37.35 
37.03 
AST feature 38.03 
37.64 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Case-sensitive BLEU scores on IWSLT 
English-French translation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>. Swap MLE 41.38 38.86 37.23 35.97 34.61 32.96 AST lexical 43.57 41.18 39.88 37.95 37.02 36.16 AST feature 44.44 42.08 40.20 38.67 36.89 35.81</figDesc><table>Replacement 

MLE 
41.38 37.21 31.40 27.43 23.94 21.03 
AST lexical 43.57 40.53 37.59 35.19 32.56 30.42 
AST feature 44.44 40.04 35.00 30.54 27.42 24.57 

Deletion 

MLE 
41.38 38.45 36.15 33.28 31.17 28.65 
AST lexical 43.57 41.89 38.56 36.14 34.09 31.77 
AST feature 44.44 41.75 39.06 36.16 33.49 30.90 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their in-sightful comments and suggestions. We also </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z. Yang, W. Chen, F. Wang, and B. Xu. 2018. Improv- ing Neural Machine Translation with Conditional</head><p>Sequence Generative Adversarial Nets. In Proceed- ings of NAACL. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A teacher-student framework for zeroresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semisupervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint training for pivot-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alias</forename><surname>Parth Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makelov</forename><surname>Aleksandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmidt</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributional smoothing with virtual adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLEU: a methof for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving nerual machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
