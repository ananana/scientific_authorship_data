<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Domain-Sensitive and Sentiment-Aware Word Embeddings *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2494</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>lyndonbing@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Domain-Sensitive and Sentiment-Aware Word Embeddings *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2494" to="2504"/>
							<date type="published">July 15-20, 2018. 2018. 2494</date>
						</imprint>
					</monogr>
					<note>* This work was partially done when Bei Shi was an intern at Tencent AI Lab. This project is substantially sup-ported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains , some existing methods for word embeddings exploit sentiment information , but they cannot produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embed-dings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification aims to predict the sen- timent polarity, such as "positive" or "negative", over a piece of review. It has been a long-standing research topic because of its importance for many applications such as social media analysis, e- commerce, and marketing <ref type="bibr" target="#b18">(Liu, 2012;</ref><ref type="bibr" target="#b23">Pang et al., 2008)</ref>. Deep learning has brought in progress in various NLP tasks, including sentiment clas- sification. Some researchers focus on design- ing RNN or CNN based models for predicting sentence level <ref type="bibr" target="#b15">(Kim, 2014)</ref> or aspect level sen- timent ( <ref type="bibr" target="#b17">Li et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2017;</ref><ref type="bibr" target="#b32">Wang et al., 2016</ref>). These works directly take the word embeddings pre-trained for general purpose as ini- tial word representations and may conduct fine tuning in the training process. Some other re- searchers look into the problem of learning task- specific word embeddings for sentiment classifi- cation aiming at solving some limitations of ap- plying general pre-trained word embeddings. For example, <ref type="bibr" target="#b30">Tang et al. (2014b)</ref> develop a neural net- work model to convey sentiment information in the word embeddings. As a result, the learned embeddings are sentiment-aware and able to dis- tinguish words with similar syntactic context but opposite sentiment polarity, such as the words "good" and "bad". In fact, sentiment information can be easily obtained or derived in large scale from some data sources (e.g., the ratings provided by users), which allows reliable learning of such sentiment-aware embeddings.</p><p>Apart from these words (e.g. "good" and "bad") with consistent sentiment polarity in differ- ent contexts, the polarity of some sentiment words is domain-sensitive. For example, the word "lightweight" usually connotes a positive senti- ment in the electronics domain since a lightweight device is easier to carry. In contrast, in the movie domain, the word "lightweight" usually connotes a negative opinion describing movies that do not in- voke deep thoughts among the audience. This ob- servation motivates the study of learning domain- sensitive word representations ( <ref type="bibr" target="#b5">Bollegala et al., 2015</ref><ref type="bibr" target="#b7">Bollegala et al., , 2014</ref>). They basically learn separate embeddings of the same word for differ- ent domains. To bridge the semantics of individual embedding spaces, they select a subset of words that are likely to be domain-insensitive and align the dimensions of their embeddings. However, the sentiment information is not exploited in these methods although they intend to tackle the task of sentiment classification.</p><p>In this paper, we aim at learning word em- beddings that are both domain-sensitive and sentiment-aware.</p><p>Our proposed method can jointly model the sentiment semantics and do- main specificity of words, expecting the learned embeddings to achieve superior performance for the task of sentiment classification. Specifically, our method can automatically determine and pro- duce domain-common embeddings and domain- specific embeddings. Domain-common embed- dings represent the fact that the semantics of a word including its sentiment and meaning in different domains are very similar. For exam- ple, the words "good" and "interesting" are usu- ally domain-common and convey consistent se- mantic meanings and positive sentiments in dif- ferent domains. Thus, they should have simi- lar embeddings across domains. On the other hand, domain-specific word embeddings represent the fact that the sentiments or meanings across domains are different. For example, the word "lightweight" represents different sentiment polar- ities in the electronics domain and the movie do- main. Moreover, some polysemous words have different meanings in different domains. For ex- ample, the term "apple" refers to the famous tech- nology company in the electronics domain or a kind of fruit in the food domain.</p><p>Our model exploits the information of sen- timent labels and context words to distinguish domain-common and domain-specific words. If a word has similar sentiments and contexts across domains, it indicates that the word has com- mon semantics in these domains, and thus it is treated as domain-common. Otherwise, the word is considered as domain-specific. The learning of domain-common embeddings can allow the ad- vantage of data augmentation of common seman- tics of multiple domains, and meanwhile, domain- specific embeddings allow us to capture the varied semantics of specific words in different domains. Specifically, for each word in the vocabulary, we design a distribution to depict the probability of the word being domain-common. The inference of the probability distribution is conducted based on the observed sentiments and contexts. As men- tioned above, we also exploit the information of sentiment labels for the learning of word embed- dings that can distinguish words with similar syn- tactic context but opposite sentiment polarity.</p><p>To demonstrate the advantages of our domain- sensitive and sentiment-aware word embeddings, we conduct experiments on four domains, includ- ing books, DVSs, electronics, and kitchen appli- ances. The experimental results show that our model can outperform the state-of-the-art mod- els on the task of sentence level sentiment clas- sification. Moreover, we conduct lexicon term sentiment classification in two common sentiment lexicon sets to evaluate the effectiveness of our sentiment-aware embeddings learned from mul- tiple domains, and it shows that our model out- performs the state-of-the-art models on most do- mains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Traditional vector space models encode individual words using the one-hot representation, namely, a high-dimensional vector with all zeroes ex- cept in one component corresponding to that word ( <ref type="bibr" target="#b1">Baeza-Yates et al., 1999</ref>). Such represen- tations suffer from the curse of dimensionality, as there are many components in these vectors due to the vocabulary size. Another drawback is that semantic relatedness of words cannot be modeled using such representations. To address these shortcomings, <ref type="bibr" target="#b26">Rumelhart et al. (1988)</ref> pro- pose to use distributed word representation in- stead, called word embeddings. Several tech- niques for generating such representations have been investigated. For example, Bengio et al. pro- pose a neural network architecture for this pur- pose ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b2">Bengio, 2009</ref>). Later, <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref> propose two methods that are considerably more efficient, namely skip-gram and CBOW. This work has made it possible to learn word embeddings from large data sets, which has led to the current popularity of word embed-dings. Word embedding models have been ap- plied to many tasks, such as named entity recogni- tion ( <ref type="bibr" target="#b31">Turian et al., 2010)</ref>, word sense disambigua- tion <ref type="bibr" target="#b9">(Collobert et al., 2011;</ref><ref type="bibr" target="#b14">Iacobacci et al., 2016;</ref><ref type="bibr" target="#b35">Zhang and Hasan, 2017;</ref><ref type="bibr" target="#b10">Dave et al., 2018)</ref>, pars- ing ( <ref type="bibr" target="#b25">Roth and Lapata, 2016)</ref>, and document clas- sification ( <ref type="bibr">Tang et al., 2014a,b;</ref><ref type="bibr" target="#b27">Shi et al., 2017</ref>).</p><p>Sentiment classification has been a long- standing research topic <ref type="bibr" target="#b18">(Liu, 2012;</ref><ref type="bibr" target="#b23">Pang et al., 2008;</ref><ref type="bibr" target="#b8">Chen et al., 2017;</ref><ref type="bibr" target="#b20">Moraes et al., 2013)</ref>. Given a review, the task aims at predicting the sentiment polarity on the sentence level <ref type="bibr" target="#b15">(Kim, 2014)</ref> or the aspect level ( <ref type="bibr" target="#b17">Li et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2017)</ref>. Supervised learning algorithms have been widely used in sentiment classification ( <ref type="bibr" target="#b22">Pang et al., 2002)</ref>. People usually use different ex- pressions of sentiment semantics in different do- mains. Due to the mismatch between domain- specific words, a sentiment classifier trained in one domain may not work well when it is directly applied to other domains. Thus cross-domain sentiment classification algorithms have been ex- plored ( <ref type="bibr" target="#b21">Pan et al., 2010;</ref><ref type="bibr" target="#b16">Li et al., 2009;</ref><ref type="bibr" target="#b12">Glorot et al., 2011</ref>). These works usually find com- mon feature spaces across domains and then share learned parameters from the source domain to the target domain. For example, <ref type="bibr" target="#b21">Pan et al. (2010)</ref> propose a spectral feature alignment algorithm to align words from different domains into unified clusters. Then the clusters can be used to reduce the gap between words of the two domains, which can be used to train sentiment classifiers in the tar- get domain. Compared with the above works, our model focuses on learning both domain-common and domain-specific embeddings given reviews from all the domains instead of only transferring the common semantics from the source domain to the target domain.</p><p>Some researchers have proposed some methods to learn task-specific word embeddings for senti- ment classification (Tang et al., 2014a,b). <ref type="bibr" target="#b30">Tang et al. (2014b)</ref> propose a model named SSWE to learn sentiment-aware embedding via incorporat- ing sentiment polarity of texts in the loss func- tions of neural networks. Without the consid- eration of varied semantics of domain-specific words in different domains, their model cannot learn sentiment-aware embeddings across multi- ple domains. Some works have been proposed to learn word representations considering multi- ple domains ( <ref type="bibr" target="#b0">Bach et al., 2016;</ref><ref type="bibr" target="#b5">Bollegala et al., 2015</ref>). Most of them learn sep- arate embeddings of the same word for differ- ent domains. Then they choose pivot words ac- cording to frequency-based statistical measures to bridge the semantics of individual embedding spaces. A regularization formulation enforcing that word representations of pivot words should be similar in different domains is added into the original word embedding framework. For exam- ple,  use Sørensen-Dice coeffi- cient <ref type="bibr">(Sørensen, 1948)</ref> for detecting pivot words and learn word representations across domains. Even though they evaluate the model via the task of sentiment classification, sentiment information associated with the reviews are not considered in the learned embeddings. Moreover, the selection of pivot words is according to frequency-based statistical measures in the above works. In our model, the domain-common words are jointly de- termined by sentiment information and context words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Description</head><p>We propose a new model, named DSE, for learn- ing Domain-sensitive and Sentiment-aware word Embeddings. For presentation clarity, we describe DSE based on two domains. Note that it can be easily extended for more than two domains, and we remark on how to extend near the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design of Embeddings</head><p>We assume that the input consists of text reviews of two domains, namely D p and D q . Each review r in D p and D q is associated with a sentiment la- bel y which can take on the value of 1 and 0 de- noting that the sentiment of the review is positive and negative respectively.</p><p>In our DSE model, each word w in the whole vocabulary Λ is associated with a domain- common vector U c w and two domain-specific vec- tors, namely U p w specific to the domain p and U q w specific to the domain q. The dimension of these vectors is d. The design of U c w , U p w and U q w re- flects one characteristic of our model: allowing a word to have different semantics across different domains. The semantic of each word includes not only the semantic meaning but also the sentiment orientation of the word. If the semantic of w is consistent in the domains p and q, we use the vec- tor U c w for both domains. Otherwise, w is repre-sented by U p w and U q w for p and q respectively. In traditional cross-domain word embedding methods ( <ref type="bibr" target="#b5">Bollegala et al., 2015</ref><ref type="bibr" target="#b6">Bollegala et al., , 2016</ref>, each word is represented by different vec- tors in different domains without differentiation of domain-common and domain-specific words. In contrast to these methods, for each word w, we use a latent variable z w to depict its domain common- ality. When z w = 1, it means that w is common in both domains. Otherwise, w is specific to the domain p or the domain q.</p><p>In the standard skip-gram model <ref type="bibr" target="#b19">(Mikolov et al., 2013)</ref>, the probability of predicting the context words is only affected by the relatedness with the target words. In our DSE model, pre- dicting the context words also depends on the domain-commonality of the target word, i.e z w . For example, assume that there are two domains, e.g. the electronics domain and the movie do- main. If z w = 1, it indicates a high probability of generating some domain-common words such as "good", "bad" or "satisfied". Otherwise, the domain-specific words are more likely to be gen- erated such as "reliable", "cheap" or "compacts" for the electronics domain. For a word w, we as- sume that the probability of predicting the context word w t is formulated as follows:</p><formula xml:id="formula_0">p(w t |w) = k∈{0,1} p(w t |w, z w = k)p(z w = k) (1)</formula><p>If w is a domain-common word without differ- entiating p and q, the probability of predicting w t can be defined as:</p><formula xml:id="formula_1">p(w t |w, z w = 1) = exp(U c w · V wt ) w ∈Λ exp(U c w · V w )<label>(2)</label></formula><p>where Λ is the whole vocabulary and V w is the output vector of the word w . If w is a domain-specific word, the probability of p(w t |w, z w = 0) is specific to the occurrence of w in D p or D q . For individual training instances, the occurrences of w in D p or D q have been estab- lished. Then the probability of p(w t |w, z w = 0) can be defined as follows:</p><formula xml:id="formula_2">p(w t |w, z w = 0) =        exp(U p w ·Vw t ) w ∈Λ exp(U p w ·V w ) , if w ∈ D p exp(U q w ·Vw t ) w ∈Λ exp(U q w ·V w ) , if w ∈ D q<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exploiting Sentiment Information</head><p>In our DSE model, the prediction of review sen- timent depends on not only the text information but also the domain-commonality. For exam- ple, the domain-common word "good" has high probability to be positive in different reviews across multiple domains. However, for the word "lightweight", it would be positive in the electron- ics domain, but negative in the movie domain. We define the polarity y w of each word w to be consis- tent with the sentiment label of the review: if we observe that a review is associated with a positive label, the words in the review are associated with a positive label too. Then, the probability of predict- ing the sentiment for the word w can be defined as:</p><formula xml:id="formula_3">p(y w |w) = k∈{0,1} p(y w |w, z w = k)p(z w = k)<label>(4)</label></formula><p>If z w = 1, the word w is a domain-common word. The probability p(y w = 1|w, z w = 1) can be de- fined as:</p><formula xml:id="formula_4">p(y w = 1|w, z w = 1) = σ(U c w · s)<label>(5)</label></formula><p>where σ(·) is the sigmoid function and the vector s with dimension d represents the boundary of the sentiment. Moreover, we have:</p><formula xml:id="formula_5">p(y w = 0|w, z w = 1) = 1−p(y w = 1|w, z w = 1)<label>(6)</label></formula><p>If w is a domain-specific word, similarly, the probability p(y w = 1|w, z w = 0) is defined as:</p><formula xml:id="formula_6">p(y w = 1|w, z w = 0) = σ(U p w · s) if w ∈ D p σ(U q w · s) if w ∈ D q<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference Algorithm</head><p>We need an inference method that can learn, given D p and D q , the values of the model pa- rameters, namely, the domain-common embed- ding U c w , and the domain-specific embeddings U p w and U q w , as well as the domain-commonality dis- tribution p(z w ) for each word w. Our inference method combines the Expectation-Maximization (EM) method with a negative sampling scheme. It is summarized in Algorithm 1. In the E-step, we use the Bayes rule to evaluate the posterior distri- bution of z w for each word and derive the objective function. In the M-step, we maximize the objec- tive function with the gradient descent method and</p><note type="other">Algorithm 1 EM negative sampling for DSE 1: Initialize U c w , U p w , U q w , V , s, p(z w ) 2: for iter = 1 to Max iter do 3: for each review r in D p and D q do 4: for each word w in r do 5: Sample negative instances from the distribution P. 6: Update p(z w |w, c w , y w ) by Eq. 11 and Eq. 15 respectively. 7: end for 8: end for 9: Update p(z w ) using Eq. 13 10: Update U c w , U p w , U q w , V , s via Maximizing Eq. 14 11: end for update the corresponding embeddings U c w , U p w and U q w .</note><p>With the input of D p and D q , the likelihood function of the whole training set is:</p><formula xml:id="formula_7">L = L p + L q<label>(8)</label></formula><p>where L p and L q are the likelihood of D p and D q respectively. For each review r from D p , to learn domain- specific and sentiment-aware embeddings, we wish to predict the sentiment label and context words together. Therefore, the likelihood function is defined as follows:</p><formula xml:id="formula_8">L p = r∈D p w∈r log p(y w , c w |w)<label>(9)</label></formula><p>where y w is the sentiment label and c w is the set of context words of w. For the simplification of the model, we assume that the sentiment label y w and the context words c w of the word w are condi- tionally dependent. Then the likelihood L p can be rewritten as:</p><formula xml:id="formula_9">L p = r∈D p w∈r wt∈cw log p(w t |w)+ r∈D p w∈r log p(y w |w)<label>(10)</label></formula><p>where p(w t |w) and p(y w |w) are defined in Eq. 1 and Eq. 4 respectively. The likelihood of the re- views from D q , i.e L q , is defined similarly. For each word w in the review r, in the E-step, the posterior probability of z w given c w and y w is:</p><formula xml:id="formula_10">p(zw = k|w, cw, yw) = p(zw = k)p(yw|w, zw = k) w t ∈cw p(wt|w, zw = k) k ∈{0,1} p(zw = k )p(yw|w, zw = k ) w t ∈cw p(wt|w, zw = k )<label>(11)</label></formula><p>In the M-step, given the posterior distribution of z w in Eq. 11, the goal is to maxmize the following Q function:</p><formula xml:id="formula_11">Q = r∈{D p ,D q } w∈r zw p(z w |w, y w , w t+j ) × log(p(z w )p(c w , y|z, w t )) = r∈{D p ,D q } w∈r zw p(z w |w, y w , c w )</formula><p>[log p(z w ) + log(y w |z, w)+ wt∈cw log p(w t |z w , w)]</p><p>Using the Lagrange multiplier, we can obtain the update rule of p(z w ), satisfying the normaliza- tion constraints that zw∈0,1 p(z w ) = 1 for each word w:</p><formula xml:id="formula_13">p(z w ) = r∈{D p ,D q } w∈r p(z w |w, y w , c w ) r∈{D p ,D q } n(w, r)<label>(13)</label></formula><p>where n(w, r) is the number of occurrence of the word w in the review r.</p><p>To obtain U c w , U p w and U q w , we collect the related items in Eq. 12 as follows:</p><formula xml:id="formula_14">Q U = r∈{D p ,D q } w∈r zw p(z w |w, y w , w t+j )</formula><p>[log(y w |z w , w) + wt∈cw log p(w t |z w , w)]</p><p>Note that computing the value p(w t |w, z w ) based on Eq. 2 and Eq. 3 is not feasible in prac- tice, given that the computation cost is propor- tional to the size of Λ. However, similar to the skip-gram model, we can rely on negative sam- pling to address this issue. Therefore we esti- mate the probability of predicting the context word p(w t |w, z w = 1) as follows:</p><formula xml:id="formula_16">log p(w t |w, z w = 1) ∝ log σ(U c w · V wt ) + n i=1 E w i ∼P [log σ(−U c w · V w i )]<label>(15)</label></formula><p>where w i is a negative instance which is sam- pled from the word distribution P (.). <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref> have investigated many choices for P (w) and found that the best P (w) is equal to the unigram distribution Unigram(w) raised to the 3/4rd power. We adopt the same setting. The probability p(w t |w, z w = 0) in Eq. 3 can be ap- proximated in a similar manner. After the substitution of p(w t |w, z w ), we use the Stochastic Gradient Descent method to maxi- mize Eq. 14, and obtain the update of U c w , U p w and U q w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">More Discussions</head><p>In our model, for simplifying the inference algo- rithm and saving the computational cost, we as- sume that the target word w t in the context and the sentiment label y w of the word w are condition- ally independent. Such technique has also been used in other popular models such as the bi-gram language model. Otherwise, we need to consider the term p(w t |w, y w ), which complicates the in- ference algorithm.</p><p>We define the formulation of the term p(w t |w, z) to be similar to the original skip- gram model instead of the CBOW model. The CBOW model averages the context words to predict the target word. The skip-gram model uses pairwise training examples which are much easier to integrate with sentiment information.</p><p>Note that our model can be easily extended to more than two domains. Similarly, we use a domain-specific vector for each word in each domain and each word is also associated with a domain-common vector. We just need to extend the probability distribution of z w from Bernoulli distribution to Multinomial distribution according to the number of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We conducted experiments on the Amazon prod- uct reviews collected by <ref type="bibr" target="#b4">Blitzer et al. (2007)</ref>. We use four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). A category corresponds to a domain. For each do- main, there are 17,457 unlabeled reviews on aver- age associated with rating scores from 1.0 to 5.0 for each domain. We use unlabeled reviews with rating score higher than 3.0 as positive reviews and unlabeled reviews with rating score lower than 3.0 as negative reviews for embedding learning. We first remove reviews whose length is less than 5 words. We also remove punctuations and the stop words. We also stem each word to its root form us- ing Porter Stemmer <ref type="bibr" target="#b24">(Porter, 1980)</ref>. Note that this review data is used for embedding learning, and the learned embeddings are used as feature vectors of words to conduct the experiments in the later two subsections.</p><p>Given the reviews from two domains, namely, D p and D q , we compare our results with the fol- lowing baselines and state-of-the-art methods: SSWE The SSWE model <ref type="bibr">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EmbeddingP and EmbeddingQ</head><p>In Embed- dingP, we use the original skip-gram method ( <ref type="bibr" target="#b19">Mikolov et al., 2013</ref>) to learn word  </p><formula xml:id="formula_17">B &amp; D B &amp; E B &amp; K D &amp; E D &amp; K E &amp; K Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 BOW 0.680</formula><formula xml:id="formula_18">U w =            if w ∈ D p U c w × p(z w ) ⊕ U p w × (1.0 − p(z w )) if w ∈ D q U c w × p(z w ) ⊕ U q w × (1.0 − p(z w ))<label>(16)</label></formula><p>where ⊕ denotes the concatenation operator. For all word embedding methods, we set the di- mension to 200. For the skip-gram based methods, we sample 5 negative instances and the size of the windows for each target word is 3. For our DSE model, the number of iterations for the whole re- views is 100 and the learning rate is set to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Review Sentiment Classification</head><p>For the task of review sentiment classification, we use 1000 positive and 1000 negative sentiment re- views labeled by <ref type="bibr" target="#b4">Blitzer et al. (2007)</ref> for each do- main to conduct experiments. We randomly se- lect 800 positive and 800 negative labeled reviews from each domain as training data, and the remain- ing 200 positive and 200 negative labeled reviews as testing data. We use the SVM classifier <ref type="bibr" target="#b11">(Fan et al., 2008</ref>) with linear kernel to train on the train- ing reviews for each domain, with each review represented as the average vector of its word em- beddings.</p><p>We use two metrics to evaluate the performance of sentiment classification. One is the standard ac- curacy metric. The other one is Macro-F1, which is the average of F1 scores for both positive and negative reviews.</p><p>We conduct multiple trials by selecting every possible two domains from books (B), DVDs (D), electronic items (E) and kitchen appliances (K). We use the average of the results of each two do- mains. The experimental results are shown in Ta- ble 1.</p><p>From <ref type="table">Table 1</ref>, we can see that compared with other baseline methods, our DSE w model can achieve the best performance of sentiment classi- fication across most combinations of the four do- mains. Our statistical t-tests for most of the com- binations of domains show that the improvement of our DSE w model over Yang and SSWE is sta- tistically significant respectively (p-value &lt; 0.05) at 95% confidence level. It shows that our method can capture the domain-commonality and senti- ment information at the same time.</p><p>Even though both of the SSWE model and our DSE model can learn sentiment-aware word embeddings, our DSE w model can outperform SSWE. It demonstrates that compared with gen- eral sentiment-aware embeddings, our learned domain-common and domain-specific word em-  Compared with the method of Yang which learns cross-domain embeddings, our DSE w model can achieve better performance. It is be- cause we exploit sentiment information to dis- tinguish domain-common and domain-specific words during the embedding learning process. The sentiment information can also help the model distinguish the words which have similar contexts but different sentiments.</p><formula xml:id="formula_19">B &amp; D B &amp; E B &amp; K D &amp; E D &amp; K E &amp; K</formula><p>Compared with EmbeddingP and EmbeddingQ, the methods of EmbeddingAll and Embedding- Cat can achieve better performance. The reason is that the data augmentation from other domains helps sentiment classification in the original do- main. Our DSE model also benefits from such kind of data augmentation with the use of reviews from D p and D q .</p><p>We observe that our DSE w variant performs better than the variant of DSE c . Compared with DSE c , our DSE w variant adds the item of p(z w ) as the weight to combine domain-common embed- dings and domain-specific embeddings. It shows that the domain-commonality distribution in our DSE model, i.e p(w z ), can effectively model the domain-sensitive information of each word and help review sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lexicon Term Sentiment Classification</head><p>To further evaluate the quality of the sentiment se- mantics of the learned word embeddings, we also conduct lexicon term sentiment classification on two popular sentiment lexicons, namely HL (Hu and <ref type="bibr" target="#b13">Liu, 2004</ref>) and MPQA ( <ref type="bibr" target="#b33">Wilson et al., 2005</ref>). The words with neutral sentiment and phrases are removed. The statistics of HL and MPQA are shown in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>We conduct multiple trials by selecting every possible two domains from books (B), DVDs (D), electronics items (E) and kitchen appliances (K).  For each trial, we learn the word embeddings. For our DSE model, we only use the domain-common part to represent each word because the lexicons are usually not associated with a particular do- main. For each lexicon, we select 80% to train the SVM classifier with linear kernel and the remain- ing 20% to test the performance. The learned em- bedding is treated as the feature vector for the lex- icon term. We conduct 5-fold cross validation on all the lexicons. The evaluation metric is Macro- F1 of positive and negative lexicons. <ref type="table" target="#tab_3">Table 2</ref> shows the experimental results of lex- icon term sentiment classification. Our DSE method can achieve competitive performance among all the methods. Compared with SSWE, our DSE is still competitive because both of them consider the sentiment information in the embed- dings. Our DSE model outperforms other methods which do not consider sentiments such as Yang, EmbeddingCat and EmbeddingAll. Note that the advantage of domain-sensitive embeddings would be insufficient for this task because the sentiment lexicons are not domain-specific. <ref type="table">Table 4</ref> shows the probabilities of "lightweight", "die", "mysterious", and "great" to be domain- common for different domain combinations. For "lightweight", its domain-common probability for the books domain and the DVDs domain ("B &amp; D") is quite high, i.e. p(z = 1) = 0.999, and the review examples in the last column show that the word "lightweight" expresses the meaning of lacking depth of content in books or movies. Note that most reviews of DVDs are about movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Term</head><p>Domain p(z = 1) Sample Reviews "lightweight"</p><p>B &amp; D 0.999 -I find Seth Godin's books incredibly lightweight. There is really nothing of any substance here.(B) -I love the fact that it's small and lightweight and fits into a tiny pocket on my camera case so I never lose track of it.(E) -These are not "lightweight" actors.   <ref type="table">Table 4</ref>: Learned domain-commonality for some words. p(z = 1) denotes the probability that the word is domain-common. The letter in parentheses indicates the domain of the review.</p><p>In the electronics domain and the kitchen appli- ances domain ("E &amp; K"), "lightweight" means light material or weighing less than average, thus the domain-common probability for these two do- mains is also high, 0.696. In contrast, for the other combinations, the probability of "lightweight" to be domain-common is much smaller, which in- dicates that the meaning of "lightweight" varies. Similarly, "die" in the domains of electronics and kitchen appliances ("E &amp; K") means that some- thing does not work any more, thus, we have p(z = 1) = 0.712. While for the books do- main, it conveys meaning that somebody passed away in some stories. The word "mysterious" con- veys a positive sentiment in the books domain, in- dicating how wonderful a story is, but it conveys a negative sentiment in the electronics domain typi- cally describing that a product breaks down unpre- dictably. Thus, its domain-common probability is small. The last example is the word "great", and it usually has positive sentiment in all domains, thus has large values of p(z = 1) for all domain com- binations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>BOW</head><label></label><figDesc>We use the traditional bag of words model to represent each review in the training data. For our DSE model, we have two variants to represent each word. The first variant DSE c rep- resents each word via concatenating the domain- common vector and the domain-specific vector. The second variant DSE w concatenates domain- common word embeddings and domain-specific word embeddings by considering the domain- commonality distribution p(z w ). For individual review instances, the occurrences of w in D p or D q have been established. The representation of w is specific to the occurrence of w in D p or D q . Specifically, each word w can be represented as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>-</head><label></label><figDesc>'m glad Brando lived long enough to get old and fat, and that he didn't die tragically young like Marilyn, JFK, or Jimi Hendrix.(B) -Like many others here, my CD-changer died after a couple of weeks and it wouldn't read any CD.(E) -I had this toaster for under 3 years when I came home one day and it smoked and died. (K)This novel really does cover the gamut: stunning twists, genuine love, beautiful settings, desire for riches, mysterious murders, detective investigations, false accusations, and self vindication.(B) -Caller ID functionality for Vonage mysteriously stopped working even though this phone's REN is rated at 0.1b. (E)This is a great book for anyone learning how to handle dogs.(B) -This is a great product, and you can get it, along with any other products on Amazon up to $500 Free!(E) -I grew up with drag racing in the 50s, 60s &amp; 70s and this film gives a great view of what it was like.(D) -This is a great mixer its a little loud but worth it for the power you get.(K)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of lexicon term sentiment classification. 

beddings can capture semantic variations of words 
across multiple domains. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the sentiment lexicons.</figDesc><table></table></figure>

			<note place="foot" n="1"> We use the implementation from https: //github.com/attardi/deepnl/wiki/ Sentiment-Specific-Word-Embeddings. 2 We use the implementation from http://statnlp. org/research/lr/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a new method of learning domain-sensitive and sentiment-aware word embeddings. Compared with existing sentiment-aware embed-dings, our model can distinguish domain-common and domain-specific words with the considera-tion of varied semantics across multiple domains. Compared with existing domain-sensitive meth-ods, our model detects domain-common words ac-cording to not only similar context words but also sentiment information. Moreover, our learned em-beddings considering sentiment information can distinguish words with similar syntactic context but opposite sentiment polarity. We have con-ducted experiments on two downstream sentiment classification tasks, namely review sentiment clas-sification and lexicon term sentiment classifica-tion. The experimental results demonstrate the ad-vantages of our approach.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification with word embeddings and canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngo Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Minh</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phuong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Symposium on Information and Communication Technology</title>
		<meeting>the Seventh Symposium on Information and Communication Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM press New York</publisher>
			<biblScope unit="volume">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain word representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification using sentiment sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Yannis</forename><surname>Goulermas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="398" to="410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to predict distributions of words across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="613" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baichuan</forename><surname>Vachik Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Al</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08774</idno>
		<title level="m">Neural-brane: Neural bayesian personalized ranking for attributed network embedding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Embeddings for word sense disambiguation: An evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="897" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge transformation for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="716" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Document-level sentiment classification: An empirical comparison between svm and ann</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">Francisco</forename><surname>Valiati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson P Gavião</forename><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="621" to="633" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web</title>
		<meeting>the 19th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural semantic role labeling with dependency path embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Jointly learning word embeddings and latent topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwun Ping</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on danish commons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorvald</forename><surname>Sørensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biologiske Skrifter</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building large-scale twitter-specific sentiment lexicon: A representation learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2005 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple regularization-based algorithm for learning crossdomain word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2898" to="2904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Name disambiguation in anonymized graphs using network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baichuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 26th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
