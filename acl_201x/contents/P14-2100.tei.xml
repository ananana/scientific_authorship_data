<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enriching Cold Start Personalized Language Model Using Social Network Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Yang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Ting</forename><surname>Kuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate Institute of Network and Multimedia</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate Institute of Network and Multimedia</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Graduate Institute of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enriching Cold Start Personalized Language Model Using Social Network Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="611" to="617"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a generalized framework to enrich the personalized language models for cold start users. The cold start problem is solved with content written by friends on social network services. Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph. The factor graph is used to incorporate prior knowledge and heuris-tics to identify the most appropriate weights. The intrinsic and extrinsic experiments show significant improvement on cold start users.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Personalized language models (PLM) on social network services are useful in many aspects ( <ref type="bibr" target="#b16">Xue et al., 2009;</ref><ref type="bibr" target="#b15">Wen et al., 2012;</ref><ref type="bibr" target="#b3">Clements, 2007)</ref>, For instance, if the authorship of a document is in doubt, a PLM may be used as a generative model to identify it. In this sense, a PLM serves as a proxy of one's writing style. Furthermore, PLMs can improve the quality of information retrieval and content-based recommendation sys- tems, where documents or topics can be recom- mended based on the generative probabilities.</p><p>However, it is challenging to build a PLM for users who just entered the system, and whose content is thus insufficient to characterize them. These are called "cold start" users. Producing better recommendations is even more critical for cold start users to make them continue to use the system. Therefore, this paper focuses on how to overcome the cold start problem and obtain a better PLM for cold start users.</p><p>The content written by friends on a social network service, such as Facebook or Twitter, is exploited. It can be either a reply to an original post or posts by friends. Here the hypothesis is that friends, who usually share common interests, tend to discuss similar topics and use similar words than non-friends. In other words, we be- lieve that a cold start user's language model can be enriched and better personalized by incorpo- rating content written by friends.</p><p>Intuitively, a linear combination of document- level language models can be used to incorporate content written by friends. However, it should be noticed that some documents are more relevant than others, and should be weighted higher. To obtain better weights, some simple heuristics could be exploited. For example, we can measure the similarity or distance between a user lan- guage model and a document language model. In addition, documents that are shared frequently in a social network are usually considered to be more influential, and could contribute more to the language model. More complex heuristics can also be derived. For instance, if two docu- ments are posted by the same person, their weights should be more similar. The main chal- lenge lies in how such heuristics can be utilized in a systematic manner to infer the weights of each document-level language model.</p><p>In this paper, we exploit the information on social network services in two ways. First, we impose the social dependency assumption via a finite mixture model. We model the true, albeit unknown, personalized language model as a combination of a biased user language model and a set of relevant document language models. Due to the noise inevitably contained in social media content, instead of using all available documents, we argue that by properly specifying the set of relevant documents, a better personalized lan- guage model can be learnt. In other words, each user language model is enriched by a personal- ized collection of background documents.</p><p>Second, we propose a factor graph model (FGM) to incorporate prior knowledge (e.g. the heuristics described above) into our model. Each mixture weight is represented by a random vari- able in the factor graph, and an efficient algo- rithm is proposed to optimize the model and infer the marginal distribution of these variables. Use- ful information about these variables is encoded by a set of potential functions.</p><p>The main contributions of this work are sum- marized below:  To solve the cold start problem encountered when estimating PLMs, a generalized frame- work based on FGM is proposed. We incorpo- rate social network information into user lan- guage models through the use of FGM. An it- erative optimization procedure utilizing per- plexity is presented to learn the parameters.</p><p>To our knowledge, this is the first proposal to use FGM to enrich language models.  Perplexity is selected as an intrinsic evalua- tion, and experiment on authorship attribution is used as an extrinsic evaluation. The results show that our model yields significant im- provements for cold start users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Social-Driven Personalized Language Model</head><p>The language model of a collection of documents can be estimated by normalizing the counts of words in the entire collection <ref type="bibr" target="#b19">(Zhai, 2008)</ref>. To build a user language model, one naïve way is to first normalize word frequency í µí±(í µí±¤, í µí±) within each document, and then average over all the documents in a user's document collection. </p><p>where í µí± í µí± (í µí±¤) is the language model of a particu- lar document, and í µí² í µí±¢ is the user's document col- lection. This formulation is basically an equal- weighted finite mixture model.</p><p>A simple yet effective way to smooth a lan- guage model is to linearly interpolate with a background language model <ref type="bibr" target="#b1">(Chen and Goodman, 1996;</ref><ref type="bibr">Zhai and Lafferty, 2001</ref>). In the line- ar interpolation method, all background docu- ments are treated equally. The entire document collection is added to the user language model í µí± í µí±¢ (í µí±¤) with the same interpolation coefficient.</p><p>Our main idea is to specify a set of relevant documents for the target user using information embedded in a social network, and enrich the smoothing procedure with these documents. Let í µí² í µí±í µí±í µí± denote the content from relevant persons (e.g. social neighbors) of u1, our idea can be con- cisely expressed as: </p><formula xml:id="formula_1">í µí± í µí±¢ 1 ′ (í µí±¤) = í µí¼ í</formula><p>where í µí¼ í µí± í µí± is the mixture weight of the language model of document di, and í µí¼ í µí±¢ 1 + ∑ í µí¼ í µí± í µí± = 1 . Documents posted by irrelevant users are not included as we believe the user language model can be better personalized by exploiting the so- cial relationship in a more structured way. In our experiment, we choose the first degree neighbor documents as í µí² í µí±í µí±í µí± . Also note that we have made no assumption about how the "base" user language model í µí± í µí±¢ 1 (í µí±¤) is built. In practice, it need not be models following maximum likelihood estimation, but any language model can be integrated into our framework to achieve a better refined model. Furthermore, any smoothing method can be ap- plied to the language model without degrading the effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Factor Graph Model (FGM)</head><p>Now we discuss how the mixture weights can be estimated. We introduce a factor graph model (FGM) to make use of the diverse information on a social network. Factor graph ( <ref type="bibr" target="#b6">Kschischang et al., 2006</ref>) is a bipartite graph consisting of a set of random variables and a set of factors which signifies the relationships among the variables. It is best suited in situations where the data is clear- ly of a relational nature ( <ref type="bibr" target="#b14">Wang et al., 2012</ref>). The joint distribution of the variables is factored ac- cording to the graph structure. Using FGM, one can incorporate the knowledge into the potential function for optimization and perform joint in- ference over documents. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the variables included in the model are described as follows:</p><p>Candidate variables í µí±¦ í µí± = 〈í µí±¢, í µí± í µí± 〉 . The ran- dom variables in the top layer stand for the de- grees of belief that a document di should be in- cluded in the PLM of the target user í µí±¢. Attribute variables xi. Local information is stored as the random variables in the bottom lay- er. For example, x1 might represent the number of common friends between the author of a doc- ument di and our target user.</p><p>The potential functions in the FGM are: Attribute-to-candidate function. This poten- tial function captures the local dependencies of a candidate variable to the relevant attributes. Let the candidate variable yi correspond to a docu- ment di, the attribute-to-candidate function of yi is defined in a log-linear form:</p><formula xml:id="formula_3">í µí±(í µí±¦ í µí± , í µí°´) = 1 í µí± í µí»¼ í µí±í µí±¥í µí±{í µí»¼ í µí± í µí°(í µí±¦ í µí± , í µí°´)} (3)</formula><p>where A is the set of attributes of either the doc- ument di or target user u; f is a vector of feature functions which locally model the value of yi with attributes in A; í µí± í µí»¼ is the local partition function and í µí»¼ is the weight vector to be learnt.</p><p>In our experiment, we define the vector of functions as í µí° = 〈í µí± í µí± í µí±í µí± , í µí± í µí±í µí±í µí±£ , í µí± í µí±í µí±í µí± , í µí± í µí±í µí±í µí± , í µí± í µí±í µí± 〉 í µí± as:  Similarity function í µí± í µí± í µí±í µí± . The similarity be- tween language models of the target user and a document should play an important role. We use cosine similarity between two unigram models in our experiments. </p><p>where g is a vector of feature functions indicat- ing whether two variables are correlated. If we further denote the set of all related variables as í µí°º(í µí±¦ í µí± ) , then for any candidate variable yi, we have the following brief expression:</p><p>í µí±(í µí±¦ í µí± , í µí°º(í µí±¦ í µí± )) = ∏ í µí±(í µí±¦ í µí± , í µí±¦ í µí± ) í µí±¦ í µí± ∈í µí°º(í µí±¦ í µí± )</p><p>For two candidate variables, let the corre- sponding document be di and dj, respectively, we define the vector í µí° = 〈í µí± í µí±í µí±í µí± , í µí± í µí±í µí±í µí±¡ 〉 í µí± as:  User relationship function í µí± í µí±í µí±í µí± . We assume that two candidate variables have higher de- pendency if they represent documents of the same author or the two authors are friends. The dependency should be even greater if two documents are similar. Let í µí±(í µí±) denote the author of a document d and í µí²©[í µí±¢] denote the closed neighborhood of a user u, we define í µí± í µí±í µí±í µí± = í µíµ{í µí±(í µí± í µí± ) ∈ í µí²©[í µí±(í µí± í µí± )]} × í µí± í µí±í µí±(í µí± í µí± , í µí± í µí± ) <ref type="formula">(7)</ref>  Co-category function í µí± í µí±í µí±í µí±¡ . For any two can- didate variables, it is intuitive that the two var- iables would have a higher correlation if di and dj are of the same category. Let í µí±(í µí±) de- note the category of document d, we define í µí± í µí±í µí±í µí±¡ = í µíµ{í µí±(í µí± í µí± ) = í µí±(í µí± í µí± )} × í µí± í µí±í µí±(í µí± í µí± , í µí± í µí± )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Inference and Optimization</head><p>Let Y and X be the set of all candidate variables and attribute variables, respectively. The joint distribution encoded by the FGM is given by multiplying all potential functions.</p><p>í µí±(í µí±, í µí±) = ∏ í µí±(í µí±¦ í µí± , í µí°´)í µí±(í µí±¦ í µí± , í µí°º(í µí±¦ í µí± )) í µí±</p><p>The desired marginal distribution can be ob- tained by marginalizing all other variables. Since under most circumstances, however, the factor graph is densely connected, the exact inference is intractable and approximate inference is required. After obtaining the marginal probabilities, the mixture weights í µí¼ í µí± í µí± in Eq. 2 are estimated by normalizing the corresponding marginal proba- bilities í µí±(í µí±¦ í µí± ) over all candidate variables, which can be written as </p><p>where the constraint í µí¼ í µí±¢ 1 + ∑ í µí¼ í µí± í µí± = 1 leads to a valid probability distribution for our mixture model. A factor graph is normally optimized by gra- dient-based methods. Unfortunately, since the ground truth values of the mixture weights are not available, we are prohibited from using su- pervised approaches. Here we propose a two-step iterative procedure to optimize our model. At first, all the model parameters (i.e. í µí»¼, í µí»½, í µí¼ í µí±¢ ) are randomly initialized. Then, we infer the marginal probabilities of candidate variables. Given these marginal probabilities, we can evaluate the per- plexity of the user language model on a held-out dataset, and search for better parameters. This procedure is repeated until convergence. Also, notice that by using FGM, we reduce the number of parameters from 1 + |í µí² í µí±í µí±í µí± | to 1 + |í µí»¼| + |í µí»½|, lowering the risk of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experiment Setup</head><p>We perform experiments on the Twitter dataset collected by <ref type="bibr" target="#b4">Galuba et al. (2010)</ref>. Twitter data have been used to verify models with different purposes <ref type="bibr" target="#b7">(Lin et al., 2011;</ref><ref type="bibr" target="#b13">Tan et al., 2011</ref>). To emphasize on the cold start scenario, we random- ly selected 15 users with about 35 tweets and 70 friends as candidates for an authorship attribution task. Our experiment corpus consists of 4322 tweets. All words with less than 5 occurrences are removed. Stop words and URLs are also re- moved and all tweets are stemmed. We identify the 100 most frequent terms as categories. The size of the vocabulary set is 1377.</p><p>We randomly partitioned the tweets of each user into training, validation and testing sets. The reported result is the average of 10 random splits. In all experiments, we vary the size of training data from 1% to 15%, and hold out the same number of tweets from each user as validation and testing data. The statistics of our dataset, given 15% training data, are shown in <ref type="table">Table 1</ref>.</p><p>Loopy belief propagation (LBP) is used to ob- tain the marginal probabilities of the variables ( <ref type="bibr" target="#b9">Murphy et al., 1999</ref>). Parameters are searched with the pattern search algorithm <ref type="bibr" target="#b0">(Audet and Dennis, 2002</ref>). To not lose generality, we use the default configuration in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of</head><p>Max. Min. Avg <ref type="table">.  Tweets  70  19  35.4  Friends  139  24  68.9  Variables 467  97  252.7  Edges  9216 231 3427.1  Table 1</ref>: Dataset statistics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline Methods</head><p>We compare our framework with two baseline methods. The first ("Cosine") is a straightfor- ward implementation that sets all mixture weights í µí¼ í µí± í µí± to the cosine similarity between the probability mass vectors of the document and user unigram language models. The second ("PS") uses the pattern search algorithm to per- form constrained optimization over the mixture weights. As mentioned in section 2.3, the main difference between this method and ours ("FGM") is that we reduce the search space of the parameters by FGM. Furthermore, social network information is exploited in our frame- work, while the PS method performs a direct search over mixture weights, discarding valuable knowledge.</p><p>Different from other smoothing methods that are usually mutually exclusive, any other smoothing methods can be easily merged into our framework. In Eq. 2, the base language model í µí± í µí±¢ 1 (í µí±¤) can be already smoothed by any techniques before being plugged into our frame- work. Our framework then enriches the user lan- guage model with social network information. We select four popular smoothing methods to demonstrate such effect, namely additive smoothing, absolute smoothing ( <ref type="bibr" target="#b10">Ney et al., 1995)</ref>, Jelinek-Mercer smoothing <ref type="bibr" target="#b5">(Jelinek and Mercer, 1980)</ref> and Dirichlet smoothing <ref type="bibr" target="#b8">(MacKay and Peto, 1994)</ref>. The results of using only the base model (i.e. set í µí¼ í µí± í µí± = 0 in Eq. 2) are denoted as "Base" in the following tables.  <ref type="table">Table 2</ref>: Testing set perplexity. ** indicates that the best score among all methods is significantly bet- ter than the next highest score, by t-test at a significance level of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Perplexity</head><p>As an intrinsic evaluation, we first compute the perplexity of unseen sentences under each user language model. The result is shown in <ref type="table">Table 2</ref>.</p><p>Our method significantly outperforms all of the methods in almost all settings. We observe that the "PS" method takes a long time to con- verge and is prone to overfitting, likely because it has to search about a few hundred parameters on average. As expected, the advantage of our model is more apparent when the data is sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Authorship Attribution (AA)</head><p>The authorship attribution (AA) task is chosen as the extrinsic evaluation metric. Here the goal is not about comparing with the state-of-the-art ap- proaches in AA, but showing that LM-based ap- proaches can benefit from our framework.</p><p>To apply PLM on this task, a naïve Bayes classifier is implemented ( <ref type="bibr" target="#b11">Peng et al., 2004</ref>). The most probable author of a document d is the one whose PLM yields the highest probability, and is determined by í µí±¢ * = argmax í µí±¢ {∏ í µí± í µí±¢ (í µí±¤) í µí±¤∈í µí± }. The result is shown in <ref type="table">Table 3</ref>. Our model im- proves personalization and outperforms the base- lines under cold start settings. When data is sparse, the "PS" method tends to overfit the noise, while the "Cosine" method contains too few information and is severely biased. Our method strikes a balance between model com- plexity and the amount of information included, and hence performs better than the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Personalization has long been studied in various textual related tasks. Personalized search is es- tablished by modeling user behavior when using search engines <ref type="bibr" target="#b12">(Shen et al., 2005;</ref><ref type="bibr" target="#b16">Xue et al., 2009</ref>). Query language model could be also ex- panded based on personalized user modeling ( <ref type="bibr" target="#b2">Chirita et al., 2007)</ref>. Personalization has also been modeled in many NLP tasks such as sum- marization <ref type="bibr" target="#b17">(Yan et al., 2011</ref>) and recommenda- tion ( <ref type="bibr" target="#b18">Yan et al., 2012)</ref>. Different from our pur- pose, these models do not aim at exploiting so- cial media content to enrich a language model. <ref type="bibr" target="#b15">Wen et al. (2012)</ref> combines user-level language models from a social network, but instead of fo- cusing on the cold start problem, they try to im- prove the speech recognition performance using a mass amount of texts on social network. On the other hand, our work explicitly models the more sophisticated document-level relationships using a probabilistic graphical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The advantage of our model is threefold. First, prior knowledge and heuristics about the social network can be adapted in a structured way through the use of FGM. Second, by exploiting a well-studied graphical model, mature inference techniques, such as LBP, can be applied in the optimization procedure, making it much more effective and efficient. Finally, different from most smoothing methods that are mutually ex- clusive, any other smoothing method can be in- corporated into our framework to be further en- hanced. Using only 1% of the training corpus, our model can improve the perplexity of base models by as much as 40% and the accuracy of authorship attribution by at most 15%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This work was sponsored by AOARD grant number No. FA2386-13-1-4045 and National Science Council, National Taiwan University and Intel Corporation under Grants NSC102- 2911-I-002-001 and NTU103R7501 and grant 102-2923-E-002-007-MY2, 102-2221-E-002-170, 101-2628-E-002-028-MY2.  <ref type="table">Table 3</ref>: Accuracy (%) of authorship attribution. ** indicates that the best score among all methods is significantly better than the next highest score, by t-test at a significance level of 0.05.</p><p>Chengxiang <ref type="bibr">Zhai and John Lafferty. 2001</ref>. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Re- trieval, SIGIR '01, pages 334-342, New York, NY, USA. ACM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A two-layered factor graph (FGM) proposed to estimate the mixture weights.</figDesc><graphic url="image-1.png" coords="2,307.90,666.88,216.12,71.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Document quality function í µí± í µí±í µí±í µí±£ . The out-of- vocabulary (OOV) ratio is used to measure the quality of a document. It is defined as í µí± í µí±í µí±í µí±£ = 1 − |{í µí±¤: í µí±¤ ∈ í µí± í µí± ∩ í µí±¤ ∉ í µí±}| |í µí± í µí± | (4) where í µí± is the vocabulary set of the entire corpus, with stop words excluded.  Document popularity function í µí± í µí±í µí±í µí± . This function is defined as the number of times di is shared to model the popularity of documents.  Common friend function í µí± í µí±í µí±í µí± . It is defined as the number of common friends between the target user u1 and the author of di.  Author friendship function í µí± í µí±í µí± . Assuming that documents posted by a user with more friends are more influential, this function is defined as the number of friends of di's author. Candidate-to-candidate function. This po- tential function defines the correlation of a can- didate variable yi with another candidate variable yj in the factor graph. The function is defined as í µí±(í µí±¦ í µí± , í µí±¦ í µí± ) = 1 í µí± í µí±í µí±,í µí»½ í µí±í µí±¥í µí±{í µí»½ í µí± í µí° (í µí±¦ í µí± , í µí±¦ í µí± )}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>í</head><label></label><figDesc>µí¼ í µí± í µí± = (1 − í µí¼ í µí±¢ 1 ) í µí±(í µí±¦ í µí± ) ∑ í µí±(í µí±¦ í µí± ) í µí±:í µí± í µí± ∈í µí² í µí±í µí±í µí±</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of generalized pattern searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Audet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Dennis</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Optimization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="889" to="903" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL &apos;96</title>
		<meeting>the 34th Annual Meeting on Association for Computational Linguistics, ACL &apos;96<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Personalized query expansion for the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><forename type="middle">S</forename><surname>Paul Alexandru Chirita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Firan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Personalization of social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Clements</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st BCS IRSG Conference on Future Directions in Information Access, FDIA&apos;07</title>
		<meeting>the 1st BCS IRSG Conference on Future Directions in Information Access, FDIA&apos;07<address><addrLine>Swinton, UK, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>British Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="14" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Outtweeting the twitterers-predicting information cascades in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Despotovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Kellerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Conference on Online Social Networks, WOSN&apos;10</title>
		<meeting>the 3rd Conference on Online Social Networks, WOSN&apos;10<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
<note type="report_type">USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpolated estimation of markov source parameters from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Pattern Recognition in Practice</title>
		<meeting>the Workshop on Pattern Recognition in Practice<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>NorthHolland</publisher>
			<date type="published" when="1980-05" />
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Factor graphs and the sum-product algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theor</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>September</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smoothing techniques for adaptive online language models: Topic tracking in tweet streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="422" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hierarchical dirichlet language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bauman Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the estimation of &apos;small&apos; probabilities by leaving-one-out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1202" to="1212" />
			<date type="published" when="1995-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Augmenting naive bayes classifiers with statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="317" to="345" />
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implicit user modeling for personalized search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM &apos;05</title>
		<meeting>the 14th ACM International Conference on Information and Knowledge Management, CIKM &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="824" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">User-level sentiment analysis incorporating social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1397" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge linking across wiki knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web, WWW &apos;12</title>
		<meeting>the 21st International Conference on World Wide Web, WWW &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Personalized language modeling by crowd sourcing with social network data for voice access of cloud applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Yuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">User language model for collaborative personalized search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno>11:1-11:28</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Summarize what you are interested in: An optimization framework for interactive personalized summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1342" to="1351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tweet recommendation with graph co-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="516" to="525" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Statistical Language Models for Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Now Publishers Inc</publisher>
			<pubPlace>Hanover, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
