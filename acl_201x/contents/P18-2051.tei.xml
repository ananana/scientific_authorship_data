<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-representation Ensembles and Delayed SGD Updates Improve Syntax-based NMT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Saunders</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrì</forename><forename type="middle">A</forename><surname>De Gispert</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SDL Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">SDL Research</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-representation Ensembles and Delayed SGD Updates Improve Syntax-based NMT</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="319" to="325"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>319</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We explore strategies for incorporating target syntax into Neural Machine Translation. We specifically focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ensembles of multiple NMT models consis- tently and significantly improve over single models ( <ref type="bibr">Garmash and Monz, 2016)</ref>. Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure <ref type="bibr" target="#b0">(Aharoni and Goldberg, 2017;</ref><ref type="bibr">Eriguchi et al., 2017</ref>) relative to those trained on plain-text, while plain-text mod- els produce shorter sequences and so may en- code lexical information more easily <ref type="bibr">(Nadejde et al., 2017)</ref>. We hypothesize that an NMT ensemble would be strengthened if its compo- nent models were complementary in this way.</p><p>However, ensembling often requires compo- nent models to make predictions relating to the same output sequence position at each time step. Models producing different sen- tence representations are necessarily synchro- nized to enable this. We propose an approach to decoding ensembles of models generating different representations, focusing on models generating syntax.</p><p>As part of our investigation we suggest strategies for practical NMT with very long target sequences.</p><p>These long sequences may arise through the use of linearized con- stituency trees and can be much longer than their plain byte pair encoded (BPE) equiv- alent representations <ref type="table">(Table 1</ref>). Long se- quences make training more difficult <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, which we address with an adjusted training procedure for the Trans- former architecture ( <ref type="bibr" target="#b4">Vaswani et al., 2017)</ref>, us- ing delayed SGD updates which accumulate gradients over multiple batches. We also sug- gest a syntax representation which results in much shorter sequences. <ref type="bibr">Nadejde et al. (2017)</ref> perform NMT with syntax annotation in the form of Combina- tory Categorial Grammar (CCG) supertags. <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref> translate from source BPE into target linearized parse trees, but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. <ref type="bibr">Eriguchi et al. (2017)</ref> combine recurrent neural net- work grammar (RNNG) models <ref type="bibr" target="#b2">(Dyer et al., 2016</ref>) with attention-based models to produce well-formed dependency trees. <ref type="bibr" target="#b5">Wu et al. (2017)</ref> similarly produce both words and arc- standard algorithm actions <ref type="bibr">(Nivre, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Previous approaches to ensembling diverse models focus on model inputs. <ref type="bibr">Hokamp (2017)</ref> shows improvements in the quality es- timation task using ensembles of NMT mod- els with multiple input representations which share an output representation. <ref type="bibr">Garmash and Monz (2016)</ref> show translation improvements with multi-source-language NMT ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ensembles of Syntax Models</head><p>We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model. We therefore propose a derivation-based representation which is much more compact than a linearized parse tree (examples in <ref type="table">Table 1</ref>). Our linearized derivation representation ((4) in <ref type="table">Table 1</ref>) con- sists of the derivation's right-hand side tokens with an end-of-rule marker, &lt;/R&gt; , marking the last non-terminal in each rule. The original tree can be directly reproduced from the se- quence, so that structure information is main- tained. We map words to subwords as de- scribed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Delayed SGD Update Training for Long Sequences</head><p>We suggest a training strategy for the Trans- former model ( <ref type="bibr" target="#b4">Vaswani et al., 2017</ref>) which gives improved performance for long se- quences, like syntax representations, without requiring additional GPU memory. The Ten- sor2Tensor framework ( <ref type="bibr" target="#b3">Vaswani et al., 2018)</ref> defines batch size as the number of tokens per batch, so batches will contain fewer sequences if their average length increases. During NMT training, by default, the gradients used to up- date model parameters are calculated over in- dividual batches. A possible consequence is that batches containing fewer sequences per update may have 'noisier' estimated gradients than batches with more sequences. Previous research has used very large batches to improve training convergence while requiring fewer model updates ( <ref type="bibr">Smith et al., 2017;</ref><ref type="bibr">Neishi et al., 2017)</ref>. However, with such large batches the model size may exceed available GPU memory. Training on multiple GPUs is one way to increase the amount of data used to estimate gradients, but it requires significant resources. Our strategy avoids this problem by using delayed SGD up- dates. We accumulate gradients over a fixed number of batches before using the accumu- lated gradients to update the model <ref type="bibr">1</ref> . This lets us effectively use very large batch sizes with- out requiring multiple GPUs. <ref type="table">Table 1</ref> shows several different representa- tions of the same hypothesis. To formulate an ensembling decoder over pairs of these representations, we assume we have a trans- ducer T that maps from one representation to the other representation. The complexity of the transduction depends on the representa- tions. Mapping from word to BPE represen- tations is straightforward, and mapping from (linearized) syntax to plain-text simply deletes non-terminals. Let P be the paths in T leading from the start state to any final state. A path  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ensembling Representations</head><formula xml:id="formula_0">p ∈ P maps an internal representation i(p) to an external representation o(p).</formula><p>The ensembling decoder produces exter- nal representations. Two NMT systems are trained, one for each representation, giving models P i and P o . An ideal equal-weight en- sembling of P i and P o yields</p><formula xml:id="formula_1">p * = argmax p∈P P i (i(p)) P o (o(p)) (1)</formula><p>with o(p * ) as the external representation of the translation.</p><p>In practice, beam decoding is performed in the external representation, i.e. over projec- tions of paths in P 2 . Let h = h 1 . . . h j be a partial hypothesis in the output representation. The set of partial paths yielding h are:</p><formula xml:id="formula_2">M (h) = (2) {(x, y)|xyz ∈ P, o(x) = h &lt;j , o(xy) = h}</formula><p>Here z is the path suffix. The ensembled score of h is then:</p><formula xml:id="formula_3">P (h j |h &lt;j ) =P o (h j |h &lt;j )× (3) max (x,y)∈M (h) P i (i(y)|i(x))</formula><p>The max performed for each partial hypothe- sis h is itself approximated by a beam search. This leads to an outer beam search over exter- nal representations with inner beam searches for the best matching internal representations. As search proceeds, each model score is up- dated separately with its appropriate represen- tation. Symbols in the internal representation are consumed as needed to stay synchronized with the external representation, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>; epsilons are consumed with a probability of 1.</p><p>Reference low -energy electron microscope ( LEEM ) and photoelectron microscope ( PEEM ) were attracted attention as new surface electron microscope . Plain BPE low energy electron microscope ( LEEM ) and photoelectron microscope ( PEEM ) are noticed as new surface electron microscope . Linearized derivation low-energy electron microscopy ( LEEM ) and photoelectron microscopy ( PEEM ) are attract- ing attention as new surface electron microscopes . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We first explore the effect of our delayed SGD update training scheme on single models, con- trasting updates every batch with accumulated updates every 8 batches. To compare target representations we train Transformer models with target representations <ref type="formula">(1)</ref>, <ref type="formula">(2)</ref>, <ref type="formula">(4)</ref> and <ref type="formula">(5)</ref> shown in <ref type="table">Table 1</ref> For comparison with earlier target syntax work, we also train two RNN attention-based seq2seq models ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) with normal SGD to produce plain BPE sequences and linearized derivations. For these models we use embedding size 400, a single BiLSTM layer of size 750, and batch size 80.</p><p>We report all experiments for Japanese- English, using the first 1M training sen- tences of the Japanese-English ASPEC data ( <ref type="bibr">Nakazawa et al., 2016</ref>). All models use plain BPE Japanese source sentences. English con- stituency trees are obtained using CKYlark ( <ref type="bibr">Oda et al., 2015</ref>), with words replaced by BPE subwords. We train separate Japanese (lowercased) and English (cased) BPE vocab- ularies on the plain-text, with 30K merges each. Non-terminals are included as separate tokens. The linearized derivation uses addi- tional tokens for non-terminals with &lt;/R&gt; .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results and Discussion</head><p>Our first results in <ref type="table">Table 3</ref> show that large batch training can significantly improve the performance of single Transformers, partic- ularly when trained to produce longer se- quences. Accumulating the gradient over 8 batches of size 4096 gives a 3 BLEU improve- ment for the linear derivation model. It has been suggested that decaying the learning rate can have a similar effect to large batch train- ing ( <ref type="bibr">Smith et al., 2017)</ref>, but reducing the ini- tial learning rate by a factor of 8 alone did not give the same improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation</head><p>Batches  <ref type="table">Table 3</ref>: Single Transformers trained to con- vergence on 1M WAT Ja-En, batch size 4096</p><p>Our plain BPE baseline <ref type="table" target="#tab_3">(Table 4)</ref> outper- forms the current best system on WAT Ja-En, an 8-model ensemble <ref type="bibr">(Morishita et al., 2017)</ref>. Our syntax models achieve similar results de- spite producing much longer sequences.    <ref type="table" target="#tab_3">Table 4</ref> (p &lt; 0.05 using bootstrap resampling ( <ref type="bibr">Koehn et al., 2007)</ref>).</p><p>3 indicates that large batch training is instru- mental in this. We find that RNN-based syntax models can equal plain BPE models as in <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref>; <ref type="bibr">Eriguchi et al. (2017)</ref> use syntax for a 1 BLEU improvement on this dataset, but over a much lower baseline. Our plain BPE Transformer outperforms all syn- tax models except POS/BPE. More compact syntax representations perform better, with POS/BPE outperforming linearized deriva- tions, which outperform linearized trees.</p><p>Ensembles of two identical models trained with different seeds only slightly improve over the single model <ref type="table" target="#tab_4">(Table 5)</ref>. However, an ensemble of models producing plain BPE and linearized derivations improves by 0.5 BLEU over the plain BPE baseline.</p><p>By ensembling syntax and plain-text we hope to benefit from their complementary strengths. To highlight these, we examine hy- potheses generated by the plain BPE and lin- earized derivation models. We find that the syntax model is often more grammatical, even when the plain BPE model may share more vocabulary with the reference <ref type="table" target="#tab_0">(Table 2)</ref>.</p><p>In ensembling plain-text with a syntax ex- ternal representation we observed that in a small proportion of cases non-terminals were over-generated, due to the mismatch in tar- get sequence lengths. Our solution was to pe- nalise scores of non-terminals under the syn- tax model by a constant factor.</p><p>It is also possible to constrain decoding of linearized trees and derivations to well- formed outputs. However, we found that this gives little improvement in BLEU over uncon- strained decoding although it remains an inter- esting line of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We report strong performance with individual models that meets or improves over the re- cent best WAT Ja-En ensemble results. We train these models using a delayed SGD up- date training procedure that is especially ef- fective for the long representations that arise from including target language syntactic in- formation in the output. We further improve on the individual results via a decoding strat- egy allowing ensembling of models produc- ing different output representations, such as subword units and syntax. We propose these techniques as practical approaches to includ- ing target syntax in NMT. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>)</head><label></label><figDesc>Linearized tree (ROOT (S (NP (DT No ) (NNS complications ) ) (VP (VBD occurred ) ) ) ) 120.0 (3) Derivation ROOT→S ; S→NP VP ; NP→DT NNS ; DT→No ; NNS→complications ; VP→VBD ; VBD→occurred - (4) Linearized derivation S&lt;/R&gt; NP VP&lt;/R&gt; DT NNS&lt;/R&gt; No complications VBD&lt;/R&gt; occurred 73.8 (5) POS/plain-text DT No NNS complications VBD occurred 53.3 Table 1: Examples for proposed representations. Lengths are for the first 1M WAT English training sentences with BPE subwords (Sennrich et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Transducer mapping internal to external representations. A partial hypothesis might be o(xy 2 ) in the external representation and i(xy 1 y 2 ) in the internal representation.</figDesc><graphic url="image-1.png" coords="3,135.80,230.14,323.68,107.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, using delayed SGD updates every 8 batches. We decode with individ- ual models and two-model ensembles, com- paring results for single-representation and multi-representation ensembles. Each multi- representation ensemble consists of the plain BPE model and one other individual model. All Transformer architectures are Ten- sor2Tensor's base Transformer model (Vaswani et al., 2018) with a batch size of 4096. In all cases we decode using SGNMT (Stahlberg et al., 2017) with beam size 4, using the average of the final 20 checkpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sample generated translations from individual models 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table>Architecture Representation 
Dev 
BLEU 

Test 
BLEU 
Seq2seq 
(8-model 
ensemble) 

Best WAT17 result 
(Morishita et al., 
2017) 

-
28.4 

Seq2seq 
Plain BPE 
21.6 
21.2 
Linearized derivation 21.9 
21.2 

Transformer 

Plain BPE 
28.0 
28.9 
Linearized tree 
28.2 
28.4 
Linearized derivation 28.5 
28.7 
POS/BPE 
28.5 
29.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Single models on Ja-En. Previous 
evaluation result included for comparison. 

External 
representation 

Internal 
representation 

Test 
BLEU 
Plain BPE 
Plain BPE 
29.2 
Linearized derivation Linearized derivation 28.8 
Linearized tree 
Plain BPE 
28.9 
Plain BPE 
Linearized derivation 28.8 
Linearized derivation Plain BPE 
29.4  † 
POS/BPE 
Plain BPE 
29.3  † 
Plain BPE 
POS/BPE 
29.4  † 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ja-En Transformer ensembles:  † 
marks significant improvement on plain BPE 
baseline shown in </table></figure>

			<note place="foot" n="1"> https://github.com/fstahlberg/ tensor2tensor</note>

			<note place="foot" n="2"> See the tokenization wrappers in https:// github.com/ucam-smt/sgnmt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by EPSRC grant EP/L027623/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tensor2tensor for neural machine translation. CoRR, abs/1803.07416</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequence-todependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
