<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wenpeng@cis.uni-muenchen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="63" to="73"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present MultiGranCNN, a general deep learning architecture for matching text chunks. MultiGranCNN supports multigranular comparability of representations: shorter sequences in one chunk can be directly compared to longer sequences in the other chunk. Multi-GranCNN also contains a flexible and modularized match feature component that is easily adaptable to different types of chunk matching. We demonstrate state-of-the-art performance of MultiGranCNN on clause coherence and paraphrase identification tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language processing (NLP) tasks can be posed as classifying the relationship be- tween two TEXTCHUNKS (cf. , <ref type="bibr" target="#b4">Bordes et al. (2014b)</ref>) where a TEXTCHUNK can be a sentence, a clause, a paragraph or any other sequence of words that forms a unit.</p><p>Paraphrasing ( <ref type="figure">Figure 1</ref>, top) is one task that we address in this paper and that can be formalized as classifying a TEXTCHUNK relation. The two classes correspond to the sentences being (e.g., the pair &lt;p, q + &gt;) or not being (e.g., the pair &lt;p, q − &gt;) paraphrases of each other. Another task we look at is clause coherence <ref type="figure">(Figure 1</ref>, bot- tom). Here the two TEXTCHUNK relation classes correspond to the second clause being (e.g., the pair &lt;x, y + &gt;) or not being (e.g., the pair &lt;x, y − &gt;) a discourse-coherent continuation of the first clause. Other tasks that can be formalized as TEXTCHUNK relations are question answering (QA) (is the second chunk an answer to the first?), textual inference (does the first chunk imply the second?) and machine translation (are the two chunks translations of each other?). p PDC will also almost certainly fan the flames of speculation about Longhorn's release.</p><p>q + PDC will also almost certainly reignite speculation about release dates of Microsoft 's new products. q − PDC is indifferent to the release of Longhorn.</p><p>x The dollar suffered its worst one-day loss in a month, y + falling to 1.7717 marks . . . from 1.7925 marks yesterday. y − up from 112.78 yen in late New York trading yesterday.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Examples for paraphrasing and clause coherence tasks</head><p>In this paper, we present MultiGranCNN, a gen- eral architecture for TEXTCHUNK relation classi- fication. MultiGranCNN can be applied to a broad range of different TEXTCHUNK relations. This is a challenge because natural language has a com- plex structure -both sequential and hierarchical - and because this structure is usually not parallel in the two chunks that must be matched, further increasing the difficulty of the task. A successful detection algorithm therefore needs to capture not only the internal structure of TEXTCHUNKS, but also the rich pattern of their interactions.</p><p>MultiGranCNN is based on two innovations that are critical for successful TEXTCHUNK re- lation classification. First, the architecture is de- signed to ensure multigranular comparability. For general matching, we need the ability to match short sequences in one chunk with long sequences in the other chunk. For example, what is expressed by a single word in one chunk ("reignite" in q + in the figure) may be expressed by a sequence of several words in its paraphrase ("fan the flames of" in p). To meet this objective, we learn rep- resentations for words, phrases and the entire sen- tence that are all mutually comparable; in particu- lar, these representations all have the same dimen- sionality and live in the same space.</p><p>Most prior work (e.g., <ref type="bibr" target="#b1">Blacoe and Lapata (2012;</ref><ref type="bibr" target="#b11">Hu et al. (2014)</ref>) has neglected the need for multi- granular comparability and performed matching within fixed levels only, e.g., only words were matched with words or only sentences with sen- tences. For a general solution to the problem of matching, we instead need the ability to match a unit on a lower level of granularity in one chunk with a unit on a higher level of granularity in the other chunk. <ref type="bibr">Unlike (Socher et al., 2011</ref>), our model does not rely on parsing and it can more ex- haustively search the hypothesis space of possible matchings, including matchings that correspond to conflicting segmentations of the input chunks (see Section 5).</p><p>Our second contribution is that MultiGranCNN contains a flexible and modularized match feature component. This component computes the ba- sic features that measure how well phrases of the two chunks match. We investigate three different match feature models that demonstrate that a wide variety of different match feature models can be implemented. The match feature models can be swapped in and out of MultiGranCNN, depending on the characteristics of the task to be solved.</p><p>Prior work that has addressed matching tasks has usually focused on a single task like QA <ref type="bibr" target="#b3">(Bordes et al., 2014a;</ref><ref type="bibr" target="#b29">Yu et al., 2014</ref>) or paraphrasing <ref type="bibr" target="#b26">(Socher et al., 2011;</ref><ref type="bibr" target="#b21">Madnani et al., 2012;</ref><ref type="bibr" target="#b13">Ji and Eisenstein, 2013)</ref>. The ARC architectures pro- posed by <ref type="bibr" target="#b11">Hu et al. (2014)</ref> are intended to be more general, but seem to be somewhat limited in their flexibility to model different matching relations; e.g., they do not perform well for paraphrasing.</p><p>Different match feature models may also be re- quired by factors other than the characteristics of the task. If the amount of labeled training data is small, then we may prefer a match feature model with few parameters that is robust against overfit- ting. If there is lots of training data, then a richer match feature model may be the right choice. This motivates the need for an architecture like MultiGranCNN that allows selection of the task- appropriate match feature model from a range of different models and its seamless integration into the architecture.</p><p>In remaining parts, Section 2 introduces some related work; Section 3 gives an overview of the proposed MultiGranCNN; Section 4 shows how to learn representations for generalized phrases (g- phrases); Section 5 describes the three matching models: DIRECTSIM, INDIRECTSIM and CON- CAT; Section 6 describes the two 2D pooling methods: grid-based pooling and phrase-based pooling; Section 7 describes the match feature CNN; Section 8 summarizes the architecture of MultiGran CNN; and Section 9 presents experi- ments; finally, Section 10 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Paraphrase identification (PI) is a typical task of sentence matching and it has been frequently stud- ied ( <ref type="bibr" target="#b25">Qiu et al., 2006;</ref><ref type="bibr" target="#b1">Blacoe and Lapata, 2012;</ref><ref type="bibr" target="#b21">Madnani et al., 2012;</ref><ref type="bibr" target="#b13">Ji and Eisenstein, 2013)</ref>. <ref type="bibr" target="#b26">Socher et al. (2011)</ref> utilized parsing to model the hierarchical structure of sentences and uses un- folding recursive autoencoders to learn represen- tations for single words and phrases acting as non- leaf nodes in the tree. The main difference to MultiGranCNN is that we stack multiple convo- lution layers to model flexible phrases and learn representations for them, and aim to address more general sentence correspondence. <ref type="bibr" target="#b0">Bach et al. (2014)</ref> claimed that elementary discourse units ob- tained by segmenting sentences play an important role in paraphrasing. Their conclusion also en- dorses <ref type="bibr" target="#b26">(Socher et al., 2011</ref>)'s and our work, for both take interactions between component phrases into account.</p><p>QA is another representative sentence matching problem. <ref type="bibr" target="#b29">Yu et al. (2014)</ref> modeled sentence rep- resentations in a simplified CNN, finally finding the match score by projecting question and answer candidates into the same space. Other relevant QA work includes ( <ref type="bibr" target="#b5">Bordes et al., 2014c;</ref><ref type="bibr" target="#b3">Bordes et al., 2014a;</ref><ref type="bibr" target="#b28">Yang et al., 2014;</ref><ref type="bibr" target="#b12">Iyyer et al., 2014)</ref> For more general matching, <ref type="bibr" target="#b7">Chopra et al. (2005)</ref> and <ref type="bibr" target="#b19">Liu (2013)</ref> used a Siamese architecture of shared-weight neural networks (NNs) to model two objects simultaneously, matching their repre- sentations and then learning a specific type of sen- tence relation. We adopt parts of their architec- ture, but we model phrase representations as well as sentence representations.</p><p>Li and Xu (2012) gave a comprehensive intro- duction to query-document matching and argued that query and document match at different levels: term, phrase, word sense, topic, structure etc. This also applies to sentence matching.</p><p>Lu and Li (2013) addressed matching of short texts. Interactions between the two texts were ob- tained via LDA ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>) and were then the basis for computing a matching score. Com- pared to MultiGranCNN, drawbacks of this ap- proach are that LDA parameters are not optimized for the specific task and that the interactions are formed on the level of single words only. <ref type="bibr" target="#b10">Gao et al. (2014)</ref> modeled interestingness be- tween two documents with deep NNs. They mapped source-target document pairs to feature vectors in a latent space in such a way that the dis- tance between the source document and its corre- sponding interesting target in that space was min- imized. Interestingness is more like topic rele- vance, based mainly on the aggregated meaning of keywords, as opposed to more structural rela- tionships as is the case for paraphrasing and clause coherence.</p><p>We briefly discussed ( <ref type="bibr" target="#b11">Hu et al., 2014</ref>)'s ARC in Section 1. MultiGranCNN is partially inspired by ARC, but introduces multigranular comparability (thus enabling crosslevel matching) and supports a wider range of match feature models.</p><p>Our unsupervised learning component (Sec- tion 4, last paragraph) resembles word2vec CBOW ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>), but learns repre- sentations of TEXTCHUNKS as well as words. It also resembles PV-DM ( <ref type="bibr" target="#b16">Le and Mikolov, 2014</ref>), but our TEXTCHUNK representation is derived us- ing a hierarchical architecture based on convolu- tion and pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of MultiGranCNN</head><p>We use convolution-plus-pooling in two differ- ent components of MultiGranCNN. The first com- ponent, the generalized phrase CNN (gpCNN), will be introduced in Section 4. This component learns representations for generalized phrases (g- phrases) where a generalized phrase is a general term for subsequences of all granularities: words, short phrases, long phrases and the sentence itself. The gpCNN architecture has L layers of convolu- tion, corresponding (for L = 2) to words, short phrases, long phrases and the sentence. We test different values of L in our experiments. We train gpCNN on large data in an unsupervised manner and then fine-tune it on labeled training data.</p><p>Using a Siamese configuration, two copies of gpCNN, one for each of the two input TEXTCHUNKS, are the input to the match feature model, presented in Section 5. This model pro- duces s 1 × s 2 matching features, one for each pair of g-phrases in the two chunks, where s 1 , s 2 are the number of g-phrases in the two chunks, respec- tively.</p><p>The s 1 ×s 2 match feature matrix is first reduced to a fixed size by dynamic 2D pooling. The re- sulting fixed size matrix is then the input to the second convolution-plus-pooling component, the match feature CNN (mfCNN) whose output is fed to a multilayer perceptron (MLP) that produces the final match score. Section 6 will give details.</p><p>We use convolution-plus-pooling for both word sequences and match features because we want to compute increasingly abstract features at multiple levels of granularity. To ensure that g-phrases are mutually comparable when computing the s 1 × s 2 match feature matrix, we impose the constraint that all g-phrase representations live in the same space and have the same dimensionality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">gpCNN: Learning Representations for g-Phrases</head><p>We use several stacked blocks, i.e., convolution- plus-pooling layers, to extract increasingly ab- stract features of the TEXTCHUNK. The input to the first block are the words of the TEXTCHUNK, represented by CW ( <ref type="bibr" target="#b8">Collobert and Weston, 2008)</ref> embeddings. Given a TEXTCHUNK of length |S|, let vector c i ∈ R wd be the concatenated embed- dings of words v i−w+1 , . . . , v i where w = 5 is the filter width, d = 50 is the dimensionality of CW embeddings and 0 &lt; i &lt; |S| + w. Embeddings for words v i , i &lt; 1 and i &gt; |S|, are set to zero. We then generate the representation p i ∈ R d of the g-phrase v i−w+1 , . . . , v i using the convolution matrix W l ∈ R d×wd :</p><formula xml:id="formula_0">p i = tanh(W l c i + b l )<label>(1)</label></formula><p>where block index l = 1, bias b l ∈ R d . We use wide convolution (i.e., we apply the convolution matrix W l to words v i , i &lt; 1 and i &gt; |S|) because this makes sure that each word v i , 1 ≤ i ≤ |S|, can be detected by all weights of W l -as opposed to only the rightmost (resp. leftmost) weights for initial (resp. final) words in narrow convolution. The configuration of convolution layers in fol- lowing blocks (l &gt; 1) is exactly the same except that the input vectors c i are not words, but the out- put of pooling from the previous layer of convo- lution -as we will explain presently. The con- figuration is the same (e.g., all W l ∈ R d×wd ) be- cause, by design, all g-phrase representations have the same dimensionality d. This also ensures that each g-phrase representation can be directly com- pared with each other g-phrase representation.</p><p>We use dynamic k-max pooling to extract the k l top values from each dimension after convolution in the l th block and the k L top values in the final block. We set</p><formula xml:id="formula_1">k l = max(α, L − l L |S||)<label>(2)</label></formula><p>where l = 1, · · · , L is the block index, and α = 4 is a constant (cf. <ref type="bibr" target="#b14">Kalchbrenner et al. (2014)</ref>) that makes sure a reasonable minimum number of val- ues is passed on to the next layer. We set k L = 1 (not 4, cf. <ref type="bibr" target="#b14">Kalchbrenner et al. (2014)</ref>) because our design dictates that all g-phrase representations, including the representation of the TEXTCHUNK itself, have the same dimensionality. Example: for L = 4, |S| = 20, the k i are <ref type="bibr">[15,</ref><ref type="bibr">10,</ref><ref type="bibr">5,</ref><ref type="bibr">1]</ref>. Dynamic k-max pooling keeps the most impor- tant features and allows us to stack multiple blocks to extract hiearchical features: units on consec- utive layers correspond to larger and larger parts of the TEXTCHUNK thanks to the subset selection property of pooling.</p><p>For many tasks, labeled data for training gpCNN is limited. We therefore employ unsu- pervised training to initialize gpCNN as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Match Feature Models</head><p>Let g 1 , . . . , g s k be an enumeration of the s k g- phrases of TEXTCHUNK S k . Let S k ∈ R s k ×d be the matrix, constructed by concatenating the four matrices of unigram, short phrase, long phrase and sentence representations shown in <ref type="figure" target="#fig_0">Figure 2</ref> that contain the learned representations from Section 4 for these s k g-phrases; i.e., row S ki is the learned representation of g i .</p><p>The basic design of a match feature model is that we produce an s 1 × s 2 matrixˆFmatrixˆ matrixˆF for a pair of TEXTCHUNKS S 1 and S 2 , shown in <ref type="figure" target="#fig_2">Figure 3</ref>. ˆ F i,j is a score that assesses the relationship be- tween g-phrase g i of S 1 and g-phrase g j of S 2 with respect to the TEXTCHUNK relation of in- terest (paraphrasing, clause coherence etc). This scorê F i,j is computed based on the vector repre- sentations S 1i and S 2j of the two g-phrases. <ref type="bibr">1</ref> We experiment with three different feature models to compute the match scorê F i,j because we would like our architecture to address a wide variety of different TEXTCHUNK relations. We can model a TEXTCHUNK relation like paraphras- ing as "for each meaning element in one sentence, there must be a similar meaning element in the other sentence"; thus, a good candidate for the match scorê F i,j is simply vector similarity. In contrast, similarity is a less promising match score for clause coherence; for clause coherence, we want a score that models how good a continuation one g-phrase is for the other. These considerations motivate us to define three different match feature models that we will introduce now.</p><p>The first match feature model is DIRECTSIM. </p><formula xml:id="formula_2">ˆ F i,j = exp( −||S 1i − S 2j || 2 2β )<label>(3)</label></formula><p>where we set β = 2 (cf. <ref type="bibr" target="#b27">Wu et al. (2013)</ref>). DIRECTSIM is an appropriate feature model for TEXTCHUNK relations like paraphrasing because in that case direct similarity features are helpful in assessing meaning equivalence.</p><p>The second match feature model is INDIRECT- SIM. Instead of computing the similarity di- rectly as we do for DIRECTSIM, we first trans- form the representation of the g-phrase in one TEXTCHUNK using a transformation matrix M ∈ R d×d , then compute the match score by inner product and sigmoid activation:</p><formula xml:id="formula_3">ˆ F i,j = σ(S 1i MS T 2j + b),<label>(4)</label></formula><p>Our motivation is that for a TEXTCHUNK rela- tion like clause coherence, the two TEXTCHUNKS need not have any direct similarity. However, if we map the representations of TEXTCHUNK S 1 into an appropriate space then we can hope that sim- ilarity between these transformed representations of S 1 and the representations of TEXTCHUNK S 2 do yield useful features. We will see that this hope is borne out by our experiments.</p><p>The third match feature model is CONCAT. This is a general model that can learn any weighted combination of the values of the two vectors:</p><formula xml:id="formula_4">ˆ F i,j = σ(w T e i,j + b)<label>(5)</label></formula><p>where e i,j ∈ R 2d is the concatenation of S 1i and S 2j . We can learn different combination weights w to solve different types of TEXTCHUNK match- ing.</p><p>We call this match feature model CONCAT be- cause we implement it by concatenating g-phrase vectors to form a tensor as shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>The match feature models implement multi- granular comparability: they match all units in one TEXTCHUNK with all units in the other TEXTCHUNK. This is necessary because a gen- eral solution to matching must match a low-level unit like "reignite" to a higher-level unit like "fan the flames of" <ref type="figure">(Figure 1</ref>). Unlike <ref type="bibr" target="#b26">(Socher et al., 2011</ref>), our model does not rely on parsing; there- fore, it can more exhaustively search the hypoth- esis space of possible matchings: mfCNN covers a wide variety of different, possibly overlapping units, not just those of a single parse tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Dynamic 2D Pooling</head><p>The match feature models generate an s 1 × s 2 ma- trix. Since it has variable size, we apply two dif- ferent dynamic 2D pooling methods, grid-based pooling and phrase-focused pooling, to transform it to a fixed size matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Grid-based pooling</head><p>We need to mapˆFmapˆ mapˆF ∈ R s 1 ×s 2 into a matrix F of fixed size s * × s * where s * is a parameter. Grid- based pooling dividesˆFdividesˆ dividesˆF into s * × s * nonover- lapping (dynamic) pools and copies the maximum value in each dynamic pool to F. This method is similar to <ref type="bibr" target="#b26">(Socher et al., 2011</ref>  If s 1 &lt; s * , we first repeat all rows in batch style with size s 1 until no fewer than s * rows remain. Then the first s * rows are kept and split into s * dynamic pools. The same principle applies to the partitioning of columns. In <ref type="figure" target="#fig_4">Figure 5</ref> (right), the ar- eas with dashed lines and dotted lines are repeated parts for rows and columns, respectively; each cell is its own dynamic pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Phrase-focused pooling</head><p>In the match feature matrixˆFmatrixˆ matrixˆF ∈ R s 1 ×s 2 , row i (resp. column j) contains all feature values for g- phrase g i of S 1 (resp. g j of S 2 ). Phrase-focused pooling attempts to pick the largest match features for a g-phrase g on the assumption that they are the best basis for assessing the relation of g with other g-phrases. To implement this, we sort the values of each row i (resp. each column j) in decreasing order giving us a matrixˆFmatrixˆ matrixˆF r ∈ R s 1 ×s 2 with sorted rows (resp. ˆ F c ∈ R s 1 ×s 2 with sorted columns). Then we concatenate the columns ofˆFofˆ ofˆF r (resp. the rows ofˆFofˆ ofˆF c ) resulting in list</p><formula xml:id="formula_5">F r = {f r 1 , . . . , f r s 1 s 2 } (resp. F c = {f c 1 , . . . , f c s 1 s 2 })</formula><p>where each f r (f c ) is an element ofˆFofˆ ofˆF r ( ˆ F c ). These two lists are merged into a list F by interleaving them so that members from F r and F c alternate. F is then used to fill the rows of F from top to bottom with each row being filled from left to right. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">mfCNN: Match feature CNN</head><p>The output of dynamic 2D pooling is further pro- cessed by the match feature CNN (mfCNN) as de- picted in <ref type="figure" target="#fig_6">Figure 6</ref>. mfCNN extracts increasingly abstract interaction features from lower-level in- teraction features, using several layers of 2D wide convolution and fixed-size 2D pooling.</p><p>We call the combination of a 2D wide convo- lution layer and a fixed-size 2D pooling layer a block, denoted by index b (b = 1, 2 . . .). In gen- eral, let tensor T b ∈ R c b ×s b ×s b denote the fea- ture maps in block b; block b has c b feature maps, each of size s b × s b (T 1 = F ∈ R 1×s * ×s * ). Let W b ∈ R c b+1 ×c b ×f b ×f b be the filter weights of 2D wide convolution in block b, f b ×f b is then the size of sliding convolution regions. Then the convolu- tion is performed as element-wise multiplication <ref type="bibr">2</ref> IfˆFIfˆ IfˆF has fewer cells than F, then we simply repeat the filling procedure to fill all cells.</p><p>between W b and T b as follows:</p><formula xml:id="formula_6">ˆ T b+1 m,i−1,j−1 = σ( W b m,:,:,: T b :,i−f b :i,j−f b :j +b b m ) (6) where 0≤m&lt;c b+1 , 1 ≤ i, j &lt; s b +f b , b b ∈ R c b+1 .</formula><p>Subsequently, fixed-size 2D pooling selects dominant features from k b × k b non-overlapping windows ofˆTofˆ ofˆT b+1 to form a tensor as input of block b + 1:</p><formula xml:id="formula_7">T b+1 m,i,j = max( ˆ T b+1 m,ik b :(i+1)k b ,jk b :(j+1)k b )<label>(7)</label></formula><p>where <ref type="bibr" target="#b11">Hu et al. (2014)</ref> used narrow convolution which would limit the number of blocks. 2D wide convo- lution in this work enables to stack multiple blocks of convolution and pooling to extract higher-level interaction features. We will study the influence of the number of blocks on performance below.</p><formula xml:id="formula_8">0 ≤ i, j &lt; s b +f b −1 k b .</formula><p>For the experiments, we set s * = 40, c b = 50, f b = 5, k b = 2 (b = 1, 2, · · ·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">MultiGranCNN</head><p>We can now describe the overall architecture of MultiGranCNN. First, using a Siamese configu- ration, two copies of gpCNN, one for each of the two input TEXTCHUNKS, produce g-phrase representations on different levels of abstraction <ref type="figure" target="#fig_0">(Figure 2</ref>). Then one of the three match feature models (DIRECTSIM, CONCAT or INDIRECTSIM) produces an s 1 × s 2 match feature matrix, each cell of which assesses the match of a pair of g- phrases from the two chunks. This match feature matrix is reduced to a fixed size matrix by dy- namic 2D pooling (Section 6). As shown in <ref type="figure" target="#fig_6">Fig- ure 6</ref>, the resulting fixed size matrix is the input for mfCNN, which extracts interaction features of increasing complexity from the basic interaction features computed by the match feature model. Fi- nally, the output of the last block of mfCNN is the input to an MLP that computes the match score.</p><p>MultiGranCNN bears resemblance to previous work on clause and sentence matching (e.g., <ref type="bibr" target="#b11">Hu et al. (2014)</ref>, Socher et al. <ref type="formula" target="#formula_0">(2011)</ref>), but it is more general and more flexible. It learns representa- tions of g-phrases, i.e., representations of parts of the TEXTCHUNK at multiple granularities, not just for a single level such as the sentence as ARC-I does ( <ref type="bibr" target="#b11">Hu et al., 2014</ref>). MultiGranCNN explores the space of interactions between the two chunks more exhaustively by considering interactions be- tween every unit in one chunk with every other unit in the other chunk, at all levels of granular- ity. Finally, MultiGranCNN supports a number of different match feature models; the corresponding module can be instantiated in a way that ensures that match features are best suited to support ac- curate decisions on the TEXTCHUNK relation task that needs to be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Experimental Setup and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Training</head><p>Suppose the triple (x, y + , y − ) is given and x matches y + better than y − . Then our objective is the minimization of the following ranking loss:</p><formula xml:id="formula_9">l(x, y + , y − ) = max(0, 1 + s(x, y − ) − s(x, y + ))</formula><p>where s(x, y) is the predicted match score for (x, y). We use stochastic gradient descent with Adagrad (Duchi et al., 2011), L 2 regularization and minibatch training.</p><p>We set initial learning rate to 0.05, batch size to 70, L 2 weight to 5 · 10 −4 .</p><p>Recall that we employ unsupervised pretraining of representations for g-phrases. We can either freeze these representations in subsequent super- vised training; or we can fine-tune them. We study the performance of both regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Clause Coherence Task</head><p>As introduced by <ref type="bibr" target="#b11">Hu et al. (2014)</ref>, the clause coherence task determines for a pair (x, y) of clauses if the sentence "xy" is a coherent sen- tence. We construct a clause coherence dataset as follows (the set used by <ref type="bibr" target="#b11">Hu et al. (2014)</ref> is not yet available). We consider all sentences from En- glish <ref type="bibr">Gigaword (Parker et al., 2009</ref>) that consist of two comma-separated clauses x and y, with each clause having between five and 30 words. For each y, we choose four clauses y . . . y randomly from the 1000 second clauses that have the highest similarity to y, where similarity is cosine similar- ity of TF-IDF vectors of the clauses; restricting the alternatives to similar clauses ensures that the task is hard. The clause coherence task then is to select y from the set y, y , . . . , y as the correct continuation of x. We create 21 million examples, each consisting of a first clause x and five second clauses. This set is divided into a training set of 19 million and development and test sets of one million each. An example from the training set is given in <ref type="figure">Figure 1</ref>.</p><p>Then, we study the performance variance of different MultiGranCNN setups from three per- spectives: a) layers of CNN in both unsuper- vised (gpCNN) and supervised (mfCNN) training phases; b) different approaches for clause relation feature modeling; c) dynamic pooling methods for generating same-sized feature matrices. <ref type="figure" target="#fig_7">Figure 7</ref> (top table) shows that ( <ref type="bibr" target="#b11">Hu et al., 2014</ref>)'s parameters are good choices for our setup as well. We get best result when both gpCNN and mfCNN have three blocks of convolution and pooling. This suggests that multiple layers of con- volution succeed in extracting high-level features that are beneficial for clause coherence.  <ref type="table" target="#tab_3">(2nd table)</ref> shows that INDIRECTSIM and CONCAT have comparable performance and both outperform DIRECTSIM. DIRECTSIM is ex- pected to perform poorly because the contents in the two clauses usually have little or no overlap- ping meaning. In contrast, we can imagine that INDIRECTSIM first transforms the first clause x into a counterpart and then matches this counter- part with the second clause y. In CONCAT, each of s 1 ×s 2 pairs of g-phrases is concatentated and supervised training can then learn an unrestricted function to assess the importance of this pair for clause coherence (cf. Eq. 5). Again, this is clearly a more promising TEXTCHUNK relation model for clause coherence than one that relies on DIRECT- SIM.    <ref type="table">)</ref> demonstrates that fine- tuning g-phrase representations gives better per- formance than freezing them. Also, grid-based and phrase-focused pooling outperform dynamic pooling <ref type="bibr" target="#b26">(Socher et al., 2011)</ref>  <ref type="table">(4th table)</ref>. Phrase- focused pooling performs best. <ref type="table">Table 1</ref> compares MultiGranCNN to ARC-I and ARC-II, the architectures proposed by <ref type="bibr" target="#b11">Hu et al. (2014)</ref>. We also test the five baseline systems from their paper: DeepMatch, WordEmbed, SEN- MLP, SENNA+MLP, URAE+MLP. For Multi- GranCNN, we use the best dev set settings: num- ber of convolution layers in gpCNN and mfCNN is 3; INDIRECTSIM; phrase-focused pooling. <ref type="table">Ta- ble 1</ref> shows that MultiGranCNN outperforms all other approaches on clause coherence test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Paraphrase Identification Task</head><p>We evaluate paraphrase identification (PI) on the PAN corpus (http://bit.ly/mt-para, ( <ref type="bibr" target="#b21">Madnani et al., 2012)</ref>), consisting of training and test sets of 10,000 and 3000 sentence pairs, respectively. Sen- tences are about 40 words long on average.</p><p>Since PI is a binary classification task, we re- place the MLP with a logistic regression layer. As phrase-focused pooling was proven to be optimal, we directly use phrase-focused pooling in PI task without comparison, assuming that the choice of dynamic pooling is task independent.</p><p>For parameter selection, we split the PAN train- ing set into a core training set (core) of size 9000 and a development set (dev) of size 1000. We then train models on core and select parameters based on best performance on dev. The best re- sults on dev are obtained for the following param- eters: freezing g-phrase representations, DIRECT- SIM, two convolution layers in gpCNN, no convo- lution layers in mfCNN. We use these parameter settings to train a model on the entire training set and report performance in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We compare MultiGranCNN to ARC-I/II ( <ref type="bibr" target="#b11">Hu et al., 2014)</ref>, and two previous papers reporting performance on PAN. <ref type="bibr" target="#b21">Madnani et al. (2012)</ref>    <ref type="table" target="#tab_3">Table 2</ref> shows that MultiGranCNN in combina- tion with MT metrics obtains state-of-the-art per- formance on PAN. Freezing weights learned in unsupervised training <ref type="figure" target="#fig_0">(Figure 2</ref>) performs better than fine-tuning them; also, <ref type="table" target="#tab_5">Table 3</ref> shows that the best result is achieved if no convolution is used in mfCNN. Thus, the best configuration for para- phrase identification is to "forward" fixed-size in- teraction matrices as input to the logistic regres- sion, without any intermediate convolution layers.</p><p>Freezing weights learned in unsupervised train- ing and no convolution layers in mfCNN both pro- tect against overfitting. Complex deep neural net- works are in particular danger of overfitting when training sets are small as in the case of PAN (cf. <ref type="bibr" target="#b11">Hu et al. (2014)</ref>). In contrast, fine-tuning weights and several convolution layers were the optimal setup for clause coherence. For clause coherence, we have a much larger training set and therefore can successfully train a much larger number of param- eters. <ref type="table" target="#tab_5">Table 3</ref> shows that CONCAT performs badly for PI while DIRECTSIM and INDIRECTSIM perform well. We can conceptualize PI as the task of deter- mining if each meaning element in S 1 has a simi- lar meaning element in S 2 . The s 1 × s 2 DIRECT- SIM feature model directly models this task and the s 1 ×s 2 INDIRECTSIM feature model also mod- els it, but learning a transformation of g-phrase representations before applying similarity. In con- trast, CONCAT can learn arbitrary relations be- tween parts of the two sentences, a model that seems to be too unconstrained for PI if insufficient training resources are available.</p><p>In contrast, for the clause coherence task, con- catentation worked well and DIRECTSIM worked poorly and we provided an explanation based on the specific properties of clause coherence (see discussion of <ref type="figure" target="#fig_7">Figure 7)</ref>. We conclude from these results that it is dependent on the task what the best feature model is for matching two linguistic ob- jects. Interestingly, INDIRECTSIM performs well on both tasks. This suggests that INDIRECTSIM is a general feature model for matching, applicable to tasks with very different properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>In this paper, we present MultiGranCNN, a gen- eral deep learning architecture for classifying the relation between two TEXTCHUNKS. Multi- GranCNN supports multigranular comparabil- ity of representations: shorter sequences in one TEXTCHUNK can be directly compared to longer sequences in the other TEXTCHUNK. Multi- GranCNN also contains a flexible and modu- larized match feature component that is eas- ily adaptable to different TEXTCHUNK relations. We demonstrated state-of-the-art performance of MultiGranCNN on paraphrase identification and clause coherence tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to CIS members and anonymous re- viewers for constructive comments. This work was supported by Baidu (through a Baidu scholarship awarded to Wenpeng Yin) and by Deutsche Forschungsgemeinschaft (grant DFG SCHU 2246/8-2, SPP 1335).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: gpCNN: learning g-phrase representations. This figure only shows two convolution layers (i.e., L = 2) for saving space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Similar to CBOW (Mikolov et al., 2013), we predict a sampled middle word v i from the av- erage of seven vectors: the TEXTCHUNK repre- sentation (the final output of gpCNN) and the three words to the left and to the right of v i . We use noise-contrastive estimation (Mnih and Teh, 2012) for training: 10 noise words are sampled for each true example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: General illustration of match feature model. In this example, both S 1 and S 2 have 10 gphrases, so the match feature matrixˆFmatrixˆ matrixˆF ∈ R s 1 ×s 2 has size 10 × 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CONCAT match feature model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 ,</head><label>5</label><figDesc>Figure 5, a s 1 × s 2 = 4 × 5 matrix (left) is split into s * × s * = 3 × 3 dynamic pools (middle): each row is split into [1, 1, 2] and each column is split into [1, 2, 2]. If s 1 &lt; s * , we first repeat all rows in batch style with size s 1 until no fewer than s * rows remain. Then the first s * rows are kept and split into s * dynamic pools. The same principle applies to the partitioning of columns. In Figure 5 (right), the areas with dashed lines and dotted lines are repeated parts for rows and columns, respectively; each cell is its own dynamic pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Partition methods in grid-based pooling. Original matrix with size 4 × 5 is mapped into matrix with size 3 × 3 and matrix with size 6 × 7, respectively. Each dynamic pool is distinguished by a border of empty white space around it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: mfCNN &amp; MLP for matching score learning. s * = 10, f b = 5, k b = 2, c b = 4 in this example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7</head><label>7</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect on dev acc (clause coherence) of different factors: # convolution blocks, match feature model, freeze vs. fine-tune, pooling method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 (</head><label>7</label><figDesc>Figure 7 (3rd table) demonstrates that finetuning g-phrase representations gives better performance than freezing them. Also, grid-based and phrase-focused pooling outperform dynamic pooling (Socher et al., 2011) (4th table). Phrasefocused pooling performs best. Table 1 compares MultiGranCNN to ARC-I and ARC-II, the architectures proposed by Hu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on PAN. "8MT" = 8 MT metrics 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Effect on dev F 1 (PI) of different factors: 
# convolution blocks, match feature model. </table></figure>

			<note place="foot" n="1"> In response to a reviewer question, recall that si is the total number of g-phrases of Si, so there is only one s1 × s2 matrix, not several on different levels of granularity.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting discourse information to identify paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngo Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Le Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shimazu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2832" to="2841" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative improvements to distributional sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="891" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond bag-of-words: machine learning for query-document matching in web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1177" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Har: Hub, authority and relevance scores in multirelational data for query search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th SIAM International Conference on Data Mining</title>
		<meeting>the 12th SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probabilistic Siamese Network for Learning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep architecture for matching short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1367" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">English gigaword fourth edition. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguistic</forename><surname>Data Consortium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Paraphrase recognition via dissimilarity significance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online multimodal deep similarity learning with application to image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haechang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS deep learning workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
