<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Forest-Based Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ehime University</orgName>
								<address>
									<settlement>Matsuyama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Forest-Based Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1253" to="1263"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest within a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model). The BLEU score of the proposed method is higher than that of the string-to-string NMT, tree-based NMT, and forest-based SMT systems .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>NMT has witnessed promising improvements re- cently. Depending on the types of input and out- put, these efforts can be divided into three cate- gories: string-to-string systems ( ; tree-to-string sys- tems ( <ref type="bibr" target="#b6">Eriguchi et al., 2016</ref><ref type="bibr" target="#b7">Eriguchi et al., , 2017</ref>; and string-to- tree systems ( <ref type="bibr" target="#b0">Aharoni and Goldberg, 2017;</ref><ref type="bibr" target="#b20">Nadejde et al., 2017</ref>). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attrac- tive features. They can use more syntactic infor- mation ( , and can conveniently in- corporate prior knowledge ( . Because of these advantages, tree-based methods * Contribution during internship at National Institute of Information and Communications <ref type="bibr">Technology.</ref> become the focus of many researches of NMT nowadays.</p><p>Based on how to represent trees, there are two main categories of tree-based NMT methods: rep- resenting trees by a tree-structured neural network ( <ref type="bibr" target="#b6">Eriguchi et al., 2016;</ref><ref type="bibr" target="#b28">Zaremoodi and Haffari, 2017)</ref>, representing trees by linearization ( <ref type="bibr" target="#b26">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Dyer et al., 2016;</ref><ref type="bibr" target="#b13">Ma et al., 2017</ref>). Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is pre- ferred from the viewpoint of computation. There- fore we focus on this kind of methods in this paper.</p><p>In spite of impressive performance of tree-based NMT systems, they suffer from a major draw- back: they only use the 1-best parse tree to di- rect the translation, which potentially introduces translation mistakes due to parsing errors <ref type="bibr" target="#b19">(Quirk and Corston-Oliver, 2006</ref>). For SMT, forest-based methods have employed a packed forest to address this problem <ref type="bibr" target="#b9">(Huang, 2008)</ref>, which represents ex- ponentially many parse trees rather than just the 1-best one ( . But for NMT, (computationally efficient) forest- based methods are still being explored. <ref type="bibr">1</ref> Because of the structural complexity of forests, the lack of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest- based NMT to some extent.</p><p>Inspired by the tree-based NMT methods based on linearization, we propose an efficient forest- based NMT approach (Section 3), which can en- code the syntactic information of a packed for-est on the basis of a novel weighted lineariza- tion method for a packed forest (Section 3.1), and can decode the linearized packed forest within the simple sequence-to-sequence framework <ref type="bibr">(Section 3.2)</ref>. Experiments demonstrate the effectiveness of our method (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We first review the general sequence-to-sequence model (Section 2.1), then describe tree-based NMT systems based on linearization (Section 2.2), and finally introduce the packed forest, through which exponentially many trees can be repre- sented in a compact manner (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence-to-sequence model</head><p>Current NMT systems usually resort to a sim- ple framework, i.e., the sequence-to-sequence model ( ). Given a source sequence (x 0 , . . . , x T ), in order to find a target sequence (y 0 , . . . , y T ) that max- imizes the conditional probability p(y 0 , . . . , y T | x 0 , . . . , x T ), the sequence-to-sequence model uses one RNN to encode the source sequence into a fixed-length context vector c and another RNN to decode this vector and generate the target se- quence. Formally, the probability of the target se- quence can be calculated as follows:</p><formula xml:id="formula_0">p(y 0 , . . . ,y T | x 0 , . . . , x T ) = T t=0 p(y t | c, y 0 , . . . , y t−1 ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">p(y t | c, y 0 , . . . , y t−1 ) = g(y t−1 , s t , c),<label>(2)</label></formula><formula xml:id="formula_2">s t = f (s t−1 , y t−1 , c), (3) c = q(h 0 , . . . , h T ),<label>(4)</label></formula><formula xml:id="formula_3">h t = f (e t , h t−1 ).<label>(5)</label></formula><p>Here, g, f , and q are nonlinear functions; h t and s t are the hidden states of the source-side RNN and target-side RNN, respectively, c is the context vector, and e t is the embedding of x t .  introduced an attention mechanism to deal with the issues related to long sequences ( ). Instead of encod- ing the source sequence into a fixed vector c, the attention model uses different c i when calculating the target-side output y i at time step i:</p><formula xml:id="formula_4">c i = T j=0 α ij h j ,<label>(6)</label></formula><formula xml:id="formula_5">α ij = exp(a(s i−1 , h j )) T k=0 exp(a(s i−1 , h k )) .<label>(7)</label></formula><p>The function a(s i−1 , h j ) can be regarded as the soft alignment between the target-side RNN hid- den state s i−1 and the source-side RNN hidden state h j . Depending on the format of the source/target sequences, this framework can be regarded as a string-to-string NMT system ), a tree-to-string NMT system ( , or a string-to-tree NMT system (Aharoni and Goldberg, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linear-structured tree-based NMT systems</head><p>Regarding the linearization adopted for tree-to- string NMT (i.e., linearization of the source side), <ref type="bibr" target="#b21">Sennrich and Haddow (2016)</ref> encoded the se- quence of dependency labels and the sequence of words simultaneously, partially utilizing the syn- tax information, while  traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-to- tree NMT (i.e., linearization of the target side), <ref type="bibr" target="#b20">Nadejde et al. (2017)</ref> used a CCG supertag se- quence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence en- semble for the annotated tree in the Penn Tree- bank ( <ref type="bibr" target="#b14">Marcus et al., 1993)</ref>. <ref type="bibr" target="#b27">Wu et al. (2017)</ref> used transition actions to linearize a dependency tree, and employed the sequence-to-sequence frame- work for NMT.</p><p>All the current tree-based NMT systems use only one tree for encoding or decoding. In con- trast, we hope to utilize multiple trees (i.e., a for- est). This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Packed forest</head><p>The packed forest gives a representation of expo- nentially many parse trees, and can compactly en- code many more candidates than the n-best list  [6] <ref type="bibr">[7]</ref> [8]</p><p>[9]</p><p>[10] <ref type="bibr">[11]</ref> (a) Packed forest  ( <ref type="bibr" target="#b9">Huang, 2008)</ref>. <ref type="figure" target="#fig_2">Figure 1a</ref> shows a packed forest, which can be unpacked into two constituent trees ( <ref type="figure" target="#fig_2">Figure 1b</ref> and <ref type="figure" target="#fig_2">Figure 1c)</ref>. Formally, a packed forest is a pair V, E, where V is the set of nodes and E is the set of hyper- edges. Each v ∈ V has the form X i,j , where X is a constituent label and i, j ∈ [0, n] are indices of words, showing that the node spans the words ranging from i (inclusive) to j (exclusive). Here, n is the length of the input sentence. Each e ∈ E is a three-tuple head(e), tails(e), score(e), where head(e) ∈ V is similar to the head node in a con- stituent tree, and tails(e) ∈ V * is similar to the set of child nodes in a constituent tree. score(e) ∈ R is the log probability that tails(e) represents the tails of head(e) calculated by the parser. Based on score(e), the score of a constituent tree T can be calculated as follows:</p><formula xml:id="formula_6">score(T ) = −λn + e∈E(T ) score(e), (8)</formula><p>where E(T ) is the set of hyperedges appearing in tree T , and λ is a regularization coefficient for the sentence length. <ref type="bibr">2</ref> 2 Following the configuration of Charniak and Johnson</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Forest-based NMT</head><p>We first propose a linearization method for the packed forest (Section 3.1), then describe how to encode the linearized forest (Section 3.2), which can then be translated by the conventional decoder (see Section 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Forest linearization</head><p>Recently, several studies have focused on the lin- earization methods of a syntax tree, both in the area of tree-based NMT (Section 2.2) and pars- ing ( <ref type="bibr" target="#b26">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Dyer et al., 2016;</ref><ref type="bibr" target="#b13">Ma et al., 2017)</ref>. Basically, these methods follow a fixed traversal order (e.g., depth-first). This does not exist for the packed forest which is a directed acyclic graph (DAG). Furthermore, the weights are attached to edges of a packed forest instead of the nodes. This further increases the difficulty of linearization.</p><p>Topological ordering algorithms for DAG <ref type="bibr" target="#b10">(Kahn, 1962;</ref><ref type="bibr" target="#b25">Tarjan, 1976)</ref> are not good solutions, because the topological ordering outputted by al- gorithms is not always optimal for machine trans- <ref type="bibr">(2005)</ref>, for all the experiments in this paper, we fixed λ to log 2 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Linearization of a packed forest</head><formula xml:id="formula_7">1: function LINEARIZEFOREST(V, E, w) 2: v ← FINDROOT(V ) 3: r ← [] 4: EXPANDSEQ(v, r, V, E, w) 5:</formula><p>return r 6: function FINDROOT(V ) 7:</p><p>for v ∈ V do 8: if v has no parent then 9:</p><p>return v 10: procedure EXPANDSEQ(v, r, V, E, w) 11:</p><p>for e ∈ E do 12:</p><p>if head(e) = v then 13:</p><p>if tails(e) = ∅ then 14:</p><p>for t ∈ SORT(tails(e)) do Sort tails(e) by word indices. 15:</p><formula xml:id="formula_8">EXPANDSEQ(t, r, V, E, w) 16: l ← LINEARIZEEDGE(head(e), w) 17: r.append(l, σ(0.0)) σ is the sigmoid function, i.e., σ(x) = 1 1+e −x , x ∈ R. 18: l ← c LINEARIZEEDGES(tails(e), w) c</formula><p>is a unary operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>r.append(l, σ(score(e))) 20:</p><formula xml:id="formula_9">else 21: l ← LINEARIZEEDGE(head(e), w) 22: r.append(l, σ(0.0)) 23: function LINEARIZEEDGE(Xi,j, w) 24: return X ⊗ ( j−1 k=i w k ) 25: function LINEARIZEEDGES(v, w) 26: return ⊕v∈vLINEARIZEEDGE(v, w)</formula><p>lation. In particular, a topological ordering could ignore "word sequential information" and "parent- child information." For example, for the packed forest in <ref type="figure" target="#fig_2">Figure 1a</ref>, <ref type="bibr">[11]</ref>" is a valid topological ordering, the word sequential in- formation of the words (e.g., "John" should be lo- cated ahead of the period), which is fairly crucial for translation of languages with fixed pragmatic word order such as Chinese or English, is lost.</p><formula xml:id="formula_10">although "[10]→[1]→[2]→ · · · →[9]→</formula><p>As another example, for the packed forest above, nodes <ref type="bibr">[2]</ref>, <ref type="bibr">[9]</ref>, and <ref type="bibr">[10]</ref> are all the children of node <ref type="bibr">[11]</ref>. However, in the topological order</p><formula xml:id="formula_11">"[1]→[2]→ · · · →[9]→[10]→[11]," node [2]</formula><p>is quite far from node <ref type="bibr">[11]</ref>, while nodes <ref type="bibr">[9]</ref> and <ref type="bibr">[10]</ref> are both close to node <ref type="bibr">[11]</ref>. The parent-child in- formation cannot be reflected in this topological order, which is not what we would expect.</p><p>To address the above two problems, we pro- pose a novel linearization algorithm for a packed forest (Algorithm 1). The algorithm linearizes the packed forest from the root node (Line 2) to leaf nodes by calling the EXPANDSEQ procedure (Line 15) recursively, while preserving the word order in the sentence <ref type="bibr">(Line 14)</ref>. In this way, word sequential information is preserved. Within the EXPANDSEQ procedure, once a hyperedge is lin- earized (Line 16), the tails are also linearized im- mediately (Line 18). In this way, parent-child in- formation is preserved. Intuitively, different parts of constituent trees should be combined in differ- ent ways, therefore we define different operators ( c , ⊗, ⊕, or ) to represent the relationships, so that the representations of these parts can be combined in different ways (see Section 3.2 for details). Words are concatenated by the operator "" with each other, a word and a constituent la- bel is concatenated by the operator "⊗", the lin- earization results of child nodes are concatenated by the operator "⊕" with each other, while the unary operator " c " is used to indicate that the node is the child node of the previous part. Fur- thermore, each token in the linearized sequence is related to a score, representing the confidence of the parser.</p><formula xml:id="formula_12">NNP⊗John / NP⊗John / c NNP⊗John / VBZ⊗has / DT⊗a / NN⊗dog / NP⊗adog / c DT⊗a⊕NN⊗dog / NP⊗adog / c DT⊗a⊕NN⊗dog / S⊗adog / c NP⊗adog / VP⊗hasadog / c VBZ⊗has⊕NP⊗adog / c VBZ⊗has⊕S⊗adog / .⊗. / S⊗Johnhasadog. / c NP⊗John⊕VP⊗hasadog⊕.⊗.</formula><p>The linearization result of the packed forest in <ref type="figure" target="#fig_2">Figure 1a</ref> is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Tokens in the lin- earized sequence are separated by slashes. Each token in the sequence is composed of different types of symbols and combined by different op- erators. We can see that word sequential infor- mation is preserved. For example, "NNP⊗John" (linearization result of node <ref type="bibr">[1]</ref>) is in front of "VBZ⊗has" (linearization result of node <ref type="bibr">[3]</ref>), which is in front of "DT⊗a" (linearization result of node <ref type="bibr">[4]</ref>). Moreover, parent-child informa- tion is also preserved. For example, "NP⊗John" (linearization result of node <ref type="bibr">[2]</ref>) is followed by " c NNP⊗John" (linearization result of node <ref type="bibr">[1]</ref>, the child of node <ref type="bibr">[2]</ref>).</p><p>Note that our linearization method does not out- put fully recoverable packed forests. What we do want to do is to encode syntax information as much as possible, so that we can improve the per- formance of NMT.</p><p>Also note that there is one more advantage of our linearization method: the linearized sequence is a weighted sequence, while all the previous studies ignored the weights during linearization.   By preserving only the nodes and hyperedges in the 1-best tree and removing all others, our linearization method can be regarded as a tree- linearization method. Compared with other tree- linearization methods, our method combines sev- eral different kinds of information within one sym- bol, retaining the parent-child information, and incorporating the confidence of the parser in the sequence. We examine whether the weights can be useful not only for linear structured tree-based NMT but also for our forest-based NMT in Sec- tion 4.</p><p>Furthermore, although our method is non- reversible for packed forests, it is reversible for constituent trees, in that the linearization is pro- cessed exactly in the depth-first traversal order and all necessary information in the tree nodes has been encoded. As far as we know, there is no pre- vious work on linearization of packed forests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding the linearized forest</head><p>The linearized packed forest forms the input of the encoder, which has two major differences from the input of a sequence-to-sequence NMT system. First, the input sequence of the encoder consists of two parts: the symbol sequence and the score se- quence. Second, the symbol sequence consists of three types of symbols: words, constituent labels, and operators ( c , ⊗, ⊕, or ) that connect the other two types of symbols. Based on these char- acteristics, we propose a method of encoding the linearized forest.</p><p>Formally, the input layer receives two se- quences: the symbol sequence l = (l 0 , . . . , l T ) and the score sequence ξ = (ξ 0 , . . . , ξ T ), where l i denotes the i-th symbol and ξ i its score. Then, the two sequences are fed into the symbol layer and the score layer, respectively. Any item l ∈ l in the symbol layer has the form</p><formula xml:id="formula_13">l = o 0 x 1 o 1 . . . x m−1 o m−1 x m ,<label>(9)</label></formula><p>where each x k (k = 1, . . . , m) is a word or a con- stituent label, m is the total number of words and constituent labels in a symbol, o 0 is " c " or empty, and each o k (k = 1, . . . , m − 1) is either "⊗", "⊕", or "". Then, in the node/operator layer, these x and o are separated and rearranged as x = (x 1 , . . . , x m , o 0 , . . . , o m−1 ), which is fed to the pre-embedding layer. The pre-embedding layer generates a sequence p = (p 1 , . . . , p m , . . . , p 2m ), which is calculated as follows:</p><formula xml:id="formula_14">p = W emb [I(x)].<label>(10)</label></formula><p>Here, the function I(x) returns a list of the indices in the dictionary for all the elements in x, includ- ing words, constituent labels, and operators. In addition, W emb is the embedding matrix of size (|w word |+|w label |+4)×d word , where |w word | and |w label | are the vocabulary size of words and con- stituent labels, respectively, d word is the dimen- sion of the word embedding, and there are four possible operators: " c ," "⊗," "⊕," and "." Note that p is a list of 2m vectors, and the dimension of each vector is d word . Hereafter, p for the k-th symbol l k is denoted by p k .</p><p>Depending on where the score layer is incor- porated, we propose two frameworks: Score-on- Embedding (SoE) and Score-on-Attention (SoA), which are illustrated in <ref type="figure" target="#fig_5">Figure 3</ref>. In SoE, the k- th element of the embedding layer is calculated as follows:</p><formula xml:id="formula_15">e k = ξ k p∈p k p,<label>(11)</label></formula><p>while in SoA, the k-th element of the embedding layer is calculated as</p><formula xml:id="formula_16">e k = p∈p k p,<label>(12)</label></formula><p>where k = 0, . . . , T . Note that e k ∈ R d word . In this manner, the proposed forest-to-string NMT framework is connected with the conventional sequence-to-sequence NMT framework. After calculating the embedding vectors in the embedding layer, the hidden vectors are calculated using Equation (5). When calculating the context vector c i , SoE and SoA differ from each other. For SoE, the c i is calculated using Equations (6) and (7), while for SoA, the α ij used to calculate the c i is determined as follows:</p><formula xml:id="formula_17">α ij = exp(ξ j a(s i−1 , h j )) T k=0 exp(ξ k a(s i−1 , h k )) .<label>(13)</label></formula><p>Then, using the decoder of the sequence-to- sequence framework, the sentence of the target language can be generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluated the effectiveness of our forest-based NMT systems on English-to-Chinese and English- to-Japanese translation tasks. <ref type="bibr">3</ref> The statistics of the corpora used in our experiments are summarized in <ref type="table">Table 1</ref>. The packed forests of English sentences were obtained by the constituent parser proposed by <ref type="bibr" target="#b9">Huang (2008)</ref>. <ref type="bibr">4</ref> We filtered out the sentences for which the parser was not able to generate any packed forests and those longer than 80 <ref type="bibr">words</ref> segmenter <ref type="bibr">5</ref> for segmentation. For Japanese sen- tences, we followed the preprocessing steps rec- ommended in WAT 2017. <ref type="bibr">6</ref> We implemented our framework based on nematus 8 ( <ref type="bibr" target="#b20">Sennrich et al., 2017)</ref>. For optimiza- tion, following previous research such as ), we used the ADADELTA al- gorithm <ref type="bibr" target="#b29">(Zeiler, 2012)</ref>. In order to avoid over- fitting, we used dropout ( <ref type="bibr" target="#b22">Srivastava et al., 2014</ref>) on the embedding layer and hidden layer, with the dropout probability set to 0.2. We used the gated recurrent unit ( ) as the recurrent unit of RNNs, which are bi-directional, with one hidden layer.</p><p>Based on the tuning result, we set the maxi- mum length of the input sequence to 300, the hid- den layer size as 512, the dimension of word em- bedding as 620, and the batch size for training as 40. We pruned the packed forest using the algo- rithm of <ref type="bibr" target="#b9">Huang (2008)</ref>, removing all hyperedges e which satisfy δ(e) &gt; 10 −5 , where δ(e) is the dif- ference between the cost of hyperedge e and that of the globally best derivation. If the lineariza- tion of the pruned forest is still longer than 300, then we linearize the 1-best parsing tree instead of the forest. As for the stopping criterion of train- ing process, we evaluated the BLEU score on the development set every 10,000 updates. If BLEU score was not increased in ten consecutive evalua- tions, then training was stopped. During decoding, we performed beam search with the beam size of 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Systems  <ref type="table">Table 2</ref>: English-Chinese experimental results (character-level BLEU). "FS," "SN," "TN," and "FN" denote forest-based SMT, string-based NMT, tree-based NMT, and forest-based NMT systems, respec- tively. The p values were obtained by the paired bootstrap resampling significance test <ref type="bibr" target="#b11">(Koehn, 2004</ref>  <ref type="table">Table 3</ref>: English-Japanese experimental results (character-level BLEU). <ref type="table">Tables 2 and 3</ref> summarize the experimental re- sults. To avoid the effect of segmentation errors, the performance was evaluated by character-level BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>). We compared our proposed models (i.e., Forest (SoE) and Forest (SoA)) with three types of baseline: a string-to- string model (s2s), forest-based models that do not use score sequences (Forest (No score)), and tree- based models that use the 1-best parsing tree (1- best (No score, SoE, SoA)). For the 1-best models, we preserved the nodes and hyperedges that were used in the 1-best constituent tree in the packed forest, while removing all other nodes and hy- peredges. For the "No score" configurations, we forced the input score sequence to be a sequence of 1.0 with the same length as the input symbol sequence, so that neither the embedding layer nor the attention layer were affected by the score se- quence.</p><note type="other">&amp; MT 03 MT 04 MT 05 p value p value</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>In addition, we also made a comparison with some state-of-the-art tree-based systems. As the SMT system, we examined . Specifically, we used the implementation of cicada. <ref type="bibr">9</ref> For NMT systems, we compared with three systems: <ref type="bibr" target="#b6">Eriguchi et al. (2016)</ref>  <ref type="bibr">10 and Chen et al. (2017)</ref>, <ref type="bibr">11</ref> both are publicly available, and we reimplemented the "Mixed RNN Encoder" model of , because of its outstanding per- formance on the NIST MT corpus.</p><p>We can see that for both English-Chinese and English-Japanese, compared with the s2s baseline system, both the 1-best and forest-based configu- rations yield better results. This indicates syntac- tic information contained in the constituent trees or forests is indeed useful for machine translation. Specifically, we observed the following facts. First, among the three different frameworks, i.e., SoE, SoA, and No-score, the SoA framework performed the best, while the No-score framework <ref type="bibr">[Source]</ref> In the Czech Republic , which was ravaged by serious floods last summer , the temperatures in its border region adjacent to neighboring Slovakia plunged to minus 18 degrees Celsius .</p><p>[Reference <ref type="table">] 去年  夏季  曾  出现  严重  水患  的  捷克  共和国 ， 其  邻近  斯洛伐克 的  边界  地区  气温  低  至  摄氏  零下  18</ref> 度 。 last summer ever appear serious floods of Czech Republic , its adjacent Slovakia of border region temperature decrease to Celsius minus 18 degree .</p><p>[s2s <ref type="table">]  去年  夏天  ，  捷克  地区  遭受  严重  洪灾  的  捷克  边境  地区  气温  下降  了 18  摄氏  度  。   last summer  ,  Czech region suffer  serious  floods  of  Czech  border region temperature decrease -ed 18</ref> Celsius degree .</p><p>[  performed the worst. This indicates that the scores of the edges in constituent trees or packed forests, which reflect the confidence of the correctness of the edges, are indeed useful. In fact, for the 1- best constituent parsing tree, the score of the edge reflects the confidence of the parser. With this in- formation, the NMT system succeeded to learn a better attention, paying more attention to the confi- dent structure and less attention to the unconfident structure, which improved the translation perfor- mance. This fact was ignored by previous stud- ies on tree-based NMT. Furthermore, it is better to use the scores to adjust the values of attention in- stead of rescaling the word embeddings, because modifying word embeddings may alter the seman- tic meanings of words. Second, compared with the cases that only use the 1-best constituent trees, with some ex- ceptions, using packed forests yielded statistical significantly better results for the SoE and SoA frameworks. This shows the effectiveness of us- ing more syntactic information. Compared with one constituent tree, the packed forest, which con- tains multiple different trees, describes the syntac- tic structure of the sentence in different aspects, which together increase the accuracy of machine translation. However, without using the scores, the 1-best constituent tree is preferred. This is because without using the scores, all trees in the packed forest are treated equally, which makes it easy to import noise into the encoder.</p><p>Compared with other types of state-of-the-art systems, our systems using only the 1-best tree (1- best (SoE, SoA)) were better than the other tree- based systems. Moreover, our NMT systems using the packed forests achieved the best performance. These results also support the usefulness of the scores of the edges and packed forests in NMT.</p><p>As for the efficiency, the training time of the SoA system was slightly longer than that of the SoE system, which was about twice of the s2s baseline. The training time of the tree-based sys- tem was about 1.5 times of the baseline. For the case of Forest (SoA), with 1 core of Tesla P100 GPU and LDC corpus as the training data, train- ing spent about 10 days, and decoding speed was about 10 sentences per second. The reason for the relatively low efficiency is that the linearized se- quences of packed forests were much longer than word sequences, enlarging the scale of the inputs. Despite this, the training process ended within rea- sonable time. <ref type="figure" target="#fig_6">Figure 4</ref> shows the translation results of an En- glish sentence using several different configura- tions: the s2s baseline, using only the 1-best tree (SoE), and using the packed forest (SoE). This is a sentence from NIST MT 03, and the training cor- pus is the LDC corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative analysis</head><p>For the s2s case, no syntactic information was utilized, and therefore the output of the system was not a grammatical Chinese sentence. The attribu- tive phrase of "Czech border region" (i.e., "last summer ... floods") is a complete sentence. How- ever, this is not grammatically allowed in Chinese.</p><p>For the case of using 1-best constituent tree, the output was a grammatical Chinese sentence. However, the phrase "adjacent to neighboring Slo- vakia" was completely ignored in the translation result. Analysis of the constituent tree revealed that this phrase was incorrectly parsed as an "ad- verb phrase," and consequently the NMT system paid a little attention to it, because of the low con- fidence given by the parser.</p><p>In contrast, the packed forest did not ignore this phrase and translated it correctly. Actually, be- sides "adverb phrase," this phrase was also cor- rectly parsed as an "adjective phrase," and covered by multiple different nodes in the forest. Because of the wide coverage, it is difficult for the encoder to ignore the phrase.</p><p>We also noticed that our method performed bet- ter on learning attention. For example, in <ref type="figure" target="#fig_6">Figure 4</ref>, we observed that for s2s model, the decoder paid attention to the word "Czech" twice, which caused the output sentence containing the corresponding Chinese translation twice. On the other hand, for our forest model, by using the syntax information, the decoder paid an attention to the phrase "In the Czech Republic" only once; therefore the decoder generated the correct output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Incorporating syntactic information into NMT systems is attracting widespread attention nowa- days. Compared with conventional string-to-string NMT systems, tree-based systems demonstrate a better performance with the help of constituent trees or dependency trees.</p><p>The first noteworthy study was <ref type="bibr" target="#b6">Eriguchi et al. (2016)</ref>, which used Tree-structured LSTM <ref type="bibr" target="#b24">(Tai et al., 2015)</ref> to encode the HPSG syntax tree of the sentence in the source-side in a bottom-up man- ner. Then, <ref type="bibr" target="#b3">Chen et al. (2017)</ref> enhanced the en- coder with a top-down tree encoder.</p><p>As a simple extension of <ref type="bibr" target="#b6">Eriguchi et al. (2016)</ref>, very recently, <ref type="bibr" target="#b28">Zaremoodi and Haffari (2017)</ref> pro- posed a forest-based NMT method by represent- ing the packed forest with a forest-structured neu- ral network. However, their method was evaluated in small-scale MT settings (each training dataset consists of under 10k parallel sentences). In con- trast, our proposed method is effective in a large- scale MT setting, and we present qualitative anal- ysis regarding the effectiveness of using forests in NMT.</p><p>Although these methods obtained good results, the tree-structured network used by the encoder made the training and decoding relatively slow, re- stricting the scope of application.</p><p>Other attempts at encoding syntactic trees have also been proposed. <ref type="bibr" target="#b7">Eriguchi et al. (2017)</ref> com- bined the Recurrent Neural Network Grammar ( <ref type="bibr" target="#b5">Dyer et al., 2016</ref>) with NMT systems, while  linearized the constituent tree and encoded it using RNNs. The training of these methods is fast, because of the linear structures of RNNs. However, all these syntax-based NMT sys- tems used only the 1-best parsing tree, making the systems sensitive to parsing errors.</p><p>Instead of using trees to represent syntactic in- formation, some studies used other data structures to represent the latent syntax of the input sen- tence. For example, <ref type="bibr" target="#b8">Hashimoto and Tsuruoka (2017)</ref> proposed translating using a latent graph. However, such systems do not enjoy the benefit of handcrafted syntactic knowledge, because they do not use a parser trained from a large treebank with human annotations.</p><p>Compared with these related studies, our frame- work utilizes a linearized packed forest, meaning the encoder can encode exponentially many trees in an efficient manner. The experimental results demonstrated these advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We proposed a new encoding method for NMT, which encodes a packed forest for the source sentence using linear-structured neural networks, such as RNN. When introducing packed forest, we confirmed that the score of each edge is indispens- able. Compared with conventional string-to-string NMT systems and tree-to-string NMT systems, our framework can utilize exponentially many lin- earized parsing trees during encoding, without sig- nificantly decreasing the efficiency. This repre- sents the first attempt to use a forest within the string-to-string NMT framework. The experimen- tal results demonstrate the effectiveness of our method.</p><p>As future work, we plan to design some more elaborate structures to incorporate the score layer into the encoder. We will also apply the proposed linearization method to other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>John</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of (a) a packed forest. The numbers in the brackets located at the upper-left corner of each node in the packed forest show one correct topological ordering of the nodes. The packed forest is a compact representation of two trees: (b) the correct constituent tree, and (c) an incorrect constituent tree. Note that the terminal nodes (i.e., words in the sentence) in the packed forest are shown only for illustration, and they do not belong to the packed forest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Linearization result of the packed forest in Figure 1a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>…</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of the forest-based NMT system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Chinese translation results of an English sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. For NIST datasets, we simply chose the first reference among the four English references of NIST cor- pora. For Chinese sentences, we used Stanford</figDesc><table>˜ huanlian/software/forest-reranker/ 
forest-charniak-v0.8.tar.bz2 

Language 
Corpus 
Usage 
#Sent. 

English-Japanese 
ASPEC 

train 
100,000 
dev. 
1790 
test 
1812 

English-Chinese 

LDC 7 
train 
1,423,695 
FBIS 
233,510 
NIST MT 02 
dev. 
876 
NIST MT 03 
test 

919 
NIST MT 04 
1,788 
NIST MT 05 
1,082 

Table 1: Statistics of the corpora. 

</table></figure>

			<note place="foot" n="1"> Zaremoodi and Haffari (2017) have proposed a forestbased NMT method based on a forest-structured neural network recently, but it is computationally inefficient (see Section 5).</note>

			<note place="foot" n="3"> English is commonly chosen as the target language. We chose English as the source language because a highperformance forest parser is not available for other languages. 4 http://web.engr.oregonstate.edu/</note>

			<note place="foot" n="5"> https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html 7 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06 8 https://github.com/EdinburghNLP/ nematus</note>

			<note place="foot" n="9"> https://github.com/tarowatanabe/ cicada 10 https://github.com/tempra28/tree2seq 11 https://github.com/howardchenhd/ Syntax-awared-NMT</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the anonymous reviewers for their insightful comments and suggestions. We thank Lemao Liu from Tencent AI Lab for his suggestions about the experiments. We thank At-sushi Fujita whose suggestions greatly improve the readability and the logical soundness of this paper. This work was done during the intern-ship of Chunpeng Ma at NICT. Akihiro Tamura is supported by JSPS KAKENHI Grant Num-ber JP18K18110. Tiejun Zhao is supported by the National Natural Science Foundation of China (NSFC) via grant 91520204 and State High-Tech Development Plan of China (863 program) via grant 2015AA015405.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and MaxEnt discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1936" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07776</idno>
		<title level="m">Recurrent neural network grammars</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to parse and translate improves neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural machine translation with source-side latent graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02265</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topological sorting of large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur B Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="558" to="562" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<meeting>EMNLP 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deterministic attention for sequence-to-sequence constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumita</forename><surname>Eiichiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3237" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Forest-based translation rule extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="206" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Forestbased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01147</idno>
	</analytic>
	<monogr>
		<title level="m">Philipp Koehn, and Alexandra Brich. 2017. Syntax-aware neural machine translation using CCG</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The impact of parse quality on syntactically-informed statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon Corston-</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02892</idno>
		<title level="m">Linguistic input features improve neural machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge-disjoint spanning trees and depth-first search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Endre Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="185" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Incorporating syntactic uncertainty in neural machine translation with forest-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poorya</forename><surname>Zaremoodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07019</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prior knowledge integration for neural machine translation using posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1514" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
