<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Text Segmentation Based on Native Language Characteristics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Wolska</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Harvard Medical School</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Text Segmentation Based on Native Language Characteristics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1457" to="1469"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1134</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most work on segmenting text does so on the basis of topic changes, but it can be of interest to segment by other, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text seg-mentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes <ref type="bibr" target="#b19">(Hearst, 1997)</ref>. There are various contexts, however, in which it is of in- terest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship ( <ref type="bibr" target="#b24">Koppel et al., 2011</ref>) and poetic voice ( <ref type="bibr" target="#b7">Brooke et al., 2012)</ref>. In this paper we investigate text segmentation on the basis of change in the native language of the writer.</p><p>Two illustrative contexts where this task might be of interest are patchwriting detection and lit- erary analysis. Patchwriting is the heavy use of text from a different source with some modifica- tion and insertion of additional words and sen- tences to form a new text. <ref type="bibr" target="#b37">Pecorari (2003)</ref> notes that this is a kind of textual plagiarism, but is a strategy for learning to write in an appropri- ate language and style, rather than for deception. <ref type="bibr" target="#b23">Keck (2006)</ref>, <ref type="bibr" target="#b15">Gilmore et al. (2010)</ref> and <ref type="bibr" target="#b42">Vieyra et al. (2013)</ref> found that non-native speakers, not surprisingly in situations of imperfect mastery of a language, are strongly over-represented in this kind of textual plagiarism. In these cases the boundaries between the writer's original text and (near-)copied native text are often quite appar- ent to the reader, as in this short example from <ref type="bibr" target="#b27">Li and Casanave (2012)</ref> (copied text italicised): "Nevertheless, doubtfulness can be cleared rea- sonably by the experiments conducted upon the 'split-brain patients', in whom intra-hemispheric communication is no longer possible. To illustrate, one experiment has the patient sit at a table with a non-transparent screen blocking the objects be- hind, who is then asked to reach the objects with different hand respectively." Because patchwrit- ing can indicate imperfect comprehension of the source <ref type="bibr" target="#b21">(Jamieson and Howard, 2013)</ref>, identifying it and supporting novice writers to improve it has become a focus of programmes like the Citation Project. <ref type="bibr">1</ref> For the second, perhaps more speculative con- text of literary analysis, consider Joseph Conrad, known for having written a number of famous English-language novels, such as Heart of Dark- ness; he was born in Poland and moved to England at the age of 21. His writings have been the subject of much manual analysis, with one particular di- rection of such research being the identification of likely influences on his English writing, including his native Polish language and the French he learnt before English. <ref type="bibr" target="#b36">Morzinski (1994)</ref>, for instance, notes aspects of his writing that exhibit Polish-like syntax, verb inflection, or other linguistic charac- teristics (e.g. "Several had still their staves in their hands" where the awkwardly placed adverb still is typical of Polish). These appear both in isolated sentences and in larger chunks of text, and part of an analysis can involve identifying these chunks.</p><p>This raises the question: Can NLP and com- putational models identify the points in a text where native language changes? Treating this as an unsupervised text segmentation problem, we present the first Bayesian model of text segmen- tation based on authorial characteristics, applied to native language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Topic Segmentation The most widely- researched text segmentation task has as its goal to divide a text into topically coherent segments. Lexical cohesion <ref type="bibr" target="#b17">(Halliday and Hasan, 1976)</ref> is an important concept here: the principle that text is not formed by a random set of words and sentences but rather logically ordered sets of related words that together form a topic. In addition to the semantic relation between words, other methods such as back-references and conjunctions also help achieve cohesion. Based on this, <ref type="bibr" target="#b35">Morris and Hirst (1991)</ref> proposed the use of lexical chains, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm <ref type="bibr" target="#b18">(Hearst, 1994</ref><ref type="bibr" target="#b19">(Hearst, , 1997</ref>) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts.</p><p>There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised Bayesian technique BAYESSEG of <ref type="bibr" target="#b13">Eisenstein and Barzilay (2008)</ref>, based on a generative model that assumes that each segment has its own language model. Un- der this assumption the task can be framed as pre- dicting boundaries at points which maximize the probability of a text being generated by a given language model. Their method is based on lexical cohesion -expressed in this context as topic seg- ments having compact and consistent lexical dis- tributions -and implements this within a prob- abilistic framework by modelling words within each segment as draws from a multinomial lan- guage model associated with that segment.</p><p>Much other subsequent work either uses this as a baseline, or extends it in some way: <ref type="bibr" target="#b22">Jeong and Titov (2010)</ref>, for example, who propose a model for joint discourse segmentation and alignment for documents with parallel structures, such as a text with commentaries or presenting alternative views on the same topic; or <ref type="bibr" target="#b11">Du et al. (2013)</ref>, who use hierarchical topic structure to improve the linear segmentation.</p><p>Bible Authorship <ref type="bibr" target="#b24">Koppel et al. (2011)</ref> consider the task of decomposing a document into its au- thorial components based on their stylistic proper- ties and propose an unsupervised method for do- ing so. The authors use as their data two bibli- cal books, Jeremiah and Ezekiel, that are gener- ally believed to be single-authored: their task was to segment a single artificial text constructed by interleaving chapters of the two books. Their most successful method used work in biblical scholar- ship on lexical choice: they give as an example the case that in Hebrew there are seven synonyms for the word fear, and that different authors may choose consistently from among them. Then, hav- ing constructed their own synsets using available biblical resources and annotations, they represent texts by vectors of synonyms and apply a modified cosine similarity measure to compare and cluster these vectors. While the general task is relevant to this paper, the particular notion of synonymy here means the approach is specific to this problem, al- though their approach is extended to other kinds of text in <ref type="bibr" target="#b0">Akiva and Koppel (2013)</ref>. <ref type="bibr" target="#b1">Aldebei et al. (2015)</ref> proposed a new approach motivated by this work, similarly clustering sentences, then using a Naive Bayes classifier with modified prior proba- bilities to classify sentences. <ref type="bibr" target="#b7">Brooke et al. (2012)</ref> perform stylistic segmentation of a well-known poem, The Waste Land by T.S. Eliot. This poem is renowned for the great number of voices that ap- pear throughout the text and has been the subject of much literary analysis <ref type="bibr" target="#b2">(Bedient and Eliot, 1986;</ref><ref type="bibr" target="#b10">Cooper, 1987)</ref>. These distinct voices, conceived of as representing different characters, have differing tones, lexis and grammatical styles (e.g. reflecting the level of formality). The transitions between the voices are not explicitly marked in the poem and the task here is to predict the breaks where these voice changes occur. The authors argue that the use of generative models is not feasible for this task, noting: "Generative models, which use a bag-of-words assumption, have a very different problem: in their standard form, they can capture only lexical cohesion, which is not the (primary) focus of stylistic analysis."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Poetry Voice Detection</head><p>They instead present a method based on a curve that captures stylistic change, similar to the Text- Tiling approach but generalised to use a range of features. The local maxima in this change curve represent potential breaks in the text. The features are both internal to the poem (e.g. word length, syllable count, POS tag) as well as external (e.g. average unigram counts in the 1T Corpus or senti- ment polarity from a lexicon). Results on an artifi- cially constructed mixed-style poem achieve a P k of 0.25. <ref type="bibr" target="#b8">Brooke et al. (2013)</ref> extend this by consid- ering clustering following an initial segmentation.</p><p>Native Language Identification (NLI) NLI casts the detecting of native language (L1) influ- ence in writing in a non-native (L2) language as a classification task: the framing of the task in this way comes from <ref type="bibr" target="#b25">Koppel et al. (2005)</ref>. There has been much activity on it in the last few years, with <ref type="bibr" target="#b41">Tetreault et al. (2012)</ref> providing a comprehensive analysis of features that had been used up until that point, and a shared task in ) that attracted 29 entrants. The shared task introduced a new, now-standard dataset, TOEFL11, and work has continued on improving classifica- tion results, e.g. by <ref type="bibr" target="#b9">Bykh and Meurers (2014)</ref> and <ref type="bibr" target="#b20">Ionescu et al. (2014)</ref>.</p><p>In addition to work on the classification task itself, there have also been investigations of the features used, and how they might be employed elsewhere. <ref type="bibr" target="#b32">Malmasi and Cahill (2015)</ref> examine the effectiveness of individual feature types used in the shared task and the diversity of those fea- tures. Of relevance to the present paper, sim- ple part-of-speech n-grams alone are fairly ef- fective, with classification accuracies of between about 40% and 65%; higher-order n-grams are more effective than lower, and the more fine- grained CLAWS2 tagset more effective than the Penn Treebank tagset. An area for application of these features is in Second Language Acquisi- tion (SLA), as a data-driven approach to finding L1-related characteristics that might be a result of cross-linguistic influence and consequently a pos- sible starting for an SLA hypothesis <ref type="bibr">(Ellis, 2008)</ref>; <ref type="bibr" target="#b39">Swanson and Charniak (2013)</ref> and <ref type="bibr" target="#b33">Malmasi and Dras (2014)</ref> propose methods for this.</p><p>Tying It Together Contra <ref type="bibr" target="#b7">Brooke et al. (2012)</ref>, we show that it is possible to develop effective generative models for segmentation on stylistic factors, of the sort used for topic segmentation. To apply it specifically to segmentation based on a writer's L1, we draw on work in NLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>We investigate the task of L1-based segmentation in three stages: 1. Can we define any models that do better than random, in a best case scenario? For this best case scenario, we determine results over a devset with the best prior found by a grid search, for a single language pair likely to be relatively easily distin- guishable. Note that as this is unsupervised seg- mentation, it is a devset in the sense that it is used to find the best prior, and also in a sense that some models as described in §4 use information from a related NLI task on the underlying data. 2. If the above is true, do the results also hold for test data, using priors derived from the devset? 3. Further, do the results also hold for all language pairs available in our dataset, not just a single eas- ily distinguishable pair?</p><p>We first describe the evaluation data -artifi- cial texts generated from learner essays, similar to the artificially constructed texts of previously de- scribed work on Bible authorship and poetry seg- mentation -and evaluation metrics, followed in §4 by the definitions of our Bayesian models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Source Data</head><p>We use as the source of data the TOEFL11 dataset used for the NLI shared task ( ) noted in §2. The data consists of 12100 essays by writers with 11 different L1s, taken from TOEFL tests where the test-taker is given a prompt 2 as the topic for the essay. The corpus is balanced across L1s and prompts (which allows us to verify that segmentation isn't occurring on the basis of topic), and is split into standard training, dev and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Generation</head><p>As the main task is to segment texts by the author's L1, we want to ensure that we are not segment- ing by topic and thus use texts written by authors from different L1 backgrounds on the same topic (prompt). We will also create one dataset to verify that segmentation by topic works in this domain; for this we use texts written by authors from the same L1 background on different topics. For our L1-varying datasets, we construct com- posite documents to be segmented as alternat-ing segments drawn from TOEFL11 from two dif- ferent L1s holding the topic (prompt) constant, broadly following a standard approach ( <ref type="bibr" target="#b7">Brooke et al., 2012</ref>, for example) (see Appendix A.1 for details). We follow the same process for our topic- varying datasets, but hold the L1 constant while alternating the topic (prompt). For our single pair of L1s, we choose German and Italian. German is the class with the highest NLI accuracy in the TOEFL11 corpus across the shared task results and Italian also performs very well. Additionally, there is very little confusion between the two; a bi- nary NLI classifier we trained on the language pair achieved 97% accuracy. For our all-pairs results, given the 11 languages in the TOEFL11 corpus, we have 55 sets of documents of alternating L1s (one of which is German-Italian).</p><p>We generate four distinct types of datasets for our experiments using the above methodology. The documents in these datasets, as described be- low, differ in the parameters used to select the es- says for each segment and what type of tokens are used. Tokens (words) can be represented in their original form and used for performing segmenta- tion. Alternatively, using an insight from Wong et al. <ref type="formula" target="#formula_0">(2012)</ref>, we can represent the documents at a level other than lexical: the text could consist of the POS tags corresponding to all of the tokens, or n-grams over those POS tags. The POS repre- sentation is motivated by the usefulness of POS- based features for capturing L1-based stylistic dif- ferences as noted in §2. Our method for encoding n-grams is described in Appendix A.2.</p><p>TOPICSEG-TOKENS This data is generated by keeping the L1 class constant and alternating seg- ments between two topics. We chose Italian for the L1 class and essays from the prompts "P7" and "P8" are used. The dataset, constructed from TOEFL11-TRAIN and TOEFL11-DEV, contains a total of 53 artificial documents, and will be used to verify that topic segmentation as discussed in <ref type="bibr" target="#b13">Eisenstein and Barzilay (2008)</ref> functions as ex- pected for data from this domain: that is, that topic change is detectable.</p><p>TOPICSEG-PTB Here the tokens in each text are replaced with their POS tags or n-grams over those tags, and the segmentation is performed over this data. In this dataset the tags are obtained via the Stanford Tagger and use the Penn Treebank (PTB) tagset. The same source data (TOEFL11- TRAIN and TOEFL11-DEV), L1 and topics as TOPICSEG-TOKENS are used for a total of 53 documents. This dataset will be used to investi- gate, inter alia, whether segmentation over these stylistically related features could take advantage of topic cues. We would expect not.</p><p>L1SEG-PTB This dataset is used for segmenta- tion based on native language, also using (n-grams over) the PTB POS tags. We choose a specific topic and then retrieve all essays from the corpus that match this; here we chose prompt "P7", since it had the largest number of essays for our cho- sen single L1 pair, German-Italian. For the dataset constructed from TOEFL11-TRAIN and TOEFL11- DEV (which we will refer to as L1SEG-PTB-GI- DEV), this resulted in 57 documents. Documents that are composites of two L1s are then generated as described above. For investigating questions 2 and 3 above, we similarly have datasets con- structed from the the smaller TOEFL11-TEST data (L1SEG-PTB-GI-TEST), which consist of 5 doc- uments of 5 segments each for the single L1 pair, and from all language pairs (L1SEG-PTB-ALL- DEV, L1SEG-PTB-ALL-TEST). We would ex- pect that these datasets should not be segmentable by topic, as all the segments are on the same topic; the segments should however, differ in stylistic characteristics related to the L1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1SEG-CLAWS2</head><p>This dataset is generated us- ing the same methodology as L1SEG-PTB, with the exception that the essays are tagged using the RASP tagger which uses the more fine-grained CLAWS2 tagset, noting that the CLAWS2 tagset performed better in the NLI classification task (Malmasi and Cahill, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>We use the standard P k ( <ref type="bibr" target="#b3">Beeferman et al., 1999</ref>) and WindowDiff (WD) <ref type="bibr" target="#b38">(Pevzner and Hearst, 2002</ref>) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations. P k and WD scores range between 0 and 1, with a lower score indicating better per- formance, and 0 a perfect segmentation. It has been noted that some "degenerate" algorithms - such as placing boundaries randomly or at every possible position -can score 0. <ref type="bibr">5 (Pevzner and Hearst, 2002</ref>). WD scores are typically similar to P k , correcting for differential penalties between false positive boundaries and false negatives im- plicit in P k . P k and WD scores reported in §5 are averages across all documents in a dataset. Formal definitions are given in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Segmentation Models</head><p>For all of our segmentation we use as a starting point the unsupervised Bayesian method of Eisen- stein and <ref type="bibr" target="#b13">Barzilay (2008)</ref>; see §2. <ref type="bibr">3</ref> We recap the important technical definitions here.</p><p>In Equation 1 of their work they define the ob- servation likelihood as,</p><formula xml:id="formula_0">p(X | z, Θ) = T t p(xt | θz t ),<label>(1)</label></formula><p>where X is the set of all T sentences, z is the vector of segment assignments for each sentence, x t is the bag of words drawn from the language model and Θ is the set of all K language models</p><formula xml:id="formula_1">Θ 1 . . . Θ K .</formula><p>As is standard in segmentation work, K is assumed to be fixed and known <ref type="bibr" target="#b31">(Malioutov and Barzilay, 2006</ref>); it is set to the actual num- ber of segments. The authors also impose an ad- ditional constraint, that z t must be equal to either z t−1 (the previous sentence's segment) or z t−1 + 1 (the next segment), in order to ensure a linear seg- mentation. This segmentation model has two parameters: the set of language models Θ and the segment as- signment indexes z. The authors note that since this task is only concerned with the segment as- signments, searching in the space of language models is not desirable. They offer two alterna- tives to overcome this: (1) taking point estimates of the language models, which is considered to be theoretically unsatisfying and (2) marginalizing them out, which yields better performance. Equa- tion 7 of <ref type="bibr" target="#b13">Eisenstein and Barzilay (2008)</ref>, repro- duced here, shows how they marginalize over the language models, supposing that each language model is drawn from a symmetric Dirichlet prior (i.e. θ j ∼ Dir(θ 0 )):</p><formula xml:id="formula_2">p(X | z, θ0) = K j p dcm ({xt : zt = j} | θ0) (2)</formula><p>The Dirichlet compound multinomial distribu- tion p dcm expresses the expectation over all the multinomial language models, when conditioned on the symmetric Dirichlet prior θ 0 :</p><formula xml:id="formula_3">p dcm ({xt : zt = j} | θ0) = Γ(W θ0) Γ(Nj + W θo) W i Γ(nj,i + θ0) Γ(θ0)<label>(3)</label></formula><p>where W is the number of words in the vocabulary and N j = W i n j,i , the total number of words in the segment j. They then observe that the optimal segmentation maximizes the joint probability</p><formula xml:id="formula_4">p(X, z | θ 0 ) = p(X | z, θ 0 )p(z)</formula><p>and assume a uniform p(z) over valid segmen- tations with no probability mass assigned to in- valid segmentations. The hyperparameter θ 0 can be chosen, or can be learned via an Expectation- Maximization process.</p><p>Inference Eisenstein and Barzilay (2008) de- fined two methods of inference, a dynamic pro- gramming (DP) one and one using Metropolis- Hastings (MH). Only MH is applicable where shifting a boundary will affect the probability of every segment, not just adjacent segments, as in their model incorporating cue phrases. Where this is not the case, they use DP inference. Their DP inference algorithm is suitable for all of our mod- els, so we also use that.</p><p>Priors For our priors, we carry out a grid search on the devsets (that is, the datasets derived from TOEFL11-TRAIN and TOEFL11-DEV) in the in- terval [0.1, 3.0], partitioned into 30 evenly spaced values; this includes both weak and strong priors. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TOPICSEG</head><p>Our first model is exactly the one proposed by Eisenstein and Barzilay (2008) described above. The aim here is to look at how we perform at seg- menting learner essays by topic in order to con- firm that topic segmentation works for this domain and these types of topics. We apply this model to the TOPICSEG-TOKENS and TOPICSEG-PTB datasets where the texts have the same L1 and boundaries are placed between essays of differing topics (prompts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">L1SEG</head><p>Our second model modifies that of <ref type="bibr" target="#b13">Eisenstein and Barzilay (2008)</ref> by revising the generative story.</p><p>Where they assume a standard generative model over words with constraints on topic change be- tween sentences, we make minor modifications to adapt the model for our task. The standard gen- erative story <ref type="bibr" target="#b6">(Blei, 2012)</ref> -an account of how a model generates the observed data -usually gen- erates words in a two-stage process:</p><note type="other">(1) For each document, randomly choose a distribution of top- ics. (2) For each word in the document: (a) Assign a topic from those chosen in step 1. (b) Randomly choose a word from that topic's vocabulary.</note><p>Here we modify this story to be over part-of- speech data instead of lexical items. By using this representation (which as noted in §2 is useful for NLI classification) we aim to segment our texts based on the L1 of the author for each segment. For this model we only make use of the L1SEG- PTB-GI-DEV dataset. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">L1SEG-COMP</head><p>It is not obvious that the same properties that produce compact distributions in standard lexical chains would also be the case for POS data, par- ticularly if extended to POS n-grams which can result in a very large number of potential tokens. In this regard Eisenstein and Barzilay (2008) note: "To obtain a high likelihood, the language mod- els associated with each segment should concen- trate their probability mass on a compact subset of words. Language models that spread their proba- bility mass over a broad set of words will induce a lower likelihood. This is consistent with the prin- ciple of lexical cohesion." <ref type="bibr" target="#b13">Eisenstein and Barzilay (2008)</ref> discuss this within the context of topic segmentation. <ref type="bibr">6</ref> How- ever, it is unclear if this would also would happen for POS tags; there is no syntactic analogue for the sort of lexical chains important in topic segmenta- tion. It may then turn out that using all POS tags or n-grams over them as in the previous model would not achieve a strong performance. We thus use knowledge from the NLI classification task to help.</p><p>Discarding Non-Discriminative Features One approach that could possibly overcome these lim- <ref type="bibr">5</ref> We also looked at including words. The results of these models were always worse, and we do not discuss them in this paper. <ref type="bibr">6</ref> For example, a topic segment related to the previously mentioned essay prompt P7 might concentrate its proba- bility mass on the following set of words: {education, learning, understanding, fact, theory, idea, concept, knowledge}.</p><p>itations is the removal of features from the in- put space that have been found to be non- discriminative in NLI classification. This would allow us to encode POS sequence information via n-grams while also keeping the model's vocabu- lary sufficiently small. Doing this requires the use of extrinsic information for filtering the n-grams. The use of such extrinsic information has proven to be useful for other similar tasks such as the po- etry style change segmentation work of <ref type="bibr" target="#b7">Brooke et al. (2012)</ref>, as noted in §2.</p><p>We perform this filtering using the discrimina- tive feature lists derived from the NLI classifica- tion task using the system and method described in <ref type="bibr" target="#b33">Malmasi and Dras (2014)</ref>, also noted in §2. We extract the top 300 most discriminative POS n- gram features for each L1 from TOEFL11-TRAIN and TOEFL11-DEV, resulting in two lists of 600 POS bigrams and trigrams; these are thus inde- pendent of our test datasets. (We illustrate a text with respect to these discriminative features in Ap- pendix A.4.) Note that discriminative n-grams can overlap with each other within the same class and also between two classes. We resolve such con- flicts by using the weights of the features from the classification task as described in <ref type="bibr" target="#b33">Malmasi and Dras (2014)</ref> and choosing the feature with the higher weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">L1SEG-ASYMP</head><p>Looking at the distribution of discriminative fea- tures in our documents, one idea is that incorpo- rating knowledge about which features are asso- ciated with which L1 could potentially help im- prove the results. One approach to do this is the use of asymmetric priors. We note that features as- sociated with an L1 often dominate in a segment. Accordingly, priors can represent evidence exter- nal to the data that some some aspect should be weighted more strongly: for us, this is evidence from the NLI classification task. The segmenta- tion models discussed so far only make use of a symmetric prior but later work mentions that it would be possible to modify this to use an asym- metric prior <ref type="bibr" target="#b12">(Eisenstein, 2009)</ref>.</p><p>Given that priors are effective for incorporat- ing external information, recent work has high- lighted the importance of optimizing over such priors, and in particular, the use of asymmetric pri- ors. Key work on this is by <ref type="bibr" target="#b43">Wallach et al. (2009)</ref> on LDA, who report that "an asymmetric Dirich- let prior over the document-topic distributions has substantial advantages over a symmetric prior", with prior values being determined through hyper- parameter optimization. Such methods have since been applied in other tasks such as sentiment anal- ysis ( <ref type="bibr" target="#b28">Lin and He, 2009;</ref><ref type="bibr" target="#b30">Lin et al., 2012</ref>) to achieve substantial improvements. For sentiment analysis, <ref type="bibr" target="#b28">Lin and He (2009)</ref> incorporate external informa- tion from a subjectivity lexicon. In applying LDA, instead of using a uniform Dirichlet prior for the document-sentiment distribution, they use asym- metric priors for positive and negative sentiment, determined empirically.</p><p>For our task, we assign a prior to each of two languages in a document, one corresponding to L1 a and the other to L1 b . Given this, we can assume that segments will alternate between L1 a and L1 b . And instead of a single θ 0 , we have two asymmetric priors that we call θ a , θ b correspond- ing to L1 a and L1 b respectively. This will require reworking the definition of p dcm in Equation 3. First adapting Equation 2,</p><formula xml:id="formula_5">p(X | z, θa, θ b ) = {jo} p dcm ({xt : zt = jo} | θa) · {je} p dcm ({xt : zt = je} | θ b ), (4)</formula><p>with {j o } = {j | j mod 2 = 1, 1 ≤ j ≤ K} the set of indices over odd segments and {j e } = {j | j mod 2 = 0, 1 ≤ j ≤ K} the set over evens. K is the (usual) total number of segments. Then</p><formula xml:id="formula_6">p dcm ({xt : zt = jo} | θa) = Γ( W k θa[k]) Γ(Nj o + W k θa[k]) W i Γ(nj,i + θa[i]) Γ(θa[i])<label>(5)</label></formula><p>W is now more generally the number of items in our vocabulary (whether words or POS n-grams). A notational addition here is θ a <ref type="bibr">[k]</ref> which refers to the L1 a prior for the kth word or POS n-gram. There is an analogous p dcm for θ b .</p><p>The next issue is how to construct the θ a and θ b . The simplest scenario would require a single constant value for all elements in one L1 and an- other for all elements in the other L1. Specifically, using discrim(L1 x ) to denote "the ranked list of discriminative n-grams for L1 x ", we define</p><formula xml:id="formula_7">θ a [i] = c 1 if θ a [i] ∈ discrim(L1 a ) c 2 if θ a [i] ∈ discrim(L1 b )</formula><p>and analogously for θ b <ref type="bibr">[i]</ref>. We would expect that c 1 &gt; c 2 (i.e. the prior is stronger for elements that come from the appropriate ranked list of discrimi- native features), but these values will be learned. In principle we would calculate versions of p(X | z, θ a , θ b ) twice: once where we assign θ a to segment 1, and the second time where we assign θ b . We'd then compare the two p(X | z, θ a , θ b ), and see which one fits better. In this work, how- ever, we will fix the initial L1: segment 1 corre- sponds to L1 a and consequently has prior θ a . <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Segmenting by Topic</head><p>We begin by testing the TOPICSEG model to en- sure that the Bayesian segmentation methodol- ogy can achieve reasonable results for segment- ing learner essays by topic. The results on the TOPICSEG-TOKENS dataset <ref type="table" target="#tab_0">(Table 1)</ref> show that content words are very effective at segmenting the writings by topic, achieving P k values in the range 0.19-0.21. These values are similar to those reported for segmenting Wall Street Journal text ( <ref type="bibr" target="#b3">Beeferman et al., 1999</ref>). On the other hand, us- ing the PTB POS tag version of the data in the TOPICSEG-PTB dataset results in very poor seg- mentation results, with P k values around 0.45. This is essentially the same as the performance of degenerate algorithms (noted in §3.3) of 0.5. This demonstrates that, as expected, POS unigrams do not provide enough information for topic segmen- tation; it is not possible to construct even an ap- proximation to lexical chains using them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">L1-based Segmentation</head><p>Having verified that the Bayesian segmentation approach is effective for topic segmentation on this data, we now turn to the L1SEG model for segmenting by the native language.</p><p>From the results in <ref type="table" target="#tab_0">Table 1</ref> we see very poor performance with a P k value of 0.466 for segment- ing the texts in L1SEG-PTB-GI-DEV using the unigrams as is. This was a somewhat unexpected result given than we know POS unigram distribu- tions are able to capture differences between L1- groups ( <ref type="bibr" target="#b32">Malmasi and Cahill, 2015)</ref>, albeit with limited accuracy. Moreover, neither bigram nor trigram encodings, which perform better at NLI, resulted in any improvement in our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Prior(s) P k WD TOPICSEG TOPICSEG-TOKENS 0.1 0.203 0.205 TOPICSEG TOPICSEG-PTB 0.8 0.444 0.480 L1SEG L1SEG-PTB-GI-DEV unigrams 0.1 0.466 0.489 L1SEG L1SEG-PTB-GI-DEV bigrams 0.8 0.466 0.487 L1SEG L1SEG-PTB-GI-DEV trigrams 0.8 0.480 0.489 L1SEG-COMP L1SEG-PTB-GI-DEV bigrams 0.1 0.476 0.490 L1SEG-COMP L1SEG-PTB-GI-DEV trigrams 0.4 0.393 0.398 L1SEG-COMP L1SEG-CLAWS2-GI-DEV bigrams 0.4 0.387 0.400 L1SEG-COMP L1SEG-CLAWS2-GI-DEV trigrams 0.4 0.370 0.373 L1SEG-ASYMP L1SEG-CLAWS2-GI-DEV trigrams (0.6,0.3) 0.316 0.318 <ref type="table" target="#tab_0">Table 1</ref>: Results on devsets for single L1 pair (German-Italian). <ref type="table">Table 2</ref>: Results on testset L1SEG-CLAWS2-GI- TEST for single L1 pair (German-Italian). Priors are the ones from the corresponding devsets in Ta- ble 1.  <ref type="table">Table 3</ref>: Results on dev and test datasets (upper: L1SEG-CLAWS2-ALL-DEV, lower: L1SEG- CLAWS2-ALL-TEST): means and standard de- viations (in parentheses) across datasets for all 55 L1 pairs.</p><formula xml:id="formula_8">Model P k WD L1SEG-COMP 0.358 0.360 L1SEG-ASYMP 0.266 0.271</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Incorporating Discriminative Features</head><p>Filtering the bigrams results in some minor im- provements over the best results from the L1SEG model. However, there are substantial improve- ments when using the filtered POS trigrams, with a P k value of 0.393. We did not test unigrams as they were the weakest NLI feature of the three.</p><p>This improvement is, we believe, because the Bayesian modelling of lexical cohesion over the input tokens requires that each segment concen- trates its probability mass on a compact subset of words. In the context of the n-gram tokenization method tested in the previous section, the L1SEG model with n-grams would most likely exacerbate the issue by substantially increasing the number of tokens in the language model: while the uni- grams do not capture enough information to distin- guish non-lexical shifts, the n-grams provide too many features.</p><p>We also see that using the CLAWS2 tagset out- performs the PTB tagset. The results achieved for bigrams are much higher, while the trigram results are also better, with P k = 0.370. NLI experiments using different POS tagsets have established that more fine-grained tagsets (i.e. those with more tag categories) provide greater classification accuracy when used as n-gram features for classification. <ref type="bibr">8</ref> Results here comport with the previous findings.</p><p>As one of the two best models, we run it on the held-out test data, using the best priors found from the grid search on the devset data <ref type="table">(Table 2)</ref>; we find the P k and WD values are comparable (and in fact slightly better), so the model still works if the filtering uses discriminative NLI features from the devset. Looking at results across all 55 L1 pairs <ref type="table">(Table 3)</ref>, we also see similar mean P k and WD values with only a small standard deviation, indi- cating the approach works just as well across all language pairs. Priors here are all also weak, in the range [0.1, 0.9].</p><p>In sum, the results here demonstrate the impor- tance of inducing a compact distribution, which we did here by reducing the vocabulary size by stripping non-informative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Applying Two Asymmetric Priors</head><p>Our final model, L1SEG-ASYMP, assesses whether setting different priors for each L1 can improve performance. Our grid search over two priors gives 900 possible prior combinations. These combinations also include cases where θ a and θ b are symmetric, which is equivalent to the L1SEG-COMP model. We observe (  <ref type="figure">Figure 1</ref>: Heatmap over asymmetric priors on L1SEG-CLAWS2-ALL-DEV the prior pair of (0.6, 0.3) achieves a P k value of 0.321, a substantial improvement over the previous best result of 0.370. Inspecting priors (see <ref type="figure">Figure 1</ref> for a heatmap over priors) shows that the best results are in the region of weak priors for both values, which is consistent with the emphasis on compactness since weak priors result in more compact models (noted by e.g. <ref type="bibr" target="#b44">Wang and Blei (2009)</ref>). Moreover, they are away from the diagonal, i.e. the L1SEG-COMP model will not produce the best results. A more fine-grained grid search, focusing on the range that provided the best results in the coarse search, can improve the results further still: over the interval [0.3, 0.9], partitioned into 60 evenly spaced values, finds a prior pair of (0.64, 0.32) that provides a slight improvement of the P k value to 0.316.</p><p>As with L1SEG-COMP, we also evaluate this on the same held-out test set ( <ref type="table">Table 2)</ref>. Applying the best asymmetric prior from the devset grid search, this improves to 0.266. Again, results across all 55 L1 pairs <ref type="table">(Table 3)</ref> show the same pattern, and much as for L1SEG-COMP, priors are all weak or neutral (range [0.1, 1.0]). These results thus demonstrate that setting an asymmetric prior gives the best performance on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Applying the approach to our two illustrative ap- plications of §1, patchwriting and literary analysis, would require development of relevant corpora. In both cases the distinction would be between native writing and writing that shows characteristics of a non-native speaker, rather than between two non- native L1s. There isn't yet a topic-balanced cor- pus like TOEFL11 which includes native speaker writing for evaluation, although we expect (given recent results on distinguishing native from non- native text in <ref type="bibr" target="#b34">Malmasi and Dras (2015)</ref>) that the techniques should carry over. For the literary anal- ysis, as well, to bridge the gap between work like <ref type="bibr" target="#b36">Morzinski (1994)</ref> and a computational applica- tion, it remains to be seen how precise an anno- tation is possible for this task. Additionally, the granularity of segmentation may need to be finer than sentence-level, as suggested by the examples in §1; this level of granularity hasn't previously been tackled in unsupervised segmentation.</p><p>In terms of possible developments for the mod- els presented for the task here, previous NLI work has shown that other, syntactic features can be useful for capturing L1-based differences. The incorporation of these features for this segmen- tation task could be a potentially fruitful avenue for future work. We have taken a fairly straight- forward approach which modifies the generative story. A more sophisticated approach would be to incorporate features into the unsupervised model. One such example is the work of <ref type="bibr" target="#b4">Berg-Kirkpatrick et al. (2010)</ref> which demonstrates that each com- ponent multinomial of a generative model can be turned into a miniature logistic regression model with the use of a modified EM algorithm. Their results showed that the feature-enhanced unsu- pervised models which incorporate linguistically- motivated features achieve substantial improve- ments for tasks such as POS induction and word segmentation. We note also that the models are potentially applicable to other stylistic segmenta- tion tasks beyond L1 influence.</p><p>As far as this initial work is concerned we have shown that, framed as a segmentation task, it is possible to identify units of text that differ stylisti- cally in their L1 influence. We demonstrated that it is possible to define a generative story and asso- ciated Bayesian models for stylistic segmentation, and further that segmentation results improve sub- stantially by compacting the n-gram distributions, achieved by incorporating knowledge about dis- criminative features extracted from NLI models. Our best results come from a model that uses al- ternating asymmetric priors for each L1, with the priors selected using a grid search and then evalu- ated on a held-out test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank John Pate for very helpful dis- cussions in the early stages of the paper, and the three anonymous referees for useful suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details on Dataset Generation and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Document Generation</head><p>For our L1-varying datasets, we construct com- posite documents to be segmented as alternating segments drawn from TOEFL11 from two differ- ent L1s. Broadly following a standard approach ( <ref type="bibr" target="#b7">Brooke et al., 2012</ref>, for example), to generate such a document, we randomly draw TOEFL11 es- says -each of which constitutes a segment - from the appropriate L1s and concatenate them, alternating the L1 class after each segment. This is repeated until the maximum number of segments per document, s, is reached. We generate multi- ple composite documents until all TOEFL11 have been used. In this work we use datasets generated with s = 5. 9 We follow the same process for our topic-varying datasets, but hold the L1 con- stant while alternating the topic (prompt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Encoding n-gram information</head><p>Lau et al. (2013) investigated the importance of n-grams within topic models over lexical items. They note that in topic modelling each token re- ceives a topic label and that the words in a collo- cation -e.g. stock market, White House or health care -may receive different topic assignments despite forming a single semantic unit. They found that identifying collocations (via a t-test) and preprocessing the text to turn these into sin- gle tokens provides a notable improvement over a unigram bag of words. We implement a similar preprocessing step that converts each sentence within each document to a set of bigrams or trigrams using a sliding window, where each n-gram is represented by a single to- ken. So, for example, the trigram DT JJ NN be- comes a single token: DT-JJ-NN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation: Metric Definitions</head><p>Given two segmentations r (reference) and h (hy- pothesis) for a corpus of N sentences, 9 This is the average number of segments per chapter in the written text used by <ref type="bibr" target="#b13">Eisenstein and Barzilay (2008)</ref>. How- ever, we have also successfully replicated our results us- ing s = 7, 9. <ref type="table" target="#tab_0">1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21</ref> Figure 2: A visualization of sentences from a sin- gle segment. Each row represents a sentence and each token is represented by a square. Token tri- grams considered discriminative for either of our two L1 classes are shown in blue or red, with the rest being considered non-discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>□ □ □ □ □ □ □ ■ ■ ■ ■ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ ■ ■ ■ ■ □ ■ ■ ■ □ □ □ ■ ■ ■ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ ■ ■ ■ □ □ □ ■ ■ ■ □ □ □ □ □ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ □ □ □ □ ■ ■ ■ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ ■ ■ ■ □ ■ ■ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ ■ ■ ■ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ ■ ■ ■ □ ■ ■ ■ ■ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ ■ ■ ■ ■ □ ■ ■ ■ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ ■ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ ■ ■ ■</head><formula xml:id="formula_9">PD(r, h) = 1≤i≤j≤N D(i, j)(δr(i, j) ¯ ⊕ δ h (i, j))<label>(6)</label></formula><p>where δ r (i, j) is an indicator function specifying whether i and j lie in the same reference segment, δ h (i, j) similarly for a hypothesised segment, ¯ ⊕ is the XNOR function, and D is a distance probabil- ity distribution over the set of possible distances between sentences. For P k , this D is defined by a fixed window of size k which contains all the probability mass, and k is set to be half the aver- age reference segment length. The WD definition is:</p><formula xml:id="formula_10">W D(r, h) = 1 N − k N −k i=1</formula><p>(|b(ri, r i+k ) − b(hi, h i+k )| &gt; 0) <ref type="bibr">(7)</ref> where b(r i , r j ) represents the number of bound- aries between positions i and j in the reference text (similarly, the hypothesis text). <ref type="figure">Figure 2</ref> shows a visualization of the discrimi- native features of a single segment where each row represents a sentence and each token is rep- resented by a square. Tokens that are part of a trigram which is considered discriminative for ei- ther of our two L1 classes are shown in blue or red. Note that discriminative n-grams can over- lap with each other within the same class (e.g. on lines 1 and 2 where two overlapping trigrams form a group of four consecutive tokens) and also be- tween two classes (e.g. on lines 10 and 11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Visualisation of Discriminative Features</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 ) that</head><label>1</label><figDesc></figDesc><table>0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1.0 
1.1 
1.2 
1.3 
1.4 
1.5 
1.6 
1.7 
1.8 
1.9 
2.0 
2.1 
2.2 
2.3 
2.4 
2.5 
2.6 
2.7 
2.8 
2.9 
3.0 
L1 B 

0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1.0 
1.1 
1.2 
1.3 
1.4 
1.5 
1.6 
1.7 
1.8 
1.9 
2.0 
2.1 
2.2 
2.3 
2.4 
2.5 
2.6 
2.7 
2.8 
2.9 
3.0 

L1 A 

Prior Grid Search Results (Coarse) 

0.330 

0.345 

0.360 

0.375 

0.390 

0.405 

0.420 

0.435 

0.450 

</table></figure>

			<note place="foot" n="1"> http://citationproject.net/</note>

			<note place="foot" n="2"> For example, prompt P7 is: &quot;Do you agree or disagree with the following statement? It is more important for students to understand ideas and concepts than it is for them to learn facts. Use reasons and examples to support your answer.&quot;</note>

			<note place="foot" n="3"> An open-source implementation of the method, called BAYESSEG, is made available by the authors at http:// groups.csail.mit.edu/rbg/code/bayesseg/</note>

			<note place="foot" n="4"> The Eisenstein and Barzilay (2008) code does implement an EM method for finding priors in the symmetric case, but we found that perhaps surprisingly the grid search almost always found better ones.</note>

			<note place="foot" n="7"> This requires an extension of the BAYESSEG software to support asymmetric priors. We will make this extended version of the code available under the same conditions as BAYESSEG. Please contact the first or second author for this.</note>

			<note place="foot" n="8"> In §2 we noted the comparison of PTB and CLAWS2 tagsets in Malmasi and Cahill (2015); also, Gyawali et al. (2013) compared Penn Treebank and Universal POS tagsets and found that the more fine-grained PTB ones did better.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Generic Unsupervised Method for Decomposing Multi-Author Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navot</forename><surname>Akiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology (JASIST)</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2256" to="2264" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised decomposition of a multi-author document based on naive-bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Aldebei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="501" to="505" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">He Do the Police in Different Voices: The Waste Land and its protagonist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Bedient</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Stearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliot</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical Models for Text Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beeferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="doi">10.1023/A:1007506220214</idno>
		<ptr target="https://doi.org/10.1023/A:1007506220214" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N10-1083" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Derrick Higgins, Aoife Cahill, and Martin Chodorow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Testing Service</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>TOEFL11: A Corpus of Non-Native English</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Stylistic Segmentation of Poetry with Change Curves and Extrinsic Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-2504" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACLHLT 2012 Workshop on Computational Linguistics for Literature. Association for Computational Linguistics</title>
		<meeting>the NAACLHLT 2012 Workshop on Computational Linguistics for Literature. Association for Computational Linguistics<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clustering voices in the waste land</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Hammond</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-1406" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Linguistics for Literature. Association for Computational Linguistics</title>
		<meeting>the Workshop on Computational Linguistics for Literature. Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring Syntactic Features for Native Language Identification: A Variationist Perspective on Feature Encoding and Ensemble Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhiy</forename><surname>Bykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1962" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TS Eliot and the politics of voice: The argument of The Waste Land</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Xiros</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cooper</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>UMI Research Press</publisher>
			<biblScope unit="volume">79</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topic segmentation with a structured topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical text segmentation from multi-scale lexical cohesion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<ptr target="www.aclweb.org/anthology/N09-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL)<address><addrLine>Boulder, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="353" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian unsupervised topic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D08-1035" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Study of Second Language Acquisition</title>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
	<note>Rod Ellis. 2nd edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weeds in the flower garden: An exploration of plagiarism in graduate students research proposals and its connection to enculturation, ESL, and contextual factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denise</forename><surname>Strickland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Briana</forename><surname>Timmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Feldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Educational Integrity</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="28" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Native Language Identification: a Simple n-gram Based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Gyawali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-1729" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="224" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cohesion in English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A K</forename><surname>Halliday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruqaiya</forename><surname>Hasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>Longman Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-paragraph segmentation of expository text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<idno type="doi">10.3115/981732.981734</idno>
		<ptr target="https://doi.org/10.3115/981732.981734" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 32nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Las Cruces, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Texttiling: Segmenting text into multi-paragraph subtopic passages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Lingustics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="64" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Can characters reveal your native language? A language-independent approach to native language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cahill</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1363" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sentence-Mining: Uncovering the Amount Of Reading and Reading Comprehension In College Writers&apos; Researched Writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">Moore</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New Digital Scholar: Exploring and Enriching the Research and Writing Practices of NextGen Students</title>
		<editor>Randall McClure and James P. Purdy</editor>
		<meeting><address><addrLine>Medford, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="111" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised discourse segmentation of documents with inherently parallel structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="151" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The use of paraphrase in summary writing: A comparison of L1 and L2 writers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Keck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Second Language Writing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="278" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised decomposition of a document into authorial components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navot</forename><surname>Akiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Dershowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachum</forename><surname>Dershowitz</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1356" to="1364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatically determining an anonymous author&apos;s native language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Zigdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligence and Security Informatics</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3495</biblScope>
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On Collocations and Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>TSLP)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two first-year students strategies for writing from sources: Patchwriting or plagiarism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Pearson Casanave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Second Language Writing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="165" to="180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="doi">10.1145/1645953.1646003</idno>
		<ptr target="http://doi.acm.org/10.1145/1645953.1646003" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly supervised joint sentimenttopic detection from text. Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Everson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1134" to="1145" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimum cut model for spoken lecture segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Malioutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="doi">10.3115/1220175.1220179</idno>
		<ptr target="https://doi.org/10.3115/1220175.1220179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Measuring feature diversity in native language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W15-0606" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language Transfer Hypotheses with Linear SVM Weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D14-1144" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1385" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilingual Native Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<idno type="doi">10.1017/S1351324915000406</idno>
		<ptr target="https://doi.org/10.1017/S1351324915000406" />
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lexical cohesion computed by thesaural relations as an indicator of the structure of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Morzinski</surname></persName>
		</author>
		<title level="m">The Linguistic influence of Polish on Joseph Conrad&apos;s style</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Columbia University Press</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Good and original: Plagiarism and patchwriting in academic second-language writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Pecorari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Second Language Writing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="345" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A critique and improvement of an evaluation metric for text segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Pevzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<idno type="doi">10.1162/089120102317341756</idno>
		<ptr target="https://doi.org/10.1162/089120102317341756" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extracting the Native Language Signal for Second Language Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A report on the first native language identification shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-1706" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Native tongues, lost and found: Resources and empirical evaluations in native language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C12-1158" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012. The COLING 2012 Organizing Committee</title>
		<meeting>COLING 2012. The COLING 2012 Organizing Committee<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2585" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Patterns in plagiarism and patchwriting in science and engineering graduate students&apos; research proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Vieyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denise</forename><surname>Strickland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Timmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal for Educational Integrity</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="49" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking lda: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Bengio, D. Schuurmans, J.D. Lafferty, C.K.I. Williams, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1982" to="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring Adaptor Grammars for Native Language Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sze-Meng Jojo</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<ptr target="http://www.aclweb.org/anthology/D12-1064" />
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="699" to="709" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
