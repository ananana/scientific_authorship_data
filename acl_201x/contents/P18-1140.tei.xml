<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Adaptive End-to-End Dialog Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Adaptive End-to-End Dialog Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1509" to="1519"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1509</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acous-tic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most of us have had frustrating experience and even expressed anger towards automated customer service systems. Unfortunately, none of the cur- rent commercial systems can detect user senti- ment and let alone act upon it. Researchers have included user sentiment in rule-based systems <ref type="bibr" target="#b0">(Acosta, 2009;</ref><ref type="bibr" target="#b22">Pittermann et al., 2010)</ref>, where there are strictly-written rules that guide the sys- tem to react to user sentiment. Because traditional modular-based systems are harder to train, to up- date with new data and to debug errors, end-to-end trainable systems are more popular. However, no work has tried to incorporate sentiment informa- tion in the end-to-end trainable systems so far to create sentiment-adaptive systems that are easy to train. The ultimate evaluators of dialog systems are users. Therefore, we believe dialog system research should strive for better user satisfaction. In this paper, we not only included user sentiment information as an additional context feature in an end-to-end supervised policy learning model, but also incorporated user sentiment information as an immediate reward in a reinforcement learning model. We believe that providing extra feedback from the user would guide the model to adapt to user behaviour and learn the optimal policy faster and better.</p><p>There are three contributions in this work: 1) an audio dataset 1 with sentiment annotation (the an- notators were given the complete dialog history); 2) an automatic sentiment detector that consid- ers conversation history by using dialogic features, textual features and traditional acoustic features; and 3) end-to-end trainable dialog policies adap- tive to user sentiment in both supervised and rein- forcement learning settings. We believe such dia- log systems with better user adaptation are benefi- cial in various domains, such as customer services, education, health care and entertainment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many studies in emotion recognition ( <ref type="bibr" target="#b24">Schuller et al., 2003;</ref><ref type="bibr" target="#b20">Nwe et al., 2003;</ref><ref type="bibr" target="#b2">Bertero et al., 2016)</ref> have used only acoustic features. But there has been work on emotion detection in spoken dialog systems incorporating extra information as well ( <ref type="bibr" target="#b11">Lee and Narayanan, 2005;</ref><ref type="bibr" target="#b5">Devillers et al., 2003;</ref><ref type="bibr" target="#b15">Liscombe et al., 2005;</ref><ref type="bibr" target="#b4">Burkhardt et al., 2009;</ref><ref type="bibr" target="#b35">Yu et al., 2017)</ref>. For example, <ref type="bibr" target="#b15">Liscombe et al. (2005)</ref> explored features like users' dialog act, lex- ical context and discourse context of the previous turns. Our approach considered accumulated di-alogic features, such as total number of interrup- tions, to predict user sentiment along with acoustic and textual features.</p><p>The traditional method to build dialog system is to train modules such as language understanding component, dialog manager and language gener- ator separately ( <ref type="bibr" target="#b12">Levin et al., 2000;</ref><ref type="bibr" target="#b33">Williams and Young, 2007;</ref><ref type="bibr" target="#b27">Singh et al., 2002)</ref>. Recently, more and more work combines all the modules in an end-to-end training framework <ref type="bibr" target="#b31">(Wen et al., 2016;</ref><ref type="bibr" target="#b14">Li et al., 2017;</ref><ref type="bibr" target="#b32">Williams et al., 2017;</ref><ref type="bibr" target="#b16">Liu and Lane, 2017a)</ref>. Specifically related to our work, <ref type="bibr" target="#b32">Williams et al. (2017)</ref> built a model, which combined the traditional rule-based system and the modern deep-learning-based sys- tem, with experts designing actions masks to regu- late the neural model. Action masks are bit vectors indicating allowed system actions at certain dialog state. The end-to-end framework made dialog sys- tem training simpler and model updating easier.</p><p>Reinforcement learning (RL) is also popular in dialog system building ( <ref type="bibr" target="#b37">Zhao and Eskenazi, 2016;</ref><ref type="bibr" target="#b17">Liu and Lane, 2017b;</ref>. A common practice is to simulate users. However, building a user simulator is not a trivial task. <ref type="bibr" target="#b37">Zhao and Eskenazi (2016)</ref> combines the strengths of reinforce- ment learning and supervised learning to acceler- ate the learning of a conversational game simula- tor.  provides a standard framework for building user simulators, which can be modi- fied and generalized to different domains. Liu and Lane (2017b) describes a more advanced way to build simulators for both the user and the agent, and train both sides jointly for better performance. We simulated user sentiment by sampling from real data and incorporated it as immediate rewards in RL, which is different from common practice of using task success as delayed rewards in RL train- ing.</p><p>Some previous module-based systems inte- grated user sentiment in dialog planning <ref type="bibr" target="#b0">(Acosta, 2009;</ref><ref type="bibr" target="#b1">Acosta and Ward, 2011;</ref><ref type="bibr" target="#b22">Pittermann et al., 2010)</ref>. They all integrated user sentiment in the dialog manager with manually defined rules to re- act to different user sentiment and showed that tracking sentiment is helpful in gaining rapport with users and creating interpersonal interaction in the dialog system. In this work, we include user sentiment into end-to-end dialog system training and make the dialog policy learn to choose dia- log actions to react to different user sentiments automatically. We achieve this through integrat- ing user sentiment into reinforcement reward de- sign. Many previous RL studies used delayed re- wards, mostly task success. However, delayed re- wards make the converging speed slow, so some studies integrated estimated per-turn immediate reward. For example, <ref type="bibr" target="#b10">Ferreira and Lefèvre (2013)</ref> explored expert-based reward shaping in dialog management and <ref type="bibr" target="#b30">Ultes et al. (2017)</ref> proposed In- teraction Quality (IQ), a less subjective variant of user satisfaction, as immediate reward in dialog training. However, both methods are not end-to- end trainable, and require manual input as prior, either in designing proper form of reward, or in annotating the IQ. Our approach is different as we detect the multimodal user sentiment on the fly and does not require manual input. Because sen- timent information comes directly from real users, our method will adapt to user sentiment as the di- alog evolves in real time. Another advantage of our model is that the sentiment scores come from a pre-trained sentiment detector, so no manual an- notation of rewards is required. Furthermore, the sentiment information is independent of the user's goal, so no prior domain knowledge is required, which makes our method generalizable and inde- pendent of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>We experimented our methods on DSTC1 dataset ( <ref type="bibr" target="#b23">Raux et al., 2005</ref>), which has a bus information search task. Although DSTC2 dataset is a more commonly-used dataset in evaluating dialog sys- tem performance, the audio recordings of DSTC2 are not publicly available and therefore, DSTC1 was chosen. There are a total of 914 dialogs in DSTC1 with both text and audio information. Statistics of this dataset are shown in <ref type="table" target="#tab_0">Table 1</ref>. We used the automatic speech recognition (ASR) as the user text inputs instead of the transcripts, be- cause the system's action decisions heavily de- pend on ASR. There are 212 system action tem- plates in this dataset. Four types of entities are involved, &lt;place&gt;, &lt;time&gt;, &lt;route&gt;, and &lt;neighborhood&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Annotation</head><p>We manually annotated 50 dialogs consisting of 517 conversation turns for user sentiment. Senti- ment is categorized into negative, neutral and positive. The annotator had access to the  entire dialog history in the annotation process be- cause the dialog context gives the annotators a holistic view of the interactions, and annotating user sentiment in a dialog without the context is re- ally difficult. Some previous studies have also per- formed similar user information annotation given context, such as <ref type="bibr" target="#b6">Devillers et al. (2002)</ref>. The an- notation scheme is described in <ref type="table" target="#tab_0">Table 10</ref> in Ap- pendix A.2. To address the concern that dialog quality may bias the sentiment annotation, we ex- plicitly asked the annotators to focus on users' be- haviour instead of the system, and hid all the de- tails of multimodal features from the annotators. Moreover, two annotators were calibrated on 37 audio files, and reached an inter-annotator agree- ment (kappa) of 0.74. The statistics of the anno- tation results are shown in <ref type="table" target="#tab_1">Table 2</ref>. The skew- ness in the dataset is due to the data's nature. In the annotation scheme, positive is defined as "excitement or other positive feelings", but peo- ple rarely express obvious excitement towards au- tomated task-oriented dialog systems. What we really want to distinguish is neutral and positive cases from negative cases so as to avoid the neg- ative sentiment, and the dataset is balanced for these two cases. To the best of our knowledge, our dataset is the first publicly available dataset that annotated user sentiment with respect to the en- tire dialog history. There are similar datasets with emotion annotations ( ) but are not labeled under dialog contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multimodal Sentiment Classification</head><p>To detect user sentiment, we extracted a set of acoustic, dialogic and textual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Acoustic features</head><p>We used openSMILE ) to ex- tract acoustic features. Specifically, we used the paralinguistics configuration from <ref type="bibr" target="#b24">Schuller et al. (2003)</ref>, which includes 1584 acoustic features, such as pitch, volume and jitter. In order to avoid possible overfitting caused by the large number of acoustic features, we performed tree-based feature selection <ref type="bibr" target="#b21">(Pedregosa et al., 2011</ref>) to reduce the size of acoustic features to 20. The selected features are listed in <ref type="table" target="#tab_0">Table 12</ref> in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dialogic features</head><p>Four categories of dialogic features are selected according to previous literature ( <ref type="bibr" target="#b15">Liscombe et al., 2005</ref>) and the statistics observed in the dataset. We used not only the per-turn statistics of these fea- tures, but also the accumulated statistics of them throughout the entire conversation so that the sen- timent classifier can also take the entire dialog context into consideration.</p><p>Interruption is defined as the user interrupting the system speech. Interruptions occurred fairly frequently in our dataset (4896 times out of 14860 user utterances). Button usage When the user is not satisfied with the ASR performance of the system, he/she would rather choose to press a button for "yes/no" questions, so the usage of buttons can be an indication of negative sentiment. During DSTC1 data collection, users were notified about the option to use buttons, so this kind of information is available in the data. Repetitions There are two kinds of repetitions: the user asks the system to repeat the previ- ous sentence, and the system keeps asking the same question due to failures to catch some important entity. In our model, we combined these two situations as one feature because very few user repetitions occur in our data (&lt;1%). But for other data, it might be helpful to separate them. Start over is active when the user chooses to restart the task in the middile of the conversa- tion. The system is designed to give the user an option to start over after several turns. If the user takes this offer, he/she might have negative sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Textual features</head><p>We also noticed that the semantic content of the utterance was relevant to sentiment. So we used the entire dataset as a corpus and created a tf-idf vector for each utterance as textual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Classification results</head><p>The sentiment classifier was trained on the 50 di- alogs annotated with sentiment labels. The pre- dictions made by this classifier were used for the supervised learning and reinforcement learning in the later sections. We used random forest as our classifier (an implementation from scikit-learn (Pedregosa et al., 2011)), as we had limited anno- tated data. We separated the data to be 60% for training, 20% for validation and 20% for testing. Due to the randomness in the experiments, we ran all the experiments 20 times and reported the aver- age results of different models in <ref type="table" target="#tab_3">Table 4</ref>. We also conducted unpaired one-tailed t-test to assess the statistical significance.</p><p>We extracted 20 acoustic features, eight dia- logic features and 164 textual features. From Ta- ble 4, we see that the model combining all the three categories of features performed the best (0.686 in F-1, p &lt; 1e6 compared to acoustic baseline). One interesting observation is that by only using eight dialogic features, the model al- ready achieved 0.596 in F-1. Another interesting observation is that using 164 textual features alone reached a comparable performance (0.664), but the combination of acoustic and textual features actually brought down the performance to 0.647. One possible reason is that the acoustic informa- tion has noise that confused the textual informa- tion when combined. But this observation doesn't necessarily apply to other datasets. The signifi- cance tests show that adding dialogic features im- proved the baseline significantly. For example, the model with both acoustic features and dialogic features are significantly better than the one with only acoustic features (p &lt; 1e6). In <ref type="table" target="#tab_2">Table 3</ref>, we listed the dialogic features with their relative importance rank, which were obtained from rank- ing their feature importance scores in the classifier. We observe that "total interruptions so far" is the most useful dialogic features to predict user senti- ment. The sentiment detector trained will be inte- grated in the end-to-end learning described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogic Features</head><p>Relative Rank of importance total interruptions so far 1 interruptions 2 total button usages so far 3 total repetitions so far 4 repetition 5 button usage 6 total start over so far 7 start over 8  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supervised Learning (SL)</head><p>We incorporated the detected user sentiment from the previous section into a supervised learning framework for training end-to-end dialog systems. There are many studies on building a dialog sys- tem in a supervised learning setting (Bordes and Weston <ref type="formula">(</ref>  <ref type="formula">(2017)</ref>). Following these approaches, we treated the problem of dialog pol- icy learning as a classification problem, which is to select actions among system action templates given conversation history. Specifically, we de- cided to adopt the framework of Hybrid Code Net- work (HCN) introduced in <ref type="bibr" target="#b32">Williams et al. (2017)</ref>, because it is the current state-of-the-art model. We reimplemented HCN and used it as the baseline system, given the absence of direct comparison on DSTC1 data. One caveat is that HCN used ac- tion masks (bit vectors indicating allowed actions at certain dialog states) to prevent impossible sys- tem actions, but we didn't use hand-crafted ac- tion masks in the supervised learning setting be- cause manually designing action masks for 212 ac- tion templates is very labor-intensive. This makes our method more general and adaptive to differ- ent tasks. All the dialog modules were trained together instead of separately. Therefore, our method is end-to-end trainable and doesn't require human expert involvement. We listed all the context features used in <ref type="bibr" target="#b32">Williams et al. (2017)</ref> in <ref type="table" target="#tab_0">Table 11</ref> in <ref type="bibr">Appendix A.3</ref>. In our model, we added one more set of con- text features, the user-sentiment-related features. For entity extraction, given that the entity values in our dataset form a simple unique fixed set, we used simple string matching. We conducted three experiments: the first one used entity presences as context features, which serves as the baseline; the second one used entity presences in addition to all the raw dialogic features mentioned in <ref type="table" target="#tab_2">Ta- ble 3</ref>; the third experiment used the baseline fea- tures plus the predicted sentiment label by the pre- built sentiment detector (converted to one-hot vec- tor) instead of the raw dialogic features. We used the entire DSTC1 dataset to train the supervised model. The input is the normalized natural lan- guage and the contexutal features, and the out- put is the action template id. We kept the same experiment setting in <ref type="bibr" target="#b32">Williams et al. (2017)</ref>, e.g. last action taken was also used as a fea- ture, along with word embeddings ( <ref type="bibr" target="#b18">Mikolov et al., 2013)</ref> and bag-of-words; LSTM with 128 hidden- units and AdaDelta optimizer <ref type="bibr" target="#b36">(Zeiler, 2012)</ref> were used to train the model.</p><p>The results of different models are shown in Ta- ble 5. We observe that using the eight raw dia- logic features did not improve turn-level F-1 score. One possible reason is that a total of eight dialogic features were added to the model, and some of them might contain noises and therefore caused the model to overfit. However, using predicted sentiment information as an extra feature, which is a more condensed information, outperformed the other models both in terms of turn-level F-1 score and dialog accuracy which indicates if all turns in a dialog are correct. The difference in absolute F- 1 score is small because we have a relatively large test set (5876 turns). But the unpaired one-tailed t-test shows that p &lt; 0.01 for both the F-1 and the dialog accuracy. This suggests that including user sentiment information in action planning is helpful in a supervised learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Reinforcement Learning (RL)</head><p>In the previous section, we discussed including sentiment features directly as a context feature in a supervised learning model for end-to-end dialog</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Weighted F-1 Dialog Acc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HCN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.4198">6.05% HCN + raw dialogic features 0.4190 5.79% HCN + predicted sentiment label⇤</head><p>0.4261 6.55% <ref type="table">Table 5</ref>: Results of different SL models. The best result is highlighted in bold. ⇤ indicates that the result is significantly better than the baseline (p &lt; 0.01). Dialog accuracy indicates if all turns in a dialog are correct, so it's low. For DSTC2 data, the state-of-art dialog accuracy is 1.9%, consistent with our results. system training, which showed promising results. But once a system operates at scale and interacts with a large number of users, it is desirable for the system to continue to learn autonomously using reinforcement learning (RL). With RL, each turn receives a measurement of goodness called reward ( <ref type="bibr" target="#b32">Williams et al., 2017</ref>). Previously, training task- oriented systems mainly relies on the delayed re- ward about task success. Due to the lack of in- formative immediate reward, the training takes a long time to converge. In this work, we propose to include user sentiment as immediate rewards to expedite the reinforcement learning training pro- cess and create a better user experience.</p><p>To use sentiment scores in the reward function, we chose the policy gradient approach <ref type="bibr" target="#b34">(Williams, 1992)</ref> and implemented the algorithm based on <ref type="bibr" target="#b38">Zhu (2017)</ref>. The traditional reward function uses a positive constant (e.g. 20) to reward the suc- cess of the task, 0 or a negative constant to penal- ize the failure of the task after certain number of turns, and gives -1 to each extra turn to encourage the system to complete the task sooner. However, such reward function doesn't consider any feed- back from the end-user. It is natural for human to consider conversational partner's sentiment in planning dialogs. So, we propose a set of new re- ward functions that incorporate user sentiment to emulate human behaviors.</p><p>The intuition of integrating sentiment in reward functions is as follows. The ultimate evaluator of dialog systems is the end-users. And user sen- timent is a direct reflection of user satisfaction. Therefore, we detected the user sentiment scores from multimodal sources on the fly, and used them as immediate rewards in an adaptive end-to-end dialog training setting. This sentiment informa- tion came directly from real users, which made the system adapt to individual user's sentiment as the dialog proceeds. Furthermore, the sentiment infor- mation is independent of the task, so our method doesn't require any prior domain knowledge and can be easily generalized to other domains. There have been works that incorporated user informa- tion into reward design ( <ref type="bibr" target="#b28">Su et al., 2015;</ref><ref type="bibr" target="#b30">Ultes et al., 2017</ref>). But they used information from one single channel and sometimes required manual la- belling of the reward. Our approach utilizes infor- mation from multiple channels and doesn't involve manual work once a sentiment detector is ready.</p><p>We built a simulated system in the same bus information search domain to test the effective- ness of using sentiment scores in the reward func- tion. In this system, there are 3 entity types - &lt;departure&gt;, &lt;arrival&gt;, and &lt;time&gt; - and 5 actions, asking for different entities, and giv- ing information. A simple action mask was used to prevent impossible actions, such as giving in- formation of an uncovered place. The inputs to the system are the simulated user's dialog acts and the simulated sentiment sampled from a subset of DSTC1, the CleanData, which will be described later. The output of the system is the system action template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">User simulator</head><p>Given that reinforcement learning requires feed- back from the environment -in our case, the users -and interacting with real users is always expen- sive, we created a user simulator to interact with the system. At the beginning of each dialog, the simulated user is initiated with a goal consisting of the three entities mentioned above and the goal remains unchanged throughout the conversation. The user responds to system's questions with enti- ties, which are placeholders like &lt;departure&gt; instead of real values. To simulate ASR errors, the simulated user's act type occasionally changes from "informing slot values" to "making noises" at certain probabilities set by hand (10% in our case). Some example dialogs along with their as- sociated rewards are shown in <ref type="table">Table 8</ref> and 9 in Appendix A.1.</p><p>We simulated user sentiment by sampling from real data, the DSTC1 dataset. There are three steps involved. First, we cleaned the DSTC1 dialogs by removing the audio files with no ASR output and high ASR errors. This resulted in a dataset CleanData with 413 dialogs and 1918 user in- puts. We observed that users accumulate their sentiment as the conversation unfolds. When the system repeatedly asks for the same entity, they express stronger sentiment. Therefore, summary statistics that record how many times certain en- tities have been asked during the conversation is representative of users' accumulating sentiment. We designed a set of summary statistics S that record the statistics of system actions, e.g. how many times the arrival place has been asked or the schedule information has been given.</p><p>The second step is to create a mapping between the five simulated system actions and the DSTC1 system actions. We do this by calculating a vector s real consisting of the values in S for each user ut- terance in CleanData. s real is used to compare the similarity between the real dialog and the sim- ulated dialog.</p><p>The final step is to sample from CleanData. For each simulated user utterance, we calculated the same vector s sim and compared it with each s real . There are two possible results. If there are s real equal to s sim ,we would randomly sample one from all the matched user utterances to rep- resent the sentiment of the simulated user. But if there is no s real matching s sim , different strate- gies would be applied based on the reward func- tion used, which will be described in details later. Once we have a sample, the eight dialogic fea- tures of the sample utterance are used to calculate the sentiment score. We didn't use the acoustic or the textual features because in a simulated setting, only the dialogic features are valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experiments</head><p>We designed four experiments with different re- ward functions. A discount factor of 0.9 was ap- plied to all the experiments. And the maximum number of turns is 15. Following <ref type="bibr" target="#b32">Williams et al. (2017)</ref>, we used LSTM with 32 hidden units for the RNN in the HCN and AdaDelta for the op- timization, and updated the reinforcement learn- ing policy after each dialog. The ✏-greedy ex- ploration strategy <ref type="bibr" target="#b29">(Tokic, 2010)</ref> was applied here. Given that the entire system was simulated, we only used the presence of each entity and the last action taken by the system as the context features, and didn't use bag-of-words or utterance embed- ding features.</p><p>In order to evaluate the method, we froze the policy after every 200 updates, and ran 500 simu- lated dialogs to calculate the task success rate. We repeated the process 20 times and reported the av- erage performance in <ref type="figure" target="#fig_2">Figure 1</ref>, 2 and <ref type="table" target="#tab_4">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Baseline</head><p>We define the baseline reward as follows without any sentiment involvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward 1 Baseline</head><note type="other">if success then R1 = 20 else if failure then R1 = 10 else if each proceeding turn then R1 = 1 end if 7.2.2 Sentiment reward with random samples (SRRS)</note><p>We designed the first simple reward function with user sentiment as the immediate reward: senti- ment with random samples (SRRS). We first drew a sample from real data with matched context; if there was no matched data, a random sample was used instead. Because the amount of CleanData is relatively small, so only 36% turns were cov- ered by matched samples. If the sampled dia- logic features were not all zeros, the sentiment reward (SR) was calculated as a linear combina- tion with tunable parameters. We chose it to be -5P neg -P neu +10P pos for simplicity. When the dialogic features were all zero, in most cases it meant the user didn't express an obvious senti- ment, we set the reward to be -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward 2 SRRS</head><p>if success then R2 = 20 else if failure then R2 = 10 else if sample with all-zero dialogic features then R2 = 1 else if sample with non-zero dialogic features then R2=-5Pneg-Pneu+10Ppos end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Sentiment reward with repetition penalty (SRRP)</head><p>Random samples in SRRS may result in extreme sentiment data. So we used dialogic features to approximate sentiment for the unmatched data. Specifically, if there were repetitions, which cor- relate with negative sentiment (see <ref type="table" target="#tab_2">Table 3</ref>), we assigned a penalty to that utterance. See Reward 3 Formula below for detailed parameters. 36% turns were covered by real data samples, 15% turns had no match in real data and had repetitions, and 33% turns had no match and no repetition. Moreover, we experimented with different penalty weights. When we increased the repeti- tion penalty to 5, the success rate was similar to penalty of 2.5. However, when we increased the penalty even further to 10, the success rate was brought down by a large margin. Our interpreta- tion is that increasing the repetition penalty to a big value made the focus less on the real sentiment samples but more on the repetitions, which did not help the learning. We observed in Section 5 that interruption is the most important feature in detecting sentiment, so if an interruption existed in the simulated user input, we assumed it had a negative sentiment and added an additional penalty of -1 to the previous sentiment reward SRRP to test the effect of inter- ruption. 7.5% turns have interruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward 3 SRRP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward 4 SRRIP</head><p>if success then R4 = 20 else if failure then</p><formula xml:id="formula_0">R4 = 10 else R4 = R3(SRRP ) if interruption then R4 = R4 1 end if end if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Experiment results</head><p>We evaluated every model on two metrics: dia- log lengths and task success rates. We observed in <ref type="figure" target="#fig_2">Figure 1</ref> that all the sentiment reward functions, even SRRS with random samples, reduced the av- erage length of the dialogs, meaning that the sys- tem finished the task faster. The rationale behind is that by adapting to user sentiment, the model can avoid unnecessary system actions to make systems more effective.</p><p>In terms of success rate, sentiment reward with both repetition and interruption penalties (SRRIP) performed the best (see <ref type="figure" target="#fig_3">Figure 2)</ref>. In <ref type="figure" target="#fig_3">Figure 2</ref>, SR- RIP is converging faster than the baseline. For ex- ample, around 5000 iterations, it outperforms the baseline by 5% in task success rate (60% vs 55%) with statistical significance (p &lt; 0.01). It also converges to a better task success rate after 10000 iterations (92.4% vs 94.3%, p &lt; 0.01).  We describe all models' performance in <ref type="table" target="#tab_4">Table  6</ref> in terms of the convergent success rate calcu- lated as the mean success rate after 10000 dialogs. We observed that incorporating various sentiment rewards improved the success rate and expedited the training process overall with statistical signifi- cance. We found that even sentiment reward with random samples (SRRS) outperformed the base- line after convergence. By adding penalties for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Convergent success rate Baseline 0.924 SRRS 0.938⇤ SRRP 0.941⇤ SRRIP 0.943⇤ repetition, the algorithm covered more data points, and therefore, the task success rate and the con- vergence speed improved. We also found that pe- nalizing interruption and repetition together (SR- RIP) achieved a slightly better performance com- pared to penalizing repetition only (SRRP). This suggests that interruptions is another factor to con- sider when approximating sentiment. But the per- formances between SRRP and SRRIP is not sig- nificant. Our guess is that only 7.5% turns in our data contains interruption and the penalty is just an extra -1, so the model confused this signal with noises. But given more interruptions in the data, interruptions could still be helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Future Work</head><p>The intuition behind the good performance of models with user sentiment is that the learned pol- icy is in general more sentiment adaptive. For ex- ample, there are some system actions that have the same intention but with different surface forms, especially for error-handling strategies. By ana- lyzing the results, we found that when the sen- timent adaptive system detected a negative senti- ment from the user, it chose to respond the user with a more detailed error-handling strategy than a general one. For example, it chose the tem- plate "Where are you leaving from? For exam- ple, you can say, &lt;place&gt;", while the baseline model would respond with "Where would you like to leave from?", which doesn't provide details to help the user compared with the previous tem- plate. As we all know, dealing with a disappointed user to proceed, providing more details is always better. One example dialog is shown in <ref type="table">Table 7</ref>. There was no written rules to force the model to choose one specific template under certain situ-  <ref type="table">Table 7</ref>: An example dialog by different sys- tems in the supervised learning setting. The sentiment-adaptive system gives a more detailed error-handling strategy than the baseline system.</p><p>ations, so the model learned these subtle differ- ences on its own. Some may argue that the sys- tem could always use a more detailed template to better guide the user instead of distinguishing be- tween two similar system templates. But this is not necessarily true. Ideally, we want the system to be succinct initially to save users' time, because we observe that users, especially repeated users, tend to interrupt long and detailed system utterances. If the user has attempted to answer the system ques- tion but failed, then it's beneficial to provide de- tailed guidance.</p><p>The performance of the sentiment detector is a key factor in our work. So in the future, we plan to incorporate features from more channels such as vision to further improve the sentiment predic- tor's performance, and potentially further improve the performance of the dialog system. We also want to explore more in user sentiment simula- tion, for example, instead of randomly sampling data for the uncovered cases, we could use linear interpolation to create a similarity score between s sim and s real , and choose the user utterance with the highest score. Furthermore, reward shaping ( <ref type="bibr" target="#b19">Ng et al., 1999;</ref><ref type="bibr" target="#b10">Ferreira and Lefèvre, 2013)</ref> is an important technique in RL. Specifically, <ref type="bibr" target="#b10">Ferreira and Lefèvre (2013)</ref> talked about incorporating ex- pert knowledge in reward design. We also plan to integrate information from different sources into reward function and apply reward shaping. Be- sides, creating a good user simulator is also very important in the RL training. There are some more advanced methods to create user simulators. For example, Liu and Lane (2017b) described how to optimize the agent and the user simulators jointly using RL. We plan to apply our sentiment reward functions in this framework in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We proposed to detect user sentiment from multi- modal channels and incorporate the detected sen- timent as feedback into adaptive end-to-end dia- log system training to make the system more ef- fective and user-adaptive. We included sentiment information directly as a context feature in the su- pervised learning framework and used sentiment scores as immediate rewards in the reinforcement learning setting. Experiments suggest that incor- porating user sentiment is helpful in reducing the dialog length and increasing the task success rate in both SL and RL settings. This work proposed an adaptive methodology to incorporate user sen- timent in end-to-end dialog policy learning and showed promising results on a bus information search task. We believe this approach can be easily generalized to other domains given its end-to-end training procedure and task independence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2016); Eric and Manning (2017); Seo et al. (2016); Liu and Lane (2017a); Li et al. (2017); Williams et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average dialog length of RL models with different reward functions.</figDesc><graphic url="image-1.png" coords="8,72.00,195.43,218.25,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average success rate of the baseline and the best performing model, SRRIP.</figDesc><graphic url="image-2.png" coords="8,72.00,424.86,218.27,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Statistics of the text data.</head><label>1</label><figDesc></figDesc><table>Category 
Total 
total dialogs 
914 
total dialogs in train 517 
total dialogs in test 
397 

Statistics 
Total 
avg dialog len 
13.8 
vocabulary size 685 

Category 
Total 
total dialogs 
50 
total audios 
517 
total audios in train 318 
total audios in dev 
99 
total audios in test 
100 

Category Total 
neutral 
254 
negative 
253 
positive 
10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the annotated audio set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Dialogic features' relative importance 
rank in sentiment detection. 

Model 
Avg. of F-1 Std. of F-1 Max of F-1 
Acoustic features only 
0.635 
0.027 
0.686 
Dialogic features only 
0.596 
0.001 
0.596 
Textual features only ⇤ 
0.664 
0.010 
0.685 
Textual + Dialogic ⇤ 
0.672 
0.011 
0.700 
Acoustic + Dialogic ⇤ 
0.680 
0.019 
0.707 
Acoustic + Textual 
0.647 
0.025 
0.686 
Acoustic + Dialogic + Text ⇤ 
0.686 
0.028 
0.756 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of sentiment detectors using dif-
ferent features. The best result is highlighted in 
bold and * indicates statistical significance com-
pared to the baseline, which is using acoustic fea-
tures only. (p &lt; 0.0001) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Convergent success rate of RL models 
with different reward functions. It is calculated as 
the mean success rate after 10000 dialogs. The 
best result is highlighted in bold. ⇤ indicates that 
the result is significantly better than the baseline 
(p &lt; 0.01). 

</table></figure>

			<note place="foot" n="1"> The dataset is available here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is partly supported by Intel Lab Re-search Gift.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using emotion to gain rapport in a spoken dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acosta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Achieving rapport with turn-by-turn, user-responsive emotional coloring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel G</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1137" to="1148" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time speech emotion and sentiment recognition for interactive dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Farhad Bin Siddique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Ho Yin Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1042" to="1047" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion detection in dialog systems: applications, strategies and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Van Ballegooy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Peter</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Polzehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Stegmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction and Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>ACII 2009. 3rd International Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emotion detection in task-oriented spoken dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Vasilescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2003. ICME&apos;03. Proceedings. 2003 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">549</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Annotation and detection of emotion in a task-oriented human-human dialog corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Lamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of ISLE Workshop</title>
		<meeting>ISLE Workshop</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">End-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00777</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04024</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recent developments in opensmile, the munich open-source multimedia feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia, MM &apos;13</title>
		<meeting>the 21st ACM International Conference on Multimedia, MM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Expertbased reward shaping and exploration scheme for boosting policy learning of dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="108" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward Detecting Emotions in Spoken Dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A stochastic model of human-machine interaction for learning dialog strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="23" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A user simulator for task-completion dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">End-to-end task-completion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01008</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using context to improve emotion detection in spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename><surname>Liscombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An end-to-end trainable neural network model with belief tracking for taskoriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05956</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06136</idno>
		<title level="m">Iterative policy learning in end-to-end trainable task-oriented neural dialog models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daishi</forename><surname>Andrew Y Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Say</forename><forename type="middle">Wei</forename><surname>Tin Lay Nwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyanage C De</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="623" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotion recognition and adaptation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Pittermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Pittermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Minker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lets go public! taking a spoken dialog system to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hidden markov model-based speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2003. ICME&apos;03. Proceedings. 2003 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">401</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The interspeech 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Marchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association</title>
		<meeting>INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Query-regression networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04582</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimizing dialogue management with reinforcement learning: Experiments with the njfun system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="133" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive &quot;-greedy exploration in reinforcement learning based on value differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Tokic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain-independent user satisfaction reward estimation for dialogue policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkšic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1721" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hybrid code networks: Practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning conversational systems that interleave task and non-task content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02560</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">tensorflow-reinforce</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
