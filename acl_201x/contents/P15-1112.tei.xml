<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1159" to="1168"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original re-cursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discrimina-tive model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chi-nese datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Feature-based discriminative supervised models have achieved much progress in dependency pars- ing <ref type="bibr" target="#b13">(Nivre, 2004;</ref><ref type="bibr" target="#b23">Yamada and Matsumoto, 2003;</ref><ref type="bibr">McDonald et al., 2005</ref>), which typically use mil- lions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness.</p><p>Recently, many methods are proposed to learn various distributed representations on both syn- tax and semantics levels. These distributed repre- sentations have been extensively applied on many * Corresponding author. natural language processing (NLP) tasks, such as syntax ( <ref type="bibr" target="#b22">Turian et al., 2010;</ref><ref type="bibr">Mikolov et al., 2010;</ref><ref type="bibr" target="#b6">Collobert et al., 2011;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014</ref>) and semantics ( <ref type="bibr">Huang et al., 2012;</ref><ref type="bibr">Mikolov et al., 2013)</ref>. Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better general- ization than discrete representations. For dependency parsing,  and <ref type="bibr" target="#b0">Bansal et al. (2014)</ref> used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks.</p><p>Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in re- cursive structure, it is intuitive to use the recur- sive neural network (RNN), which is used for con- stituent parsing <ref type="bibr" target="#b18">(Socher et al., 2013a</ref>). However, recursive neural network can only process the bi- nary combination and is not suitable for depen- dency parsing, since a parent node may have two or more child nodes in dependency tree.</p><p>In this work, we address the problem to rep-resent all level nodes (words or phrases) with dense representations in a dependency tree. We propose a recursive convolutional neural net- work (RCNN) architecture to capture syntac- tic and compositional-semantic representations of phrases and words. RCNN is a general architec- ture and can deal with k-ary parsing tree, there- fore it is very suitable for dependency parsing. For each node in a given dependency tree, we first use a RCNN unit to model the interactions between it and each of its children and choose the most infor- mative features by a pooling layer. Thus, we can apply the RCNN unit recursively to get the vector representation of the whole dependency tree. The output of each RCNN unit is used as the input of the RCNN unit of its parent node, until it outputs a single fixed-length vector at root node. <ref type="figure" target="#fig_0">Figure 1</ref> il- lustrates an example how a RCNN unit represents the phrases "a red bike" as continuous vectors. The contributions of this paper can be summa- rized as follows.</p><p>• RCNN is a general architecture to model the distributed representations of a phrase or sen- tence with its dependency tree. Although RCNN is just used for the re-ranking of the dependency parser in this paper, it can be regarded as semantic modelling of text se- quences and handle the input sequences of varying length into a fixed-length vector. The parameters in RCNN can be learned jointly with some other NLP tasks, such as text clas- sification.</p><p>• Each RCNN unit can model the complicated interactions of the head word and its children. Combined with a specific task, RCNN can capture the most useful semantic and struc- ture information by the convolution and pool- ing layers.</p><p>• When applied to the re-ranking model for parsing, RCNN improve the accuracy of base parser to make accurate parsing decisions. The experiments on two benchmark datasets show that RCNN outperforms the state-of- the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recursive Neural Network</head><p>In this section, we briefly describe the recur- sive neural network architecture of <ref type="bibr" target="#b18">(Socher et al., 2013a</ref>). The idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure <ref type="bibr" target="#b14">(Pollack, 1990;</ref><ref type="bibr" target="#b20">Socher et al., 2013c</ref>). RNN can be also regarded as a general structure to model sen- tence. At every node in the tree, the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer com- puted at the top node gives a representation for the whole sentence.</p><p>Following the binary tree structure, RNN can assign a fixed-length vector to each word at the leaves of the tree, and combine word and phrase pairs recursively to create intermediate node vec- tors of the same length, eventually having one fi- nal vector representing the whole sentence. Multi- ple recursive combination functions have been ex- plored, from linear transformation matrices to ten- sor products <ref type="bibr" target="#b20">(Socher et al., 2013c</ref>). <ref type="figure" target="#fig_1">Figure 2</ref> illus- trates the architecture of RNN.</p><p>The binary tree can be represented in the form of branching triplets (p → c 1 c 2 ). Each such triplet denotes that a parent node p has two children and each c k can be either a word or a non-terminal node in the tree.</p><p>Given a labeled binary parse tree, ((p 2 → ap 1 ), (p 1 → bc)), the node represen- tations are computed by</p><formula xml:id="formula_0">p 1 = f (W b c ), p 2 = f (W a p 1 ),<label>(1)</label></formula><p>where (p 1 , p 2 , a, b, c) are the vector representa- tion of (p 1 , p 2 , a, b, c) respectively, which are de- noted by lowercase bold font letters; W is a matrix of parameters of the RNN. Based on RNN, Socher et al. (2013a) intro- duced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector rep- resentations. In order to compute the score of how plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all p i :</p><formula xml:id="formula_1">s(p i ) = v · p i ,<label>(2)</label></formula><p>where v is a vector of parameters that need to be trained. This score will be used to find the high- est scoring tree. For more details on how standard RNN can be used for parsing, see <ref type="bibr" target="#b17">(Socher et al., 2011</ref>). <ref type="bibr" target="#b7">Costa et al. (2003)</ref> applied recursive neural net- works to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough in- formation to make the correct parsing decisions. <ref type="bibr">Menchetti et al. (2005)</ref> used RNNs to re-rank dif- ferent constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser <ref type="bibr" target="#b5">(Collins, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recursive Convolutional Neural Network</head><p>The dependency grammar is a widely used syntac- tic structure, which directly reflects relationships among the words in a sentence. In a dependency tree, all nodes are terminal (words) and each node may have more than two children. Therefore, the standard RNN architecture is not suitable for de- pendency grammar since it is based on the binary tree. In this section, we propose a more general architecture, called recursive convolutional neu- ral network (RCNN), which borrows the idea of convolutional neural network (CNN) and can deal with to k-ary tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RCNN Unit</head><p>For ease of exposition, we first describe the ba- sic unit of RCNN. A RCNN unit is to model a head word and its children. Different from the constituent tree, the dependency tree does not have non-terminal nodes. Each node consists of a word and its POS tags. Each node should have a differ- ent interaction with its head node.</p><p>Word Embeddings Given a word dictionary W, each word w ∈ W is represented as a real-valued vector (word embedding) w ∈ R m where m is the dimensionality of the vector space. The word em- beddings are then stacked into a embedding ma- trix M ∈ R m|W| . For a word w ∈ W, its cor- responding word embedding Embed(w) ∈ R m is retrieved by the lookup table layer. The matrix M is initialized with pre-training embeddings and up- dated by back-propagation.</p><formula xml:id="formula_2">x h = x K x 2 x 1 w h d (h,</formula><p>Distance Embeddings Besides word embed- dings, we also use distributed vector to represent the relative distance of a head word h and one of its children c. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the relative distances of "bike" to "a" and "red" are -2 and -1, respectively. The relative distances also are mapped to a vector of dimension m d (a hy- perparameter); this vector is randomly initialized. Distance embedding is a usual way to encode the distance information in neural model, which has been proven effectively in several tasks. Our ex- perimental results also show that the distance em- bedding gives more benefits than the traditional representation. The relative distance can encode the structure information of a subtree.</p><p>Convolution The word and distance embed- dings are subsequently fed into the convolution component to model the interactions between two linked nodes.</p><p>Different with standard RNN, there are no non- terminal nodes in dependency tree. Each node h in dependency tree has two associated distributed representations:</p><p>1. word embedding w h ∈ R m , which is denoted as its own information according to its word form;</p><p>2. phrase representation x h ∈ R m , which is de- noted as the joint representation of the whole subtree rooted at h. In particular, when h is leaf node, x h = w h .</p><p>Given a subtree rooted at h in dependency tree, we define c i , 0 &lt; i ≤ L as the i-th child node of h, where L represents the number of children.</p><p>For each pair (h, c i ), we use a convolutional hidden layer to compute their combination repre- sentation z i .</p><formula xml:id="formula_3">z i = tanh(W (h,c i ) p i ), 0 &lt; i ≤ K,<label>(3)</label></formula><p>where W (h,c i ) ∈ R m×n is the linear composition matrix, which depends on the POS tags of h and c i ; p i ∈ R n is the concatenated representation of h and the i-th child, which consists of the head word embeddings w h , the child phrase represen- tation x c i and the distance embeddings d h,c i of h and c i ,</p><formula xml:id="formula_4">p i = x h ⊕ x c i ⊕ d (h,c i ) ,<label>(4)</label></formula><p>where ⊕ represents the concatenation operation.</p><p>The distances d h,c i is the relative distance of h and c i in a given sentence. Then, the relative dis- tances also are mapped to m-dimensional vectors. Different from constituent tree, the combination should consider the order or position of each child in dependency tree.</p><p>In our model, we do not use the POS tags em- beddings directly. Since the composition matrix varies on the different pair of POS tags of h and c i , it can capture the different syntactic combina- tions. For example, the combination of adjective and noun should be different with that of verb and noun.</p><p>After the composition operations, we use tanh as the non-linear activation function to get a hid- den representation z.</p><formula xml:id="formula_5">Max Pooling After convolution, we get Z (h) = [z 1 , z 2 , · · · , z K ],</formula><p>where K is dynamic and de- pends on the number of children of h. To trans- form Z to a fixed length and determine the most useful semantic and structure information, we per- form a max pooling operation to Z on rows.</p><formula xml:id="formula_6">x (h) j = max i Z (h) j,i , 0 &lt; j ≤ m.<label>(5)</label></formula><p>Thus, we obtain the vector representation x h ∈ R m of the whole subtree rooted at node h. <ref type="figure" target="#fig_2">Figure 3</ref> shows the architecture of our proposed RCNN unit.  <ref type="figure">Figure 4</ref>: Example of a RCNN unit Given a whole dependency tree, we can apply the RCNN unit recursively to get the vector rep- resentation of the whole sentence. The output of each RCNN unit is used as the input of the RCNN unit of its parent node.</p><p>Thus, RCNN can be used to model the dis- tributed representations of a phrase or sentence with its dependency tree and applied to many NLP tasks. The parameters in RCNN can be learned jointly with the specific NLP tasks. Each RCNN unit can model the complicated interactions of the head word and its children. Combined with a spe- cific task, RCNN can select the useful semantic and structure information by the convolution and max pooling layers. <ref type="figure">Figure 4</ref> shows an example of RCNN to model the sentence "I eat sashimi with chopsitcks".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parsing</head><p>In order to measure the plausibility of a subtree rooted at h in dependency tree, we use a single- unit linear layer neural network to compute the score of its RCNN unit.</p><p>For constituent parsing, the representation of a non-terminal node only depends on its two chil- dren. The combination is relative simple and its correctness can be measured with the final repre- sentation of the non-terminal node <ref type="bibr" target="#b18">(Socher et al., 2013a)</ref>.</p><p>However for dependency parsing, all combina- tions of the head h and its children c i (0 &lt; i ≤ K) are important to measure the correctness of the subtree. Therefore, our score function s(h) is computed on all of hidden layers z i (0 &lt; i ≤ K):</p><formula xml:id="formula_7">s(h) = K i=1 v (h,c i ) · z i ,<label>(6)</label></formula><p>where v (h,c i ) ∈ R m×1 is the score vector, which also depends on the POS tags of h and c i . Given a sentence x and its dependency tree y, the goodness of a complete tree is measured by summing the scores of all the RCNN units.</p><formula xml:id="formula_8">s(x, y, Θ) = h∈y s(h),<label>(7)</label></formula><p>where h ∈ y is the node in tree y; Θ = {Θ W , Θ v , Θ w , Θ d } including the combination matrix set Θ W , the score vector set Θ v , the word embeddings Θ w and distance embeddings Θ d . Finally, we can predict dependency treê y with highest score for sentence x.</p><formula xml:id="formula_9">ˆ y = arg max y∈gen(x) s(x, y, Θ),<label>(8)</label></formula><p>where gen(x) is defined as the set of all possible trees for sentence x. When applied in re-ranking, gen(x) is the set of the k-best outputs of a base parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>For a given training instance (x i , y i ), we use the max-margin criterion to train our model. We first predict the dependency treê y i with the highest score for each x i and define a structured margin loss ∆(y i , ˆ y i ) between the predicted treê y i and the given correct tree y i . ∆(y i , ˆ y i ) is measured by counting the number of nodes y i with an incor- rect span (or label) in the proposed tree <ref type="bibr" target="#b10">(Goodman, 1998)</ref>.</p><formula xml:id="formula_10">∆(y i , ˆ y i ) = d∈ˆyd∈ˆ d∈ˆy i κ1{d / ∈ y i }<label>(9)</label></formula><p>where κ is a discount parameter and d represents the nodes in trees. Given a set of training dependency parses D, the final training objective is to minimize the loss function J(Θ), plus a l 2 -regulation term:</p><formula xml:id="formula_11">J(Θ) = 1 |D| (x i ,y i )∈D r i (Θ) + λ 2 Θ 2 2 ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">r i (Θ) = maxˆy maxˆ maxˆy i ∈Y (x i ) ( 0, s t (x i , ˆ y i , Θ) + ∆(y i , ˆ y i ) − s t (x i , y i , Θ) ) .<label>(11)</label></formula><p>By minimizing this object, the score of the cor- rect tree y i is increased and the score of the highest scoring incorrect treê y i is decreased.</p><p>We use a generalization of gradient descent called subgradient method ( <ref type="bibr" target="#b15">Ratliff et al., 2007)</ref> which computes a gradient-like direction. The subgradient of equation is:</p><formula xml:id="formula_13">∂J ∂Θ = 1 |D| (x i ,y i )∈D ( ∂s t (x i , ˆ y i , Θ) ∂Θ − ∂s t (x i , y i , Θ) ∂Θ ) + λΘ.<label>(12)</label></formula><p>To minimize the objective, we use the diagonal variant of AdaGrad ( <ref type="bibr" target="#b8">Duchi et al., 2011</ref>). The pa- rameter update for the i-th parameter Θ t,i at time step t is as follows:</p><formula xml:id="formula_14">Θ t,i = Θ t−1,i − ρ t τ =1 g 2 τ,i g t,i ,<label>(13)</label></formula><p>where ρ is the initial learning rate and g τ ∈ R |θ i | is the subgradient at time step τ for parameter θ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Re-rankers</head><p>Re-ranking k-best lists was introduced by Collins and <ref type="bibr" target="#b4">Koo (2005)</ref> and <ref type="bibr" target="#b1">Charniak and Johnson (2005)</ref>. They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, <ref type="bibr" target="#b16">Sangati et al. (2009)</ref> used a third-order generative model for re-ranking k-best lists of base parser. <ref type="bibr" target="#b11">Hayashi et al. (2013)</ref> used a discriminative for- est re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy ( <ref type="bibr" target="#b11">Hayashi et al., 2013;</ref><ref type="bibr">Le and Mikolov, 2014)</ref>, which is a combination of the our model and the base parser.</p><formula xml:id="formula_15">ˆ y i = arg max y∈T (x i ) αs t (x i , y, Θ) + (1 − α)s b (x i , y) (14)</formula><p>where α ∈ [0, 1] is a hyperparameter; s t (x i , y, Θ) and s b (x i , y) are the scores given by RCNN and the base parser respectively.</p><p>To apply RCNN into re-ranking model, we first get the k-best outputs of all sentences in train set with a base parser. Thus, we can train the RCNN in a discriminative way and optimize the re-ranking strategy for a particular base parser.</p><p>Note that the role of RCNN is not fully valued when applied in re-ranking model since that the gen(x) in Eq. <ref type="formula" target="#formula_9">(8)</ref> is just the k-best outputs of a base parser, not the set of all possible trees for sentence x. The parameters of RCNN could overfit to k- best outputs of training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Datasets</head><p>To empirically demonstrate the effectiveness of our approach, we use two datasets in different lan- guages (English and Chinese) in our experimen- tal evaluation and compare our model against the other state-of-the-art methods using the unlabeled attachment score (UAS) metric ignoring punctua- tion.</p><p>English For English dataset, we follow the stan- dard splits of Penn Treebank (PTB), using sections 2-21 for training, section 22 as de- velopment set and section 23 as test set. We tag the development and test sets using an au- tomatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknif- ing similar to <ref type="bibr" target="#b4">(Collins and Koo, 2005</ref>).</p><p>Chinese For Chinese dataset, we follow the same split of the Penn Chinese Treeban (CTB5) as described in <ref type="bibr" target="#b24">(Zhang and Clark, 2008)</ref> and use sections 001-815, 1001-1136 as training set, sections 886-931, 1148-1151 as devel- opment set, and sections 816-885, 1137-1147 as test set. Dependencies are converted by us- ing the Penn2Malt tool with the head-finding rules of <ref type="bibr" target="#b24">(Zhang and Clark, 2008)</ref>. And fol- lowing ( <ref type="bibr" target="#b24">Zhang and Clark, 2008</ref>) (Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input.</p><p>We use the linear-time incremental parser ( <ref type="bibr" target="#b12">Huang and Sagae, 2010)</ref> as our base parser and calculate the 64-best parses at the top cell of the chart. Note that we optimize the training settings for base parser and the results are slightly im- proved on <ref type="bibr" target="#b12">(Huang and Sagae, 2010)</ref>. Then we use max-margin criterion to train RCNN. Finally, we use the mixture strategy to re-rank the top 64-best parses.</p><p>For initialization of parameters, we train word2vec embeddings ( <ref type="bibr">Mikolov et al., 2013</ref>) on Wikipedia corpus for English and Chinese respec- tively. For the combination matrices and score vectors, we use the random initialization within (0.01, 0.01). The parameters which achieve the best unlabeled attachment score on the develop- ment set will be chosen for the final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">English Dataset</head><p>We first evaluate the performances of the RCNN and re-ranker (Eq. <ref type="formula" target="#formula_0">(14)</ref>) on the development set. <ref type="figure" target="#fig_3">Figure 5</ref> shows UASs of different models with varying k. The base parser achieves 92.45%. When k = 64, the oracle best of base parser achieves 97.34%, while the oracle worst achieves 73.30% <ref type="bibr">(-19.15%)</ref> . RCNN achieves the maxi- mum improvement of 93.00%(+0.55%) when k = 6. When k &gt; 6, the performance of RCNN de- clines with the increase of k but is still higher than baseline (92.45%). The reason behind this is that RCNN could require more negative sam- ples to avoid overfitting when k is large. Since the negative samples are limited in the k-best outputs of a base parser, the learnt parameters could easily overfits to the training set.</p><p>The mixture re-ranker achieves the maximum improvement of 93.50%(+1.05%) when k = 64. In mixture re-ranker, α is optimised by searching with the step-size 0.005.</p><p>Therefore, we use the mixture re-ranker in the following experiments since it can take the advan- tages of both the RCNN and base models. <ref type="figure">Figure 6</ref> shows the accuracies on the top ten POS tags of the modifier words with the largest improvements. We can see that our re-ranker can improve the accuracies of CC and IN, and therefore may indirectly result in rising the the well-known coordinating conjunction and PP- attachment problems.</p><p>The final experimental results on test set are shown in <ref type="table">Table 1</ref>. The hyperparameters of our model are set as in <ref type="table" target="#tab_2">Table 2</ref>. Our re-ranker achieves the maximum improvement of 93.83%(+1.48%) on test set. Our system performs slightly better than many state-of-the-art systems such as <ref type="bibr" target="#b24">Zhang and Clark (2008)</ref> and <ref type="bibr" target="#b12">Huang and Sagae (2010)</ref>. It outperforms <ref type="bibr" target="#b11">Hayashi et al. (2013)</ref> and <ref type="bibr">Le and Zuidema (2014)</ref>, which also use the mixture re- ranking strategy.</p><p>Since the result of ranker is conditioned to k- best results of base parser, we also do an experi- ment to avoid this limitation by adding the oracle to k-best candidates. With including oracle, the re-ranker can achieve 94.16% on UAS, which is shown in the last line ("our re-ranker (with ora- cle)") of <ref type="table">Table 1</ref>. <ref type="formula" target="#formula_0">1 2 3 4 5 6 7 8 9 10 32</ref>   Base Paser Re-ranker <ref type="figure">Figure 6</ref>: Accuracies on the top ten POS tags of the modifier words with the largest improvements on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Chinese Dataset</head><p>We also make experiments on the Penn Chinese Treebank (CTB5). The hyperparameters is the same as the previous experiment on English except that α is optimised by searching with the step-size 0.005. The final experimental results on the test set are shown in <ref type="table" target="#tab_3">Table 3</ref>. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-the- art methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line ("our re-ranker (with oracle)") of <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Methods</head><p>Zhang and Clark <ref type="formula" target="#formula_1">(2008)</ref> 91.4 <ref type="bibr" target="#b12">Huang and Sagae (2010)</ref> 92.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributed Representations</head><p>Stenetorp <ref type="formula" target="#formula_0">(2013)</ref> 86.25  93.74 <ref type="bibr" target="#b2">Chen and Manning (2014)</ref> 92.0 Re-rankers <ref type="bibr" target="#b11">Hayashi et al. (2013)</ref> 93.12 <ref type="bibr">Le and Zuidema (2014)</ref> 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle)</p><p>94.16 <ref type="table">Table 1</ref>: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.</p><p>Compared with the re-ranking model of <ref type="bibr" target="#b11">Hayashi et al. (2013)</ref>, that use a large number of handcrafted features, our model can achieve a competitive per- formance with the minimal feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Discussions</head><p>The performance of the re-ranking model is af- fected by the base parser. The small divergence of the dependency trees in the output list also results to overfitting in training phase. Although our re-Word embedding size m = 25 Distance embedding size m d = 25 Initial learning rate ρ = 0.1  ranker outperforms the state-of-the-art methods, it can also benefit from improving the quality of the candidate results. It was also reported in other re- ranking works that a larger k (eg. k &gt; 64) results the worse performance. We think the reason is that the oracle best increases when k is larger, but the oracle worst decrease with larger degree. The er- ror types increase greatly. The re-ranking model requires more negative samples to avoid overfit- ting. When k is larger, the number of negative samples also needs to multiply increase for train- ing. However, we just can obtain at most k neg- ative samples from the k-best outputs of the base parser.</p><formula xml:id="formula_16">Margin loss discount κ = 2.0 Regularization λ = 10 −4 k-best k = 64</formula><p>The experiments also show that the our model can achieves significant improvements by adding the oracles into the output lists of the base parser. This indicates that our model can be boosted by a better set of the candidate results, which can be implemented by combining the RCNN in the de- coding algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>There have been several works to use neural net- works and distributed representation for depen- dency parsing. Stenetorp (2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his model is still unsatisfactory. <ref type="bibr" target="#b2">Chen and Manning (2014)</ref> improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their in- teractions with neural network to make predictions of actions. Their methods aim to transition-based parsing and can not model the sentence in seman- tic vector space for other NLP tasks. <ref type="bibr" target="#b19">Socher et al. (2013b)</ref> proposed a composi- tional vectors computed by dependency tree RNN (DT-RNN) to map sentences and images into a common embedding space. However, there are two major differences as follows. 1) They first summed up all child nodes into a dense vector v c and then composed subtree representation from v c and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pool- ing layer. 2) We represent the relative position of each child and its parent with distributed rep- resentation (position embeddings), which is very useful for convolutional layer. <ref type="figure" target="#fig_4">Figure 7</ref> shows an example of DTRNN to illustrates how RCNN rep- resents phrases as continuous vectors.</p><p>Specific to the re-ranking model, <ref type="bibr">Le and Zuidema (2014)</ref> proposed a generative re-ranking model with Inside-Outside Recursive Neural Net- work (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate re- sults. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generaliza- tion of simple recurrent neural network (SRNN) <ref type="bibr" target="#b9">(Elman, 1990)</ref>. Unlike IORNN, our proposed RCNN is a discriminative model and can opti- mize the re-ranking strategy for a particular base parser. Another difference is that RCNN computes the score of tree in a recursive way, which is more natural for the hierarchical structure of natural lan- guage. Besides, the RCNN can not only be used for the re-ranking, but also be regarded as general model to represent sentence with its dependency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we address the problem to rep- resent all level nodes (words or phrases) with dense representations in a dependency tree. We propose a recursive convolutional neural net- work (RCNN) architecture to capture the syntac- tic and compositional-semantic representations of phrases and words. RCNN is a general architec- ture and can deal with k-ary parsing tree, there- fore RCNN is very suitable for many NLP tasks to minimize the effort in feature engineering with a external dependency parser. Although RCNN is just used for the re-ranking of the dependency parser in this paper, it can be regarded as seman- tic modelling of text sequences and handle the in- put sequences of varying length into a fixed-length vector. The parameters in RCNN can be learned jointly with some other NLP tasks, such as text classification.</p><p>For the future research, we will develop an inte- grated parser to combine RCNN with a decoding algorithm. We believe that the integrated parser can achieve better performance without the limi- tation of base parser. Moreover, we also wish to investigate the ability of our model for other NLP tasks. <ref type="bibr">Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012</ref>. Improving word represen- tations via global context and multiple word proto- types. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 873-882, Jeju Island, Korea, July. Association for Computational Linguis- tics.</p><p>Quoc V. <ref type="bibr">Le and Tomas Mikolov. 2014</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of a RCNN unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of a RNN unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of a RCNN unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: UAS with varying k on the development set. Oracle best: always choosing the best result in the k-best of base parser; Oracle worst: always choosing the worst result in the k-best of base parser; RCNN: choosing the most probable candidate according to the score of RCNN; Re-ranker: a combination of the RCNN and base parser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example of a DT-RNN unit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Hyperparameters of our model 

UAS 
Traditional Methods 
Zhang and Clark (2008) 
84.33 
Huang and Sagae (2010) 
85.20 
Distributed Representations 
Chen et al. (2014) 
82.94 
Chen and Manning (2014) 
83.9 
Re-rankers 
Hayashi et al. (2013) 
85.9 
Our baseline 
85.46 
Our re-ranker 
85.71(+0.25) 
Our re-ranker (with oracle) 
87.43 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on Chinese test set.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments. This work was partially funded by the National Natural Sci-ence Foundation of China (61472088, 61473092), the National High Technology Research and De-velopment Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200), Shanghai Leading Aca-demic Discipline Project (B114).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature embedding for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="816" to="826" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards incremental parsing of natural language using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Soda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="9" to="25" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>cmp-lg/9805007</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Parsing inside-out. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient stacked dependency parsing by forest reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="139" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan B Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">(online) subgradient methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nathan D Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Conference on Artificial Intelligence and Statistics (AIStats)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generative re-ranking model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Sangati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies</title>
		<meeting>the 11th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="238" to="241" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
