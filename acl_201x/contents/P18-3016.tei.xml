<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-based Filtering of Out-of-Vocabulary Words for Encoder-Decoder Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Katsumata</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Metropolitan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukio</forename><surname>Matsumura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Metropolitan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayahide</forename><surname>Yamagishi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Metropolitan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Metropolitan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-based Filtering of Out-of-Vocabulary Words for Encoder-Decoder Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="112" to="119"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>112</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Encoder-decoder models typically only employ words that are frequently used in the training corpus to reduce the computational costs and exclude noise. However , this vocabulary set may still include words that interfere with learning in encoder-decoder models. This paper proposes a method for selecting more suitable words for learning encoders by utilizing not only frequency but also co-occurrence information, which we capture using the HITS algorithm. We apply our proposed method to two tasks: machine translation and grammatical error correction. For Japanese-to-English translation, this method achieves a BLEU score that is 0.56 points more than that of a baseline. Furthermore, it outperforms the baseline method for English grammatical error correction , with an F 0.5-measure that is 1.48 points higher.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Encoder-decoder models <ref type="bibr" target="#b19">(Sutskever et al., 2014)</ref> are effective in tasks such as machine translation ( <ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> and grammatical error correction <ref type="bibr" target="#b23">(Yuan and Briscoe, 2016)</ref>. Vocabulary in encoder-decoder models is generally selected from the training corpus in de- scending order of frequency, and low-frequency words are replaced with an unknown word token &lt;unk&gt;. The so-called out-of-vocabulary (OOV) words are replaced with &lt;unk&gt; to not increase the decoder's complexity and to reduce noise. However, naive frequency-based OOV replace- ment may lead to loss of information that is nec- essary for modeling context in the encoder.</p><p>This study hypothesizes that vocabulary con- structed using unigram frequency includes words that interfere with learning in encoder-decoder models. That is, we presume that vocabulary selection that considers co-occurrence informa- tion selects fewer noisy words for learning robust encoders in encoder-decoder models. We apply the hyperlink-induced topic search (HITS) algo- rithm to extract the co-occurrence relations be- tween words. Intuitively, the removal of words that rarely co-occur with others yields better en- coder models than ones that include noisy low- frequency words.</p><p>This study examines two tasks, machine transla- tion (MT) and grammatical error correction (GEC) to confirm the effect of decreasing noisy words, with a focus on the vocabulary of the encoder side, because the vocabulary on the decoder side is rela- tively limited. In a Japanese-to-English MT exper- iment, our method achieves a BLEU score that is 0.56 points more than that of the frequency-based method. Further, it outperforms the frequency- based method for English GEC, with an F 0.5 - measure that is 1.48 points higher.</p><p>The main contributions of this study are as fol- lows:</p><p>1. The simple but effective preprocessing method we propose for vocabulary selec- tion improves encoder-decoder model perfor- mance.</p><p>2. This study is the first to address noise re- duction in the source text of encoder-decoder models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is currently a growing interest in apply- ing neural models to MT <ref type="bibr" target="#b19">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b20">Wu et al., 2016)</ref> and GEC <ref type="bibr" target="#b23">(Yuan and Briscoe, 2016;</ref><ref type="bibr" target="#b21">Xie et al., 2016;</ref><ref type="bibr" target="#b7">Ji et al., 2017</ref>); hence, this study focuses on improving the simple attentional encoder-decoder models that are applied to these tasks.</p><p>In the investigation of vocabulary restriction in neural models, <ref type="bibr" target="#b18">Sennrich et al. (2016)</ref> applied byte pair encoding to words and created a partial char- acter string set that could express all the words in the training data. They increased the number of words included in the vocabulary to enable the encoder-decoder model to robustly learn contex- tual information. In contrast, we aim to improve neural models by using vocabulary that is appro- priate for a training corpus-not to improve neural models by increasing their vocabulary. <ref type="bibr" target="#b6">Jean et al. (2015)</ref> proposed a method of re- placing and copying an unknown word token with a bilingual dictionary in neural MT. They auto- matically constructed a translation dictionary from a training corpus using a word-alignment model (GIZA++), which finds a corresponding source word for each unknown target word token. They replaced the unknown word token with the corre- sponding word into which the source word was translated by the bilingual dictionary. <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref> used a similar method for neural GEC. Because our proposed method is performed as preprocessing, it can be used simultaneously with this replace-and-copy method.</p><p>Algorithms that rank words using co- occurrence are employed in many natural language processing tasks.</p><p>For example, TextRank ( <ref type="bibr" target="#b12">Mihalcea and Tarau, 2004</ref>) uses PageRank ( <ref type="bibr" target="#b1">Brin and Page, 1998</ref>) for keyword extraction. TextRank constructs a word graph in which nodes represent words, and edges represent co-occurrences between words within a fixed window; TextRank then executes the PageRank algorithm to extract keywords. Although this is an unsupervised method, it achieves nearly the same precision as one state-of-the-art supervised method <ref type="bibr" target="#b5">(Hulth, 2003)</ref>. <ref type="bibr" target="#b8">Kiso et al. (2011</ref><ref type="bibr">) used HITS (Kleinberg, 1999</ref>) to select seeds and create a stop list for bootstrapping in natural language processing. They reported significant improvements over a baseline method using unigram frequency. Their graph-based algorithm was effective at extracting the relevance between words, which cannot be grasped with a simple unigram frequency. In this study, we use HITS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 HITS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require: hubness vector i0</head><p>Require: adjacency matrix A Require: iteration number τ Ensure: hubness vector i Ensure: authority vector p 1: function HITS(i0, A, τ ) 2: i ← i0 3:</p><p>for t = 1, 2, ..., τ do 4:</p><p>p ← A T i 5:</p><p>i ← Ap 6:</p><p>normalize i and p 7:</p><p>return i and p 8: end function to retrieve co-occurring words from a training corpus to reduce noise in the source text.</p><p>3 Graph-based Filtering of OOV Words 3.1 Hubness and authority scores from HITS HITS, which is a web page ranking algorithm pro- posed by <ref type="bibr" target="#b9">Kleinberg (1999)</ref>, computes hubness and authority scores for a web page (node) using the adjacency matrix that represents the web page's link (edge) transitions. A web page with high au- thority is linked from a page with high hubness scores, and a web page with a high hubness score links to a page with a high authority score. Algo- rithm 1 shows pseudocode for the HITS algorithm. Hubness and authority scores converge by setting the iteration number τ to a sufficiently large value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vocabulary selection using HITS</head><p>In this study, we create an adjacency matrix from a training corpus by considering a word as a node and the co-occurrence between words as an edge. Unlike in web pages, co-occurrence be- tween words is nonbinary; therefore, several co- occurrence measures can be used as edge weights. Section 3.3 describes the co-occurrence measures and the context in which co-occurrence is defined.</p><p>The HITS algorithm is executed using the adja- cency matrix created in the way described above. As a result, it is possible to obtain a score indi- cating importance of each word while considering contextual information in the training corpus. <ref type="figure" target="#fig_0">Figure 1</ref> shows a word graph example. A word that obtains a high score in the HITS al- gorithm is considered to co-occur with a variety of words. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates that second or- der co-occurrence scores (the scores of words co- occurring with words that co-occur with various words <ref type="bibr" target="#b17">(Schütze, 1998)</ref>) are also high. In this study, words with high hubness scores are considered to co-occur with an important word, and low-scoring words are excluded from the vocabulary. Using this method appears to gen- erate a vocabulary that includes words that are more suitable for representing a context vector for encoder models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word graph construction</head><p>To acquire co-occurrence relations, we use a com- bination of each word and its peripheral words. Specifically, we combine the target word with sur- rounding words within window width N and count the occurrences. When defining the context in this way, because the adjacency matrix becomes sym- metric, the same hubness and authority scores can be obtained. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of co- occurrence in which N is set to two.</p><p>We use raw co-occurrence frequency (Freq) and positive pointwise mutual information (PPMI) be- tween words as the (x, y) element A xy of the adja- cency matrix. However, naive PPMI reacts sensi- tively to low-frequency words in a training corpus. To account for high-frequency, we weight the PMI by the logarithm of the number of co-occurrences and use PPMI based on this weighted PMI <ref type="bibr">(Equa1</ref> In this study, singleton words and their co-occurrences are excluded from the graph. </p><formula xml:id="formula_0">A f req xy = |x, y|<label>(1)</label></formula><formula xml:id="formula_1">A ppmi xy = max(0, pmi(x, y) + log 2 |x, y|) (2)</formula><p>Equation 3 is the PMI of target word x and co- occurrence word y. M is the number of tokens of the combination, |x, * | and | * , y| are the number of token combinations when fixing target word x and co-occurrence word y, respectively.</p><formula xml:id="formula_2">pmi(x, y) = log 2 M · |x, y| |x, * || * , y|<label>(3)</label></formula><p>4 Machine Translation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>In the first experiment, we conduct a Japanese- to-English translation using the Asian Scientific Paper Excerpt Corpus (ASPEC; <ref type="bibr" target="#b14">Nakazawa et al., 2016)</ref>. We follow the official split of the train, de- velopment, and test sets. As training data, we use only the first 1.5 million sentences sorted by sen- tence alignment confidence to obtain a Japanese- English parallel corpus (sentences of more than 60 words are excluded). Our training set consists of 1,456,278 sentences, development set consists of 1,790 sentences, and test set consists of 1,812 sentences. The training set has 247,281 Japanese word types and 476,608 English word types. The co-occurrence window width N is set to two. For combinations that co-occurred only once within the training corpus, we set the value of el- ement A xy of the adjacency matrix to zero. The iteration number τ of the HITS algorithm is set to 300. As mentioned in Section 1, we only use the proposed method on the encoder side.</p><p>For this study's neural MT model 2 , we imple- ment global dot attention ( <ref type="bibr" target="#b11">Luong et al., 2015</ref>). We train a baseline model that uses vocabulary that is determined by its frequency in the training corpus. Vocabulary size is set to 100K on the encoder side and 50K on the decoder side. Additionally, we  conduct an experiment of varying vocabulary size of the encoder to 50K in the baseline and PPMI to investigate the effect of vocabulary size. Un- less otherwise noted, we conduct an analysis of the model using the vocabulary size of 100K. The number of dimensions for each of the hidden and embedding layers is 512. The mini-batch size is 150. AdaGrad is used as an optimization method with an initial learning rate of 0.01. Dropout is applied with a probability of 0.2. For this experiment, a bilingual dictionary is prepared for postprocessing unknown words ( <ref type="bibr" target="#b6">Jean et al., 2015</ref>). When the model outputs an unknown word token, the word with the highest attention score is used as a query to replace the unknown token with the corresponding word from the dic- tionary. If not in the dictionary, we replace the un- known word token with the source word (unk rep). This dictionary is created based on word align- ment obtained using fast align <ref type="bibr" target="#b4">(Dyer et al., 2013)</ref> on the training corpus.</p><note type="other">baseline HITS (Freq) HITS (PPMI) BLEU (50K</note><p>We evaluate translation results using BLEU scores ( <ref type="bibr" target="#b16">Papineni et al., 2002</ref>). <ref type="table">Table 1</ref> shows the translation accuracy (BLEU scores) and p-value of a significance test (p &lt; 0.05) by bootstrap resampling <ref type="bibr" target="#b10">(Koehn, 2004</ref>). The PPMI model improves translation accuracy by 0.56 points in Japanese-to-English translation, which is a significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Next, we examine differences in vocabulary by comparing each model with the baseline. Com- pared to the vocabulary of the baseline in 100K setting, Freq and PPMI replace 16,107 and 17,166 types, respectively; compared to the vocabulary of the baseline in 50K setting, PPMI replaces 4,791 types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>According to <ref type="table">Table 1</ref>, the performance of Freq is almost the same as that of the baseline. When examining the differences in selected words in vocabulary between PPMI and Freq, we find that PPMI selects more low-frequency words in the training corpus compared to Freq, because PPMI deals with not only frequency but also co- occurrence.</p><p>The effect of unk rep is almost the same in the baseline as in the proposed method, which indi- cates that the proposed method can be combined with other schemes as a preprocessing step.</p><p>As a comparison of the vocabulary size 50K and 100K, the BLEU score of 100K is higher than that of 50K in PPMI. Moreover, the BLEU scores are almost the same in the baseline. We suppose that the larger the vocabulary size of encoder, the more noisy words the baseline includes, while the PPMI filters these words. That is why the pro- posed method works well in the case where the vocabulary size is large.</p><p>To examine the effect of changing the vocab- ulary on the source side, the test set is divided into two subsets: COMMON and DIFF. The for- mer (1,484 sentences) consists of only the com- mon vocabulary between the baseline and PPMI, whereas the latter (328 sentences) includes at least one word excluded from the common vocabulary. <ref type="table" target="#tab_1">Table 2</ref> shows the translation accuracy of the COMMON and DIFF outputs. Translation perfor- mance of both corpora is improved.</p><p>In order to observe how PPMI improves COM- MON outputs, we measure the similarity of the baseline and PPMI output sentences by count- ing the exact same sentences. In the COMMON outputs, 72 sentence pairs (4.85%) are the same, whereas 9 sentence pairs are the same in the DIFF outputs (2.74%). Surprisingly, even though it uses the same vocabulary, PPMI often outputs different but fluent sentences. <ref type="table" target="#tab_2">Table 3</ref> shows an example of Japanese-to- English translation. The outputs of the proposed method (especially PPMI) are improved, despite the source sentence being expressed with common vocabulary; this is because the proposed method yielded a better encoder model than the baseline. src , , , , baseline there are fields such as separation , extraction , extraction , improvement of new material creation , waste treatment , analysis , etc . Freq there are separation and extraction of useful substances , the improvement of reactivity , new material creation , waste treatment and analysis . PPMI there are the fields such as separation and extraction of useful materials , the reaction improvement , new material creation , waste treatment , analysis , etc ... ref the application fields are separation and extraction of useful substances , reactivity improvement , creation of new products , waste treatment , and chemical analysis .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Grammatical Error Correction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setting</head><p>The second experiment addresses GEC. We com- bine the FCE public dataset (Yannakoudakis et al., 2011), NUCLE corpus ( <ref type="bibr" target="#b3">Dahlmeier et al., 2013)</ref>, and English learner corpus from the Lang-8 learner corpus <ref type="bibr" target="#b13">(Mizumoto et al., 2011</ref>) and re- move sentences longer than 100 words to create a training corpus. From the Lang-8 learner cor- pus, we use only the pairs of erroneous and cor- rected sentences. We use 1,452,584 sentences as a training set (502,908 types on the encoder side and 639,574 types on the decoder side). We evalu- ate the models' performances on the standard sets from the CoNLL-14 shared task ( <ref type="bibr" target="#b15">Ng et al., 2014</ref>) using CoNLL-13 data as a development set (1,381 sentences) and CoNLL-14 data as a test set (1,312 sentences) <ref type="bibr">4</ref> . We employ F 0.5 as an evaluation measure for the CoNLL-14 shared task.</p><p>We use the same model as in Section 4.1 as a neural model for GEC. The models' parameter set- tings are similar to the MT experiment, except for the vocabulary and batch sizes. In this experiment, we set the vocabulary size on the encoder and de- coder sides to 150K and 50K, respectively. Ad-src</p><p>Genetic refers the chance of inheriting a dis- order or disease . baseline Genetic refers the chance of inheriting a dis- order or disease . PPMI Genetic refers to the chance of inheriting a disorder or disease . gold</p><p>Genetic risk refers to the chance of inherit- ing a disorder or disease . <ref type="table">Table 6</ref>: An example of GEC using a source sen- tence from COMMON.</p><p>ditionally, we conduct the experiment of changing vocabulary size of the encoder to 50K to investi- gate the effect of the vocabulary size. Unless oth- erwise noted, we conduct an analysis of the model using the vocabulary size of 150K. The mini-batch size is 100. <ref type="table" target="#tab_3">Table 4</ref> shows the performance of the baseline and proposed method. The PPMI model improves pre- cision and recall; it achieves a F 0.5 -measure 1.48 points higher than the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Result</head><p>In setting the vocabulary size of encoder to 150K, PPMI replaces 37,185 types from the base- line; in the 50K setting, PPMI replaces 10,203 types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>The F 0.5 of the baseline is almost the same while the PPMI model improves the score in the case where the vocabulary size increases. Similar to MT, we suppose that the PPMI filters noisy words.</p><p>As in Section 4.3, we perform a follow-up ex- periment using two data subsets: COMMON and DIFF, which contain 1,072 and 240 sentences, re- spectively. <ref type="table" target="#tab_4">Table 5</ref> shows the accuracy of the error correc- tion of the COMMON and DIFF outputs. Preci- sion increases by 11.81 points, whereas recall re- mains the same for the COMMON outputs.</p><p>In GEC, approximately 20% of COMMON's output pairs differ, which is caused by the dif-  ferences in the training environment. Unlike MT, we can copy OOV in the target sentence from the source sentence without loss of fluency; therefore, our model has little effect on recall, whereas its precision improves because of noise reduction. <ref type="table">Table 6</ref> shows an example of GEC. The pro- posed method's output improves when the source sentence is expressed using common vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We described that the proposed method has a pos- itive effect on learning the encoder. However, we have a question; what affects the performance? We conduct an analysis of this question in this sec- tion.</p><p>First, we count the occurrence of the words in- cluded only in the baseline or PPMI in the training corpus. We also show the number of the tokens per types ("Ave. tokens") included only in either the baseline or PPMI vocabulary.</p><p>The result is shown in <ref type="table" target="#tab_6">Table 7</ref>. We find that the proposed method uses low-frequency words in- stead of high-frequency words in the training cor- pus. This result suggests that the proposed method works well despite the fact that the encoder of the proposed method encounters more &lt;unk&gt; than the baseline. This is because the proposed method excludes words that may interfere with the learn- ing of encoder-decoder models.</p><p>Second, we conduct an analysis of the POS of the words in GEC to find why increasing OOV improves the learning of encoder-decoder models. Specifically, we apply POS tagging to the training corpus and calculate the occurrence of the POS of the words only included in the baseline or PPMI. We use NLTK as a POS tagger. <ref type="table" target="#tab_8">Table 8</ref> shows the result. It is observed that NOUN is the most affected POS by the proposed method and becomes often represented by &lt;unk&gt;. NOUN words in the vocabulary of the baseline contain some non-English words, such as Japanese or Korean. These words should be treated as OOV but the baseline fails to exclude them using only the frequency. According to <ref type="table" target="#tab_1">Table 8, NUM is also   POS  baseline PPMI  ALL  NOUN  92,693 44,472  4,644,478  VERB  11,066 10,099  3,597,895  PRON  127  107  1,869,422  ADP  626  685  1,836,193  DET  128  202  1,473,</ref>  affected by the proposed method. NUM words of the baseline include a simple numeral such as "119", in addition to incorrectly segmented nu- merals such as "514&amp;objID". This word appears 25 times in the training corpus owing to the noisy nature of Lang-8. We suppose that the proposed method excludes these noisy words and has a pos- itive effect on training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed an OOV filtering method, which considers word co-occurrence in- formation for encoder-decoder models. Unlike conventional OOV handling, this graph-based method selects the words that are more suitable for learning encoder models by considering con- textual information. This method is effective for not only machine translation but also grammatical error correction. This study employed a symmetric matrix (sim- ilar to skip-gram with negative sampling) to ex- press relationships between words. In future re- search, we will develop this method by using vocabulary obtained by designing an asymmetric matrix to incorporate syntactic relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example word graph created for ten sentences in the training corpus used for GEC 1 .</figDesc><graphic url="image-1.png" coords="3,78.38,62.81,205.50,242.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of co-occurrence context.</figDesc><graphic url="image-2.png" coords="3,307.28,63.80,220.80,65.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores of the COMMON and DIFF 
outputs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>An example of Japanese-to-English translation on a source sentence from COMMON. 

baseline 
PPMI 
50K 150K 
50K 150K 
Precision 
48.09 46.53 49.45 49.23 
Recall 
8.30 
8.50 
8.61 
9.02 
F0.5 
24.55 24.55 25.37 26.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F 0.5 results on the CoNLL-14 test set 4 . 

COMMON outputs 
DIFF outputs 
baseline 
PPMI baseline PPMI 
P 
48.26 
60.07 
9.40 17.32 
R 
0.01 
0.01 
0.01 
0.02 
F0.5 
0.04 
0.04 
0.04 
0.08 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : F 0.5 of COMMON and DIFF outputs.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 : Number of words included only in either the baseline or PPMI vocabulary.</head><label>7</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Number of the POS of words only in-
cluded in the baseline or PPMI. 

</table></figure>

			<note place="foot" n="2"> https://github.com/yukio326/nmt-chainer</note>

			<note place="foot" n="3"> BLEU score for postprocessing (unk rep) improves by 0.46, 0.44, and 0.46 points in the baseline, Freq, and PPMI, respectively.</note>

			<note place="foot" n="4"> We do not consider alternative answers suggested by the participating teams.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yangyang Xi of Lang-8, Inc. for allow-ing us to use the Lang-8 learner corpus. We also thank Masahiro Kaneko and anonymous review-ers for their insightful comments. We thank Roam Analytics for a travel grant to support the presen-tation of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<idno type="doi">10.1016/S0169-7552(98)00110-X</idno>
		<ptr target="https://doi.org/10.1016/S0169-7552(98)00110-X" />
	</analytic>
	<monogr>
		<title level="j">Comput. Netw. ISDN Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-7</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS corpus of learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-1703" />
	</analytic>
	<monogr>
		<title level="m">Proc. of BEA</title>
		<meeting>of BEA</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1073" />
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for WMT15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-1070" />
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HITS-based seed selection and stop list construction for bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuo</forename><surname>Kiso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="doi">10.1145/324133.324140</idno>
		<ptr target="https://doi.org/10.1145/324133.324140" />
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W04-3252" />
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning sns for automated Japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/I11-1017" />
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNLP</title>
		<meeting>of IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ASPEC: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2204" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL Shared Task</title>
		<meeting>of CoNLL Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=972719.972724" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09727</idno>
		<title level="m">Neural language correction with character-based attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-" />
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-HLT</title>
		<meeting>of ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1042" />
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="380" to="386" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
