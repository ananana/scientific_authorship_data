<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Entity Clustering via Phylogenetic Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Andrews</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 N. Charles St</addrLine>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 N. Charles St</addrLine>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 N. Charles St</addrLine>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Entity Clustering via Phylogenetic Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="775" to="785"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Entity clustering must determine when two named-entity mentions refer to the same entity. Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity. In this paper, we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data. The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context. Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process. We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Variation poses a serious challenge for determin- ing who or what a name refers to. For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. Presi- dent article, including:</p><p>President Obama Barack H. Obama, Jr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Barak Obamba Barry Soetoro</head><p>To relate different names, one solution is to use specifically tailored measures of name similarity such as Jaro-Winkler similarity <ref type="bibr" target="#b38">(Winkler, 1999;</ref><ref type="bibr" target="#b9">Cohen et al., 2003)</ref>. This approach is brittle, however, and fails to adapt to the test data. Another option is to train a model like stochastic edit distance from known pairs of similar names ( <ref type="bibr" target="#b31">Ristad and Yianilos, 1998;</ref><ref type="bibr" target="#b18">Green et al., 2012)</ref>, but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar- even identical-do not necessarily corefer. Docu- ment context is needed to determine whether they may be talking about two different people.</p><p>In this paper, we propose a method for jointly (1) learning similarity between names and (2) clus- tering name mentions into entities, the two major components of cross-document coreference reso- lution systems ( <ref type="bibr" target="#b3">Baron and Freedman, 2008;</ref><ref type="bibr" target="#b17">Finin et al., 2009;</ref><ref type="bibr" target="#b29">Rao et al., 2010;</ref><ref type="bibr" target="#b33">Singh et al., 2011;</ref><ref type="bibr" target="#b24">Lee et al., 2012;</ref><ref type="bibr" target="#b18">Green et al., 2012)</ref>. Our model is an evolutionary generative process based on the name variation model of , which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the fre- quent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process:</p><p>Taylor Swift → T-Swift → T-Swizzle Our model learns without supervision that these all refer to the the same entity. Such creative spellings are especially common on Twitter and other so- cial media; we give more examples of coreferents learned by our model in Section 8.4.</p><p>Our primary contributions are improvements on  for the entity clustering task. Their inference procedure only clustered types (dis- tinct names) rather than tokens (mentions in con- text), and relied on expensive matrix inversions for learning. Our novel approach features: §4.1 A topical model of which entities from previ- ously written text an author tends to mention from previously written text. §4.2 A name mutation model that is sensitive to features of the input and output characters and takes a reader's comprehension into account. §5 A scalable Markov chain Monte Carlo sam- pler used in training and inference. §7 A minimum Bayes risk decoding procedure to pick an output clustering. The procedure is applicable to any model capable of producing a posterior over coreference decisions. We evaluate our approach by comparing to sev- eral baselines on datasets from three different gen- res: Twitter, newswire, and blogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview and Related Work</head><p>Cross-document coreference resolution (CDCR) was first introduced by <ref type="bibr" target="#b2">Bagga and Baldwin (1998b)</ref>. Most approaches since then are based on the intu- itions that coreferent names tend to have "similar" spellings and tend to appear in "similar" contexts. The distinguishing feature of our system is that both notions of similarity are learned together without supervision.</p><p>We adopt a "phylogenetic" generative model of coreference. The basic insight is that coreference is created when an author thinks of an entity that was mentioned earlier in a similar context, and men- tions it again in a similar way. The author may alter the name mention string when copying it, but both names refer to the same entity. Either name may later be copied further, leading to an evolution- ary tree of mentions-a phylogeny. Phylogenetic models are new to information extraction. In com- putational historical linguistics, <ref type="bibr" target="#b7">Bouchard-Côté et al. (2013)</ref> have also modeled the mutation of strings along the edges of a phylogeny; but for them the phylogeny is observed and most mentions are not, while we observe the mentions only.</p><p>To apply our model to the CDCR task, we ob- serve that the probability that two name mentions are coreferent is the probability that they arose from a common ancestor in the phylogeny. So we design a Monte Carlo sampler to reconstruct likely phylo- genies. A phylogeny must explain every observed name. While our model is capable of generating each name independently, a phylogeny will gener- ally achieve higher probability if it explains similar names as being similar by mutation (rather than by coincidence). Thus, our sampled phylogenies tend to make similar names coreferent-especially long or unusual names that would be expensive to generate repeatedly, and especially in contexts that are topically similar and therefore have a higher prior probability of coreference.</p><p>For learning, we iteratively adjust our model's parameters to better explain our samples. That is, we do unsupervised training via Monte Carlo EM.</p><p>What is learned? An important component of a CDCR system is its model of name similarity <ref type="bibr" target="#b38">(Winkler, 1999;</ref><ref type="bibr" target="#b27">Porter and Winkler, 1997)</ref>, which is often fixed up front. This role is played in our sys- tem by the name mutation model, which we take to be a variant of stochastic edit distance ( <ref type="bibr" target="#b30">Ristad and Yianilos, 1996)</ref>. Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies.</p><p>Name similarity is also an important component of within-document coreference resolution, and ef- forts in that area bear resemblance to our approach. <ref type="bibr" target="#b21">Haghighi and Klein (2010)</ref> describe an "entity- centered" model where a distance-dependent Chi- nese restaurant process is used to pick previous coreferent mentions within a document. Similarly, <ref type="bibr" target="#b15">Durrett and Klein (2013)</ref> learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibili- ties, a challenging inference problem.</p><p>The second major component of CDCR is context-based disambiguation of similar or iden- tical names that refer to the same entity. Like <ref type="bibr" target="#b23">Kozareva and Ravi (2011)</ref> and <ref type="bibr" target="#b18">Green et al. (2012)</ref> we use topics as the contexts, but learn mention topics jointly with other model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generative Model of Coreference</head><p>Let x = (x 1 , . . . , x N ) denote an ordered sequence of distinct named-entity mentions in documents</p><formula xml:id="formula_0">d = (d 1 , . . . , d D )</formula><p>. We assume that each doc- ument has a (single) known language, and that its mentions and their types have been identified by a named-entity recognizer. We use the object- oriented notation x.v for attribute v of mention x.</p><p>Our model generates an ordered sequence x al- though we do not observe its order. Thus each men- tion x has latent position x.i (e.g., x 729 .i = 729). The entire corpus, including these entities, is gen- erated according to standard topic model assump- tions; we first generate a topic distribution for a document, then sample topics and words for the document ( <ref type="bibr" target="#b6">Blei et al., 2003)</ref>. However, any topic may generate an entity type, e.g. PERSON, which is then replaced by a specific name: when PERSON is generated, the model chooses a previous mention of any person and copies it, perhaps mutating its name. 1 Alternatively, the model may manufacture <ref type="bibr">1</ref> We make the closed-world assumption that the author is a name for a new person, though the name itself may not be new.</p><p>If all previous mentions were equally likely, this would be a Chinese Restaurant Process (CRP) in which frequently mentioned entities are more likely to be mentioned again ("the rich get richer"). We refine that idea by saying that the current topic, lan- guage, and document influence the choice of which previous mention to copy, similar to the distance- dependent CRP ( <ref type="bibr" target="#b5">Blei and Frazier, 2011)</ref>. <ref type="bibr">2</ref> This will help distinguish multiple John Smith entities if they tend to appear in different contexts.</p><p>Formally, each mention x is derived from a par- ent mention x.p where x.p.i &lt; x.i (the parent came first), x.e = x.p.e (same entity) and x.n is a copy or mutation of x.p.n. In the special case where x is a first mention of x.e, x.p is the special symbol ♦, x.e is a newly allocated entity of some appropriate type, and the name x.n is generated from scratch.</p><p>Our goal is to reconstruct mappings p, i, z that specify the latent properties of the mentions x. The mapping p : x → x.p forms a phylogenetic tree on the mentions, with root ♦. Each entity corresponds to a subtree that is rooted at some child of ♦. The mapping i : x → x.i gives an ordering consistent with that tree in the sense that (∀x)x.p.i &lt; x.i. Finally, the mapping z : x → x.z specifies, for each mention, the topic that generated it. While i and z are not necessary for creating coref clusters, they are needed to produce p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detailed generative story</head><p>Given a few constants that are referenced in the main text, we assume that the corpus d was gener- ated as follows.</p><p>First, for each topic z = 1, . . . K and each lan- guage , choose a multinomial β z over the word vocabulary, from a symmetric Dirichlet with con- centration parameter η. Then set m = 0 (entity only aware of previous mentions from our corpus. This means that two mentions cannot be derived from a common ancestor outside our corpus. To mitigate this unrealistic assumption, we allow any ordering x of the observed mentions, not respecting document timestamps or forcing the mentions from a given document to be generated as a contiguous subsequence of x.</p><p>2 Unlike the ddCRP, our generative story is careful to pro- hibit derivational cycles: each mention is copied from a previ- ous mention in the latent ordering. This is why our phylogeny is a tree, and why our sampler is more complex. Also unlike the ddCRP, we permit asymmetric "distances": if a certain topic or language likes to copy mentions from another, the compliment is not necessarily returned. count), i = 0 (mention count), and for each docu- ment index d = 1, . . . , D:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Choose the document's length L and language</head><p>. (The distributions used to choose these are unimportant because these variables are always observed. </p><formula xml:id="formula_1">.t = w dk x.d = d x.. = x.i = i x.z = z dk x.k = k ii.</formula><p>Choose the parent x.p from a distri- bution conditioned on the attributes just set (see §4.1). iii. If x.p = ♦, increment m and set</p><p>x.e = a new entity e m . Else set x.e = x.p.e. iv. Choose x.n from a distribution con- ditioned on x.p.n and x.. (see §4.2).</p><p>Notice that the tokens w dk in document d are exchangeable: by collapsing out ψ d , we can re- gard them as having been generated from a CRP. Thus, for fixed values of the non-mention tokens and their topics, the probability of generating the mention sequence x is proportional to the prod- uct of the probabilities of the choices in step 3 at the positions dk where mentions were generated. These choices generate a topic x.z (from the CRP for document d), a type x.e.t (from β x.z ), a par- ent mention (from the distribution over previous mentions), and a name string (conditioned on the parent's name if any). §5 uses this fact to construct an MCMC sampler for the latent parts of x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sub-model for parent selection</head><p>To select a parent for a mention x of type t = x.e.t, a simple model (as mentioned above) would be a CRP: each previous mention of the same type is selected with probability proportional to 1, and ♦ is selected with probability proportional to α t &gt; 0. A larger choice of α t results in smaller entity clusters, because it prefers to create new entities of type t rather than copying old ones.</p><p>We modify this story by re-weighting ♦ and previous mentions according to their relative suit- ability as the parent of x:</p><formula xml:id="formula_2">Pr φ (x.p | x) = exp (φ · f (x.p, x)) Z(x)<label>(1)</label></formula><p>where x.p ranges over ♦ and all previous mentions of the same type as x, that is, mentions p such that p.i &lt; x.i and p.e.t = x.e.t. The normalizing con-</p><formula xml:id="formula_3">stant Z(x) def = p exp (φ · f (x.p, x)</formula><p>) is chosen so that the probabilities sum to 1. This is a conditional log-linear model parameter- ized by φ, where φ k ∼ N (0, σ 2 k ). The features f are extracted from the attributes of x and x.p. Our most important feature tests whether x.p.z = x.z. This binary feature has a high weight if authors mainly choose mentions from the same topic. To model which (other) topics tend to be selected, we also have a binary feature for each parent topic x.p.z and each topic pair (x.p.z, x.z). <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sub-model for name mutation</head><p>Let x denote a mention with parent p = x.p. As in , its name x.n is a stochastic transduction of its parent's name p.n. That is,</p><formula xml:id="formula_4">Pr θ (x.n | p.n)<label>(2)</label></formula><p>is given by the probability that applying a random sequence of edits to the characters of p.n would yield x.n. The contextual probabilities of different edits depend on learned parameters θ.</p><p>(2) is the total probability of all edit sequences that derive x.n from p.n. It can be computed in time O(|x.n| · |p.n|) by dynamic programming.</p><p>The probability of a single edit sequence, which corresponds to a monotonic alignment of x.n to p.n, is a product of individual edit probabilities of the form Pr θ (( a b ) | ˆ a), which is conditioned on the next input characterâcharacterˆcharacterâ. The edit ( a b ) replaces input a ∈ {, ˆ a} with output b ∈ {} ∪ Σ (where is the empty string and Σ is the alphabet of language x..). Insertions and deletions are the cases where respectively a = or b = -we do not allow both at once. All other edits are substitutions. Whenâ WhenˆWhenâ is the special end-of-string symbol #, the only allowed edits are the insertion ( b ) and the substi- tution ( # # ). We define the edit probability using a locally normalized log-linear model:</p><formula xml:id="formula_5">Pr θ (( a b ) | ˆ a) = exp(θ · f (ˆ a, a, b)) a ,b exp(θ · f (ˆ a, a , b ))<label>(3)</label></formula><p>We use a small set of simple feature functions f , which consider conjunctions of the attributes of the charactersâcharactersˆcharactersâ and b: character, character class (letter, digit, etc.), and case (upper vs. lower).</p><p>More generally, the probability (2) may also be conditioned on other variables such as on the lan- guages p.. and x..-this leaves room for a translit- eration model when x.. = p..-and on the entity type x.t. The features in (3) may then depend on these variables as well.</p><p>Notice that we use a locally normalized proba- bility for each edit. This enables faster and sim- pler training than the similar model of <ref type="bibr" target="#b13">Dreyer et al. (2008)</ref>, which uses a globally normalized probabil- ity for the whole edit sequence.</p><p>When p = ♦, we are generating a new name x.n. We use the same model, taking ♦.n to be the empty string (but with # ♦ rather than # as the end-of- string symbol). This yields a feature-based unigram language model (whose character probabilities may differ from usual insertion probabilities because they see # ♦ as the lookahead character).</p><p>Pragmatics. We can optionally make the model more sophisticated. Authors tend to avoid names x.n that readers would misinterpret (given the pre- viously generated names). The edit model thinks that Pr θ (CIA | ♦) is relatively high (because CIA is a short string) and so is Pr θ (CIA | Chuck's Ice Art). But in fact, if CIA has already been frequently used to refer to the Central Intelligence Agency, then an author is unlikely to use it for a different entity.</p><p>To model this pragmatic effect, we multiply our definition of Pr θ (x.n | p.n) by an extra fac- tor Pr(x.e | x) γ , where γ ≥ 0 is the effect strength. <ref type="bibr">5</ref> Here Pr(x.e | x) is the probability that a reader correctly identifies the entity x.e. We take this to be the probability that a reader who knows our sub-models would guess some parent having the correct entity (or ♦ if x is a first men- tion):</p><p>p :p .e=x.e w(p , x)/ p w(p , x). Here p ranges over mentions (including ♦) that precede x in the ordering i, and w(p , x)-defined later in sec. 5.3-is proportional to the posterior probabil- ity that x.p = p , given name x.n and topic x.z. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Inference by Block Gibbs Sampling</head><p>We use a block Gibbs sampler, which from an ini- tial state (p 0 , i 0 , z 0 ) repeats these steps:</p><p>1. Sample the ordering i from its conditional distribution given all other variables. 2. Sample the topic vector z likewise. 3. Sample the phylogeny p likewise. 4. Output the current sample s t = (p, i, z).</p><p>It is difficult to draw exact samples at steps 1 and 2. Thus, we sample i or z from a simpler proposal distribution, but correct the discrepancy using the Independent Metropolis-Hastings (IMH) strategy: with an appropriate probability, reject the proposed new value and instead use another copy of the current value <ref type="bibr" target="#b35">(Tierney, 1994)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Resampling the ordering i</head><p>We resample the ordering i of the mentions x, conditioned on the other variables. The current phylogeny p already defines a partial order on x, since each parent must precede its children. For instance, phylogeny (a) below requires ♦ x and ♦ y. This partial order is compatible with 2 total orderings, ♦ x y and ♦ y x. By contrast, phylogeny (b) requires the total ordering ♦ x y.</p><formula xml:id="formula_6">♦ y x (a) ♦ x y (b)</formula><p>We first sample an ordering i ♦ (the ordering of mentions with parent ♦, i.e. all mentions) uni- formly at random from the set of orderings compat- ible with the current p. (We provide details about this procedure in Appendix A.) <ref type="bibr">7</ref> However, such or- derings are not in fact equiprobable given the other variables-some orderings better explain why that phylogeny was chosen in the first place, according to our competitive parent selection model ( §4.1). To correct for this bias using IMH, we accept the proposed ordering i ♦ with probability</p><formula xml:id="formula_7">a = min 1, Pr(p, i ♦ , z, x | θ, φ) Pr(p, i, z, x | θ, φ)<label>(4)</label></formula><p>where i is the current ordering. Otherwise we reject i ♦ and reuse i for the new sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Resampling the topics z</head><p>Each context word and each named entity is asso- ciated with a latent topic. The topics of context words are assumed exchangeable, and so we re- sample them using Gibbs sampling ( <ref type="bibr" target="#b19">Griffiths and Steyvers, 2004</ref>). Unfortunately, this is prohibitively expensive for the (non-exchangeable) topics of the named men- tions x. A Gibbs sampler would have to choose a new value for x.z with probability proportional to the resulting joint probability of the full sample. This probability is expensive to evaluate because changing x.z will change the probability of many edges in the current phylogeny p. (Equation (1) puts x is in competition with other parents, so ev- ery mention y that follows x must recompute how happy it is with its current parent y.p.)</p><p>Rather than resampling one topic at a time, we re- sample z as a block. We use a proposal distribution for which block sampling is efficient, and use IMH to correct the error in this proposal distribution.</p><p>Our proposal distribution is an undirected graph- ical model whose random variables are the topics z and whose graph structure is given by the current phylogeny p:</p><formula xml:id="formula_8">Q(z) ∝ x =♦ Ψ x (x.z)Ψ x.p,x (x.p.z, x.z) (5)</formula><p>Q(z) is an approximation to the posterior distri- bution over z. As detailed below, a proposal can be sampled from Q(z) in time O(|z|K 2 ) where K is the number of topics, because the only interac- tions among topics are along the edges of the tree p. The unary factor Ψ x gives a weight for each possible value of x.z, and the binary factor Ψ x.p,x gives a weight for each possible value of the pair (x.p.z, x.z).</p><p>The Ψ x (x.z) factors in (5) approximate the topic model's prior distribution over z. Ψ x (x.z) is pro- portional to the probability that a Gibbs sampling step for an ordinary topic model would choose this value of x.z. This depends on whether-in the current sample-x.z is currently common in x's document and x.t is commonly generated by x.z. It ignores the fact that we will also be resampling the topics of the other mentions.</p><p>The Ψ x.p,x factors in (5) approximate Pr(p | z, i) (up to a constant factor), where p is the current phylogeny. Specifically, Ψ x.p,x approximates the probability of a single edge. It ought to be given by (1), but we use only the numerator of (1), which avoids modeling the competition among parents.</p><p>We sample from Q using standard methods, sim- ilar to sampling from a linear-chain CRF by run- ning the backward algorithm followed by forward sampling. Specifically, we run the sum-product algorithm from the leaves up to the root ♦, at each node x computing the following for each topic z:</p><formula xml:id="formula_9">β x (z) def = Ψ x (z) · y∈children(x) z Ψ x,y (z, z ) · β y (z )</formula><p>Then we sample from the root down to the leaves, first sampling ♦.z from β ♦ , then at each x = ♦ sampling the topic x.z to be z with probability proportional to Ψ x.p,x (x.p.z, z) · β x (z).</p><p>Again we use IMH to correct for the bias in Q: we accept the resulting proposaî z with probability</p><formula xml:id="formula_10">min 1, Pr(p, i, ˆ z, x | θ, φ) Pr(p, i, z, x | θ, φ) · Q(z) Q( ˆ z)<label>(6)</label></formula><p>While Pr(p, i, ˆ z, x | θ, φ) might seem slow to compute because it contains many factors (1) with different denominators Z(x), one can share work by visiting the mentions x in their order i. Most summands in Z(x) were already included in Z(x ), where x is the latest previous mention having the same attributes as x (e.g., same topic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Resampling the phylogeny p</head><p>It is easy to resample the phylogeny. For each x, we must choose a parent x.p from among the possible parents p (having p.i &lt; x.i and p.e.t = x.e.t).</p><p>Since the ordering i prevents cycles, the resulting phylogeny p is indeed a tree.</p><p>Given the topics z, the ordering i, and the ob- served names, we choose an x.p value according to its posterior probability. This is proportional to w(x.p, x) def = Pr φ (x.p | x) · Pr θ (x.n | x.p.n), independent of any other mention's choice of par- ent. The two factors here are given by <ref type="formula" target="#formula_2">(1)</ref> and <ref type="formula" target="#formula_4">(2)</ref> respectively. As in the previous section, the de- nominators Z(x) in the Pr(x.p | x) factors can be computed efficiently with shared work. With the pragmatic model (section 4.2), the par- ent choices are no longer independent; then the samples of p should be corrected by IMH as usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Initializing the sampler</head><p>The initial sampler state (z 0 , p 0 , i 0 ) is obtained as follows. <ref type="formula" target="#formula_2">(1)</ref> We fix topics z 0 via collapsed Gibbs sampling ( <ref type="bibr" target="#b19">Griffiths and Steyvers, 2004</ref>). The sam- pler is run for 1000 iterations, and the final sam- pler state is taken to be z 0 . This process treats all topics as exchangeable, including those associated with named entities.(2) Given the topic assignment z 0 , initialize p 0 to the phylogeny rooted at ♦ that maximizes x log w(x.p, x). This is a maximum rooted directed spanning tree problem that can be solved in time O(n 2 ) <ref type="bibr" target="#b34">(Tarjan, 1977)</ref>. The weight w(x.p, x) is defined as in section 5.3-except that since we do not yet have an ordering i, we do not restrict the possible values of x.p to mentions p with p.i &lt; x.p.i. (3) Given p 0 , sample an ordering i 0 using the procedure described in §5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parameter Estimation</head><p>Evaluating the likelihood and its partial derivatives with respect to the parameters of the model requires marginalizing over our latent variables. As this marginalization is intractable, we resort to Monte Carlo EM procedure ( <ref type="bibr" target="#b25">Levine and Casella, 2001</ref>) which iterates the following two steps: E-step: Collect samples by MCMC simulation as in §5, given current model parameters θ and φ. M-step: Improve θ and φ to increase 8</p><formula xml:id="formula_11">L def = 1 S S s=1</formula><p>log Pr θ,φ (x, p s , i s , z s )</p><p>It is not necessary to locally maximize L at each M-step, merely to improve it if it is not already at a local maximum <ref type="bibr" target="#b10">(Dempster et al., 1977</ref>). We improve it by a single update: at the tth M-step, we update our parameters to Φ t = (θ t , φ t )</p><formula xml:id="formula_13">Φ t = Φ t−1 + εΣ t Φ L(x, Φ t−1 )<label>(8)</label></formula><p>where ε is a fixed scaling term and Σ t is an adap- tive learning rate given by AdaGrad ( <ref type="bibr" target="#b14">Duchi et al., 2011</ref>). We now describe how to compute the gradient Φ L. The gradient with respect to the parent se-lection parameters φ is</p><formula xml:id="formula_14">1 S   f (p, x) − p Pr φ (p | x)f (p , x)   (9)</formula><p>The outer summation ranges over all edges in the S samples. The other variables in (9) are associ- ated with the edge being summed over. That edge explains a mention x as a mutation of some parent p in the context of a particular sample (p s , i s , z s ).</p><p>The possible parents p range over ♦ and the men- tions that precede x according to the ordering i s , while the features f and distribution Pr φ depend on the topics z s . As for the mutation parameters, let c p,x be the fraction of samples in which p is the parent of x. This is the expected number of times that the string p.n mutated into x.n. Given this weighted set of string pairs, let c ˆ a,a,b be the expected number of times that edit ( a b ) was chosen in contextâcontextˆcontextâ: this can be computed using dynamic programming to marginalize over the latent edit sequence that maps p.n to x.n, for each (p, x). The gradient of L with respect to θ isâ</p><formula xml:id="formula_15">isˆisâ,a,b c ˆ a,a,b (f (ˆ a, a, b) − a ,b Pr θ (a , b | ˆ a)f (ˆ a, a , b ))<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Consensus Clustering</head><p>From a single phylogeny p, we deterministically obtain a clustering e by removing the root ♦. Each of the resulting connected components corresponds to a cluster of mentions. Our model gives a distribu- tion over phylogenies p (given observations x and learned parameters Φ)-and thus gives a posterior distribution over clusterings e, which can be used to answer various queries. A traditional query is to request a single cluster- ing e. We prefer the clustering e * that minimizes Bayes risk (MBR) ( <ref type="bibr" target="#b4">Bickel and Doksum, 1977)</ref>:</p><formula xml:id="formula_16">e * = argmin e e L(e , e) Pr(e | x, θ, φ) (11)</formula><p>This minimizes our expected loss, where L(e , e) denotes the loss associated with picking e when the true clustering is e. In practice, we again esti- mate the expectation by sampling e values.</p><p>The Rand index <ref type="bibr" target="#b28">(Rand, 1971</ref>)-unlike our actual evaluation measure-is an efficient choice of loss function L for use with (11):</p><formula xml:id="formula_17">R(e , e) def = TP + TN TP + FP + TN + FN = TP + TN N 2</formula><p>where the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) use the clustering e to evaluate how well e classi- fies the N 2 mention pairs as coreferent or not.</p><p>More similar clusterings achieve larger R, with R(e , e) = 1 iff e = e. In all cases, 0 ≤ R(e , e) = R(e, e ) ≤ 1.</p><p>The MBR decision rule for the (negated) Rand index is easily seen to be equivalent to</p><formula xml:id="formula_18">e * = argmax e E[TP] + E[TN]<label>(12)</label></formula><p>= argmax</p><formula xml:id="formula_19">e i,j: x i ∼x j s ij + i,j: x i ∼x j (1 − s ij )</formula><p>where ∼ denotes coreference according to e . As explained above, the s ij are coreference probabil- ities s ij that can be estimated from a sample of clusterings e. This objective corresponds to min-max graph cut ( <ref type="bibr" target="#b11">Ding et al., 2001</ref>), an NP-hard problem with an approximate solution ( <ref type="bibr" target="#b26">Nie et al., 2010)</ref>. <ref type="bibr">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>In this section, we describe experiments on three different datasets. Our main results are described first: Twitter features many instances of name vari- ation that we would like our model to be able to learn. We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits. We report additional experiments on the ACE 2008 cor- pus, and on a political blog corpus, to demonstrate that our approach is applicable in different settings.</p><p>For Twitter and ACE 2008, we report the stan- dard B 3 metric ( <ref type="bibr" target="#b1">Bagga and Baldwin, 1998a</ref>). For the political blog dataset, the reference does not consist of entity annotations, and so we follow the evaluation procedure of <ref type="bibr" target="#b40">Yogatama et al. (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Twitter</head><p>Data. We use a novel corpus of Twitter posts dis- cussing the 2013 Grammy Award ceremony. This is a challenging corpus, featuring many instances of name variation. The dataset consists of five splits (by entity), the smallest of which is 604 mentions and the largest is 1374. We reserve the largest split for development purposes, and report our results on the remaining four. Appendix B provides more detail about the dataset.</p><p>Baselines. We use the discriminative entity cluster- ing algorithm of <ref type="bibr" target="#b18">Green et al. (2012)</ref> as our baseline; their approach was found to outperform another generative model which produced a flat cluster- ing of mentions via a Dirichlet process mixture model. Their method uses Jaro-Winkler string sim- ilarity to match names, then clusters mentions with matching names (for disambiguation) by compar- ing their unigram context distributions using the Jenson-Shannon metric. We also compare to the EXACT-MATCH baseline, which assigns all strings with the same name to the same entity.</p><p>Procedure. We run four test experiments in which one split is used to pick model hyperparameters and the remaining three are used for test. For the discriminative baseline, we tune the string match threshold, context threshold, and the weight of the context model prior (all via grid search). For our model, we tune only the fixed weight of the root feature, which determines the precision/recall trade- off (larger values of this feature result in more attachments to ♦ and hence more entities). We leave other hyperparameters fixed: 16 latent top- ics, and Gaussian priors N (0, 1) on all log-linear parameters. For PHYLO, the entity clustering is the result of (1) training the model using EM, (2) sampling from the posterior to obtain a distribu- tion over clusterings, and (3) finding a consensus clustering. We use 20 iterations of EM with 100 samples per E-step for training, and use 1000 sam- ples after training to estimate the posterior. We report results using three variations of our model: PHYLO does not consider mention context (all men- tions effectively have the same topic) and deter- mines mention entities from a single sample of p (the last); PHYLO+TOPIC adds context ( §5.2); PHYLO+TOPIC+MBR uses the full posterior and consensus clustering to pick the output clustering ( §7). Our results are shown in <ref type="table">Table 1. 10</ref> 10 Our single-threaded implementation took around 15 min- utes per fold of the Twitter corpus on a personal laptop with a 2.3 Ghz Intel Core i7 processor (including time required to parse the data files). Typical acceptance rates for ordering and topic proposals ranged from 0.03 to 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Test B 3 P R F1</head><p>EXACT-MATCH 99.6 53.7 69.8 <ref type="bibr" target="#b18">Green et al. (2012)</ref> 92  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Newswire</head><p>Data. We use the ACE 2008 dataset, which is described in detail in <ref type="bibr" target="#b18">Green et al. (2012)</ref>. It is split into a development portion and a test portion. The baseline system took the first mention from each (gold) within-document coreference chain as the canonical mention, ignoring other mentions in the chain; we follow the same procedure in our experiments. 11</p><p>Baselines &amp; Procedure. We use the same base- lines as in §8.1. On development data, modeling pragmatics as in §4.2 gave large improvements for organizations (8 points in F-measure), correcting the tendency to assume that short names like CIA were coincidental homonyms. Hence we allowed γ &gt; 0 and tuned it on development data. 12 Results are in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Blogs</head><p>Data. The CMU political blogs dataset consists of 3000 documents about U.S. politics ( <ref type="bibr" target="#b39">Yano et al., 2009)</ref>. Preprocessed as described in <ref type="bibr" target="#b40">Yogatama et al. (2012)</ref>, the data consists of 10647 entity mentions.</p><p>Unlike our other datasets, mentions are not anno- tated with entities: the reference consists of a table of 126 entities, where each row is the canonical name of one entity.</p><p>Baselines. We compare to the system results reported in <ref type="figure" target="#fig_0">Figure 2</ref> of <ref type="bibr" target="#b40">Yogatama et al. (2012)</ref>. This includes a baseline hierarchical clustering ap- proach, the "EEA" name canonicalization system of <ref type="bibr" target="#b16">Eisenstein et al. (2011)</ref>, as well the model pro- posed by <ref type="bibr" target="#b40">Yogatama et al. (2012)</ref>. Like the output of our model, the output of their hierarchical clus- tering baseline is a mention clustering, and there- fore must be mapped to a  <ref type="figure" target="#fig_0">Figure 2</ref> of <ref type="bibr" target="#b40">Yogatama et al. (2012)</ref>, which is specif- ically designed for the task of canonicalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Discussion</head><p>On the Twitter dataset, we obtained a 12.6-point F1 improvement over the baseline. To understand our model's behavior, we looked at the sampled phy- logenetic trees on development data. One reason our model does well in this noisy domain is that it is able to relate seemingly dissimilar names via successive steps. For instance, our model learned to relate many variations of LL Cool J:</p><p>Cool James LLCoJ El-El Cool John</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LL LL COOL JAMES LLCOOLJ</head><p>In the sample we inspected, these mentions were also assigned the same topic, further boosting the probability of the configuration. The ACE dataset, consisting of editorialized newswire, naturally contains less name variation than Twitter data. Nonetheless, we find that the variation that does appear is often properly handled by our model. For instance, we see several in- stances of variation due to transliteration that were all correctly grouped together, such as Megawati Soekarnoputri and Megawati Sukarnoputri. The prag- matic model was also effective in grouping com- mon acronyms into the same entity.</p><p>We found that multiple samples tend to give dif- ferent phylogenies (so the sampler is mobile), but essentially the same clustering into entities (which is why consensus clustering did not improve much over simply using the last sample). Random restarts of EM might create more variety by choosing dif- ferent locally optimal parameter settings. It may also be beneficial to explore other sampling tech- niques <ref type="bibr" target="#b8">(Bouchard-Côté, 2014</ref>).</p><p>Our method assembles observed names into an evolutionary tree. However, the true tree must in- clude many names that fall outside our small ob- served corpora, so our model would be a more appropriate fit for a far larger corpus. Larger cor- pora also offer stronger signals that might enable our Monte Carlo methods to mix faster and detect regularities more accurately.</p><p>A common error of our system is to connect mentions that share long substrings, such as dif- ferent PERSONs who share a last name, or differ- ent ORGANIZATIONs that contain University of. A more powerful name mutation than the one we use here would recognize entire words, for example inserting a common title or replacing a first name with its common nickname. Modeling the internal structure of names <ref type="bibr" target="#b22">(Johnson, 2010;</ref><ref type="bibr" target="#b16">Eisenstein et al., 2011;</ref><ref type="bibr" target="#b40">Yogatama et al., 2012</ref>) in the mutation model is a promising future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>Our primary contribution consists of new model- ing ideas, and associated inference techniques, for the problem of cross-document coreference resolu- tion. We have described how writers systematically plunder (φ) and then systematically modify (θ) the work of past writers. Inference under such models could also play a role in tracking evolving memes and social influence, not merely in establishing strict coreference. Our model also provides an al- ternative to the distance-dependent CRP. <ref type="bibr">2</ref> Our implementation is available for re- search use at: https://bitbucket.org/ noandrews/phyloinf.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>) 2 .</head><label>2</label><figDesc>Choose its topic distribution ψ d from an asymmetric Dirichlet prior with parameters m (Wallach et al., 2009). 3 3. For each token position k = 1, . . . , L: (a) Choose a topic z dk ∼ ψ d . (b) Choose a word conditioned on the topic and language, w dk ∼ β z dk . (c) If w dk is a named entity type (PERSON, PLACE, ORG, . . . ) rather than an ordinary word, then increment i and: i. create a new mention x with x.e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Results for the ACE 2008 newswire dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>table of canonical</head><label>of</label><figDesc></figDesc><table>entity 
names to compare to the reference table. 

Procedure &amp; Results We tune our method as in 
previous experiments, on the initialization data 
used by Yogatama et al. (2012) which consists of 
a subset of 700 documents of the full dataset. The 
tuned model then produced a mention clustering 
on the full political blog corpus. As the mapping 
from clusters to a table is not fully detailed in Yo-
gatama et al. (2012), we used a simple heuristic: 
the most frequent name in each cluster is taken as 
the canonical name, augmented by any titles from 
a predefined list appearing in any other name in 
the cluster. The resulting table is then evaluated 
against the reference, as described in Yogatama et 
al. (2012). We achieved a response score of 0.17 
and a reference score of 0.61. Though not state-of-
the-art, this result is close to the score of the "EEA" 
system of Eisenstein et al. (2011), as reported in 
</table></figure>

			<note place="foot" n="3"> Extension: This choice could depend on the language d...</note>

			<note place="foot" n="4"> Many other features could be added. In a multilingual setting, one would similarly want to model whether English authors select Arabic mentions. One could also imagine features that reward proximity in the generative order (x.p.i ≈ x.i), local linguistic relationships (when x.p.d = x.d and x.p.k ≈ x.k), or social information flow (e.g., from mainstream media to Twitter). One could also make more specific versions of any feature by conjoining it with the entity type t.</note>

			<note place="foot" n="5"> Currently we omit the step of renormalizing this deficient model. Our training procedure also ignores the extra factor.</note>

			<note place="foot" n="6"> Better, one could integrate over the reader&apos;s guess of x.z. 7 The full version of this paper is available at http://cs.jhu.edu/ ˜ noa/publications/ phylo-acl-14.pdf</note>

			<note place="foot" n="8"> We actually do MAP-EM, which augments (7) by adding the log-likelihoods of θ and φ under a Gaussian prior.</note>

			<note place="foot" n="9"> In our experiments, we run the clustering algorithm five times, initialized from samples chosen at random from the last 10% of the sampler run, and keep the clustering that achieved highest expected Rand score.</note>

			<note place="foot" n="11"> That is, each within-document coreference chain is mapped to a single mention as a preprocessing step. 12 We used only a simplified version of the pragmatic model, approximating w(p , x) as 1 or 0 according to whether p .n = x.n. We also omitted the IMH step from section 5.3. The other results we report do not use pragmatics at all, since we found that it gave only a slight improvement on Twitter.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Name phylogeny: A generative model of string variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="344" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entitybased cross-document coreferencing using the vector space model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="79" to="85" />
		</imprint>
	</monogr>
	<note>ACL &apos;98. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who is who and what is what: Experiments in crossdocument co-reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjorie</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mathematical Statistics : Basic Ideas and Selected Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjell</forename><forename type="middle">A</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doksum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Holden-Day, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distance dependent chinese restaurant processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">I</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2488" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automated reconstruction of ancient languages using probabilistic models of sound change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo (SMC) for Bayesian phylogenetics. Bayesian phylogenetics: methods, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparison of string metrics for matching names and records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on data cleaning and object consolidation</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A min-max cut algorithm for graph partitioning and data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE International Conference on</title>
		<meeting>IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
	<note>Data Mining</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Carmen: A twitter geolocation system with applications to public health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
	<note>Honolulu, Hawaii, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured databases of named entities from bayesian nonparametrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP &apos;11</title>
		<meeting>the First Workshop on Unsupervised Learning in NLP, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using Wikitology for cross-document entity coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Learning by Reading and Learning to Read</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entity clustering across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">To link or not to link? a study on end-to-end tweet entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Kıcıman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1020" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1148" to="1157" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised name ambiguity resolution using a generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP &apos;11</title>
		<meeting>the First Workshop on Unsupervised Learning in NLP, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint entity and event coreference resolution across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLPCoNLL</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implementations of the Monte Carlo EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="422" to="439" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved minmax cut graph clustering with nonnegative relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Q</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dijun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<editor>José L. Balcázar, Francesco Bonchi, Aristides Gionis, and Michèle Sebag</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6322</biblScope>
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Approximate String Comparison and its Effect on an Advanced Record Linkage System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="190" to="199" />
		</imprint>
	</monogr>
	<note>U.S. Bureau of the Census</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Streaming cross document entity coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delip</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1050" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning string edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sven Ristad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Yianilos</surname></persName>
		</author>
		<idno>CS-TR- 532-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>Princeton University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning string edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sven Ristad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="532" />
			<date type="published" when="1998-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale cross-document coreference using distributed inference and hierarchical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R E</forename><surname>Tarjan</surname></persName>
		</author>
		<title level="m">Finding optimum branchings. Networks</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Markov Chains for Exploring Posterior Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Tierney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1701" to="1728" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking lda: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A discriminative hierarchical model for fast coreference at large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The state of record linkage and current research problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>U.S. Census Bureau</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Statistical Research Division</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting response to political blog posts with topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A probabilistic model for canonicalizing named entity mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchuan</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
