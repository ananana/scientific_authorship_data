<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hippocratic Abbreviation Expansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<addrLine>79 Ninth Avenue</addrLine>
									<postCode>10011</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<addrLine>79 Ninth Avenue</addrLine>
									<postCode>10011</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hippocratic Abbreviation Expansion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="364" to="369"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Incorrect normalization of text can be particularly damaging for applications like text-to-speech synthesis (TTS) or typing auto-correction, where the resulting nor-malization is directly presented to the user, versus feeding downstream applications. In this paper, we focus on abbreviation expansion for TTS, which requires a &quot;do no harm&quot;, high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations un-expanded. In the context of a large-scale, real-world TTS scenario, we present methods for training classifiers to establish whether a particular expansion is apt. We achieve a large increase in correct abbreviation expansion when combined with the baseline text normalization component of the TTS system, together with a substantial reduction in incorrect expansions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text normalization <ref type="bibr" target="#b18">(Sproat et al., 2001</ref>) is an im- portant initial phase for many natural language and speech applications. The basic task of text normal- ization is to convert non-standard words (NSWs) -numbers, abbreviations, dates, etc. -into stan- dard words, though depending on the task and the domain a greater or lesser number of these NSWs may need to be normalized. Perhaps the most de- manding such application is text-to-speech synthe- sis (TTS) since, while for parsing, machine trans- lation and information retrieval it may be accept- able to leave such things as numbers and abbre- viations unexpanded, for TTS all tokens need to be read, and for that it is necessary to know how to pronounce them. Which normalizations are re- quired depends very much on the application.</p><p>What is also very application-dependent is the cost of errors in normalization. For some applica- tions, where the normalized string is an interme- diate stage in a larger application such as trans- lation or information retrieval, overgeneration of normalized alternatives is often a beneficial strat- egy, to the extent that it may improve the accu- racy of what is eventually being presented to the user. In other applications, such as TTS or typing auto-correction, the resulting normalized string it- self is directly presented to the user; hence errors in normalization can have a very high cost relative to leaving tokens unnormalized.</p><p>In this paper we concentrate on abbreviations, which we define as alphabetic NSWs that it would be normal to pronounce as their expansion. This class of NSWs is particularly common in personal ads, product reviews, and so forth. For example:</p><p>home health care svcs stat home health llc osceola aquatic ctr stars rating write audi vw repair ser quality and customer</p><p>Each of the examples above contains an abbrevi- ation that, unlike, e.g., conventionalized state ab- breviations such as ca for California, is either only slightly standard (ctr for center) or not standard at all (ser for service).</p><p>An important principle in text normalization for TTS is do no harm. If a system is unable to re- liably predict the correct reading for a string, it is better to leave the string alone and have it default to, say, a character-by-character reading, than to expand it to something wrong. This is particularly true in accessibility applications for users who rely on TTS for most or all of their information needs. Ideally a navigation system should read turn on 30N correctly as turn on thirty north; but if it can- not resolve the ambiguity in 30N, it is far better to read it as thirty N than as thirty Newtons, since lis- teners can more easily recover from the first kind of error than the second.</p><p>We present methods for learning abbreviation expansion models that favor high precision (incor- rect expansions &lt; 2%). Unannotated data is used to collect evidence for contextual disambiguation and to train an abbreviation model. Then a small amount of annotated data is used to build models to determine whether to accept a candidate expan-sion of an abbreviation based on these features. The data we report on are taken from Google Maps TM and web pages associated with its map en- tries, but the methods can be applied to any data source that is relatively abbreviation rich.</p><p>We note in passing that similar issues arise in automatic spelling correction work <ref type="bibr">(WilcoxO'Hearn et al., 2008)</ref>, where it is better to leave a word alone than to "correct" it wrongly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There has been a lot of interest in recent years on "normalization" of social media such as Twitter, but that work defines normalization much more broadly than we do here ( <ref type="bibr" target="#b21">Xia et al., 2006;</ref><ref type="bibr" target="#b5">Choudhury et al., 2007;</ref><ref type="bibr" target="#b11">Kobus et al., 2008;</ref><ref type="bibr" target="#b2">Beaufort et al., 2010;</ref><ref type="bibr" target="#b10">Kaufmann, 2010;</ref><ref type="bibr" target="#b16">Pennell and Liu, 2011;</ref><ref type="bibr" target="#b1">Aw and Lee, 2012;</ref><ref type="bibr" target="#b13">Liu et al., 2012a;</ref><ref type="bibr" target="#b14">Liu et al., 2012b;</ref><ref type="bibr" target="#b8">Hassan and Menezes, 2013;</ref><ref type="bibr" target="#b23">Yang and Eisenstein, 2013)</ref>. There is a good reason for us to focus more narrowly. For Twit- ter, much of the normalization task involves non- standard language such as ur website suxx brah (from <ref type="bibr" target="#b23">Yang and Eisenstein (2013)</ref>). Expanding the latter to your website sucks, brother certainly nor- malizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviations in that case is neutral with respect to preserving the intent of the original text.</p><p>One other difference between the work we re- port from much of the recent work cited above is that that work focuses on getting high F scores, whereas we are most concerned with getting high precision. While this may seem like a trivial trade off between precision and recall, our goal motivates developing measures that minimize the "risk" of expanding a term, something that is im- portant in an application such as TTS, where one cannot correct a misexpansion after it is spoken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Since our target application is text-to-speech, we define the task in terms of an existing TTS lexi- con. If a word is already in the lexicon, it is left unprocessed, since there is an existing pronuncia- tion for it; if a word is out-of-vocabulary (OOV), we consider expanding it to a word in the lexicon. We consider a possible expansion for an abbrevi- ation to be any word in the lexicon from which the abbreviation can be derived by only deletion of letters. <ref type="bibr">1</ref> For present purposes we use the Google English text-to-speech lexicon, consisting of over 430 thousand words. Given an OOV item (possi- ble abbreviation) in context, we make use of fea- tures of the context and of the OOV item itself to enumerate and score candidate expansions.</p><p>Our data consists of 15.1 billion words of text data from Google Maps TM , lower-cased and tok- enized to remove punctuation symbols. We used this data in several ways. First, we used it to boot- strap a model for assigning a probability of an ab- breviation/expansion pair. Second, we used it to extract contextual n-gram features for predicting possible expansions. Finally, we sampled just over 14 thousand OOV items in context and had them manually labeled with a number of categories, in- cluding 'abbreviation'. OOVs labeled as abbrevia- tions were also labeled with the correct expansion. We present each of these uses in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Abbreviation modeling</head><p>We collect potential abbreviation/full-word pairs by looking for terms that could be abbreviations of full words that occur in the same context. Thus: the svc/service center heating clng/cooling system dry clng/cleaning system contributes evidence that svc is an abbreviation of service. Similarly instances of clng in con- texts that can contain cooling or cleaning are evi- dence that clng could be an abbreviation of either of these words. (The same contextual information of course is used later on to disambiguate which of the expansions is appropriate for the context.) To compute the initial guess as to what can be a possible abbreviation, a Thrax grammar <ref type="bibr" target="#b17">(Roark et al., 2012</ref>) is used that, among other things, speci- fies that: the abbreviation must start with the same letter as the full word; if a vowel is deleted, all ad- jacent vowels should also be deleted; consonants may be deleted in a cluster, but not the last one; and a (string) suffix may be deleted. <ref type="bibr">2</ref> We count a pair of words as 'co-occurring' if they are ob- served in the same context. For a given context C, e.g., the center, let W C be the set of words found in that context. Then, for any pair of words u, v, we can assign a pair count based on the count of contexts where both occur:</p><formula xml:id="formula_0">c(u, v) = |{C : u ∈ W C and v ∈ W C }| blvd boulevard rd road</formula><p>yrs years ca california fl florida ctr center mins minutes def definitely ste suite <ref type="table">Table 1</ref>: Examples of automatically mined abbrevia- tion/expansion pairs.</p><p>Let c(u) be defined as v c(u, v). From these counts, we can define a 2×2 table and calculate statistics such as the log likelihood statistic <ref type="bibr" target="#b7">(Dunning, 1993</ref>), which we use to rank possible abbre- viation/expansion pairs. Scores derived from these type (rather than token) counts highly rank pairs of in-vocabulary words and OOV possible abbrevia- tions that are substitutable in many contexts.</p><p>We further filter the potential abbreviations by removing ones that have a lot of potential expan- sions, where we set the cutoff at 10. This removes mostly short abbreviations that are highly ambigu- ous. The resulting ranked list of abbreviation ex- pansion pairs is then thresholded before building the abbreviation model (see below) to provide a smaller but more confident training set. For this paper, we used 5-gram contexts (two words on ei- ther side) to extract abbreviations and their expan- sions. See <ref type="table">Table 1</ref> for some examples.</p><p>Our abbreviation model is a pair character lan- guage model (LM), also known as a joint multi- gram model <ref type="bibr" target="#b3">(Bisani and Ney, 2008)</ref>, whereby aligned symbols are treated as a single token and a smoothed n-gram model is estimated. This de- fines a joint distribution over input and output sequences, and can be efficiently encoded as a weighted finite-state transducer. The extracted abbreviation/expansion pairs are character-aligned and a 7-gram pair character LM is built over the alignments using the OpenGrm n-gram library <ref type="bibr" target="#b17">(Roark et al., 2012</ref>). For example: c:c :e :n t:t :e r:r Note that, as we've defined it, the alignments from abbreviation to expansion allow only identity and insertion, no deletions or substitutions. The cost from this LM, normalized by the length of the ex- pansion, serves as a score for the quality of a pu- tative expansion for an abbreviation.</p><p>For a small set of frequent, conventionalized abbreviations (e.g., ca for California -63 pairs in total -mainly state abbreviations and similar items), we assign an fixed pair LM score, since these examples are in effect irregular cases, where the regularities of the productive abbreviation pro- cess do not capture their true cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contextual features</head><p>To predict the expansion given the context, we ex- tract n-gram observations for full words in the TTS lexicon. We do this in two ways. First, we sim- ply train a smoothed n-gram LM from the data. Because of the size of the data set, this is heav- ily pruned using relative entropy pruning <ref type="bibr" target="#b19">(Stolcke, 1998)</ref>. Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram con- texts for words. Space precludes a detailed treat- ment of these two statistics, but, briefly, both can be derived from contingency table values calcu- lated from the frequencies of <ref type="formula">(1)</ref> </p><note type="other">the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to de- rive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in ei- ther position.</note><p>We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Manual annotations</head><p>We randomly selected 14,434 OOVs in their full context, and had them manually annotated as falling within one of 8 categories, along with the expansion if the category was 'abbreviation'. Note that these are relatively lightweight annotations that do not require extensive linguistics expertise. The abbreviation class is defined as cases where pronouncing as the expansion would be normal. Other categories included letter sequence (expan- sion would not be normal, e.g., TV); partial let- ter sequence (e.g., PurePictureTV); misspelling; leave as is (part of a URL or pronounced as a word, e.g., NATO); foreign; don't know; and junk. Abbreviations accounted for nearly 23% of the cases, and about 3/5 of these abbreviations were instances from the set of 63 conventional abbrevi- ation/expansion pairs mentioned in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Abbreviation expansion systems</head><p>We have three base systems that we compare here. The first is the hand-built TTS normalization sys- tem. This system includes some manually built patterns and an address parser to find common ab- breviations that occur in a recognizable context. For example, the grammar covers several hundred city-state combinations, such as Fairbanks AK, yielding good performance on such cases.</p><p>The other two systems were built using data ex- tracted as described above. Both systems make use of the pair LM outlined in Section 3.1, but differ in how they model context. The first sys-tem, which we call "N-gram", uses a pruned <ref type="bibr" target="#b9">Katz (1987)</ref> smoothed trigram model. The second sys- tem, which we call "SVM", uses a Support Vec- tor Machine <ref type="bibr" target="#b6">(Cortes and Vapnik, 1995</ref>) to classify candidate expansions as being correct or not. For both systems, for any given input OOV, the pos- sible expansion with the highest score is output, along with the decision of whether to expand.</p><p>For the "N-gram" system, n-gram negative log probabilities are extracted as follows. Let w i be the position of the target expansion. We extract the part of the n-gram probability of the string that is not constant across all competing expansions, and normalize by the number of words in that window. Thus the score of the word is:</p><formula xml:id="formula_1">S(w i ) = − 1 k + 1 i+k j=i log P(w j | w j−1 w j−2 )</formula><p>In our experiments, k = 2 since we have a trigram model, though in cases where the target word is the last word in the string, k = 1, because there only the end-of-string symbol must be predicted in ad- dition to the expansion. We then take the Bayesian fusion of this model with the pair LM, by adding them in the log space, to get prediction from both the context and abbreviation model.</p><p>For the "SVM" model, we extract features from the log likelihood and log odds scores associated with contextual n-grams, as well as from the pair LM probability and characteristics of the abbrevi- ation itself. We train a linear model on a subset of the annotated data (see section 4). Multiple con- textual n-grams may be observed, and we take the maximum log likelihood and log odds scores for each candidate expansion in the observed context. We then quantize these scores down into 16 bins, using the histogram in the training data to define bin thresholds so as to partition the training in- stances evenly. We also create 16 bins for the pair LM score. A binary feature is defined for each bin that is set to 1 if the current candidate's score is less than the threshold of that bin, otherwise 0. Thus multiple bin features can be active for a given candidate expansion of the abbreviation.</p><p>We also have features that fire for each type of contextual feature (e.g., trigram with expansion as middle word, etc.), including 'no context', where none of the trigrams or bigrams from the current example that include the candidate expansion are present in our list. Further, we have features for the length of the abbreviation (shorter abbrevia- tions have more ambiguity, hence are more risky to expand); membership in the list of frequent, conventionalized abbreviations mentioned earlier; and some combinations of these, along with bias features. We train the model using standard op- tions with Google internal SVM training tools.</p><p>Note that the number of n-grams in the two models differs. The N-gram system has around 200M n-grams after pruning; while the SVM model uses around a quarter of that. We also tried a more heavily pruned n-gram model, and the re- sults are only very slightly worse, certainly accept- able for a low-resource scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We split the 3,209 labeled abbreviations into a training set of 2,209 examples and a held aside de- velopment set of 1,000 examples. We first evaluate on the development set, then perform a final 10- fold cross validation over the entire set of labeled examples. We evaluate in terms of the percent- age of abbreviations that were correctly expanded (true positives, TP) and that were incorrectly ex- panded (false positives, FP).</p><p>Results are shown in <ref type="table" target="#tab_0">Table 2</ref>. The first two rows show the baseline TTS system and SVM model. On the development set, both systems have a false positive rate near 3%, i.e., three abbreviations are expanded incorrectly for every 100 examples; and over 50% true positive rate, i.e., more than half of the abbreviations are expanded correctly. To re- port true and false positive rates for the N-gram system we would need to select an arbitrary de- cision threshold operating point, unlike the deter- ministic TTS baseline and the SVM model with its decision threshold of 0. Rather than tune such a meta-parameter to the development set, we instead present an ROC curve comparison of the N-gram and SVM models, and then propose a method for "intersecting" their output without requiring a tuned decision threshold. <ref type="figure">Figure 1</ref> presents an ROC curve for the N-gram and SVM systems, and for the simple Bayesian fusion (sum in log space) of their scores. We can see that the SVM model has very high precision for its highest ranked examples, yielding nearly 20% of the correct expansions without any in- correct expansions. However the N-gram system achieves higher true positive rates when the false  </p><note type="other">0 1 2 3 4 0 10 20 30 40 50 60 N-­gram N-­gram N-­gram SVM SVM SVM SVM + N-­gram SVM + N-­gram SVM + N-­gram SVM intersect N-­gram SVM intersect N-­gram SVM intersect N-­gram Incorrect expansion percentage (FP)</note><p>Correct expansion percentage (TP)</p><p>Figure 1: ROC curve plotting true positive (correct expan- sion) percentages versus false positive (incorrect expansion) percentages for several systems on the development set.</p><p>at the SVM's decision threshold corresponding to around 3.3% false positive rate. The simple com- bination of their scores achieves strong improve- ments over either model, with an operating point associated with the SVM decision boundary that yields a couple of points improvement in true pos- itives and a full 1% reduction in false positive rate. One simple way to combine these two system outputs in a way that does not require tuning a de- cision threshold is to expand the abbreviation if and only if (1) both the SVM model and the N- gram model agree on the best expansion; and <ref type="formula">(2)</ref> the SVM model score is greater than zero. In a slight abuse of the term 'intersection', we call this combination 'SVM intersect N-gram' (or 'SVM \ N-gram' in <ref type="table" target="#tab_0">Table 2</ref>). Using this approach, our true positive rate on the dev set declines a bit to just over 50%, but our false positive rate declines over two full percentage points to 1.1%, yielding a very high precision system. Taking this very high precision system combi- nation of the N-gram and SVM models, we then combine with the baseline TTS system as follows.</p><p>First we apply our system, and expand the item if it scores above threshold; for those items left un- expanded, we let the TTS system process it in its own way. In this way, we actually reduce the false positive rate on the dev set over the baseline TTS system by over 1% absolute to less than 2%, while increase of 18.5% Of course, at whether an OOV we also looked at of the collected d neously suggests a the 11,157 examp non-abbreviations, panded 45 items, of 0.4% under the should be expande found that 20% of of abbreviations th During system mented with a num sion approaches th ing in detail here, ber of expansion ca guage model score; pansion when at le text is present for and CART tree (B with real valued s very high precision leaving many more found that, for use line TTS system, l positive rate were a tem with substanti higher FP rates, sin then passed along u tem, with its relativ To ensure that w tems to the dev se performed 10-fold set of abbreviation in <ref type="table" target="#tab_0">Table 2</ref>. Most no has a much lower tr systems achieve pe the development se with the TTS base than the numbers o</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Notes for internal r • Maybe more likelihood and positive rate falls between 1 and 3 percent, though both systems reach roughly the same performance at the SVM's decision threshold corresponding to around 3.3% false positive rate. The simple com- bination of their scores achieves strong improve- ments over either model, with an operating point associated with the SVM decision boundary that yields a couple of points improvement in true pos- itives and a full 1% reduction in false positive rate. One simple way to combine these two system outputs in a way that does not require tuning a de- cision threshold is to expand the abbreviation if and only if (1) both the SVM model and the N- gram model agree on the best expansion; and (2) the SVM model score is greater than zero. In a slight abuse of the term 'intersection', we call this combination 'SVM intersect N-gram' (or 'SVM ∩ N-gram' in <ref type="table" target="#tab_0">Table 2</ref>). Using this approach, our true positive rate on the development set declines a bit to just over 50%, but our false positive rate declines over two full percentage points to 1.1%, yielding a very high precision system.</p><p>Taking this very high precision system combi- nation of the N-gram and SVM models, we then combine with the baseline TTS system as follows. First we apply our system, and expand the item if it scores above threshold; for those items left un- expanded, we let the TTS system process it in its own way. In this way, we actually reduce the false positive rate on the development set over the base- line TTS system by over 1% absolute to less than 2%, while also increasing the true positive rate to 73.5%, an increase of 18.5% absolute.</p><p>Of course, at test time, we will not know whether an OOV is an abbreviation or not, so we also looked at the performance on the rest of the collected data, to see how often it erro- neously suggests an expansion from that set. Of the 11,157 examples that were hand-labeled as non-abbreviations, our SVM ∩ N-gram system ex- panded 45 items, which is a false positive rate of 0.4% under the assumption that none of them should be expanded. In fact, manual inspection found that 20% of these were correct expansions of abbreviations that had been mis-labeled.</p><p>We also experimented with a number of alter- native high precision approaches that space pre- cludes our presenting in detail here, including: pruning the number of expansion candidates based on the pair LM score; only allowing abbreviation expansion when at least one extracted n-gram con- text is present for that expansion in that context; and CART tree <ref type="bibr" target="#b4">(Breiman et al., 1984)</ref> training with real valued scores. Some of these yielded very high precision systems, though at the cost of leaving many more abbreviations unexpanded. We found that, for use in combination with the baseline TTS system, large overall reductions in FP rate were achieved by using an initial system with substantially higher TP and somewhat higher FP rates, since far fewer abbreviations were then passed along unexpanded to the baseline system, with its relatively high 3% FP rate.</p><p>To ensure that we did not overtune our systems to the development set through experimentation, we performed 10-fold cross validation over the full set of abbreviations. These results are presented in <ref type="table" target="#tab_0">Table 2</ref>. Most notably, the TTS baseline system has a much lower true positive rate; yet we find our systems achieve performance very close to that for the development set, so that our final combination with the TTS baseline was actually slighly better than the numbers on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we have presented methods for high precision abbreviation expansion for a TTS appli- cation. The methods are largely self-organizing, using in-domain unannotated data, and depend on only a small amount of annotated data. Since the SVM features relate to general properties of ab- breviations, expansions and contexts, the classi- fier parameters will likely carry over to new (En- glish) domains. We demonstrate that in combi- nation with a hand-built TTS baseline, the meth- ods afford dramatic improvement in the TP rate (to about 74% from a starting point of about 40%) and a reduction of FP to below our goal of 2%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Figure 1: ROC curve plotting true positive (correct expansion) percentages versus false positive (incorrect expansion) percentages for several systems on the development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on held-out labeled data, and with final 

10-fold cross-validation over the entire labeled set. Percent-
age of abbreviations expanded correctly (TP) and percentage 
expanded incorrectly (FP) are reported for each system. 

367 

</table></figure>

			<note place="foot" n="1"> We do not deal here with phonetic spellings in abbreviations such as 4get, or cases where letters have been transposed due to typographical errors (scv). 2 This Thrax grammar can be found at http://openfst.cs.nyu.edu/twiki/bin/ view/Contrib/ThraxContrib</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Daan van Esch and the Google Speech Data Operations team for their work on preparing the annotated data. We also thank the reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Categorical data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Agresti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Personalized normalization for a multilingual chat system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Ti</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Hau Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A hybrid rule/model-based finite-state framework for normalizing SMS messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Beaufort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Roekhaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise-Amélie</forename><surname>Cougnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédrick</forename><surname>Fairon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointsequence models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Bisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="434" to="451" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<pubPlace>Wadsworth &amp; Brooks, Pacific Grove CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Investigation and modeling of the structure of texting language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudesha</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="157" to="174" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supportvector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Social text normalization using contextual graph random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recogniser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Syntactic normalization of Twitter messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kaufmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on NLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Normalizing SMS: are two metaphors better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Kobus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Damnati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Manchester, UK, August. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A broad-coverage normalization system for social media language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint inference of named entity recognition and normalization for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="526" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fightin&apos;words: Lexical feature selection and evaluation for identifying the content of political conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burt L Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin M</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="372" to="403" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A character-level machine translation approach for normalization of SMS abbreviations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP. Papers/pennellliu3.pdf</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The OpenGrm opensource finite-state grammar software libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Normalization of non-standard words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="333" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Entropy-based pruning of backoff language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>DARPA Broadcast News Transcription and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-word spelling correction with trigrams: A reconsideration of the Mays, Damerau, and Mercer model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-O&amp;apos;</forename><surname>Hearn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Budanitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">4919</biblScope>
			<biblScope unit="page" from="605" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A phonetic-based approach to Chinese chat text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Sydney, Australia, July. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
	<note>Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A log-linear model for unsupervised text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
