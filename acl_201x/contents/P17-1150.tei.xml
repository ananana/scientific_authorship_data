<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
						</author>
						<title level="a" type="main">Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1634" to="1644"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1150</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs. Grounded verb semantics are acquired mainly based on the parallel data of the use of a verb phrase and its corresponding sequences of primitive actions demonstrated by humans. The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored. To address this limitation, this paper presents a new interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn models for grounded verb semantics. The proposed approach uses reinforcement learning to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the long-term reward. Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties. Compared to previous work, the models acquired from interactive learning result in a 48% to 145% performance gain when applied in new situations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In communication with cognitive robots, one of the challenges is that robots do not have sufficient linguistic or world knowledge as humans do. For example, if a human asks a robot to boil the wa- ter but the robot has no knowledge what this verb phrase means and how this verb phrase relates to its own actuator, the robot will not be able to exe- cute this command. Thus it is important for robots to continuously learn the meanings of new verbs and how the verbs are grounded to its underlying action representations from its human partners.</p><p>To support learning of grounded verb seman- tics, previous works <ref type="bibr" target="#b19">Misra et al., 2015;</ref><ref type="bibr" target="#b27">She and Chai, 2016</ref>) rely on multiple in- stances of human demonstrations of correspond- ing actions. From these demonstrations, robots capture the state change of the environment caused by the actions and represent verb semantics as the desired goal state. One advantage of such state-based representation is that, when robots en- counter the same verbs/commands in a new situa- tion, the desired goal state will trigger the action planner to automatically plan a sequence of prim- itive actions to execute the command.</p><p>While the state-based verb semantics provides an important link to connect verbs to the robot's actuator, previous works also present several limi- tations. First of all, previous approaches were de- veloped under the assumption of perfect percep- tion of the environment <ref type="bibr" target="#b19">Misra et al., 2015;</ref><ref type="bibr" target="#b27">She and Chai, 2016)</ref>. However, this assumption does not hold in real-world situated interaction. The robot's representation of the envi- ronment is often incomplete and error-prone due to its limited sensing capabilities. Thus it is not clear whether previous approaches can scale up to handle noisy and incomplete environment.</p><p>Second, most previous works rely on multi- ple demonstration examples to acquire grounded verb models. Each demonstration is simply a sequence of primitive actions associated with a verb. No other type of interaction between humans and robots is explored. Previous cognitive stud- ies ( <ref type="bibr" target="#b3">Bransford et al., 2000</ref>) on how people learn have shown that social interaction (e.g., conver-sation with teachers) can enhance student learn- ing experience and improve learning outcomes. For robotic learning, previous work <ref type="bibr" target="#b4">(Cakmak and Thomaz, 2012</ref>) has also demonstrated the neces- sity of question answering in the learning process. Thus, in our view, interactive learning beyond demonstration of primitive actions should play a vital role in the robot's acquisition of more reliable models of grounded verb semantics. This is es- pecially important because the robot's perception of the world is noisy and incomplete, human lan- guage can be ambiguous, and the robot may lack the relevant linguistic or world knowledge during the learning process.</p><p>To address these limitations, we have developed a new interactive learning approach where robots actively engage with humans to acquire models of grounded verb semantics. Our approach ex- plores the space of interactive question answering between humans and robots during the learning process. In particular, motivated by previous work on robot learning <ref type="bibr" target="#b4">(Cakmak and Thomaz, 2012)</ref>, we designed a set of questions that are pertinent to verb semantic representations. We further ap- plied reinforcement learning to learn an optimal policy that guides the robot in deciding when to ask what questions. Our empirical results have shown that this interactive learning process leads to more reliable representations of grounded verb semantics, which contribute to significantly better action performance in new situations. When the environment is noisy and uncertain (as in a realis- tic situation), the models acquired from interactive learning result in a performance gain between 48% and 145% when applied in new situations. Our re- sults further demonstrate that the interaction pol- icy acquired from reinforcement learning leads to the most efficient interaction and the most reliable verb models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To enable human-robot communication and col- laboration, recent years have seen an increasing amount of works which aim to learn semantics of language that are grounded to agents' percep- tion <ref type="bibr" target="#b9">(Gorniak and Roy, 2007;</ref><ref type="bibr" target="#b32">Tellex et al., 2014;</ref><ref type="bibr" target="#b10">Kim and Mooney, 2012;</ref><ref type="bibr" target="#b16">Matuszek et al., 2012a;</ref><ref type="bibr" target="#b14">Liu et al., 2014;</ref><ref type="bibr" target="#b13">Liu and Chai, 2015;</ref><ref type="bibr" target="#b34">Thomason et al., 2015</ref><ref type="bibr" target="#b33">Thomason et al., , 2016</ref>) and action <ref type="bibr" target="#b17">(Matuszek et al., 2012b;</ref><ref type="bibr" target="#b0">Artzi and Zettlemoyer, 2013;</ref><ref type="bibr" target="#b18">Misra et al., 2014</ref><ref type="bibr" target="#b19">Misra et al., , 2015</ref><ref type="bibr" target="#b27">She and Chai, 2016)</ref>. Specif- ically for verb semantics, recent works explored the connection between verbs and action plan- ning ( <ref type="bibr" target="#b18">Misra et al., 2014</ref><ref type="bibr" target="#b19">Misra et al., , 2015</ref><ref type="bibr" target="#b27">She and Chai, 2016)</ref>, for example, by represent- ing grounded verbs semantics as the desired goal state of the physical world that is a result of the corresponding actions. Such representations are learned based on example actions demonstrated by humans. Once acquired, these representations will allow agents to interpret verbs/commands is- sued by humans in new situations and apply action planning to execute actions. Given its clear advan- tage in connecting verbs with actions, our work also applies the state-based representation for verb semantics. However, we have developed a new ap- proach which goes beyond learning from demon- strated examples by exploring how rich interaction between humans and agents can be used to acquire models for grounded verb semantics. This approach was motivated by previous cog- nitive studies ( <ref type="bibr" target="#b3">Bransford et al., 2000</ref>) on how peo- ple learn as well as recent findings on robot skill learning <ref type="bibr" target="#b4">(Cakmak and Thomaz, 2012)</ref>. One of the principles for human learning is that "learning is enhanced through socially supported interac- tions". Studies have shown that social interaction with teachers and peers (e.g., substantive conver- sation) can enhance student learning experience and improve learning outcomes. In recent work on interactive robot learning of new skills <ref type="bibr" target="#b4">(Cakmak and Thomaz, 2012)</ref>, researchers identified three types of questions that can be used by a hu- man/robot student to enhance learning outcomes: 1) demonstration query (i.e., asking for a full or partial demonstration of the task), 2) label query (i.e., asking whether an execution is correct), and 3) feature query (i.e., asking for a specific feature or aspect of the task). Inspired by these previous findings, our work explores interactive learning to acquire grounded verb semantics. In particular, we aim to address when to ask what questions during interaction to improve learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Acquisition of Grounded Verb Semantics</head><p>This section gives a brief review on acquisition of grounded verb semantics and illustrates the differ- ences between previous approaches and our ap- proach using interactive learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">State-based Representation</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the verb semantics (e.g., boil(x)) is represented by the goal state (e.g., Status(x, T empHigh)) which is the result of the demonstrated primitive actions. Given the verb phrase boil the water (i.e., L i ), the human teaches the robot how to accomplish the corresponding ac- tion based on a sequence of primitive actions − → A i . By comparing the final environment E i with the initial environment E i , the robot is able to iden- tify the state change of the environment, which be- comes a hypothesis of goal state to represent verb semantics. Compared to procedure-based repre- sentations, the state-based representation supports automated planning at the execution time. It is environment-independent and more generalizable. In <ref type="bibr" target="#b27">(She and Chai, 2016)</ref>, instead of one hypoth- esis, it maintains a specific-to-general hypothesis space as shown in <ref type="figure" target="#fig_1">Figure 2</ref> to capture all goal hy- potheses of a particular verb frame. Specifically, it assumes that one verb frame may lead to differ- ent outcomes under different environments, where each possible outcome is represented by one node in the hierarchical graph and each node is a con- junction of multiple atomic fluents. <ref type="bibr">1</ref> Given a language command (i.e., a verb phrase), a robot will engage in the following processes:</p><p>• Execution. In this process, the robot will se- lect a hypothesis from the space of hypothe- ses that is most relevant to the current situa- tion and use the corresponding goal state to plan for actions to execute.</p><p>• Learning. When the robot fails to select a hypothesis or fails to execute the action, it will ask the human for a demonstration.</p><p>1 In this work, we assume the set of atomic fluents rep- resenting environment state are given and do not address the question of whether these predicates are adequate to represent a domain. Based on the demonstrated actions, the robot will learn a new representation (i.e., new nodes) and update the hypothesis space. Previous works represent the environment E i as a conjunction of grounded state fluents. Each flu- ent consists of a predicate and one or more argu- ments (i.e., objects in the physical world, or ob- ject status), representing one aspect of the per- ceived environment. An example of a fluent is "Has(Kettle1, W AT ER)" meaning object <ref type="bibr">Kettle1</ref> has some water inside, where Has is the predi- cate, and Kettle1 and W AT ER are arguments. The set of fluents include the status of the robot (e.g.,  <ref type="table">Table 1</ref>: Examples to show differences between learning through demonstrations as in the previous works <ref type="bibr" target="#b27">(She and Chai, 2016)</ref> and the proposed learning from interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noisy Environment</head><p>ronment has a perfect, deterministic representa- tion, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This is clearly not the case in the realistic physical world. In reality, given limitations of sensor capabili- ties, the environment representation is often par- tial, error prone, and full of uncertainties. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example of a more realistic representa- tion where each fluent comes with a confidence between 0 and 1 to indicate how likely that par- ticular fluent can be detected in the current envi- ronment. Thus, it is unclear whether the previous work is able to handle representations with uncer- tainties. Our interactive learning approach aims to address these uncertainties through interactive question answering with human partners. <ref type="figure" target="#fig_3">Figure 4</ref> shows a general framework for interac- tive learning of action verbs. It aims to support a life-long learning cycle for robots, where the robot can continuously (1) engage in collaboration and communication with humans based on its exist- ing knowledge; (2) acquire new verbs by learn- ing from humans and experiencing the change of the world (i.e., grounded verb semantics as in this work); and (3) learn how to interact (i.e., update interaction policies). The lifelong learning cycle is composed by a sequence of interactive learn- ing episodes (Episode 1, 2...) where each episode consists of either an execution phase or a learning phase or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interactive Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework of Interactive Learning</head><p>The execution phase starts with a human request for action (e.g., boil the water). According to its interaction policy, the robotic agent may choose to ask one or more questions (i.e., Q + i ) and wait for human answers (i.e., A + i ), or select a hypothesis from its existing knowledge base to execute the command (i.e., Execute). With the human feed- back of the execution, the robot can update its in- teraction policy and existing knowledge.</p><p>In the learning phase, the robot can initiate the learning by requesting a demonstration from the human. After the human performs the task, the robotic agent can either choose to update its knowledge if it feels confident, or it can choose to ask the human one or more questions before up- dating its knowledge. <ref type="table">Table 1</ref> illustrates the differences between the pre- vious approach that acquires verb models based solely on demonstrations and our current work that acquires models based on interactive learning. As shown in <ref type="table">Table 1</ref>, under the demonstration setting, humans only provide a demonstration of primitive actions and there's no interactive question answer- ing. In the interactive learning setting, the robot can proactively choose to ask questions regard- ing the uncertainties either about the environment (e.g., R1), the goal (e.g., R2), or the demonstra- tions (e.g., R6). Our hypothesis is that rich inter- actions based on question answering will allow the robot to learn more reliable models for grounded verb semantics, especially in a noisy environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Examples of Interactive Learning</head><p>Then the question is how to manage such inter- action: when to ask and what questions to ask to most efficiently acquire reliable models and apply them in execution. Next we describe the appli- cation of reinforcement learning to manage inter- active question answering for both the execution phase and the learning phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Formulation of Interactive Learning</head><p>Markov Decision Process (MDP) and its closely related Reinforcement Learning (RL) have been applied to sequential decision-making problems in dynamic domains with uncertainties, e.g., dialogue/interaction management ( <ref type="bibr" target="#b29">Singh et al., 2002;</ref><ref type="bibr" target="#b24">Paek and Pieraccini, 2008;</ref><ref type="bibr" target="#b35">Williams and Zweig, 2016)</ref>, mapping language commands to actions <ref type="bibr" target="#b2">(Branavan et al., 2009)</ref>, interactive robot learning ( <ref type="bibr" target="#b11">Knox and Stone, 2011</ref>), and interactive information retrieval ( <ref type="bibr" target="#b12">Li et al., 2017)</ref>. In this work, we formulate the choice of when to ask what ques- tions during interaction as a sequential decision- making problem and apply reinforcement learning to acquire an optimal policy to manage interaction.</p><p>Specifically, each of the execution and learning phases is governed by one policy (i.e., θ E and θ D ), which is updated by the reinforcement learning al- gorithm. The use of RL intends to obtain opti- mal policies that can lead to the highest long-term reward by balancing the cost of interaction (e.g., the length of interaction and difficulties of ques- tions) and the quality of the acquired models. The reinforcement formulation for both the execution phase and the learning phase are described below.</p><p>State For the execution phase, each state s e ∈ S E is a five tuple: s e = &lt;l, e, KB, Grd, Goal&gt;. l is a language command, including a verb and multiple noun phrases extracted by the Stan- ford parser. For example, the command "Mi- crowave the ramen" is represented as l = microwave(ramen). The environment e is a probabilistic representation of the currently per- ceived physical world, consisting of a set of grounded fluents and the confidence of perceiv- ing each fluent (an example is shown in <ref type="figure" target="#fig_2">Figure 3)</ref>. KB stands for the existing knowledge of verb models. Grd accounts for the agent's current be- lief of object grounding: the probability of each noun in the l being grounded to different objects. Goal represents the agent's belief of different goal state hypotheses of the current command. Within one interaction episode, command l and knowl- edge KB will stay the same, while e, Grd, and Goal may change accordingly due to interactive question answering and robot actions. In the ex- ecution phase, Grd and Goal are initialized with existing knowledge of learned verb models. For the learning phase, a state s d ∈ S D is a four tu- ple: s d = &lt;l, e start , e end , Grd&gt;. e start and e end stands for the environment before the demonstra- tion and after the demonstration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action</head><p>Motivated by previous studies on how humans ask questions while learning new skills <ref type="bibr" target="#b4">(Cakmak and Thomaz, 2012)</ref>, the agent's question set includes two categories: yes/no ques- tions and wh-questions. These questions are designed to address ambiguities in noun phrase grounding, uncertain environment sensing, and goal states. They are domain independent in nature. For example, one of the questions is np grd ynq(n, o). It is a yes/no question asking whether the noun phrase n refers to an object o (e.g., "I see a silver object, is that the pot?"). Other questions are env pred ynq(p) (i.e., whether a fluent p is present in the environment; e.g., "Is the microwave door open?") and goal pred ynq(p) (i.e., whether a predicate p should be part of the goal; "Should the pot be on a pot stand?"). Ta- ble 2 lists all the actions available in the execu- tion and learning phases. The select hypo action (i.e., select a goal hypothesis to execute) is only for the execution. Ideally, after asking questions, the agent should be more likely to select a goal hy-Action Name</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explanation</head><p>Question Example Reward</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">np grd whq(n)</head><p>Ask for the grounding of a np. "Which is the cup, can you show me?" -6.5 1 2. np grd ynq(n, o) Confirm the grounding of a np. "I see a silver object, is that the pot?" -1. pothesis that best describes the current situation. For the learning phase, the include fluent(∧p) ac- tion forms a goal hypothesis by conjoining a set of fluents ps where each p should have high probabil- ity of being part of the goal. Transition The transition function takes action a in state s, and gives the next state s according to human feedback. Note that the command l does not change during interaction. But the agent's be- lief of environment e, object grounding Grd, and goal hypotheses Goal is changed according to the questions and human answers. For example, sup- pose the agent asks whether noun phrase n refers to the object o, if the human confirms it, the prob- ability of n being grounded to o becomes 1.0, oth- erwise it will become 0.0. Reward Finding a good reward function is a hard problem in reinforcement learning. Our cur- rent approach has followed the general practice in the spoken dialogue community ( <ref type="bibr" target="#b26">Schatzmann et al., 2006;</ref><ref type="bibr" target="#b30">Su et al., 2016</ref>). The immediate robot questions are assigned small costs to favor shorter and more efficient interac- tion. Furthermore, motivated by how humans ask <ref type="bibr">1</ref> According to the study in <ref type="bibr" target="#b4">(Cakmak and Thomaz, 2012)</ref>, the frequency of y/n questions used by humans is about 6.5 times the frequency of open questions (wh question), which motivates our assignment of -6.5 to wh questions.</p><p>2 bulk np grd ynq asks multiple object grounding all at once. This is harder to answer than asking for a single np. Therefore, its cost is assigned three times of the other yes/no questions. Choose a ∼ P (a |s ; θ) with greedy; questions <ref type="bibr" target="#b4">(Cakmak and Thomaz, 2012)</ref>, yes/no questions are easier for a human to answer than the open questions (e.g., wh-questions) and thus are given smaller costs. A large positive reward is given at the end of interaction when the task is completed successfully. Detailed reward assign- ment for different actions are shown in <ref type="table">Table 2</ref>.</p><formula xml:id="formula_0">δ ← r + γ · θ T · φ(s , a ) − θ T · φ(s, a); 5 θ ← θ + δ · η · φ(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning</head><p>The SARSA algorithm with linear function approximation is utilized to update poli- cies θ E and θ D <ref type="bibr" target="#b31">(Sutton and Barto, 1998)</ref>. Specif- ically, the objective of training is to learn an opti- mal value function Q(s, a) (i.e., the expected cu-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features shared by both phases</head><p>If a is a np grd whq(n). The entropy of candidate groundings of n. If n has more than 4 grounding candidates. If a is a np grd ynq(n, o). The probability of n grounded to o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Features specific for the Execution phase</head><p>If a is a select hypo(h) action. The probability of hypo h not satisfied in current envi- ronment. Similarity between the ns used by command l and the commands from previous experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Features specific for the Learning phase</head><p>If a is a pred change ynq(p). The probability of p been changed by demo.  <ref type="table">Table 2</ref>  Example features used by the two phases are listed in <ref type="table" target="#tab_1">Table 3</ref>. These features in- tend to capture different dimensions of informa- tion such as specific types of questions, how well noun phrases are grounded to the environment, un- certainties of the environment, and consistencies between a hypothesis and the current environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Dataset. To evaluate our approach, we utilized the benchmark made available by <ref type="bibr" target="#b19">(Misra et al., 2015)</ref>. Individual language commands and corre- sponding action sequences are extracted similarly as <ref type="bibr" target="#b27">(She and Chai, 2016)</ref>. This dataset includes common tasks in the kitchen and living room do- mains, where each data instance comes with a lan- guage command (e.g., "boil the water", "throw the beer into the trashcan") and the correspond- ing sequence of primitive actions. In total, there are 979 instances, including 75 different verbs and 215 different noun phrases. The length of primi- tive action sequences range from 1 to 51 with an average of 4.82 (+/-4.8). We divided the dataset into three groups: (1) 200 data instances were used by reinforcement learning to acquire optimal inter- action policies; (2) 600 data instances were used by different approaches (i.e., previous approaches and our interactive learning approach) to acquire grounded verb semantics models; and (3) 179 data instances were used as testing data to evaluate the learned verb models. The performance on apply- ing the learned models to execute actions for the testing data is reported.</p><p>To learn interaction policies, a simulated human model is created from the dataset ( <ref type="bibr" target="#b26">Schatzmann et al., 2006</ref>) to continuously interact with the robot learner <ref type="bibr">3</ref> . This simulated user can answer the robot's different types of questions and make decisions on whether the robot's execution is correct. During policy learning, one data instance can be used multiple times. At each time, the in- teraction sequence is different due to exploitation and exploration in RL in selecting the next action. The RL discount factor γ is set to 0.99, the in - greedy is 0.1, and the learning rate is 0.01.</p><p>Noisy Environment Representation. The origi- nal data provided by <ref type="bibr" target="#b19">(Misra et al., 2015</ref>) is based on the assumption that environment sensing is per- fect and deterministic. To enable incomplete and noisy environment representation, for each fluent (e.g., grasping(Cup 3 ), near(robot 1 , Cup 3 )) in the original data, we independently sampled a con- fidence value to simulate the likelihood that a par- ticular fluent can be detected correctly from the environment. We applied the following four dif- ferent variations in sampling the confidence val- ues, which correspond to different levels of sensor reliability.</p><p>(1) PerfectEnv represents the most reliable sensor. If a fluent is true in the original data, its sampled confidence is 1, and 0 otherwise. (2) NormStd3 represents a relatively reliable sen- sor. For each fluent in the original environment, a confidence is sampled according to a normal dis- tribution N (1, 0.3 2 ) with an interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. This distribution has a large probability of sampling a number larger than 0.5, meaning the correspond- ing fluent is still more likely to be true. (3) NormStd5 represents a less reliable sensor. The sampling distribution is N (1, 0.5 2 ), which has a larger probability of generating a number smaller than 0.5 compared to NormStd3.</p><p>(4) UniEnv represents an unreliable sensor. Each number is sampled with a uniform distribution be- tween 0 and 1. This means the sensor works ran- domly. A fluent has a equal change to be true or false no matter what the true environment is. Evaluation Metrics. We used the same evalua- tion metrics as in the previous works <ref type="bibr" target="#b19">(Misra et al., 2015;</ref><ref type="bibr" target="#b27">She and Chai, 2016)</ref> to evaluate the perfor- mance of applying the learned models to testing instances on action planning.</p><p>• IED: Instruction Editing Distance. This is a number between 0 and 1 measuring the sim- ilarity between the predicted action sequence and the ground-truth action sequence. IED equals 1 if the two sequences are exactly the same.</p><p>• SJI: State Jaccard Index. This is a num- ber between 0 and 1 measuring the similarity between the predicted and the ground-truth state changes. SJI equals 1 if action planning leads to exactly the same state change as in the ground-truth.</p><p>Configurations. To understand the role of interac- tive learning in model acquisition and action plan- ning, we first compared the interactive learning ap- proach with the previous leading approach (pre- sented as She16). To further evaluate the interac- tion policies acquired by reinforcement learning, we also compared the learned policy (i.e., RLPol- icy) with the following two baseline policies:</p><p>• RandomPolicy which randomly selects ques- tions to ask during interaction.</p><p>• ManualPolicy which continuously asks for yes/no confirmations (i.e., object grounding questions (GroundQ), environment ques- tions (EnvQ), goal prediction questions (GoalQ)) until there's no more questions be- fore making a decision on model acquisition or action execution.    environment conditions. When the environment becomes noisy (i.e., NormStd3, NormStd5, and UniEnv), the performance of She16 that only relies on demonstrations decreases significantly. While the interactive learning improves the performance under the perfect environment condition, its effect in noisy environment is more remarkable. It leads to a significant performance gain between 48% and 145%. These results validate our hypothe- sis that interactive question answering can help to alleviate the problem of uncertainties in environ- ment representation and goal prediction. <ref type="figure" target="#fig_9">Figure 5</ref> shows the performance of the vari- ous learned models on the testing data, based on a varying number of training instances and dif- ferent interaction policies. The interactive learn- ing guided by the policy acquired from RL out- performs the previous approach She16. The RL policy slightly outperforms interactive learning us- ing manually defined policy (i.e., ManualPolicy). However, as shown in the next section, the Man-  ualPolicy results in much longer interaction (i.e., more questions) than the RL acquired policy. These results further demonstrate that the policy learned from RL enables efficient interactions and the acquisition of more reliable verb models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">The Effect of Interactive Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison of Interaction Policies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Robots live in a noisy environment. Due to the limitations in their external sensors, their repre- sentations of the shared environment can be er- ror prone and full of uncertainties. As shown in previous work <ref type="bibr" target="#b22">(Mourão et al., 2012)</ref>, learning ac- tion models from the noisy and incomplete obser- vation of the world is extremely challenging. The same problem applies to the acquisition of verb se- mantics that are grounded to the perceived world.</p><p>To address this problem, this paper presents an interactive learning approach which aims to han- dle uncertainties of the environment as well as in- completeness and conflicts in state representation by asking human partners intelligent questions. The interaction strategies are learned through re- inforcement learning. Our empirical results have shown a significant improvement in model acqui- sition and action prediction. When applying the learned models in new situations, the models ac- quired through interactive learning leads to over 140% performance gain in noisy environment.</p><p>The current investigation also has several lim- itations. As in previous works, we assume the world can be described by a closed set of predi- cates. This causes significant simplification for the physical world. One of the important questions to address in the future is how to learn new predicates through interaction with humans. Another limita- tion is that the current utility function is learned based on a set of pre-identified features. Future work can explore deep neural network to alleviate feature engineering.</p><p>As cognitive robots start to enter our daily lives, data-driven approaches to learning may not be possible in new situations. Human partners who work side-by-side with these cognitive robots are great resources that the robots can directly learn from. Recent years have seen an increasing amount of work on task learning from human part- ners ( <ref type="bibr" target="#b25">Saunders et al., 2006;</ref><ref type="bibr" target="#b6">Chernova and Veloso, 2008;</ref><ref type="bibr" target="#b5">Cantrell et al., 2012;</ref><ref type="bibr" target="#b20">Mohan et al., 2013;</ref><ref type="bibr" target="#b1">Asada et al., 2009;</ref><ref type="bibr" target="#b21">Mohseni-Kabir et al., 2015;</ref><ref type="bibr" target="#b23">Nejati et al., 2006</ref>; . Our future work will incorporate interactive learning of verb semantics with task learning to enable autonomy that can learn by communicating with humans.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of acquiring state-based representation for verb semantics based on an initial environment E i , and a language command L i , the primitive action sequence − → A i demonstrated by the human, and the final environment E i that results from the execution of − → A i in E i .</figDesc><graphic url="image-1.png" coords="3,94.68,62.81,408.19,74.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example hypothesis space for the verb frame f ill(x, y).</figDesc><graphic url="image-2.png" coords="3,325.70,213.43,181.42,140.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example probabilistic sensing result.</figDesc><graphic url="image-3.png" coords="3,314.36,486.40,204.10,51.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A general framework of robot interactive learning. KB stands for knowledge base, θ E stands for Interaction Strategy for Execution, and θ D stands for Interaction Strategy for Learning. Only demonstration(She and Chai, 2016) Interactive learning (this work) Execution Example Execution Example H1: Boil the water. H1: Boil the water. R1: (select a hypothesis and execute the action accordingly) R1: I see a silver object on the table, is that a pot? H2: (provide true/false feedback on whether the execution is H2: Yes. correct) R2: Should I leave the pot on the stove after boil? H3: Yes. R3: Ok. (select a hypothesis to execute) H4: Thank you. Learning Example Learning Example H3: Boil the water. H5: Boil the water. R3: I don't know how to boil, could you show me an example? R5: I don't know how to boil, could you show me an example? H4: (human shows the robot how to achieve the corresponding action through a sequence of primitive actions) H6: Ok. (demonstrate the task, at the end put the pot on a heat pad) R4: (indicate the acceptance and induce a hypothesis to rep-R6: So after boiling something, should I put it on a heat pad? resent verb semantics) H7: No, you don't have to. R7: I see. (induce goal state hypothesis)</figDesc><graphic url="image-4.png" coords="4,83.34,62.81,430.85,117.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>bulk np grd ynq(n, o) Confirm the grounding of multiple nps. "I think the pot is the red object and milk is in the white box, am I right?" -3.0 / -6.0 2 7. pred change ynq(p) Ask whether a predicate p has been changed by the action demonstration. "The pot is on a stand after the action, is that correct?" -1.0 / -2.0 8. include fluent(∧p) Include ∧p into the goal state representa- tion. Update the verb semantic knowledge. 100 / -2.0 Table 2: The action space for reinforcement learning, where n stands for a noun phrase, o a physical object, p a fluent representation of the current state of the world, h a goal hypothesis. Action 1 and 2 are shared by both the execution and learning phases. Action 3, 4, 5 are for the execution phase, and 6, 7, 8 are only used for the learning phase. -1.0/-2.0 are typically used for yes/no questions. When the human answers the question with a "yes", the reward is -1.0, otherwise it's -2.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 :</head><label>1</label><figDesc>Policy learning. The execution and learning phases share the same learning process, but with different state s, action a spaces, and feature vectors φ. The e end is only available to the learning phase. Input : e, l (, e end ); Feature function φ; Old policy θ (i.e., a weight vector) Verb Goal States Hypotheses H; Initialize : state s initialized with e, l (, e end ); first action a ∼ P (a|s; θ) with greedy 1 while s is not terminal do 2 Take action a, receive reward r; 3 s = T (s, a); 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>.</head><label></label><figDesc>The"If" features are binary, and the other features are real-valued. mulative reward of taking action a in a state s). This value function is approximated by a linear function Q(s, a) = θ · φ(s, a), where φ(s, a) is a feature vector and θ is a weight updated during training. Details of the algorithm is shown in Al- gorithm 1. During testing, the agent can take an action a that maximizes the Q value at a state s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance (SJI) comparison by applying models acquired based on different interaction policies to the testing data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Example features used by the two phases. a stands for action. Other notations are the same as used in</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 shows</head><label>4</label><figDesc></figDesc><table>the performance comparison on the 
testing data between the previous approach She16 
and our interactive learning approach based on en-
vironment representations with different levels of 
noise. The verb models acquired by interactive 
learning perform better consistently across all four 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance comparison between She16 
and our interactive learning based on environment 
representations with different levels of noise. All 
the improvements (marked *) are statistically sig-
nificant (p &lt; 0.01). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison between different policies including the average number (and standard deviation) 
of different types of questions asked during the execution phase and the learning phase respectively, and 
the performance on action planning for the testing data. The results are based on the noisy environment 
sampled by NormStd3. * indicates statistically significant difference (p &lt; 0.05) comparing RLPolicy 
with ManualPolicy. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 compares</head><label>5</label><figDesc></figDesc><table>the performance of different in-
teraction policies. It shows the average number of 
questions asked under different policies. It is not 
surprising the RandomPolicy has the worst perfor-
mance. For the ManualPolicy, its performance is 
similar to the RLPolicy. However, the average in-
teraction length of ManualPolicy is 6.792, which 
is much longer than the RLPolicy (which is 3.127). 
</table></figure>

			<note place="foot">Grasping(Kettle1)), the status of different objects (e.g., Status(W AT ER, T empHigh)), and relations between objects (e.g., On(Kettle1, Stove)). One limitation of the previous works is that the envi</note>

			<note place="foot" n="3"> In our future work, interacting with real humans will be conducted through Amazon Mechanical Turk. And the policies acquired with a simulated user in this work will be used as initial policies.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Science Foundation (IIS-1208390 and IIS-1617682) and the DARPA SIMPLEX program under a subcon-tract from UCLA (N66001-15-C-4035). The au-thors would like to thank Dipendra K. Misra and colleagues for providing the evaluation data, and the anonymous reviewers for valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cognitive developmental robotics: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minoru</forename><surname>Asada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koh</forename><surname>Hosoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuo</forename><surname>Kuniyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshio</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Ogino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chisato</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="34" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reinforcement learning for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA, ACL &apos;09</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How People Learn: Brain, Mind, Experience, and School: Expanded Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Bransford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><forename type="middle">R</forename><surname>Cocking</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>National Academy Press</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Designing robot learners that ask good questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction</title>
		<meeting>the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tell me when and why to do it! run-time planner model updates via natural language instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cantrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schermerhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction</title>
		<meeting>the 7th Annual ACM/IEEE International Conference on Human-Robot Interaction<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching multi-robot coordination using demonstration of communication and state sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems</title>
		<meeting>the 7th international joint conference on Autonomous agents and multiagent systems</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1183" to="1186" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collaborative models for referring expression generation in situated dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence. AAAI Press, AAAI&apos;14</title>
		<meeting>the 28th AAAI Conference on Artificial Intelligence. AAAI Press, AAAI&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1544" to="1550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Physical causality of action verbs in grounded language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Situated language understanding as filtering perceived affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gorniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Science</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="197" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised pcfg induction for grounded language learning with highly ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLP-CoNLL &apos;12)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLP-CoNLL &apos;12)<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding human teaching modalities in reinforcement learning environments: A preliminary report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2011 Workshop on Agents Learning Interactively from Human Teachers (ALIHT)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning through dialogue interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to mediate perceptual differences in situated humanrobot dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. AAAI Press, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence. AAAI Press, AAAI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2288" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic labeling for efficient referential grounding based on collaborative discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jointly learning grounded task structures from language instruction and visual demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sari</forename><surname>Saba-Sadiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Song-Chun Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1482" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<editor>John Langford and Joelle Pineau</editor>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ICML-12</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to parse natural language commands to a robot control system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Tracts in Advanced Robotics</title>
		<editor>Jaydev P. Desai, Gregory Dudek, Oussama Khatib, and Vijay Kumar</editor>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="403" to="415" />
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tell me dave: Contextsensitive grounding of natural language to manipulation instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyong</forename><surname>Dipendra K Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)<address><addrLine>Berkeley, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Environment-driven lexicon induction for high-level instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejia</forename><surname>Dipendra Kumar Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A computational model for situated task learning with interactive instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwali</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCM 2013-12th International Conference on Cognitive Modeling</title>
		<meeting>ICCM 2013-12th International Conference on Cognitive Modeling</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive hierarchical task learning from a single demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anahita</forename><surname>Mohseni-Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Annual ACM/IEEE International Conference on HumanRobot Interaction. ACM, HRI &apos;15</title>
		<meeting>the Tenth Annual ACM/IEEE International Conference on HumanRobot Interaction. ACM, HRI &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning STRIPS operators from noisy and incomplete observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Mourão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Catalina Island, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning hierarchical task networks by observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negin</forename><surname>Nejati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Konik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="665" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automating spoken dialogue management design using machine learning: An industry perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="716" to="729" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Teaching robots by moulding behavior and scaffolding the environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chrystopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerstin</forename><surname>Nehaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM SIGCHI/SIGART conference on Human-robot interaction</title>
		<meeting>the 1st ACM SIGCHI/SIGART conference on Human-robot interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Stuttle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Eng. Rev</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental acquisition of verb hypothesis space towards physical world interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Back to the blocks world: Learning new actions through situated human-robot dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanbo</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2014 Conference, The 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the SIGDIAL 2014 Conference, The 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-20" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimizing dialogue management with reinforcement learning: Experiments with the njfun system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="133" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On-line active reward learning for policy optimisation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M Rojas</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2431" to="2441" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning perceptually grounded word meanings from unaligned parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratiksha</forename><surname>Thaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="167" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning multi-modal grounded linguistic semantics by playing &quot;i spy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jivko</forename><surname>Sinapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Svetlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI16)</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence (IJCAI16)<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3477" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language commands through human-robot dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1923" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Endto-end lstm-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01269</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grounded semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
