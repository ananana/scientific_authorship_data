<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tables as Semi-structured Knowledge for Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
							<email>petert@allenai.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence Seattle</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA, WA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tables as Semi-structured Knowledge for Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="474" to="483"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Question answering requires access to a knowledge base to check facts and reason about information. Knowledge in the form of natural language text is easy to acquire , but difficult for automated reasoning. Highly-structured knowledge bases can facilitate reasoning, but are difficult to acquire. In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this trade-off. We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations, easily and efficiently via crowd-sourcing. We then use this annotated data to train a semi-structured feature-driven model for question answering that uses tables as a knowledge base. In benchmark evaluations, we significantly outperform both a strong un-structured retrieval baseline and a highly-structured Markov Logic Network model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) has emerged as a prac- tical research problem for pushing the boundaries of artificial intelligence (AI). Dedicated projects and open challenges to the research community in- clude examples such as Facebook AI Research's challenge problems for AI-complete QA ( <ref type="bibr" target="#b22">Weston et al., 2015</ref>) and the Allen Institute for AI's (AI2) Aristo project <ref type="bibr" target="#b3">(Clark, 2015)</ref> along with its recently completed Kaggle competition <ref type="bibr">1</ref> . The reason for this emergence is the diversity of core language and reasoning problems that a complex, integrated task like QA exposes: information extraction <ref type="bibr" target="#b17">(Srihari and Li, 1999</ref>), semantic modelling <ref type="bibr" target="#b16">(Shen and Lapata, 2007;</ref><ref type="bibr" target="#b12">Narayanan and Harabagiu, 2004</ref>), logic and reasoning ( <ref type="bibr" target="#b11">Moldovan et al., 2003)</ref>, and inference ( <ref type="bibr" target="#b10">Lin and Pantel, 2001</ref>).</p><p>Complex tasks such as QA require some form of knowledge base to store facts about the world and reason over them. By knowledge base, we mean any form of knowledge: structured (e.g., ta- bles, ontologies, rules) or unstructured (e.g., nat- ural language text). For QA, knowledge has been harvested and used in a number of different modes and formalisms: large-scale extracted and curated knowledge bases <ref type="bibr" target="#b5">(Fader et al., 2014</ref>), structured models such as Markov Logic Networks ( <ref type="bibr" target="#b9">Khot et al., 2015)</ref>, and simple text corpora in information retrieval approaches ( <ref type="bibr" target="#b20">Tellex et al., 2003)</ref>.</p><p>There is, however, a fundamental trade-off in the structure and regularity of a formalism and its ability to be curated, modelled or reasoned with easily. For example, simple text corpora contain no structure, and are therefore hard to reason with in a principled manner. Nevertheless, they are eas- ily and abundantly available. In contrast, Markov Logic Networks come with a wealth of theoretical knowledge connected with their usage in princi- pled inference. However, they are difficult to in- duce automatically from text or to build manually.</p><p>In this paper we explore tables as semi- structured knowledge for multiple-choice question (MCQ) answering. Specifically, we focus on ta- bles that represent general knowledge facts, with cells that contain free-form text (Secton 3 details the nature and semantics of these tables). The structural properties of tables, along with their free-form text content represents a semi-structured balanced compromise in the trade-off between de- gree of structure and ubiquity. We present two main contributions, with tables and their structural properties playing a crucial role in both. First, we crowd-source a collection of over 9000 MCQs with alignment annotations to table elements, us- ing tables as guidelines in efficient data harvest- ing. Second, we develop a feature-driven model that uses these MCQs to perform QA, while fact- checking and reasoning over tables.</p><p>Others have used tables in the context of QA. Question bank creation for tables has been inves- tigated <ref type="bibr" target="#b14">(Pasupat and Liang, 2015)</ref>, but without structural guidelines or the alignment information that we propose. Similarly, tables have been used in QA reasoning ( <ref type="bibr" target="#b24">Yin et al., 2015b;</ref><ref type="bibr" target="#b13">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b18">Sun et al., 2016</ref>) but have not explic- itly attempted to encode all the semantics of ta- ble structure (see Section 3.1). To the best of our knowledge, no previous work uses tables for both creation and reasoning in a connected framework.</p><p>We evaluate our model on MCQ answering for three benchmark datasets. Our results consis- tently and significantly outperform a strong re- trieval baseline as well as a Markov Logic network model ( <ref type="bibr" target="#b9">Khot et al., 2015</ref>). We thus show the ben- efits of semi-structured data and models over un- structured or highly-structured counterparts. We also validate our curated MCQ dataset and its an- notations as an effective tool for training QA mod- els. Finally, we find that our model learns general- izations that permit inference when exact answers may not even be contained in the knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work with tables, semi-structured knowledge bases and QA relates to several parallel lines of research. In terms of dataset creation via crowd- sourcing, <ref type="bibr" target="#b0">Aydin et al. (2014)</ref> harvest MCQs via a gamified app, although their work does not involve tables. <ref type="bibr" target="#b14">Pasupat and Liang (2015)</ref> use tables from Wikipedia to construct a set of QA pairs. However their annotation setup does not impose structural constraints from tables, and does not collect fine- grained alignment to table elements.</p><p>On the inference side Pasupat and Liang (2015) also reason over tables to answer questions. Un- like our approach, they do not require alignments to table cells. However, they assume knowledge of the table that contains the answer, a priori -which we do not. <ref type="bibr" target="#b24">Yin et al. (2015b)</ref> and <ref type="bibr" target="#b13">Neelakantan et al. (2015)</ref> also use tables in the context of QA, but deal with synthetically generated query data. <ref type="bibr" target="#b18">Sun et al. (2016)</ref> perform cell search over web tables via relational chains, but are more generally inter- ested in web queries. <ref type="bibr" target="#b2">Clark et al. (2016)</ref>  <ref type="table">combine  different levels of knowledge for QA, including  an integer-linear program for searching over table  cells. None of these other efforts leverage tables  for generation of data.</ref> Our research more generally pertains to natu- ral language interfaces for databases. Answer- ing questions in this context refers to executing queries over relational databases <ref type="bibr" target="#b1">(Cafarella et al., 2008;</ref><ref type="bibr" target="#b15">Pimplikar and Sarawagi, 2012)</ref>. <ref type="bibr" target="#b23">Yin et al. (2015a)</ref> consider databases where information is stored in n-tuples, which are essentially ta- bles. Also, investigation of the relational structure of tables is connected with research on database schema analysis and induction <ref type="bibr" target="#b21">(Venetis et al., 2011;</ref><ref type="bibr" target="#b19">Syed et al., 2010)</ref>. Finally, unstructured text and structured formats links to work on open infor- mation extraction ( <ref type="bibr" target="#b4">Etzioni et al., 2008)</ref> and knowl- edge base population (Ji and Grishman, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tables as Semi-structured Knowledge Representation</head><p>Tables can be found on the web containing a wide range of heterogenous data. To focus and fa- cilitate our work on QA we select a collection of tables that were specifically designed for the task. Specifically we use AI2's Aristo Tablestore 2 .</p><p>However, it should be noted that the contributions of this paper are not tied to specific tables, as we provide a general methodology that could equally be applied to a different set of tables. The struc- tural properties of this class of tables is further de- scribed in Section 3.1. The Aristo Tablestore consists of 65 hand- crafted tables organized by topic. Some of the topics are bounded, containing only a fixed num- ber of facts, such as the possible phase changes of matter (see <ref type="table">Table 1</ref>). Other topics are unbounded, containing a very large or even infinite number of facts, such as the kind of energy used in perform- ing an action (the corresponding tables can only contain a sample subset of these facts from the publicly available Regents dataset 3 . The rest were targeted at an unreleased dataset called Monarch. In both cases only the training partition of each dataset was used to formulate and hand- craft tables. However, for unbounded topics, addi- tional facts were added to each table, using science education text books and websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Table Semantics and Relations</head><p>Part of a table from the Aristo Tablestore is given as an example in <ref type="table">Table 1</ref>. The format is semi- structured: the rows of the table (with the excep- tion of the header) are a list of sentences, but with well-defined recurring filler patterns. Together with the header, these patterns divide the rows into meaningful columns. This semi-structured data format is flexible. Since facts are presented as sentences, the tables can act as a text corpus for information retrieval. At the same time the struc- ture can be used -as we do -to focus on specific nuggets of information. The flexibility of these ta- bles allows us to compare our table-based system to an information retrieval baseline. Such tables have some interesting structural se- mantics, which we will leverage throughout the paper. A row in a table corresponds to a fact 4 . The cells in a row correspond to concepts, enti- ties, or processes that participate in this fact. A content column 5 corresponds to a group of con- cepts, entities, or processes that are the same type. The header cell of the column is an abstract de- scription of the type. We may view the head as a hypernym and the cells in the column below as co-hyponyms of the head. The header row defines a generalization of which the rows in the table are specific instances.</p><p>This structure is directly relevant to multiple- choice QA. Facts (rows) form the basis for creat-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Crowd-sourcing Multiple-choice Questions from Tables</head><p>We use Amazon's Mechanical Turk (MTurk) ser- vice to generate MCQs by imposing constraints derived from the structure of the tables. These constraints help annotators create questions with scaffolding information, and lead to consistent quality in the generated output. An additional ben- efit of this format is the alignment information, linking cells in the tables to the MCQs generated by the Turkers. The alignment information is gen- erated as a by-product of making the MCQs. We present Turkers with a table such as the one in <ref type="figure" target="#fig_0">Figure 1</ref>. Given this table, we choose a target cell to be the correct answer for a new MCQ; for example, the red cell in <ref type="figure" target="#fig_0">Figure 1</ref>. First, Turkers create a question by using information from the rest of the row containing the target (i.e., the blue cells in <ref type="figure" target="#fig_0">Figure 1</ref>), such that the target is its cor- rect answer. Then they select the cells in the row that they used to construct the question. Follow- ing this, they construct four succinct choices for the question, one of which is the correct answer and the other three are distractors. Distractors are formed from other cells in the column containing the target (i.e. yellow cells in <ref type="figure" target="#fig_0">Figure 1</ref>). If there are insufficient unique cells in the column Turk- ers create their own. Annotators can rephrase and shuffle the contents of cells as required.</p><p>In addition to an MCQ, we obtain alignment information with no extra effort from annotators. We know which table, row, and column contains the answer, and thus we know which header cells might be relevant to the question. We also know the cells of a row that were used to construct a question.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The TabMCQ Dataset</head><p>We created a HIT (the MTurk acronym for Hu- man Intelligence Task) for every non-filler cell (see Section 3) from each one of the 65 manually constructed tables of the Aristo Tablestore. We paid annotators 10 cents per MCQ, and asked for 1 annotation per HIT for most tables. For an initial set of four tables which we used in a pilot study, we asked for three annotations per HIT <ref type="bibr">6</ref> . We re- quired Turkers to have a HIT approval rating of 95% or higher, with a minimum of at least 500 HITs approved. We restricted the demographics of our workers to the US. <ref type="table" target="#tab_2">Table 2</ref> compares our method with other studies conducted at AI2 to generate MCQs. These meth- ods attempt to generate new MCQs from existing ones, or write them from scratch, but do not in- volve tables in any way. Our annotation procedure leads to faster data creation, with consistent out- put quality that resulted in the lowest percentage of rejected HITs. Manual inspection of the gener- ated output also revealed that questions are of con- sistently good quality. They are good enough for training machine learning models and many are good enough as evaluation data for QA. A sample of generated MCQs is presented in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>We implemented some simple checks to eval- uate the data before approving HITs. These in- cluded things like checking whether an MCQ has at least three choices and whether choices are re- peated. We had to further prune our data to dis- card some MCQs due to corrupted data or badly constructed MCQs. A total of 159 MCQs were lost through the cleanup. In the end our com- plete data consists of 9092 MCQs, which is -to the best of our knowledge -orders of magnitude larger than any existing collection of science exam style MCQs available for research. These MCQs also come with alignment information to tables, rows, columns and cells. The dataset, bundled to- gether with the Aristo Tablestore, can be freely downloaded 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Solving MCQs with Table Cell Search</head><p>Consider the MCQ "What is the process by which water is changed from a liquid to a gas?" with choices "melting, sublimation, vaporization, con- densation", and the table given in <ref type="figure" target="#fig_0">Figure 1</ref>. Find- ing the correct answer amounts to finding a cell in the table that is most relevant to a candidate QA pair. In other words, a relevant cell should confirm the assertion made by a particular QA pair.</p><p>By applying the reasoning used to create MCQs (see Section 4) in the inverse direction, finding these relevant cells becomes the task of finding an intersection between rows and columns of interest.</p><p>Consider the table in <ref type="figure" target="#fig_0">Figure 1</ref>: assuming we have some way of aligning a question to a row (blue cells) and choices to a column (yellow cells), then the relevant cell is at the intersection of the two (the red cell). This alignment is precisely what we get as a by-product of the annotation task we setup in Section 4 to harvest MCQs. We can thus featurize connections between MCQs and elements of tables and use the align- ment data to train a model over the features. This is outlined in the next section, describing our Fea- ture Rich Table Embedding Solver (FRETS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model and Training Objective</head><p>Let Q = {q 1 , ..., q N } denote a set of MCQs, and A n = {a 1 n , ..., a k n } be the set of candidate answer choices for a given question q n . Let the set of ta- bles be defined as T = {T 1 , ..., T M }. Given a ta- ble T m , let t ij m be the cell in that table correspond- ing to the ith row and jth column.</p><p>We define a log-linear model that scores every cell t ij m of every table in our collection according to a set of discrete weighted features, for a given QA pair. We have the following: log p(t ij m |q n , a k n ; A n , T ) =</p><formula xml:id="formula_0">d λ d f d (q n , a k n , t ij m ; A n , T ) − log Z (1)</formula><p>Here λ d are weights and f d (q n , a k n , t ij m ; A n , T ) are features. These features should ideally leverage both structure and content of tables to assign high scores to relevant cells, while assigning low scores to irrelevant cells. Z is the partition function, de- fined as follows:</p><formula xml:id="formula_1">Z = m,i,j exp d λ d f d (q n , a k n , t ij m ; A n , T )<label>(2)</label></formula><p>Z normalizes the scores associated with every cell over all the cells in all the tables to yield a prob- ability distribution. During inference the partition term log Z can be ignored, making scoring cells of every table for a given QA pair efficient. These scores translate to a solution for an MCQ. Every QA pair produces a hypothetical fact, and as noted in Section 3.1, the row of a table is in essence a fact. Relevant cells (if they exist) should confirm the hypothetical fact asserted by a given QA pair. During inference, we assign the score of the highest scoring row (or the most likely fact) to a hypothetical QA pair. Then the correct solu- tion to the MCQ is simply the answer choice as- sociated with the QA pair that was assigned the highest score. Mathematically, this is expressed as follows:</p><formula xml:id="formula_2">a * n = arg max a k n max m,i j d λ d f d (q n , a k n , t ij m ; A n , T ) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Training</head><p>Since FRETS is a log-linear model, training in- volves optimizing a set of weights λ d . As train- ing data, we use alignment information between MCQs and table elements (see Section 4.1). The predictor value that we try to maximize with our model is an alignment score that is closest to the true alignments in the training data. True align- ments to table cells for a given QA pair are es- sentially indicator values but we convert them to numerical scores as follows <ref type="bibr">8</ref> . For a correct QA hypothesis we assign a score of 1.0 to cells whose row and column and both aligned to the MCQ (i.e. cells that exactly answer the question), 0.5 to cells whose row but not column is aligned in some way to the question (i.e. cells that were used to construct the question), and 0.0 otherwise. For an incorrect QA hypothesis we assign a score of 0.1 to random cells from tables that contain no alignments to the QA (so all except one), with a probability of 1%, while all other cells are scored 0.0. The intuition behind this scoring scheme is to guide the model to pick relevant cells for cor- rect answers, while encouraging it to pick faulty evidence with low scores for incorrect answers.</p><p>Given these scores assigned to all cells of all ta- bles for all QA pairs in the training set, suitably normalized to a probability distribution over ta- bles for a given QA pair, we can then proceed to train our model. We use cross-entropy, which min- imizes the following loss: <ref type="bibr">Level</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Description Intuition S-Var Cmpct <ref type="table">Table  Table score</ref> Ratio of words in t to q+a Topical consistency ♦ †TF- <ref type="table">IDF table score</ref> Same but TF-IDF weights Topical consistency ♦ Row-question score Ratio of words in r to q Question align ♦ Row Row-question w/o focus score Ratio of words in r to q-(a f +q f ) Question align ♦ Header-question score Ratio of words in h to q Prototype align ♦ Column overlap Ratio of elements in c and A Choices align ♦ Column Header answer-type match Ratio of words in ch to a f Choices hypernym align ♦ Header question-type match Ratio of words in ch to q f Question hypernym align ♦ †Cell salience Salience of s to q+a QA hypothesis assert ♦ Cell †Cell answer-type entailment Entailment score between s and a f Hypernym-hyponym align Cell answer-type similarity Avg. vector sim between s and a f Hypernym-hyponym sim. </p><formula xml:id="formula_3">L( λ) = qn a k n ∈An m,i,j p(t * ij m |q n , a k n ; T )· log p(t ij m |q n , a k n ; A n , T ) (4) Here p(t * ij m |q n , a k n ; T )</formula><p>is the normalized probabil- ity of the true alignment scores.</p><p>While this is an indirect way to train our model to pick the best answer, in our pilot experiments it worked better than direct maximum likelihood or ranking with hinge loss, achieving a training accuracy of almost 85%. Our experimental re- sults on the test suite, presented in the next section, also support the empirical effectiveness of this ap- proach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Features</head><p>The features we use are summarized in Ta- ble 4. These features compute statistics be- tween question-answer pairs and different struc- tural components of tables. While the features are weighted and summed for each cell individually, they can capture more global properties such as scores associated with tables, rows or columns in which the specific cell is contained. Features are divided into four broad categories based on the level of granularity at which they operate. In what follows we give some details of <ref type="table" target="#tab_4">Table 4</ref> that re- quire further elaboration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Soft matching</head><p>Many of the features that we implement are based on string overlap between bags of words. How- ever, since the tables are defined statically in terms of a fixed vocabulary (which may not necessarily match words contained in an MCQ), these over- lap features will often fail. We therefore soften the constraint imposed by hard word overlap by a more forgiving soft variant. More specifically we introduce a word-embedding based soft match- ing overlap variant for every feature in the table marked with ♦. The soft variant targets high recall while the hard variant aims at providing high pre- cision. We thus effectively have almost twice the number of features listed.</p><p>Mathematically, let a hard overlap feature de- fine a score |S 1 ∩ S 2 | / |S 1 | between two bags of words S 1 and S 2 . We can define the denominator S 1 here, without loss of generality. Then, a corre- sponding word-embedding soft overlap feature is given by this formula:</p><formula xml:id="formula_4">1 |S 1 | w i ∈S 1 max w j ∈S 2 sim( w i , w j )<label>(5)</label></formula><p>Intuitively, rather than matching a word to its exact string match in another set, we instead match it to its most similar word, discounted by the score of that similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Question parsing</head><p>We parse questions to find the desired answer- type and, in rarer cases, question-type words. For example, in the question "What form of energy is required to convert water from a liquid to a gas?", the type of the answer we are expecting is a "form of energy". Generally, this answer-type corresponds to a hypernym of the answer choices, and can help find relevant information in the table, specifically related to columns.</p><p>By carefully studying the kinds of question pat- terns in our data, we implemented a rule-based parser that finds answer-types from queries. This parser uses a set of hand-coded regular expres- sions over phrasal chunks. The parser is designed to have high accuracy, so that we only produce an output for answer-types in high confidence situa- tions. In addition to producing answer-types, in some rarer cases we also detect hypernyms for parts of the questions. We call this set of words question-type words. Together, the question-type and answer-type words are denoted as focus words in the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">TF-IDF weighting</head><p>TF-IDF scores for weighting terms are pre- computed for all words in all the tables. We do this by treating every table as a unique document. At run-time we discount scores by table length as well as length of the QA pair under consideration to avoid disproportionately assigning high scores to large tables or long MCQs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Salience</head><p>The salience of a string for a particular QA pair is an estimate of how relevant it is to the hypoth- esis formed from that QA pair. It is computed by taking words in the question, pairing them with words in an answer choice and then computing PMI statistics between these pairs from a large corpus. A high salience score indicates words that are particularly relevant for a given QA pair hy- pothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Entailment</head><p>To calculate the entailment score between two strings, we use several features, such as overlap, paraphrase probability, lexical entailment likeli- hood, and ontological relatedness, computed with n-grams of varying lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Normalization</head><p>All the features in <ref type="table" target="#tab_4">Table 4</ref> produce numerical scores, but the range of these scores vary to some extent. To make our final model more robust, we normalize all feature scores to have a range be- tween 0.0 and 1.0. We do this by finding the maxi- mum and minimum values for any given feature on a training set. Subsequently, instead of using the raw feature value of a feature f d , we instead re- place it with</p><formula xml:id="formula_5">(f d − min f d ) / (max f d − min f d ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We train FRETS (Section 5) on the TabMCQ dataset (Section 4) using adaptive gradient descent with an L2 penalty of 1.0 and a mini-batch size of 500 training instances. We train two variants: one consisting of all the features from <ref type="table" target="#tab_4">Table 4</ref>, the other -a compact model -consisting of the most important features (above a threshold) from the first model by feature-weight. These features are noted by in the final column of <ref type="table" target="#tab_4">Table 4</ref>.</p><p>We run experiments on three 4th grade science exam MCQ datasets: the publicly available Re- gents dataset, the larger but unreleased dataset called Monarch, and a third even larger public dataset of Elementary School Science Questions (ESSQ) <ref type="bibr">9</ref> . For the first two datasets we use the test splits only, since the training sets were directly studied to construct the Aristo Tablestore, which was in turn used to generate our TabMCQ training data. On ESSQ we use all the questions since they are independent of the tables. The Regents test set consists of 129 MCQs, the Monarch test set of 250 MCQs, and ESSQ of 855 MCQs.</p><p>Since we are investigating semi-structured mod- els, we compare against two baselines. The first is an unstructured information retrieval method, which uses the Lucene search engine. To ap- ply Lucene to the tables, we ignore their struc- ture and simply use rows as plain-text sentences. The score for top retrieved hits are used to rank the different choices of MCQs. The second base- line is the highly-structured Markov-logic Net- work (MLN) model from <ref type="bibr" target="#b9">Khot et al. (2015)</ref> as re- ported in <ref type="bibr" target="#b2">Clark et al. (2016)</ref>, who use the model as a baseline 10 . Note that <ref type="bibr" target="#b2">Clark et al. (2016)</ref> achieve a score of 71.3 on Regents Test, which is higher than FRETS' scores (see <ref type="table" target="#tab_6">Table 5</ref>), but their results are not comparable to ours because they use an ensemble of algorithms. In contrast, we use a sin- gle algorithm with a much smaller collection of knowledge. FRETS rivals the best individual al- gorithm from their work.</p><p>We primarily use the tables from the Aristo Ta- blestore as knowledge base data in three different settings: with only tables constructed for Regents (40 tables), with only supplementary tables con- structed for Monarch (25 tables), and with all ta-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Regents <ref type="table" target="#tab_2">Test Monarch Test ESSQ   Lucene   Regents Tables  37.5  32.6  36.9  Monarch Tables  28.4  27.3  27.7  Regents+Monarch Tables  34.8</ref>    <ref type="table" target="#tab_4">Table 4</ref>) for all our experiments were trained on 300 million words of Newswire English from the monolingual section of the WMT-2011 shared task data. These vectors were improved post-training by retrofitting ( <ref type="bibr" target="#b6">Faruqui et al., 2014</ref>) them to PPDB ( <ref type="bibr" target="#b7">Ganitkevitch et al., 2013)</ref>.</p><p>The results of these experiments is presented in <ref type="table" target="#tab_6">Table 5</ref>. All numbers are reported in percentage accuracy. We perform statistical significance test- ing on these results using Fisher's exact test with a p-value of 0.05 and report them in our discussions.</p><p>First, FRETS -in both full and compact form -consistently outperforms the baselines, often by large margins. For Lucene, the improvements over all but the Waterloo corpus baseline are statisti- cally significant. Thus FRETS is able to capital- ize on data more effectively and rival an unstruc- tured model with access to orders of magnitude more data. For MLN, the improvements are sta- tistically significant in the case of Regents and Re- gents+Monarch tables. FRETS is thus performing better than a highly structured model while mak- ing use of a much simpler data formalism.</p><p>Our models are able to effectively generalize. With Monarch tables, the Lucene baseline is lit- tle better than random (25%  Finally, dropping some features in the compact model doesn't always hurt performance, in com- parison with the full model. This indicates that potentially higher scores are possible by a prin- cipled and detailed feature selection process. In these experiments the difference between the two FRETS models on equivalent data is statistically insignificant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study</head><p>To evaluate the contribution of different features we perform an ablation study, by individually re- moving groups of features from the full FRETS model, and re-training. Evaluation of these partial models is given in <ref type="table" target="#tab_8">Table 6</ref>. In this experiment we use all tables as knowledge base data.</p><p>Judging by relative score differential, cell fea- tures are by far the most important group, fol- lowed by row features. In both cases the drops in score are statistically significant. Intuitively, these results make sense, since row features are crucial in alignment to questions, while cell fea- tures capture the most fine-grained properties. It is less clear which among the other three feature groups is dominant, since the differences are not statistically significant. It is possible that cell fea- tures replicate information of other feature groups. For example, the cell answer-type entailment fea- ture indirectly captures the same information as the header answer-type match feature (a column feature). Similarly, salience captures weighted statistics that are roughly equivalent to the coarse- grained table features. Interestingly, the success of these fine-grained features would explain our im- provements over the Lucene baseline in <ref type="table" target="#tab_6">Table 5</ref>, which is incapable of such fine-grained search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented tables as knowledge bases for question answering. We explored a connected framework in which tables are first used to guide the creation of MCQ data with alignment infor- mation to table elements, then jointly with this data are used in a feature-driven model to answer unseen MCQs. A central research question of this paper was the trade-off between the degree of structure in a knowledge base and its ability to be harvested or reasoned with. On three bench- mark evaluation sets our consistently and signif- icantly better scores over an unstructured and a highly-structured baseline strongly suggest that ta- bles can be considered a balanced compromise in this trade-off. We also showed that our model is able to generalize from content to structure, thus reasoning about questions whose answer may not even be contained in the knowledge base.</p><p>We are releasing our dataset of more than 9000 MCQs and their alignment information, to the re- search community. We believe it offers interesting challenges that go beyond the scope of this paper -such as question parsing, or textual entailment - and are exciting avenues for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example table from MTurk annotation task illustrating constraints. We ask Turkers to construct questions from blue cells, such that the red cell is the correct answer, and yellow cells form distractors.</figDesc><graphic url="image-1.png" coords="4,117.36,62.81,362.85,107.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>table from MTurk</head><label>from</label><figDesc>annotation task illustrating constraints. We ask Turkers to construct questions from blue cells, such that the red cell is the correct answer, and yellow cells form distractors.</figDesc><table>Task 
Avg. Time (s) $/hour % Reject 
Rewrite 
345 
2.61 
48 
Paraphrase 
662 
1.36 
49 
Add choice 291 
2.47 
24 
Write new 
187 
5.78 
38 
TabMCQ 
72 
5.00 
2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of different ways of generat-
ing MCQs with MTurk. 

What is the orbital event with 
the longest day and the shortest night? 
A) Summer solstice 
B) Winter solstice 
C) Spring equinox 
D) Fall equinox 
Steel is a/an 
of electricity 
A) Separator 
B) Isolator 
C) Insulator 
D) Conductor 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of MCQs generated by MTurk. 
Correct answer choices are in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Summary of features. For a question (q) and answer (a) we compute scores for elements 
of tables: whole tables (t), rows (r), header rows (h), columns (c), column headers (c h ) and cells (s). 
Answer-focus (a f ) and question-focus (q f ) terms added where appropriate. Features marked ♦ denote 
soft-matching variants, marked with while those marked with a  † are described in further detail in Sec-
tion 5.2. Finally, features denote those that received high weights during training with all features, and 
were subsequently selected to form a compact FRETS model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Evaluation results on three benchmark datasets using different sets of tables as knowledge bases. 
Best results on a dataset are highlighted in bold. 

bles together (all 65 tables; see Section 3). For the 
Lucene baseline we also experiment with several 
orders of magnitude more data by indexing over 
the 5 × 10 10 words Waterloo corpus compiled by 
Charles Clarke at the University of Waterloo. Data 
is not a variable for MLN, since we directly cite 
results from Clark et al. (2016). 
The word vectors we used in soft matching fea-
ture variants (i.e., ♦ features from </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>). But with the same knowledge base data, FRETS is competitive and sometimes scores higher than the best Lucene or MLN models (although this difference is statisti-</figDesc><table>Model 
REG MON ESSQ 
FRETS 
59.1 
52.4 
54.9 
w/o tab features 
59.1 
47.6 
52.8 
w/o row features 49.0 
40.4 
44.3 
w/o col features 
59.9 
47.2 
53.1 
w/o cell features 
25.7 
25.0 
24.9 
w/o ♦ features 
62.2 
47.5 
53.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Ablation study on FRETS, removing 
groups of features based on level of granularity. ♦ 
refers to the soft matching features from Table 4. 
Best results on a dataset are highlighted in bold. 

cally insignificant). These results indicate that our 
models are able to effectively capture both con-
tent and structure, reasoning approximately (and 
effectively) when the knowledge base may not 
even contain the relevant information to answer 
a question. The Monarch tables themselves seem 
to add little value, since results for Regents tables 
by themselves are just as good or better than Re-
gents+Monarch tables. This is not a problem with 
FRETS, since the same phenomenon is witnessed 
with the Lucene baseline. It is noteworthy, how-
ever, that our models do not suffer from the addi-
tion of more tables, showing that our search pro-
cedure over table cells is robust. 
</table></figure>

			<note place="foot" n="1"> https://www.kaggle.com/c/ the-allen-ai-science-challenge</note>

			<note place="foot" n="2"> http://allenai.org/content/data/ AristoTablestore-Nov2015Snapshot.zip</note>

			<note place="foot" n="3"> http://allenai.org/content/data/ Regents.zip 4 Also predicates, or more generally frames with typed arguments. 5 Different from filler columns, which only contain a recurring pattern, and no information in their header cells. ing or answering questions, while instances of a type (columns) act as the choices of an MCQ. We use these observations both for crowd-sourcing MCQ creation as well as for designing features to answer MCQs with tables.</note>

			<note place="foot" n="6"> The goal was to obtain diversity in the MCQs created for a target cell. The results were not sufficiently conclusive to warrant a threefold increase in the cost of creation.</note>

			<note place="foot" n="7"> http://ai2-website.s3.amazonaws.com/ data/TabMCQ_v_1.0.zip</note>

			<note place="foot" n="8"> On training data, we experimented with a few different scoring heuristics and found that these ones worked well.</note>

			<note place="foot" n="9"> http://aristo-public-data.s3. amazonaws.com/AI2-Elementary-NDMC-Feb2016. zip 10 We do not re-implement the MLN, and therefore only cite results from previous work on part of our test suite.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We'd like to thank AI2 for funding this research and the creation of our MCQ dataset. The first and third authors of this paper were also supported in part by the following grants: NSF grant IIS-1143703, NSF award IIS-1147810, DARPA grant FA87501220342. Thanks also go to the anony-mous reviewers, whose valuable comments helped to improve the quality of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdsourcing for multiple-choice question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavuz</forename><surname>Bahadir Ismail Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Selim Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demirbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth IAAI Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Webtables: exploring the power of tables on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Michael J Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="538" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining retrieval, statistics, and inference to answer elementary science questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Elementary school science and math tests as a driver for ai: Take the aristo challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IAAI</title>
		<meeting>IAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4166</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge base population: Successful approaches and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring markov logic networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gribkoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovery of inference rules for question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="343" to="360" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cogex: A logic prover for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Maiorano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Question answering based on semantic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">693</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04834</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00305</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Answering table queries on the web using column keywords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Pimplikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="908" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using semantic roles to improve question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Information extraction supported question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohini</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Table cell search for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting a web of semantic data for interpreting tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zareen</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varish</forename><surname>Mulwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Web Science Conference</title>
		<meeting>the Second Web Science Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recovering semantics of tables on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Venetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengxin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: a set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Answering questions with complex semantic constraints on open knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1301" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00965</idno>
		<title level="m">Neural enquirer: Learning to query tables</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
