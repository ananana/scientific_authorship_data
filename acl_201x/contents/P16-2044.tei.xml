<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tweet2Vec: Character-Based Distributed Representations for Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Zhou</surname></persName>
							<email>zhongzhou@cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Heinz College</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Fitzpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Heinz College</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Muehl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tweet2Vec: Character-Based Distributed Representations for Social Media</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="269" to="274"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Text from social media provides a set of challenges that can cause traditional NLP approaches to fail. Informal language, spelling errors, abbreviations, and special characters are all commonplace in these posts, leading to a prohibitively large vocabulary size for word-level approaches. We propose a character composition model, tweet2vec, which finds vector-space representations of whole tweets by learning complex, non-local dependencies in character sequences. The proposed model outperforms a word-level baseline at predicting user-annotated hashtags associated with the posts, doing significantly better when the input contains many out-of-vocabulary words or unusual character sequences. Our tweet2vec encoder is publicly available 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We understand from Zipf's Law that in any nat- ural language corpus a majority of the vocabu- lary word types will either be absent or occur in low frequency. Estimating the statistical proper- ties of these rare word types is naturally a diffi- cult task. This is analogous to the curse of di- mensionality when we deal with sequences of to- kens -most sequences will occur only once in the training data. Neural network architectures over- come this problem by defining non-linear compo- sitional models over vector space representations of tokens and hence assign non-zero probability even to sequences not seen during training <ref type="bibr" target="#b0">(Bengio et al., 2003;</ref><ref type="bibr" target="#b7">Kiros et al., 2015)</ref>. In this work, we explore a similar approach to learning dis- tributed representations of social media posts by <ref type="bibr">1</ref> https://github.com/bdhingra/tweet2vec composing them from their constituent characters, with the goal of generalizing to out-of-vocabulary words as well as sequences at test time.</p><p>Traditional Neural Network Language Models (NNLMs) treat words as the basic units of lan- guage and assign independent vectors to each word type. To constrain memory requirements, the vocabulary size is fixed before-hand; therefore, rare and out-of-vocabulary words are all grouped together under a common type 'UNKNOWN'. This choice is motivated by the assumption of ar- bitrariness in language, which means that surface forms of words have little to do with their semantic roles. Recently, ( <ref type="bibr" target="#b9">Ling et al., 2015</ref>) challenge this assumption and present a bidirectional Long Short Term Memory (LSTM) (Hochreiter and Schmid- huber, 1997) for composing word vectors from their constituent characters which can memorize the arbitrary aspects of word orthography as well as generalize to rare and out-of-vocabulary words.</p><p>Encouraged by their findings, we extend their approach to a much larger unicode character set, and model long sequences of text as functions of their constituent characters (including white- space). We focus on social media posts from the website Twitter, which are an excellent test- ing ground for character based models due to the noisy nature of text. Heavy use of slang and abundant misspellings means that there are many orthographically and semantically similar tokens, and special characters such as emojis are also im- mensely popular and carry useful semantic infor- mation. In our moderately sized training dataset of 2 million tweets, there were about 0.92 mil- lion unique word types. It would be expensive to capture all these phenomena in a word based model in terms of both the memory requirement (for the increased vocabulary) and the amount of training data required for effective learning. Ad- ditional benefits of the character based approach include language independence of the methods, and no requirement of NLP preprocessing such as word-segmentation.</p><p>A crucial step in learning good text representa- tions is to choose an appropriate objective function to optimize. Unsupervised approaches attempt to reconstruct the original text from its latent rep- resentation ( <ref type="bibr" target="#b11">Mikolov et al., 2013;</ref><ref type="bibr" target="#b0">Bengio et al., 2003)</ref>. Social media posts however, come with their own form of supervision annotated by mil- lions of users, in the form of hashtags which link posts about the same topic together. A natural as- sumption is that the posts with the same hashtags should have embeddings which are close to each other. Hence, we formulate our training objective to maximize cross-entropy loss at the task of pre- dicting hashtags for a post from its latent represen- tation.</p><p>We propose a Bi-directional Gated Recurrent Unit (Bi-GRU) ( <ref type="bibr" target="#b1">Chung et al., 2014</ref>) neural net- work for learning tweet representations. Treat- ing white-space as a special character itself, the model does a forward and backward pass over the entire sequence, and the final GRU states are lin- early combined to get the tweet embedding. Pos- terior probabilities over hashtags are computed by projecting this embedding to a softmax out- put layer. Compared to a word-level baseline this model shows improved performance at predicting hashtags for a held-out set of posts. Inspired by recent work in learning vector space text represen- tations, we name our model tweet2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Using neural networks to learn distributed repre- sentations of words dates back to ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>. More recently, ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>) re- leased word2vec -a collection of word vectors trained using a recurrent neural network. These word vectors are in widespread use in the NLP community, and the original work has since been extended to sentences <ref type="bibr" target="#b7">(Kiros et al., 2015)</ref>, doc- uments and paragraphs ( <ref type="bibr" target="#b8">Le and Mikolov, 2014</ref>), topics ( <ref type="bibr" target="#b12">Niu and Dai, 2015</ref>) and queries ( <ref type="bibr" target="#b3">Grbovic et al., 2015)</ref>. All these methods require storing an extremely large table of vectors for all word types and cannot be easily generalized to unseen words at test time ( <ref type="bibr" target="#b9">Ling et al., 2015</ref>). They also require preprocessing to find word boundaries which is non-trivial for a social network domain like Twit- ter.</p><p>In ( <ref type="bibr" target="#b9">Ling et al., 2015)</ref>, the authors present a compositional character model based on bidirec- tional LSTMs as a potential solution to these prob- lems. A major benefit of this approach is that large word lookup tables can be compacted into char- acter lookup tables and the compositional model scales to large data sets better than other state- of-the-art approaches. While ( <ref type="bibr" target="#b9">Ling et al., 2015)</ref> generate word embeddings from character repre- sentations, we propose to generate vector repre- sentations of entire tweets from characters in our tweet2vec model.</p><p>Our work adds to the growing body of work showing the applicability of character models for a variety of NLP tasks such as Named Entity Recog- nition ( <ref type="bibr" target="#b13">Santos and Guimarães, 2015)</ref>, POS tag- ging ( <ref type="bibr" target="#b14">Santos and Zadrozny, 2014</ref>), text classifica- tion ( <ref type="bibr" target="#b17">Zhang et al., 2015</ref>) and language modeling ( <ref type="bibr" target="#b5">Karpathy et al., 2015;</ref><ref type="bibr" target="#b6">Kim et al., 2015)</ref>.</p><p>Previously, ( <ref type="bibr" target="#b10">Luong et al., 2013</ref>) dealt with the problem of estimating rare word representa- tions by building them from their constituent mor- phemes. While they show improved performance over word-based models, their approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter. Also the space of all morphemes, though smaller than the space of all words, is still large enough that modelling all morphemes is impractical.</p><p>Hashtag prediction for social media has been addressed earlier, for example in ( <ref type="bibr" target="#b16">Weston et al., 2014;</ref><ref type="bibr" target="#b2">Godin et al., 2013)</ref>. ( <ref type="bibr" target="#b16">Weston et al., 2014</ref>) also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can gener- alize to an unrelated task of document recommen- dation, justifying the use of hashtags as supervi- sion for learning text representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tweet2Vec</head><p>Bi-GRU Encoder: <ref type="figure">Figure 1</ref> shows our model for encoding tweets. It uses a similar structure to the C2W model in ( <ref type="bibr" target="#b9">Ling et al., 2015)</ref>, with LSTM units replaced with GRU units.</p><p>The input to the network is defined by an al- phabet of characters C (this may include the en- tire unicode character set). The input tweet is bro- ken into a stream of characters c 1 , c 2 , ...c m each of which is represented by a 1-by-|C| encoding. These one-hot vectors are then projected to a char- acter space by multiplying with the matrix P C ∈ <ref type="figure">Figure 1</ref>: Tweet2Vec encoder for social media text R |C|×dc , where d c is the dimension of the char- acter vector space. Let x 1 , x 2 , ...x m be the se- quence of character vectors for the input tweet af- ter the lookup. The encoder consists of a forward- GRU and a backward-GRU. Both have the same architecture, except the backward-GRU processes the sequence in reverse order. Each of the GRU units process these vectors sequentially, and start- ing with the initial state h 0 compute the sequence h 1 , h 2 , ...h m as follows:</p><formula xml:id="formula_0">r t = σ(W r x t + U r h t−1 + b r ), z t = σ(W z x t + U z h t−1 + b z ), ˜ h t = tanh(W h x t + U h (r t h t−1 ) + b h ), h t = (1 − z t ) h t−1 + z t ˜ h t .</formula><p>Here r t , z t are called the reset and update gates respectively, and˜hand˜ and˜h t is the candidate output state which is converted to the actual output state h t .</p><formula xml:id="formula_1">W r , W z , W h are d h × d c matrices and U r , U z , U h are d h × d h matrices,</formula><p>where d h is the hidden state dimension of the GRU. The final states h f m from the forward-GRU, and h b 0 from the backward GRU are combined using a fully-connected layer to the give the final tweet embedding e t :</p><formula xml:id="formula_2">e t = W f h f m + W b h b 0<label>(1)</label></formula><p>Here W f , W b are d t × d h and b is d t × 1 bias term, where d t is the dimension of the final tweet embedding. In our experiments we set d t = d h . All parameters are learned using gradient descent. Softmax: Finally, the tweet embedding is passed through a linear layer whose output is the same size as the number of hashtags L in the data set. We use a softmax layer to compute the poste- rior hashtag probabilities:</p><formula xml:id="formula_3">P (y = j|e) = exp(w T j e + b j ) L i=1 exp(w T i e + b j )</formula><p>.</p><p>Objective Function: We optimize the cate- gorical cross-entropy loss between predicted and true hashtags:</p><formula xml:id="formula_5">J = 1 B B i=1 L j=1 −t i,j log(p i,j ) + λΘ 2 . (3)</formula><p>Here B is the batch size, L is the number of classes, p i,j is the predicted probability that the i- th tweet has hashtag j, and t i,j ∈ {0, 1} denotes the ground truth of whether the j-th hashtag is in the i-th tweet. We use L2-regularization weighted by λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Level Baseline</head><p>Since our objective is to compare character-based and word-based approaches, we have also imple- mented a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a min- imum. The encoder is essentially the same as tweet2vec, with the input as words instead of char- acters. A lookup table stores word vectors for the V (20K here) most common words, and the rest are grouped together under the 'UNK' token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data</head><p>Our dataset consists of a large collection of global posts from Twitter 2 between the dates of June 1, 2013 to June 5, 2013. Only English language posts (as detected by the lang field in Twitter API) and posts with at least one hashtag are retained. We removed infrequent hashtags (&lt; 500 posts) since they do not have enough data for good general- ization. We also removed very frequent tags (&gt; 19K posts) which were almost always from auto- matically generated posts (ex: #androidgame) which are trivial to predict. The final dataset con- tains 2 million tweets for training, 10K for valida- tion and 50K for testing, with a total of 2039 dis- tinct hashtags. We use simple regex to preprocess the post text and remove hashtags (since these are to be predicted) and HTML tags, and replace user- names and URLs with special tokens. We also re- moved retweets and convert the text to lower-case.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Word vectors and character vectors are both set to size d L = 150 for their respective models. There were 2829 unique characters in the training set and we model each of these independently in a charac- ter look-up table. Embedding sizes were chosen such that each model had roughly the same num- ber of parameters <ref type="table" target="#tab_1">(Table 2)</ref>. Training is performed using mini-batch gradient descent with Nesterov's momentum. We use a batch size B = 64, initial learning rate η 0 = 0.01 and momentum parame- ter µ 0 = 0.9. L2-regularization with λ = 0.001 was applied to all models. Initial weights were drawn from 0-mean gaussians with σ = 0.1 and initial biases were set to 0. The hyperparame- ters were tuned one at a time keeping others fixed, and values with the lowest validation cost were chosen. The resultant combination was used to train the models until performance on validation set stopped increasing. During training, the learn- ing rate is halved everytime the validation set pre- cision increases by less than 0.01 % from one epoch to the next. The models converge in about 20 epochs. Code for training both the models is publicly available on github.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We test the character and word-level variants by predicting hashtags for a held-out test set of posts. Since there may be more than one correct hashtag per post, we generate a ranked list of tags for each  post from the output posteriors, and report aver- age precision@1, recall@10 and mean rank of the correct hashtags. These are listed in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>To see the performance of each model on posts containing rare words (RW) and frequent words (FW) we selected two test sets each containing 2,000 posts. We populated these sets with posts which had the maximum and minimum number of out-of-vocabulary words respectively, where vocabulary is defined by the 20K most frequent words. Overall, tweet2vec outperforms the word model, doing significantly better on RW test set and comparably on FW set. This improved perfor- mance comes at the cost of increased training time (see <ref type="table" target="#tab_1">Table 2</ref>), since moving from words to charac- ters results in longer input sequences to the GRU.</p><p>We also study the effect of model size on the performance of these models. For the word model we set vocabulary size V to 8K, 15K and 20K re- spectively. For tweet2vec we set the GRU hidden state size to 300, 400 and 500 respectively.   <ref type="table">Table 4</ref>: Precision @1 as training data size and number of output labels is increased. Note that the test set is different for each setting.</p><p>set described above. There is not much variation in the performance, and moreover tweet2vec always outperforms the word based model for the same number of parameters. <ref type="table">Table 4</ref> compares the models as complexity of the task is increased. We created 3 datasets (small, medium and large) with an increasing number of hashtags to be predicted. This was done by vary- ing the lower threshold of the minimum number of tags per post for it to be included in the dataset. Once again we observe that tweet2vec outperforms its word-based counterpart for each of the three settings.</p><p>Finally, table 1 shows some predictions from the word level model and tweet2vec. We selected these to highlight some strengths of the character based approach -it is robust to word segmenta- tion errors and spelling mistakes, effectively inter- prets emojis and other special characters to make predictions, and also performs comparably to the word-based approach for in-vocabulary tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented tweet2vec -a character level encoder for social media posts trained using super- vision from associated hashtags. Our result shows that tweet2vec outperforms the word based ap- proach, doing significantly better when the input post contains many rare words. We have focused only on English language posts, but the character model requires no language specific preprocessing and can be extended to other languages. For fu- ture work, one natural extension would be to use a character-level decoder for predicting the hash- tags. This will allow generation of hashtags not seen in the training dataset. Also, it will be in- teresting to see how our tweet2vec embeddings can be used in domains where there is a need for semantic understanding of social media, such as tracking infectious diseases <ref type="bibr" target="#b15">(Signorini et al., 2011</ref>). Hence, we provide an off-the-shelf en- coder trained on medium dataset described above to compute vector-space representations of tweets along with our code on github.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Precision @1 v Number of model parameters for word model and tweet2vec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Model sizes and training time/epoch</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Hashtag prediction results. Best numbers 
for each test set are in bold. 

</table></figure>

			<note place="foot" n="2"> https://twitter.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Alex Smola, Yun Fu, Hsiao-Yu Fish Tung, Ruslan Salakhutdinov, and Barnabas Poczos for useful discussions. We would also like to thank Juergen Pfeffer for providing ac-cess to the Twitter data, and the reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using topic models for twitter hashtag recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Slavkovikj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van De Walle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web companion</title>
		<meeting>the 22nd international conference on World Wide Web companion</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="593" to="596" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-and content-aware embeddings for query rewriting in sponsored search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06615</idno>
		<title level="m">Character-aware neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<title level="m">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Qiang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08422</idno>
		<title level="m">Topic2vec: Learning distributed representations of topics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Guimarães</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05008</idno>
		<title level="m">Boosting named entity recognition with neural character embeddings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The use of twitter to track levels of disease activity and public concern in the us during the influenza a h1n1 pandemic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Signorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Maria</forename><surname>Segre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip M</forename><surname>Polgreen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">19467</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">tagspace: Semantic embeddings from hashtags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1822" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
