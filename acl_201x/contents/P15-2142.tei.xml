<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Incremental Dependency Parsing with Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Incremental Dependency Parsing with Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="863" to="869"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a neural network model for scalable generative transition-based dependency parsing. A probability distribution over both sentences and transition sequences is parameterised by a feed-forward neural network. The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efficient integration of syntax into neural models for language generation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transition-based dependency parsers that perform incremental local inference with a discrimina- tive classifier offer an appealing trade-off be- tween speed and accuracy <ref type="bibr" target="#b30">(Nivre, 2008;</ref><ref type="bibr" target="#b38">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b12">Choi and Mccallum, 2013)</ref>. Recently neural network transition-based depen- dency parsers have been shown to give state-of- the-art performance <ref type="bibr" target="#b11">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b18">Dyer et al., 2015;</ref><ref type="bibr" target="#b36">Weiss et al., 2015)</ref>. However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically.</p><p>Neural networks have also been shown to be powerful generative models for language mod- elling ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b27">Mikolov et al., 2010)</ref> and machine translation <ref type="bibr" target="#b22">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b15">Devlin et al., 2014;</ref><ref type="bibr" target="#b32">Sutskever et al., 2014</ref>). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used.</p><p>In this paper we propose a generative model for incremental parsing that offers an efficient way to incorporate syntactic information into a generative model. It relies on the strength of neural networks to overcome sparsity in the long conditioning con- texts required for an accurate model, while also of- fering a principled approach to learn dependency- based word representations ( <ref type="bibr" target="#b25">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b2">Bansal et al., 2014</ref>).</p><p>Generative models for graph-based dependency parsing <ref type="bibr" target="#b19">(Eisner, 1996;</ref><ref type="bibr" target="#b35">Wallach et al., 2008)</ref> are much less accurate than their discriminative coun- terparts. Syntactic language models based on PCFGs <ref type="bibr" target="#b31">(Roark, 2001;</ref><ref type="bibr" target="#b8">Charniak, 2001</ref>) and incre- mental parsing <ref type="bibr" target="#b9">(Chelba and Jelinek, 2000;</ref><ref type="bibr" target="#b20">Emami and Jelinek, 2005</ref>) have been proposed for speech recognition and machine translation. However, these models are also limited in either scalability, expressiveness, or both. A generative transition- based dependency parser based on recurrent neu- ral networks <ref type="bibr" target="#b33">(Titov and Henderson, 2007)</ref> obtains high accuracy, but training and decoding is pro- hibitively expensive.</p><p>We perform efficient linear-time decoding with a particle filtering-based beam-search method where derivations after pruned after every word generation and the beam size depends on the un- certainty in the model ( <ref type="bibr" target="#b7">Buys and Blunsom, 2015)</ref>.</p><p>The model obtains 91.1% UAS on the WSJ, which is 0.2% UAS better than the previous high- est accuracy generative dependency parser <ref type="bibr" target="#b33">(Titov and Henderson, 2007)</ref>, while also being much more efficient. As a language model its perplex- ity reaches 111.8, a 23% reduction over an n- gram baseline, when combining supervised train- ing with unsupervised fine-tuning. Finally, we find that the model is able to generate sentences that display both local and syntactic coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generative Transition-based Parsing</head><p>Our parsing model is based on transition-based arc-standard projective dependency parsing ( <ref type="bibr" target="#b29">Nivre and Scholz, 2004</ref>). The generative formulation is similar to previous generative transition-based parsers <ref type="bibr" target="#b33">(Titov and Henderson, 2007;</ref><ref type="bibr" target="#b13">Cohen et al., 2011;</ref><ref type="bibr" target="#b7">Buys and Blunsom, 2015)</ref>, and also related to the joint tagging and parsing model of <ref type="bibr" target="#b4">Bohnet and Nivre (2012)</ref>.</p><p>The model predicts a sequence of parsing tran- sitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label.</p><p>The joint probability distribution over a sen- tence with words w 1:n , tags t 1:n and a transition sequence a 1:2n is defined as</p><formula xml:id="formula_0">n i=1 p(t i |h m i )p(w i |t i , h m i ) m i+1 j=m i +1 p(a j |h j ) ,</formula><p>where m i is the number of transitions that have been performed when (t i , w i ) is shifted and h j is the conditioning context at the jth transition. A parser configuration (σ, β, A) for sentence s consists of a stack σ of indices in s, an index β to the next word to be generated, and a set of arcs A. The stack elements are referred to as σ 1 , . . . , σ |σ| , where σ 1 is the top element. For any node a, lc 1 (a) refers to the leftmost child of a in A, and rc 1 (a) to its rightmost child. A root node is added to the beginning of the sentence, and the head word of the sentence (we assume there is only one) is the dependent of the root.</p><p>The initial configuration is ([], 0, ∅), while A terminal configuration is reached when β &gt; |s| and |σ| = 1.</p><p>The transition types are shift, left-arc and right- arc. Shift generates the next word of the sentence and pushes it on the stack. Left-arc adds an arc (σ 1 , l, σ 2 ) and removes σ 2 from the stack. Right- arc adds (σ 2 , l, σ 1 ) and pops σ 1 .</p><p>The parsing strategy adds arcs bottom-up. In a valid transition sequence the last transition is a right-arc from the root to the head word, and the root node is not involved in any other dependen- cies. We use an oracle to extract transition se- quences from the training data: The oracle prefers reduce over shift transitions when both may lead to a valid derivation. <ref type="table">Table 1</ref>: Conditioning context elements for neural network input: First, second and third order de- pendencies are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Order Elements</head><formula xml:id="formula_1">1 σ 1 , σ 2 , σ 3 , σ 4 2 lc 1 (σ 1 ), rc 1 (σ 1 ), lc 1 (σ 2 ), rc 1 (σ 2 ) lc 2 (σ 1 ), rc 2 (σ 1 ), lc 2 (σ 2 ), rc 2 (σ 2 ) 3 lc 1 (lc 1 (σ 1 )), rc 1 (rc 1 (σ 1 )) lc 1 (lc 1 (σ 2 )), rc 1 (rc 1 (σ 2 ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Network Model</head><p>Our probability model is based on neural net- work language models with distributed represen- tations ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b28">Mnih and Hinton, 2007)</ref>, as well as feed-forward neural network models for transition-based dependency pars- ing <ref type="bibr" target="#b11">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b36">Weiss et al., 2015)</ref>. We estimate the distributions p(t i |h i ), p(w i |t i , h i ) and p(a j |h j ) with neural networks with shared in- put and hidden layers but separate output layers.</p><p>The templates for the conditioning context used are defined in <ref type="table">Table 1</ref>. In the templates we ob- tain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number of elements without suffering from sparsity.</p><p>In the input layer we make use of additive rep- resentations ( <ref type="bibr" target="#b5">Botha and Blunsom, 2014</ref>) so that for each word input position i we can include the word type, tag and other features, and learn input representations for each of these. Each context feature f has an input representation q f ∈ R D . The composite representation is computed as q i = f ∈µ(w i ) q f , where µ(w i ) are the word features. The hidden layer is then defined as</p><formula xml:id="formula_2">φ(h) = g( L j=1 C j q h j ),</formula><p>where C j ∈ R D×D are transformation matrices defined for each position in sequence h, L = |h| and g is a (usually non-linear) activation function applied element-wise. The matrices C j can be ap- proximated to be diagonal to reduce the number of model parameters and speed up the model by avoiding expensive matrix multiplications. For the output layer predicting the next transi- tion a, the hidden layer is mapped with a scoring</p><formula xml:id="formula_3">function χ(a, h) = k T a φ(h) + e a ,</formula><p>where k a is the transition output representation and e a is the bias weight. The score is normalised with the soft-max function:</p><formula xml:id="formula_4">p(a|h) = exp(χ(a, h)) a ∈A exp(χ(a , h)) .</formula><p>The output layer for predicting the next tag has a similar form, using the scoring function</p><formula xml:id="formula_5">τ (t, h) = t T t φ(h) + o t</formula><p>for tag representation t t and bias o t . The probability p(w|t, h) can be estimated similarly. However, to reduce the computa- tional cost of normalising over the entire vocab- ulary, we factorize the probability as P (w|h) = P (c|t, h)P (w|c, t, h), where c = c(w) is the unique class of word w. For each c, let Γ(c) be the set of words in that class. The vocabulary is clustered into approximately |V | classes using Brown clustering <ref type="bibr" target="#b6">(Brown et al., 1992)</ref>, reducing the number of items to sum over in the normal- isation factor from O(|V |) to O( |V |). Class- based factorization has been shown to be an effec- tive strategy in normalizing neural language mod- els ( <ref type="bibr" target="#b0">Baltescu and Blunsom, 2015)</ref>,</p><p>The class prediction score is defined as ψ(c, h) = s T c φ(h) + d c , where s c ∈ R D is the output weight vector for class c and d c is the class bias weight. The output layer then consists of a softmax function for p(c|h) and another softmax for the word prediction</p><formula xml:id="formula_6">p(w|c, h) = exp(Φ(w, h)) w ∈Γ(c) exp(Φ(w , h)) ,</formula><p>where Φ(w, h) = r T w φ(h)+b w is the word scoring function with output word representation r w and bias weight b w .</p><p>The model is trained with minibatch stochas- tic gradient descent (SGD) with Adagrad (Duchi et al., 2011) and L2 regularisation, to minimise the negative log likelihood of the joint distribu- tion over parsed training sentences. For our ex- periments we train the model while the training objective improves, and choose the parameters of the iteration with the best development set accu- racy (early stopping). The model obtains high ac- curacy with only a few training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding</head><p>Beam-search decoders for transition-based pars- ing ( <ref type="bibr" target="#b37">Zhang and Clark, 2008</ref>) keep a beam of par- tial derivations, advancing each derivation by one transition at a time. When the size of the beam exceeds a set threshold, the lowest-scoring deriva- tions are removed. However, in an incremental generative model we need to compare derivations with the same number of words shifted, rather than transitions performed. To let the decoding time re- main linear, we also need to bound the total num- ber of reduce transitions that can be performed over all derivations between two shift transitions.</p><p>To achieve this, we use a decoding method re- cently proposed for generative incremental pars- ing (Buys and Blunsom, 2015) based on particle filtering ( <ref type="bibr" target="#b16">Doucet et al., 2001</ref>), a sequential Monte Carlo sampling method.</p><p>In the algorithm, a fixed number of particles are divided among the partial derivations in the beam. Suppose i words have been shifted in all the derivations on the beam. To predict the next tran- sition from derivation d j , its particles are divided according to p(a|h). In practice, adding only shift and the most likely reduce transition leads to al- most no accuracy loss. After all the derivations have been advanced to shift word i + 1, a selection step is performed: The number of particles of each derivation is redistributed according to its proba- bility, weighted by its current number of particles. Some derivations may be assigned 0 particles, in which case they are removed.</p><p>The particle filtering method lets the beam size depend of the uncertainty of the model, somewhat similar to <ref type="bibr" target="#b12">Choi and Mccallum (2013)</ref>, while fixing the total number of particles constrains the decod- ing time to be linear. The particle filter also allow us to sample outputs, and to marginalise over the syntax when generating.  Our neural network implementation is partly based on the OxLM neural language modelling framework ( <ref type="bibr" target="#b1">Baltescu et al., 2014</ref>). The model pa- rameters are initialised randomly by drawing from a Gaussian distribution with mean 0 and variance 0.1, except for the bias weights, which are ini- tialised by the unigram distributions of their out- put. We use minibatches of size 128, the L2 regu- larization parameter is 10, and the word represen- tation and hidden layer of size is 256. The Ada- grad learning rate is initialised to 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>POS tags for the development and test sets are obtained with the Stanford POS tagger ( <ref type="bibr" target="#b34">Toutanova et al., 2003)</ref>, with 97.5% test set accuracy. Words that occur only once in the training data are treated as unknown words. Unknown words are replaced by tokens representing morphological surface fea- tures (based on capitalization, numbers, punctua- tion and common suffixes) similar to those used in the implementation of generative constituency parsers ( .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parsing results</head><p>We report unlabelled attachment score (UAS) and labelled attachment score (LAS) in our results, excluding punctuation. On the development set, we consider the effect of the choice of activation function ( <ref type="table" target="#tab_1">Table 2</ref>), finding that a sigmoid activa- tion (logistic function) performs best, following by tanh. Under our training setup the model can ob- tain up to 91.0 UAS after only 1 training iteration, thereby performing pure online learning.</p><p>We found that including third order depen- dencies in the conditioning context performs just 0.1% UAS better than including only first and sec- ond order dependencies. Including additional ele- ments does not improve performance further. The model can obtain 91.18 UAS, 89.02 LAS when <ref type="bibr">3</ref> Converted with version 3.4.1 of the Stanford parser, available at http::/nlp.stanford.edu/software/lex-parser.shtml.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>UAS LAS <ref type="bibr" target="#b35">Wallach et al. (2008)</ref> 85.7 - Titov and <ref type="bibr">Henderson (2007) 90.93 89.42 NN-GenDP 91.11 89.41 Chen and</ref><ref type="bibr" target="#b11">Manning (2014)</ref> 92.0 90.7 <ref type="table">Table 3</ref>: Parsing accuracies for dependency parsers on the WSJ test set, CoNLL dependencies.</p><p>trained only on words, not POS tags. Dependency parsers that do not use distributed representations tend to rely much more on the tags. Test set results comparing generative depen- dency parsers are given in <ref type="table">Table 3</ref> (our model is refered to as NN-GenDP). The graph-based gen- erative baseline ( <ref type="bibr" target="#b35">Wallach et al., 2008</ref>), parame- terised by Pitman-Yor Processes, is quite weak. Our model outperforms the generative model of <ref type="bibr" target="#b33">Titov and Henderson (2007)</ref>, which we retrained on our dataset, by 0.2%, despite that model be- ing able to condition on arbitrary-sized contexts. The decoding speed of our model is around 20 sen- tences per second, against less than 1 sentence per second for Titov and Henderson's model. Using diagonal transformation matrices further increases our model's speed, but reduces parsing accuracy.</p><p>On the Stanford dependency representation our model obtains 90.63% UAS, 88.27% LAS. Al- though this performance is promising, it is still below the discriminative neural network models of <ref type="bibr" target="#b18">Dyer et al. (2015)</ref> and <ref type="bibr" target="#b36">Weiss et al. (2015)</ref>, who ob- tained 93.1% UAS and 94.0% UAS respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language modelling</head><p>We also evaluate our parser as a language model, on the same WSJ data used for the parsing eval- uation <ref type="bibr">4</ref> . We perform unlabelled parsing, as ex- periments show that including labels in the con- ditioning context has a very small impact on per- formance. Neither do we use POS tags, as they are too expensive to predict in language genera- tion applications.</p><p>Perplexity results on the WSJ are given in Ta- ble 4. As baselines we report results on modified Knesser-Ney ( <ref type="bibr" target="#b24">Kneser and Ney, 1995)</ref> and neu- ral network 5-gram models. For our dependency- based language models we report perplexities based on the most likely parse found by the de- coder, which gives an upper bound on the true the u.s. union board said revenue rose 11 % to $ NUM million , or $ NUM a share . mr. bush has UNK-ed a plan to buy the company for $ NUM to NUM million , or $ NUM a share . the plan was UNK-ed by the board 's decision to sell its $ NUM million UNK loan loan funds . in stocks coming months , china 's NUM shares rose 10 cents to $ NUM million , or $ NUM a share . in the case , mr. bush said it will sell the company business UNK concern to buy the company . it was NUM common shares in addition , with $ NUM million , or $ NUM a share , according to mr. bush . in the first quarter , 1989 shares closed yesterday at $ NUM , mr. bush has increased the plan . last year 's retrenchment price index index rose 11 cents to $ NUM million , or $ NUM million is asked . last year earlier , net income rose 11 million % to $ NUM million , or 91 cents a share . the u.s. union has UNK-ed $ NUM million , or 22 cents a share , in 1990 , payable nov. 9 .  value of the model perplexity.</p><p>First we only perform standard supervised train- ing with the model -this already leads to an im- provement of 10 perplexity points over the neu- ral n-gram model. Second we consider a train- ing setup where we first perform 5 supervised it- erations, and then perform unsupervised training, treating the transition sequence as latent. For each minibatch parse trees are sampled with a parti- cle filter. This approach further improves the per- plexity to 111.8, a 23% reduction relative to the Knesser-Ney model. The unsupervised training stage lets the parsing accuracy fall from 91.48 to 89.49 UAS. We pos- tulate that the model is learning to make small ad- justments to favour of parsing structures that ex- plain the data better than the annotated parse trees, leading to the improvement in perplexity.</p><p>To test the scalability of our model, we also trained it on a larger unannotated corpus -a sub- set (of around 7 million words) of the billion word language modeling benchmark dataset ( <ref type="bibr" target="#b10">Chelba et al., 2013)</ref>. After training the model on the WSJ, we parsed the unannotated data with the model, and continued to train on the obtained parses. We observed a small increase in perplexity, from 203.5 for a neural n-gram model to 200.7 for the generative dependency model. We expect larger improvements when training on more data and with more sophisticated inference.</p><p>To evaluate our generative model qualitatively, we perform unconstrained generation of sentences (and parse trees) from the model, and found that sentences display a higher degree of syntactic co- herence than sentences generated by an n-gram model. See <ref type="table" target="#tab_2">Table 5</ref> for examples generated by the model. The highest-scoring sentences of length 20 or more are given, from 1000 samples generated. Note that the generation includes unknown word tokens (here NUM, UNK and UNK-ed are used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented an incremental generative depen- dency parser that can obtain accuracies competi- tive with discriminative models. The same model can be applied as an efficient syntactic language model, and for future work it should be integrated into language generation tasks such as machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>We evaluate our model for parsing and language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup 1 . Constituency trees are converted to projective CoNLL syntac- tic dependencies (Johansson and Nugues, 2007) with the LTH converter 2 . For some experiments</figDesc><table>Activation UAS LAS 
linear 
88.40 86.48 
rectifier 
89.99 88.31 
tanh 
90.91 89.22 
sigmoid 
91.48 89.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parsing accuracies using different neural 
network activation functions. 

we also use the Stanford dependency representa-
tion (De Marneffe and Manning, 2008) (SD) 3 . 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 5 : Sentences of length 20 or greater generated by the neural generative dependency model.</head><label>5</label><figDesc></figDesc><table>Model 
Perplexity 
KN 5-gram 
145.7 
NN 5-gram 
142.5 
NN-GenDP 
132.2 
NN-GenDP + unsup 
111.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>WSJ Language modelling test results. 
We compare our model, with and without unsu-
pervised tuning, to n-gram baselines. 

</table></figure>

			<note place="foot" n="1"> Training on sections 02-21, development on section 22, and testing on section 23. 2 http://nlp.cs.lth.se/software/treebank converter/</note>

			<note place="foot" n="4"> However instead of using multiple unknown word classes, we replace all numbers by 0 and have a single unknown word token.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We acknowledge the financial support of the Ox-ford Clarendon Fund and the Skye Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pragmatic neural language modelling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HTL</title>
		<meeting>NAACL-HTL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="820" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Oxlm: A neural language modelling framework for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Baltescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CONLL</title>
		<meeting>EMNLP-CONLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Bayesian model for generative transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04334</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Immediate-head parsing for language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="332" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with selectional branching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exact inference for generative probabilistic non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1234" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo methods in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Joural of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A neural syntactic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="195" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extended constituent-to-dependency conversion for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Nordic Conference of Computational Linguistics</title>
		<meeting><address><addrLine>Tartu, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Short Papers</title>
		<meeting>ACL: Short Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine learning</title>
		<meeting>the 24th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deterministic dependency parsing of English text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A latent variable model for generative dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Parsing Technologies</title>
		<meeting>the Tenth International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="144" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bayesian modeling of dependency trees using hierarchical Pitman-Yor priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Hanna M Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Prior Knowledge for Text and Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT: Short papers</title>
		<meeting>ACL-HLT: Short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
