<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning of Distributional Relation Vectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>M.S.Jameel@kent.ac.uk</roleName><forename type="first">Zied</forename><surname>Bouraoui</surname></persName>
							<email>bouraoui@cril.fr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
							<email>schockaerts1@cardiff.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computing</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Informatics</orgName>
								<orgName type="laboratory">CRIL CNRS and Artois University</orgName>
								<orgName type="institution">Medway Campus University of Kent</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cardiff University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Learning of Distributional Relation Vectors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="23" to="33"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>23</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings are vector space representations of word meaning <ref type="bibr" target="#b21">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b24">Pennington et al., 2014)</ref>. A remarkable property of these models is that they capture various lexical rela- tionships, beyond mere similarity. For example, ( <ref type="bibr" target="#b21">Mikolov et al., 2013b)</ref> found that analogy ques- tions of the form "a is to b what c is to ?" can often be answered by finding the word d that max- imizes cos(w b − w a + w c , w d ), where we write w x for the vector representation of a word x.</p><p>Intuitively, the word vector w a represents a in terms of its most salient features. For example, w paris implicitly encodes that Paris is located in France and that it is a capital city, which is intu- itively why the 'capital of' relation can be mod- eled in terms of a vector difference. Other rela- tionships, however, such as the fact that Macron succeeded Hollande as president of France, are un- likely to be captured by word embeddings. Rela- tion extraction methods can discover such infor- mation by analyzing sentences that contain both of the words or entities involved ( <ref type="bibr" target="#b22">Mintz et al., 2009;</ref><ref type="bibr" target="#b25">Riedel et al., 2010;</ref><ref type="bibr" target="#b8">dos Santos et al., 2015</ref>), but they typically need a large number of training ex- amples to be effective.</p><p>A third alternative, which we consider in this paper, is to characterize the relatedness between two words s and t by learning a relation vector r st in an unsupervised way from corpus statistics. Among others, such vectors can be used to find word pairs that are similar to a given word pair (i.e. finding analogies), or to find the most pro- totypical examples among a given set of relation instances. They can also be used as an alternative to the aforementioned relation extraction methods, by subsequently training a classifier that uses the relation vectors as input, which might be particu- larly effective in cases where only limited amounts of training data are available (with the case of anal- ogy finding from a single instance being an ex- treme example).</p><p>The most common unsupervised approach for learning relation vectors consists of averaging the embeddings of the words that occur in between s and t, in sentences that contain both ( <ref type="bibr" target="#b33">Weston et al., 2013;</ref><ref type="bibr" target="#b10">Fan et al., 2015;</ref><ref type="bibr" target="#b11">Hashimoto et al., 2015)</ref>. While this strategy is often surprisingly effective ( <ref type="bibr" target="#b13">Hill et al., 2016)</ref>, it is sub-optimal for two rea- sons. First, many of the words co-occurring with s and t will be semantically related to s or to t, but will not actually be descriptive for the relationship between s and t; e.g. the vector describing the re- lation between Paris and France should not be af- fected by words such as eiffel (which only relates to Paris). Second, it gives too much weight to stop- words, which cannot be addressed in a straightfor- ward way as some stop-words are actually crucial for modeling relationships (e.g. prepositions such as 'in' or 'of' or Hearst patterns <ref type="bibr" target="#b14">(Indurkhya and Damerau, 2010)</ref>).</p><p>In this paper, we propose a method for learn- ing relation vectors directly from co-occurrence statistics. We first introduce a variant of GloVe, in which word vectors can be directly interpreted as smoothed PMI-weighted bag-of-words represen- tations. We then represent relationships between words as weighted bag-of-words representations, using generalizations of PMI to three arguments, and learn vectors that correspond to smoothed ver- sions of these representations.</p><p>As far as the possible applications of our methodology is concerned, we imagine that rela- tion vectors can be used in various ways to enrich the input to neural network models. As a sim- ple example, in a question answering system, we could "annotate" mentions of entities with relation vectors encoding their relationship to the differ- ent words from the question. As another exam- ple, we could consider a recommendation system which takes advantage of vectors expressing the relationship between items that have been bought (or viewed) by a customer and other items from the catalogue. Finally, relation vectors should also be useful for knowledge completion, especially in cases where few training examples per relation type are given (meaning that neural network mod- els could not be used) and where relations cannot be predicted from the already available knowledge (meaning that knowledge graph embedding meth- ods could not be used, or are at least not sufficient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The problem of characterizing the relationship be- tween two words has been studied in various set- tings. From a learning point of view, the most straightforward setting is where we are given la- beled training sentences, with each label explic- itly indicating what relationship is expressed in the sentence. This fully supervised setting has been the focus of several evaluation campaigns, in- cluding as part of ACE ( <ref type="bibr" target="#b7">Doddington et al., 2004</ref>) and at <ref type="bibr">SemEval 2010</ref><ref type="bibr" target="#b12">(Hendrickx et al., 2010</ref>. A key problem with this setting, however, is that la- beled training data is hard to obtain. A popular alternative is to use known instances of the rela- tions of interest as a form of distant supervision <ref type="bibr" target="#b22">(Mintz et al., 2009;</ref><ref type="bibr" target="#b25">Riedel et al., 2010)</ref>. Some au- thors have also considered unsupervised relation extraction methods <ref type="bibr" target="#b27">(Shinyama and Sekine, 2006;</ref><ref type="bibr" target="#b1">Banko et al., 2007)</ref>, in which case the aim is es- sentially to find clusters of patterns that express similar relationships, although these relationships may not correspond to the ones that are needed for the considered application. Finally, several sys- tems have also used bootstrapping strategies <ref type="bibr" target="#b4">(Brin, 1998;</ref><ref type="bibr" target="#b0">Agichtein and Gravano, 2000;</ref><ref type="bibr">Carlson et al., 2010)</ref>, where a small set of instances are used to find extraction patterns, which are used to find more instances, which can in turn be used to find better extraction patterns, etc.</p><p>Traditionally, relation extraction systems have relied on a variety of linguistic features, such as lexical patterns, part-of-speech tags and depen- dency parsers. More recently, several neural net- work architectures have been proposed for the re- lation extraction problem. These architectures rely on word embeddings to represent the words in the input sentence, and manipulate these word vectors to construct a relation vector. Some approaches simply represent the sentence (or the phrase con- necting the entities whose relationship we want to determine) as a sequence of words, and use e.g. convolutional networks to aggregate the vectors of the words in this sequence ( <ref type="bibr" target="#b35">Zeng et al., 2014;</ref><ref type="bibr" target="#b8">dos Santos et al., 2015)</ref>. Another possibility, explored in ( <ref type="bibr" target="#b28">Socher et al., 2012</ref>), is to use parse trees to cap- ture the structure of the sentence, and to use re- cursive neural networks (RNNs) to aggregate the word vectors in a way which respects this struc- ture. A similar approach is taken in ( <ref type="bibr" target="#b34">Xu et al., 2015)</ref>, where LSTMs are applied to the shortest path between the two target words in a depen- dency parser. A straightforward baseline method is to simply take the average of the word vec- tors ( <ref type="bibr" target="#b23">Mitchell and Lapata, 2010)</ref>. While conceptu- ally much simpler, variants of this approach have obtained state-of-the-art performance for relation classification ( <ref type="bibr" target="#b11">Hashimoto et al., 2015</ref>) and a va- riety of tasks that require sentences to be repre- sented as a vector ( <ref type="bibr" target="#b13">Hill et al., 2016)</ref>.</p><p>Given the effectiveness of word vector averag- ing, in ( <ref type="bibr" target="#b17">Kenter et al., 2016</ref>) a model was proposed that explicitly tries to learn word vectors that gen- eralize well when being averaged. Similarly, the model proposed in ( <ref type="bibr" target="#b11">Hashimoto et al., 2015</ref>) aims to produce word vectors that perform well for the specific task of relation classification. The Para- graphVector method from ( <ref type="bibr" target="#b18">Le and Mikolov, 2014</ref>) is related to the aformentioned approaches, but it explicitly learns a vector representation for each paragraph along with the word embeddings. How- ever, this method is computationally expensive, and often fails to outperform simpler approaches ( <ref type="bibr" target="#b13">Hill et al., 2016)</ref>.</p><p>To the best of our knowledge, existing methods for learning relation vectors are all based on ma- nipulating pre-trained word vectors. In contrast, we will directly learn relation vectors from cor- pus statistics, which will have the important ad- vantage that we can focus on words that describe the interaction between the two words s and t, i.e. words that commonly occur in sentences that con- tain both s and t, but are comparatively rare in sen- tences that only contain s or only contain t.</p><p>Finally, note that our work is fundamentally dif- ferent from Knowledge Graph Embedding (KGE) ( <ref type="bibr" target="#b32">Wang et al., 2014b</ref>), ( <ref type="bibr" target="#b31">Wang et al., 2014a</ref>), <ref type="bibr" target="#b3">(Bordes et al., 2011</ref>) in at least two ways: (i) KGE models start from a structured knowledge graph whereas we only take a text corpus as input, and (ii) KGE models represent relations as geometric objects in the "entity embedding" itself (e.g. as translations, linear maps, combinations of projec- tions and translations, etc), whereas we represent words and relations in different vector spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Vectors as PMI Encodings</head><p>Our approach to relation embedding is based on a variant of the GloVe word embedding model ( <ref type="bibr" target="#b24">Pennington et al., 2014)</ref>. In this section, we first briefly recall the GloVe model itself, after which we discuss our proposed variant. A key advantage of this variant is that it allows us to directly inter- pret word vectors in terms of the Pointwise Mu- tual Information (PMI), which will be central to the way in which we learn relation vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>The GloVe model ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>) learns a vector w i for each word i in the vocabulary, based on a matrix of co-occurrence counts, en- coding how often two words appear within a given window. Let us write x ij for the number of times word j appears in the context of word i in some text corpus. More precisely, assume that there are m sentences in the corpus, and let P l i ⊆ {1, ..., n l } be the set of positions from the l th sentence where the word i can be found (with n l the length of the sentence). Then x ij is defined as follows:</p><formula xml:id="formula_0">m l=1 p∈P l i q∈P l j weight(p, q)</formula><p>where weight(p, q) = 1 |p−q| if 0 &lt; |p − q| ≤ W , and weight(p, q) = 0 otherwise, where the win- dow size W is usually set to 5 or 10.</p><p>The GloVe model learns for each word i two vectors w i and˜wand˜ and˜w i by optimizing the following ob- jective:</p><formula xml:id="formula_1">i j:x ij =0 f (x ij )(w i · ˜ w j + b i + ˜ b j − log x ij ) 2</formula><p>where f is a weighting function, aimed at re- ducing the impact of rare terms, and b i and˜band˜ and˜b j are bias terms. The GloVe model is closely re- lated to the notion of pointwise mutual informa- tion (PMI), which is defined for two words i and j as PMI(i, j) = log P (i,j)</p><formula xml:id="formula_2">P (i)P (j)</formula><p>, where P (i, j) is the probability of seeing the words i and j if we ran- domly pick a word position from the corpus and a second word position within distance W from the first position. The PMI between i and j is usually estimated as follows:</p><formula xml:id="formula_3">PMI X (i, j) = log x ij x * * x i * x * j</formula><p>where</p><formula xml:id="formula_4">x i * = j x ij , x * j = i x ij and x * * = i j x ij .</formula><p>In particular, it is straightforward to see that after the reparameterization given by b i → b i + log x i * − log x * * and b j → b j + log x * j , the GloVe model is equivalent to</p><formula xml:id="formula_5">i j x ij =0 f (x ij )(w i · ˜ w j + b i + ˜ b j − PMI X (i, j)) 2</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Variant of GloVe</head><p>In this paper, we will use the following variant of the formulation in (1):</p><formula xml:id="formula_6">i j∈J i 1 σ 2 j (w i · ˜ w j + ˜ b j − PMI S (i, j)) 2<label>(2)</label></formula><p>Despite its similarity, this formulation differs from the GloVe model in a number of important ways. First, we use smoothed frequency counts instead of the observed frequency counts x ij . In particu- lar, the PMI between words i and j is given as:</p><formula xml:id="formula_7">PMI S (i, j) = log P (i, j) P (i)P (j)</formula><p>where the probabilities are estimated as follows:</p><formula xml:id="formula_8">P (i) = x i * + α x * * + nα P (j) = x * j + α x * * + nα P (i, j) = x ij + α x * * + n 2 α</formula><p>where α ≥ 0 is a parameter controlling the amount of smoothing and n is the size of the vocabulary. This ensures that the estimation of PMI(i, j) is well-defined even in cases where x ij = 0, mean- ing that we no longer have to restrict the inner summation to those j for which x ij &gt; 0. For efficiency reasons, in practice, we only consider a small subset of all context words j for which x ij = 0, which is similar in spirit to the use of negative sampling in Skip-gram ( <ref type="bibr" target="#b21">Mikolov et al., 2013b</ref>). In particular, the set J i contains each j such that x ij &gt; 0 as well as M uniformly 1 sam- pled context words j for which x ij = 0, where we choose M = 2 · |{j :</p><formula xml:id="formula_9">x ij &gt; 0}|.</formula><p>Second, following <ref type="bibr" target="#b15">(Jameel and Schockaert, 2016)</ref>, the weighting function f (x ij ) has been re- placed by 1 σ 2 j , where σ 2 j is the residual variance of the regression problem for context word j, esti- mated follows:</p><formula xml:id="formula_10">σ 2 j = 1 |J −1 j | i∈J −1 j (w i · ˜ w j + ˜ b j − PMI S (i, j)) 2 with J −1 j = {i : j ∈ J i }.</formula><p>Since we need the word vectors to estimate this residual variance, we re- estimate σ 2 j after every five iterations of the SGD optimization. For the first 5 iterations, where no estimation for σ 2 j is available, we use the GloVe weighting function.</p><p>The use of smoothed frequency counts and residual variance based weighting make the word embedding model more robust for rare words. For instance, if w only co-occurs with a handful of other terms, it is important to prioritize the most informative context words, which is exactly what the use of the residual variance achieves, i.e. σ 2 j is small for informative terms and large for stop words; see ( <ref type="bibr" target="#b15">Jameel and Schockaert, 2016</ref>). This will be important for modeling relations, as the re- lation vectors will often have to be estimated from very sparse co-occurrence counts.</p><p>Finally, the bias term b i has been omitted from the model in <ref type="bibr">(2)</ref>. We have empirically found that omitting this bias term does not affect the perfor- mance of the model, while it allows us to have a more direct connection between the vector w i and the corresponding PMI scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Vectors and PMI</head><p>Let us define PMI W as follows:</p><formula xml:id="formula_11">PMI W (i, j) = w i · ˜ w j + ˜ b j</formula><p>Clearly, when the word vectors are trained accord- ing to (2), it holds that</p><formula xml:id="formula_12">PMI W (i, j) ≈ PMI S (i, j).</formula><p>In other words, we can think of the word vector w i as a low-dimensional encoding of the vector (PMI S (i, 1), ..., PMI S (i, n)), with n the number of words in the vocabulary. This view allows us to assign a natural interpretation to some word vec- tor operations. In particular, the vector difference w i − w k is commonly used as a model for the rela- tionship between words i and k. For a given con- text word j, we have</p><formula xml:id="formula_13">(w i − w k ) · ˜ w j = PMI W (i, j) − PMI W (k, j)</formula><p>The latter is an estimation of log</p><formula xml:id="formula_14">P (i,j) P (i)P (j) − log P (k,j) P (k)P (j) = log P (j|i) P (j|k)</formula><p>. In other words, the vector translation w i − w k encodes for each context word j the (log) ratio of the probability of seeing j in the context of i and in the context of k, which is in line with the original motivation under- lying the GloVe model ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>). In the following section, we will propose a num- ber of alternative vector representations for the re- lationship between two words, based on general- izations of PMI to three arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Global Relation Vectors</head><p>We now turn to the problem of learning a vector r ik that encodes how the source word i and tar- get word k are related. The main underlying idea is that r ik will capture which context words j are most closely associated with the word pair (i, k). Whereas the GloVe model is based on statistics about (main word, context word) pairs, here we will need statistics on (source word, context word, target word) triples. First, we discuss how co- occurrence statistics among three words can be ex- pressed using generalizations of PMI to three ar- guments. Then we explain how this can be used to learn relation vectors in natural way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Co-occurrence Statistics for Triples</head><p>Let P l i ⊆ {1, ..., n l } again be the set of positions from the l th sentence corresponding to word i. We define:</p><formula xml:id="formula_15">y ijk = m l=1 p∈P l i q∈P l j r∈P l k weight(p, q, r)</formula><p>where weight(p, q, r) = max( 1 q−p , 1 r−q ) if p &lt; q &lt; r and r−p ≤ W , and weight(p, q, r) = 0 oth- erwise. In other words, y ijk reflects the (weighted) number of times word j appears between words i and k in a sentence in which i and k occur suffi- ciently close to each other, in that order. Note that by taking word order into account in this way, we will be able to model asymmetric relationships.</p><p>To model how strongly a context word j is asso- ciated with the word pair (i, k), we will consider the following two well-known generalizations of PMI to three arguments (Van de Cruys, 2011):</p><formula xml:id="formula_16">SI 1 (i, j, k) = log P (i, j)P (i, k)P (j, k) P (i)P (j)P (k)P (i, j, k) SI 2 (i, j, k) = log P (i, j, k) P (i)P (j)P (k)</formula><p>where P (i, j, k) is the probability of seeing the word triple (i, j, k) when randomly choosing a sentence and three (ordered) word positions in that sentence within a window size of W . In addition we will also consider two ways in which PMI can be used more directly:</p><formula xml:id="formula_17">SI 3 (i, j, k) = log P (i, j, k) P (i, k)P (j) SI 4 (i, j, k) = log P (i, k|j) P (i|j)P (k|j)</formula><p>Note that SI 3 (i, j, k) corresponds to the PMI be- tween (i, k) and j, whereas SI 4 (i, j, k) is the PMI between i and k conditioned on the fact that j oc- curs. The measures SI 3 and SI 4 are closely related to SI 1 and SI 2 respectively 2 . In particular, the fol- lowing identities are easy to show:</p><formula xml:id="formula_18">PMI(i, j) + PMI(j, k) − SI 1 (i, j, k) = SI 3 (i, j, k) SI 2 (i, j, k) − PMI(i, j) − PMI(j, k) = SI 4 (i, j, k)</formula><p>2 Note that probabilities of the form P (i, j) or P (i) here refer to marginal probabilities over ordered triples. In con- trast, the PMI scores from the word embedding model are based on probabilities over unordered word pairs, as is com- mon for word embeddings.</p><p>Using smoothed versions of the counts y ijk , we can use the following probability estimates for SI 1 (i, j, k)-SI 4 (i, j, k):</p><formula xml:id="formula_19">P (i, j, k) = y ijk + α y * * * + n 3 α P (i, j) = y ij * + α y * * * + n 2 α P (i, k) = y i * k + α y * * * + n 2 α P (j, k) = y * jk + α y * * * + n 2 α P (i) = y i * * + α y * * * + nα P (j) = y * j * + α y * * * + nα P (k) = y * * k + α y * * * + nα</formula><p>where y ij * = k y ijk , and similar for the other counts. For efficiency reasons, the counts of the form y ij * , y i * k and y * jk are pre-computed for all word pairs, which can be done efficiently due to the sparsity of co-occurrence counts (i.e. these counts will be 0 for most pairs of words), sim- ilarly to how to the counts x ij are computed in GloVe. From these counts, we can also efficiently pre-compute the counts y i * * , y * j * , y * * k and y * * * . On the other hand, the counts y ijk cannot be pre- computed, since the total number of triples for which y ijk = 0 is prohibitively high in a typi- cal corpus. However, using an inverted index, we can efficiently retrieve the sentences that contain the words i and k, and since this number of sen- tences is typically small, we can efficiently obtain the counts y ijk corresponding to a given pair (i, k) whenever they are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relation Vectors</head><p>Our aim is to learn a vector r ik that models the relationship between i and k. Computing such a vector for each pair of words (which co-occur at least once) is not feasible, given the number of triples (i, j, k) that would need to be considered. Instead, we first learn a word embedding, by op- timizing (2). Then, fixing the context vectors˜wvectors˜ vectors˜w j and bias terms b j , we learn a vector representation for a given pair (i, k) of interest by solving the fol- lowing objective:</p><formula xml:id="formula_20">j∈J i,k (r ik · ˜ w j + ˜ b j − SI(i, j, k)) 2<label>(3)</label></formula><p>where SI refers to one of SI 1 S , SI 2 S , SI 3 S , SI 4 S . Note that (3) is essentially the counterpart of (1), where we have replaced the role of the PMI measure by SI. In this way, we can exploit the representations of the context words from the word embedding model for learning relation vectors. Note that the factor 1 σ 2 j has been omitted. This is because words j that are normally relatively uninformative (e.g. stop words), for which σ 2 j would be high, can actu- ally be very important for characterizing the rela- tionship between i and k. For instance, the phrase "X such as Y " clearly suggests a hyponomy re- lationship between X and Y , but both 'such' and 'as' would be associated with a high residual vari- ance σ 2 j . The set J i,k contains every j for which y ijk &gt; 0 as well as a random sample of m words for which y ijk = 0, where m = 2 · |{j : y ijk &gt; 0|. Note that because˜wbecause˜ because˜w j is now fixed, (3) is a lin- ear least squares regression problem, which can be solved exactly and efficiently.</p><p>The vector r ik is based on words that appear between i and k. In the same way, we can learn a vector s ik based on the words that appear be- fore i and a vector t ik based on the words that appear after k, in sentences where i occurs be- fore k. Furthermore, we also learn vectors r ki , s ki and t ki from the sentences where k occurs before i. As the final representation R ik of the relation- ship between i and k, we concatenate the vectors r ik , r ki , s ik , s ki , t ik , t ki as well as the word vectors w i and w k . We write R l ik to denote the vector that results from using measure SI l (l ∈ {1, 2, 3, 4}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In our experiments, we have used the Wikipedia dump from November 2nd, 2015, which consists of 1,335,766,618 tokens. We have removed punc- tuations and HTML/XML tags, and we have low- ercased all tokens. Words with fewer than 10 occurrences have been removed from the corpus. To detect sentence boundaries, we have used the Apache sentence segmentation tool. In all our experiments, we have set the number of dimen- sions to 300, which was found to be a good choice in previous work, e.g. ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>). We use a context window size W of 10 words. The number of iterations for SGD was set to 50. For our model, we have tuned the smooth- ing parameter α based on held-out tuning data, considering values from {0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001}. We have noticed that in most of the cases the value of α was automatically se- lected as 0.00001. To efficiently compute the triples, we have used the Zettair 3 retrieval engine.</p><p>As our main baselines, we use three popular un- supervised methods for constructing relation vec- 3 http://www.seg.rmit.edu.au/zettair/  <ref type="bibr">et al., 2016)</ref>. Second, Conc uses the concatenation of w i and w k . This model is more general than Diff but it uses twice as many dimensions, which may make it harder to learn a good classifier from few examples. The use of concatenations is popular e.g. in the context of hypernym detection ( <ref type="bibr" target="#b2">Baroni et al., 2012</ref>). Finally, Avg averages the vector rep- resentations of the words occurring in sentences that Diff, contain i and k. In particular, let r avg ik be obtained by averaging the word vectors of the con- text words appearing between i and k for each sen- tence containing i and k (in that order), and then averaging the vectors obtained from each of these sentences. Let s avg ik and t avg ik be similarly obtained from the words occurring before i and the words occurring after k respectively. The considered re- lation vector is then defined as the concatenation of r avg ik , r avg ki , s avg ik , s avg ki , t avg ik , t avg ki , w i and w k . The Avg will allow us to directly compare how much we can improve relation vectors by deviating from the common strategy of averaging word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relation Induction</head><p>In the relation induction task, we are given word pairs (s 1 , t 1 ), ..., (s k , t k ) that are related in some way, and the task is to decide for a number of test examples (s, t) whether they also have this rela- tionship. Among others, this task was considered in ( <ref type="bibr" target="#b30">Vylomova et al., 2016)</ref>, and a ranking version of this task was studied in ( <ref type="bibr" target="#b9">Drozd et al., 2016)</ref>. As test sets we use the Google Analogy Test Set ( <ref type="bibr" target="#b20">Mikolov et al., 2013a</ref>), which contains instances of 14 different types of relations, and the DiffVec dataset, which was introduced in ( <ref type="bibr" target="#b30">Vylomova et al., 2016)</ref>. This dataset contains instances of 36 dif- <ref type="table">Table 2</ref>: Results for the relation induction task using alternative word embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GloVe</head><p>SkipGram <ref type="table" target="#tab_0">CBOW  Google  DiffVec  Google  DiffVec  Google  DiffVec  Acc  F1  Acc  F1  Acc  F1  Acc  F1  Acc  F1  Acc  F1  Diff</ref> 90.0 81.9 21.2 13.9 89.8 81.9 21.7 14.5 89.9 82.1 17.4 9.7 Conc 88.9 80.4 20.2 11.9 89.2 81.6 20.5 12.0 89.1 81.1 16.4</p><p>7.7 Avg 89.8 82.1 21.4 13.9 90.2 82.4 21.8 14.4 89.8 82.2 17. ferent types of relations 4 . Note that both datasets contain a mix of semantic and syntactic relations.</p><p>In our evaluation, we have used 10-fold cross- validation (or leave-one-out for relations with fewer than 10 instances). In the experiments, we consider for each relation in the test set a separate binary classification task, which was found to be considerably more challenging than a multi-class classification setting in ( <ref type="bibr" target="#b30">Vylomova et al., 2016)</ref>. To generate negative examples in the training data (resp. test data), we have used three strategies, fol- lowing ( <ref type="bibr" target="#b30">Vylomova et al., 2016)</ref>. First, for a given positive example (s, t) of the considered relation, we add (t, s) as a negative example. Second, for each positive example (s, t), we generate two neg- ative examples (s, t 1 ) and (s, t 2 ) by randomly se- lecting two tail words t 1 , t 2 from the other training (resp. test) examples of the same relation. Finally, for each positive example, we also generate a neg- ative example by randomly selecting two words from the vocabulary. For each relation, we then train a linear SVM classifier. To set the parameters of the SVM, we initially use 25% of the training data for tuning, and then retrain the SVM with the optimal parameters on the full training data.</p><p>The results are summarized in <ref type="table" target="#tab_0">Table 1</ref> in terms of accuracy and (macro-averaged) precision, recall and F1 score. As can be observed, our model out- performs the baselines on both datasets, with the R 2 ik variant outperforming the others.</p><p>To analyze the benefit of our proposed word embedding variant, <ref type="table">Table 2</ref> shows the results that were obtained when we use standard word embed- ding models. In particular, we show results for the standard GloVe model, SkipGram and the Contin- uous Bag of Words (CBOW) model. As can be observed, our variant leads to better results than the original GloVe model, even for the baselines. The difference is particularly noticeable for Diff- Vec. The difference is also larger for our relation vectors than for the baselines, which is expected as our method is based on the assumption that con- text word vectors can be interpreted in terms of PMI scores, which is only true for our variant.</p><p>Similar as in the GloVe model, the context words in our model are weighted based on their distance to the nearest target word. <ref type="table" target="#tab_1">Table 3</ref> shows the results of our model without this weighting, for the relation induction task. Comparing these re- sults with those in <ref type="table" target="#tab_0">Table 1</ref> shows that the weight- ing scheme indeed leads to a small improvement (except for the accuracy of R 1 ik for DiffVec). Sim- ilarly, in <ref type="table" target="#tab_1">Table 3</ref>, we show what happens if the re- lation vectors s ik , s ki , t ik and t ki are omitted. In other words, for the results in <ref type="table" target="#tab_1">Table 3</ref>, we only use context words that appear between the two target words. Again, the results are worse than those in <ref type="table" target="#tab_0">Table 1</ref> (with the accuracy of R 1 ik for Diff- Vec again being an exception), although the dif- ferences are very small in this case. While includ- ing the vectors s ik , s ki , t ik , t ki should be helpful, it also significantly increases the dimensionality of the vectors R l ik . Given that the number of in- stances per relation is typically quite small for this <ref type="table">Table 4</ref>: Results for measuring degrees of proto- typicality (Spearman ρ × 100).</p><formula xml:id="formula_21">Diff Conc Avg R 1 ik R 2 ik R 3 ik R 4 ik 17.3</formula><p>16.7 21.1 22.7 23.9 21.8 22.2 task, this can also make it harder to learn a suitable classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Measuring Degrees of Prototypicality</head><p>Instances of relations can often have different de- grees of prototypicality. For example, for the rela- tion "X characteristically makes the sound Y ", the pair (dog,bark) should be considered more proto- typical than the pair (floor,squeak), even though both pairs might be considered to be instances of the relation ( <ref type="bibr" target="#b16">Jurgens et al., 2012)</ref>. A suit- able relation vector should allow us to rank word pairs according to how prototypical they are as instances of that relation. We evaluate this abil- ity using a dataset that was produced in the after- math of SemEval 2012 Task 2. In particular, we have used the "Phase2AnswerScaled" data from the platinum rankings dataset, which is available from the SemEval 2012 Task 2 website 5 . In this dataset, 79 ranked list of word pairs are provided, each of which corresponds to a particular relation. For each relation, we first split the associated rank- ing into 60% training, 20% tuning, and 20% test- ing (i.e. we randomly select 60% of the word pairs and use their ranking as training data, and similar for tuning and test data). We then train a linear SVM regression model on the ranked word pairs. Note that this task slightly differs from the task that was considered at SemEval 2012, to allow us to use an SVM based model for consistency with the rest of the paper. We report results using Spearman's ρ in <ref type="table">Table  4</ref>. Our model again outperforms the baselines, with R 2 ik again being the best variant. Interest- ingly, in this case, the Avg baseline is consider- ably stronger than Diff and Conc. Intuitively, we might indeed expect that this ranking problem re- quires a more fine-grained representation than the relation induction setting. Note that the Diff repre- sentations were found to achieve near state-of-the- art performance on a closely related task in ( <ref type="bibr" target="#b36">Zhila et al., 2013</ref>). The only model that was found to perform (slightly) better was a hybrid model, com- bining Diff representations with linguistic patterns </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head><formula xml:id="formula_22">Precision R 1 ik R 2 ik R 2 ik (Quadratic) R 3 ik R 4</formula><p>ik Avg Diff Conc <ref type="figure">Figure 1</ref>: Results for the relation extraction from the NYT corpus: comparison with the main base- lines.</p><p>(inspired by <ref type="bibr" target="#b26">(Rink and Harabagiu, 2012)</ref>) and lex- ical databases, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Relation Extraction</head><p>Finally, we consider the problem of relation ex- traction from a text corpus. Specifically, we con- sider the task proposed in ( <ref type="bibr" target="#b25">Riedel et al., 2010)</ref>, which is to extract (subject,predicate,object) triples from the New York Times (NYT) corpus.</p><p>Rather than having labelled sentences as training data, the task requires using the existing triples from Freebase as a form of distant supervision, i.e. for some pairs of entities we know some of the relations that hold between them, but not which sentences assert these relationships (if any). To be consistent with published results for this task, we have used a word embedding that was trained from the NYT corpus 6 , rather than Wikipedia (using the same preprocessing and set-up). We have used the training and test data that was shared publicly for this task 7 , which consist of sentences from arti- cles published in <ref type="bibr">[2005]</ref><ref type="bibr">[2006]</ref> and in 2007, respec- tively. Each of these sentences contains two en- tities, which are already linked to Freebase. We learn relation vectors from the sentences in the training and test sets, and learn a linear SVM clas- sifier based on the Freebase triples that are avail- able in the training set. Initially, we split the train- ing data into 75% training and 25% tuning to find the optimal parameters of the linear SVM model. We tuned the parameters for each test fold sepa- rately. For each test fold, we used 25% of the 9 training folds as tuning data. After the optimal parameters have been determined, we retrain the model on the full training data, and apply it on the test fold. We used this approach (rather than e.g. fixing a train/tune/test split) because the to- tal number of examples for some of the relations is very small. After tuning, we re-train the SVM models on the full training data. As the number of training examples is larger for this task, we also consider SVMs with a quadratic kernel. Following earlier work on this task, we re- port our results on the test set as a precision- recall graph in <ref type="figure">Figure 1</ref>. This shows that the best performance is again achieved by R 2 ik , espe- cially for larger recall values. Furthermore, us- ing a quadratic kernel (only shown for R 2 ik ) out- performs the linear SVM models. Note that the differences between the baselines are more pro- nounced in this task, with Avg being clearly bet- ter than Diff, which is in turn better than Conc. For this relation extraction task, a large number of methods have already been proposed in the lit- erature, with variants of convolutional neural net- work models with attention mechanisms achiev- ing state-of-the-art performance <ref type="bibr">8</ref> . A comparison with these models 9 is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. The per- formance of R 2 ik is comparable with the state-of-the-art PCNN+ATT model ( <ref type="bibr" target="#b19">Lin et al., 2016)</ref>, out- performing it for larger recall values. This is re- markable, as our model is conceptually much sim- pler, and has not been specifically tuned for this task. For instance, it could easily be improved by incorporating the attention mechanism from the PCNN+ATT model to focus the relation vectors on the considered task. Similarly, we could con- sider a supervised variant of <ref type="formula" target="#formula_20">(3)</ref>, in which a learned relation-specific weight is added to each term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed an unsupervised method which uses co-occurrences statistics to represent the re- lationship between a given pair of words as a vec- tor. In contrast to neural network models for rela- tion extraction, our model learns relation vectors in an unsupervised way, which means that it can be used for measuring relational similarities and related tasks. Moreover, even in (distantly) super- vised tasks (where we need to learn a classifier on top of the unsupervised relation vectors), our model has proven competitive with state-of-the-art neural network models. Compared to approaches that rely on averaging word vectors, our method is able to learn more faithful representations by fo- cusing on the words that are most strongly related to the considered relationship.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results for the relation extraction from the NYT corpus: comparison with state-of-the-art neural network models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Results for the relation induction task.</head><label>1</label><figDesc></figDesc><table>Google Analogy 
Diff Conc Avg R 1 

ik 

R 2 

ik 

R 3 

ik 

R 4 

ik 

Acc 90.0 89.0 89.9 90.0 92.3 90.9 90.4 
Pre 81.6 78.7 80.8 79.9 87.1 83.2 81.1 
Rec 82.6 83.9 83.9 86.0 84.8 84.8 85.5 
F1 
82.1 81.2 82.3 82.8 85.9 84.0 83.3 
DiffVec 
Diff Conc Avg R 1 

ik 

R 2 

ik 

R 3 

ik 

R 4 

ik 

Acc 29.5 28.9 29.7 29.7 31.3 30.4 30.1 
Pre 19.6 18.7 20.4 21.5 22.9 21.9 22.3 
Rec 23.8 22.9 23.7 24.5 25.7 25.3 22.9 
F1 
21.5 20.6 21.9 22.4 24.2 23.5 22.6 

tors. First, Diff uses the vector difference w k −w i , 
following the common strategy of modeling rela-
tions as vector differences, as e.g. in (Vylomova 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Relation induction without position 
weighting (left) and without the relation vectors 
s ik and t ik (right). 

Google 
DiffVec 
Acc 
F1 
Acc 
F1 
R 1 

ik 

89.7 82.4 30.2 22.2 
R 2 

ik 

91.0 83.4 30.8 24.1 
R 3 

ik 

90.4 83.2 30.1 22.3 
R 4 

ik 

90.2 82.9 29.1 21.2 
Google 
DiffVec 
Acc 
F1 
Acc 
F1 
R 1 

ik 

90.0 82.5 29.9 22.3 
R 2 

ik 

92.3 85.8 31.2 24.2 
R 3 

ik 

90.5 83.2 30.2 23.0 
R 4 

ik 

90.3 83.1 29.8 22.3 

</table></figure>

			<note place="foot" n="1"> While the negative sampling method used in Skip-gram favors more frequent words, initial experiments suggested that deviating from a uniform distribution almost had no impact in our setting.</note>

			<note place="foot" n="4"> Note that in contrast to (Vylomova et al., 2016) we use all 36 relations from this dataset, including those with very few instances.</note>

			<note place="foot" n="6"> https://catalog.ldc.upenn.edu/LDC2008T19 7 http://iesl.cs.umass.edu/riedel/ecml/</note>

			<note place="foot" n="8"> Note that such models would not be suitable for the evaluation tasks in Sections 5.1 and 5.2, due to the very limited number of training examples. 9 Results for the neural network models have been obtained from https://github.com/thunlp/ TensorFlow-NRE/tree/master/data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by ERC Starting Grant 637277. Experiments in this work were performed using the computational facilities of the Advanced Research Computing at Cardiff (ARCCA) Divi-sion, Cardiff University and the ICARUS compu-tational facility from Information Services, at the University of Kent.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snowball: Extracting relations from large plain-text collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM Conference on Digital libraries</title>
		<meeting>the Fifth ACM Conference on Digital libraries</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting patterns and relations from the world wide web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey Brin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on The World Wide Web and Databases</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="172" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word embeddings, analogies, and machine learning: Beyond king-man + woman = queen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3519" to="3530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jointly embedding relations and mentions for knowledge population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Miao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Task-oriented learning of word embeddings for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Handbook of natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Indurkhya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">D-GloVe: A feasible least squares model for estimating word embedding densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1849" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 2: Measuring degrees of relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David A Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holyoak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. *SEM</title>
		<meeting>*SEM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Siamese CBOW: optimizing word embeddings for sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML/PKDD</title>
		<meeting>ECML/PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">UTD: Determining relational similarity using lexical patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="413" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Preemptive information extraction using unrestricted relation discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Shinyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two multivariate generalizations of pointwise mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Distributional Semantics and Compositionality</title>
		<meeting>the Workshop on Distributional Semantics and Compositionality</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining heterogeneous models for measuring relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Zhila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1000" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
