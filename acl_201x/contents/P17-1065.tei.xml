<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence-to-Dependency Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence-to-Dependency Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="698" to="707"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1065</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art base-lines on Chinese-English and Japanese-English translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder frame- work ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) has achieved sig- nificant improvements in translation quality of many language pairs ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015a;</ref><ref type="bibr" target="#b25">Tu et al., 2016;</ref>. In a conventional NMT model, an encoder reads in source sentences of various lengths, and trans- forms them into sequences of intermediate hidden vector representations. After weighted by atten- tion operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs). * Contribution during internship at Microsoft Research.</p><p>Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by <ref type="bibr" target="#b24">Sutskever et al. (2014)</ref> and <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>. Previous work ranges from addressing the problem of out-of- vocabulary words ( <ref type="bibr" target="#b12">Jean et al., 2015</ref>), designing at- tention mechanism ( <ref type="bibr" target="#b16">Luong et al., 2015a)</ref>, to more efficient parameter learning <ref type="bibr" target="#b23">(Shen et al., 2016)</ref>, using source-side syntactic trees for better encod- ing ( <ref type="bibr" target="#b7">Eriguchi et al., 2016</ref>) and so on. All these NMT models employ a sequential recurrent neu- ral network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect trans- lations which violate long-distance syntactic con- straints. This suggests that it is still very challeng- ing for a linear RNN to learn models that effec- tively capture many subtle long-range word de- pendencies. For example, <ref type="figure">Figure 1</ref> shows an in- correct translation related to the long-distance de- pendency. The translation fragment in italic is lo- cally fluent around the word is, but from a global view the translation is ungrammatical. Actually, this part of translation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby.</p><p>Fortunately, such long-distance word corre- spondence can be well addressed and modeled by syntactic dependency trees. In <ref type="figure">Figure 1</ref>, the head word foreigners in the partial dependency tree (top dashed box) can provide correct structural con- text for the next target word, with this informa- tion it is more likely to generate the correct word will rather than is. This structure has been suc- cessfully applied to significantly improve the per- formance of statistical machine translation <ref type="bibr" target="#b22">(Shen et al., 2008</ref>). On the NMT side, introducing tar- get syntactic structures could help solve the prob- lem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models: a) syntactic trees can be used to model the gram- matical validity of translation candidates; b) par- tial syntactic structures can be used as additional context to facilitate future target word prediction.</p><formula xml:id="formula_0">Source : 他 还 说 , 来 委 外国人 若 攻击 委内瑞拉 政府 会 面临 严重 后果 , 将 被 驱逐出境 . partial tree decoder</formula><p>Ref : He added that foreign visitors to Venezuela who criticize the Venezuelan government will face serious consequences and will be deported .</p><p>NMT : He also said that foreigners to Venezuela who attack the Venezuelan government is facing serious consequences, will be deported . However, it is not trivial to build and leverage syntactic structures on the target side in current NMT framework. Several practical challenges arise:</p><p>(1) How to model syntactic structures such as dependency parse trees with recurrent neural net- work;</p><p>(2) How to efficiently perform both target word generation and syntactic structure construction tasks simultaneously in a single neural network;</p><p>(3) How to effectively leverage target syntactic context to help target word generation.</p><p>To address these issues, we propose and empir- ically evaluate a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) model in our paper. An SD-NMT model encodes source in- puts with bi-directional RNNs and associates them with target word prediction via attention mecha- nism as in most NMT models, but it comes with a new decoder which is able to jointly generate target translations and construct their syntactic de- pendency trees. The key difference from conven- tional NMT decoders is that we use two RNNs, one for translation generation and the other for de- pendency parse tree construction, in which incre- mental parsing is performed with the arc-standard shift-reduce algorithm proposed by <ref type="bibr" target="#b20">Nivre (2004)</ref>.</p><p>We will describe in detail how these two RNNs work interactively in Section 3.</p><p>We evaluate our method on publicly avail- able data sets with Chinese-English and Japanese- English translation tasks. Experimental results show that our model significantly improves trans- lation accuracy over the conventional NMT and SMT baseline systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>As a new paradigm to machine translation, NMT is an end-to-end framework <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) which directly mod- els the conditional probability P (Y |X) of target translation Y = y 1 ,y 2 ,...,y n given source sentence X = x 1 ,x 2 ,...,x m . An NMT model consists of two parts: an encoder and a decoder. Both of them utilize recurrent neural networks which can be a Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>) or a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) in practice. The en- coder bidirectionally encodes a source sentence into a sequence of hidden vectors H = h 1 ,h 2 ,...,h m with a forward RNN and a backward RNN. Then the decoder predicts target words one by one with probability</p><formula xml:id="formula_1">P (Y |X) = n j=1 P (y j |y &lt;j , H)<label>(1)</label></formula><p>Typically, for the jth target word, the probability P (y j |y &lt;j , H) is computed as</p><formula xml:id="formula_2">P (y j |y &lt;j , H) = g(s j , y j−1 , c j )<label>(2)</label></formula><p>where g is a nonlinear function that outputs the probability of y j , and s j is the RNN hidden state. The context c j is calculated at each timestamp j based on H by the attention network</p><formula xml:id="formula_3">c j = m k=1 a jk h k (3) a jk = exp(e jk ) m i=1 exp(e ji )<label>(4)</label></formula><formula xml:id="formula_4">e jk = v T a tanh(W a s j−1 + U a h k )<label>(5)</label></formula><p>where v a , W a , U a are the weight matrices. The attention mechanism is effective to model the cor- respondences between source and target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dependency Tree Construction</head><p>We use a shift-reduce transition-based dependency parser to build the syntactic structure for the target language in our work. Specially, we adopt the arc- standard algorithm <ref type="bibr" target="#b20">(Nivre, 2004</ref>) to perform incre- mental parsing during the translation process. In this algorithm, a stack and a buffer are maintained to store the parsing state over which three kinds of transition actions are applied. Let w 0 and w 1 be two topmost words in the stack, and ¯ w be the cur- rent new word in a sequence of input, three transi- tion actions are described as below.</p><p>• Shift(SH) : Push ¯ w to the stack. During parsing, an specific structure is used to record the dependency relationship between dif- ferent words of input sentence. The parsing fin- ishes when the stack is empty and all input words are consumed. As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2n, where n is the length of the sentence <ref type="bibr" target="#b20">(Nivre, 2004)</ref>. Because each valid transition action se- quence corresponds to a unique dependency tree, a dependency tree can also be equivalently repre- sented by a sequence of transition actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence-to-Dependency Neural Machine Translation</head><p>An SD-NMT model is an extension to the con- ventional NMT model augmented with syntactic structural information of target translation. Given a source sentence X = x 1 ,x 2 ,..,x m , its target trans- lation Y = y 1 ,y 2 ,..,y n and Y 's dependency parse tree T , the goal of the extension is to enable us to compute the joint probability P (Y, T |X). As in most structural learning tasks, the full prediction of Y and T is further decomposed into a chain of smaller predictions. For translation Y , it is gen- erated in the left-to-right order as y 1 , y 2 , .., y n fol- lowing the way in a normal sequence-to-sequence model. For Y 's parse tree T , instead of directly modeling the tree itself, we predict a parsing ac- tion sequence A which can map Y to T . Thus at top level our SD-NMT model can be formulated as</p><formula xml:id="formula_5">P (Y, T |X) = P (Y, A|X) = P (y 1 y 2 ..y n , a 1 , a 2 ..a l |X)(6)</formula><p>where A = a 1 ,a 2 ,..,a j ,..,a l 1 with length</p><formula xml:id="formula_6">l (l = 2n), a j ∈ {SH, RR(d), LR(d)} 2 .</formula><p>Two recurrent neural networks, Word-RNN and Action-RNN, are used to model generation pro- cesses of translation sequence Y and parsing ac- tion sequence A respectively. <ref type="figure" target="#fig_2">Figure 2</ref> shows an example how translation Y and its parsing actions are predicted step by step. Because the lengths of Word-RNN and Action- RNN are different, they are designed to work in a mutually dependent way: a target word is only allowed to be generated when the SH action is predicted in the action sequence. In this way, we can perform incremental dependency parsing for translation Y and at the same time track the par- tial parsing status through the translation genera- tion process.</p><p>For notational clarity, we introduce a virtual translation sequencê Y =ˆy=ˆy 1 ,ˆ y 2 ,..,ˆ y j ,..,ˆ y l for Word- RNN which has the same length l with transition action sequence. ˆ y j is defined asˆy  to Y is deterministic, and Y can be easily derived givenˆYgivenˆ givenˆY and A.</p><formula xml:id="formula_7">asˆ asˆy j = y v j δ(SH, a j ) = 1 y v j−1 δ(SH, a j ) = 0 where δ(SH, a j ) is 1 when a j = SH, otherwise 0. v j is the index of Y , computed by v j = j i=1 δ(SH, a i ).</formula><p>With the notation ofˆYofˆ ofˆY , the sequence probability of Y and A can be written as</p><formula xml:id="formula_8">P (A|X, ˆ Y &lt;l ) = l j=1 P (a j |a &lt;j , X, ˆ Y &lt;j )<label>(7)</label></formula><formula xml:id="formula_9">P ( ˆ Y |X, A ≤l ) = l j=1 P (ˆ y j |ˆy|ˆy &lt;j , X, A ≤j ) δ(SH,aj )<label>(8)</label></formula><p>wherê Y &lt;j refers to the subsequencêsubsequencê y 1 , ˆ y 2 , .., ˆ y j−1 , and A ≤j to a 1 , a 2 , .., a j . Based on Equation 7 and 8, the overall joint model can be computed as</p><formula xml:id="formula_10">P (Y, T |X) = P (A|X, ˆ Y &lt;l ) × P ( ˆ Y |X, A ≤l )<label>(9)</label></formula><p>As we have two RNNs in our model, the termina- tion condition is also different from a conventional NMT model. In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced. <ref type="figure" target="#fig_3">Figure 3</ref> (a) gives an overview of our SD-NMT model. Due to space limitation, the detailed inter- connections between two RNNs are only illus- trated at timestamp j. The encoder of our model follows standard bidirectional RNN configuration. At timestamp j during decoding, our model first predicts an action a j by Action-RNN, then Word- RNN checks the condition gate δ according to a j . If a j = SH, the Word-RNN will generate a new state (solid arrow) and predict a new target word y v j , otherwise it just copies previous state (dashed arrow) to the current state. For example, at times- tamp 3, a 3 = SH, the state of Word-RNN is copied from its previous one. Meanwhile, ˆ y 3 = y 2 is used as the immediate proceeding word in translation history.</p><p>When computing attention scores, we extend Equation 5 by replacing the decoder hidden state with the concatenation of Word-RNN hidden state s and Action-RNN hidden state s (gray boxes in <ref type="figure" target="#fig_3">Figure 3</ref>). The new attention score is then updated as</p><formula xml:id="formula_11">e jk = v T a tanh(W a [s j−1 ; s j−1 ] + U a h k )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Syntactic Context for Target Word Prediction</head><p>Syntax has been proven useful for sentence gen- eration task <ref type="bibr" target="#b6">(Dyer et al., 2016)</ref>. We propose to leverage target syntax to help translation genera- tion. In our model, the syntactic context K j at timestamp j is defined as a vector which is com- puted by a feed-forward network based on current parsing configuration of Action-RNN. Denote that w 0 and w 1 are two topmost words in the stack, w 0l and w 1l are their leftmost modifiers in the partial tree, w 0r and w 1r their rightmost modifiers respec- tively. We define two unigram features and four bigram features. The unigram features are w 0 <ref type="bibr" target="#b29">and Clark, 2008)</ref>. Based on these features, the syntactic context vector K j is computed as</p><note type="other">and w 1 which are represented by the word embedding vectors. The bigram features are w 0 w 0l , w 0 w 0r , w 1 w 1l and w 1 w 1r . Each of them is computed by b hc = tanh(W b Ew h + U b Ew hc ), h ∈ {0, 1}, c ∈ {l, r}. These kinds of feature template have beeb proven effective in dependency parsing task (Zhang</note><formula xml:id="formula_12">K j = tanh(W k [Ew 0 ; Ew 1 ] + U k [b 0l ; b 0r ; b 1l ; b 1r ])<label>(11)</label></formula><p>where gives an overview of the construction of K j . Note that zero vector is used for padding the words which are not available in the partial tree, so that all the K vectors have the same input size in com- putation.</p><formula xml:id="formula_13">W k , U k , W b , U b</formula><p>Adding K j to Equation 2, the probabilities of transition action and word in Equation 7 and 8 are then updated as</p><formula xml:id="formula_14">P (a j |a &lt;j , X, ˆ Y &lt;j ) = g(s j , a j−1 , c j , K j ) (12) P (ˆ y j |ˆy|ˆy &lt;j , X, A ≤j ) = g(s j , ˆ y j−1 , c j , K j )<label>(13)</label></formula><p>After each prediction step in Word-RNN and Action-RNN, the syntax context vector K will be updated accordingly. Note that K is not used to calculate the recurrent states s in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training and Decoding</head><p>For SD-NMT model, we use the sum of log- likelihoods of word sequence and action sequence as objective function for training algorithm, so that the joint probability of target translations and their parsing trees can be maximized:</p><formula xml:id="formula_15">J(θ) = (X,Y,A)∈D log P (A|X, ˆ Y &lt;l )+ log P ( ˆ Y |X, A ≤l )<label>(14)</label></formula><p>We also use mini-batch for model training. As the target dependency trees are known in the bilin- gual corpus during training, we pre-compute the partial tree state and syntactic context at each time stamp for each training instance. Thus it is easy for the model to process multiple trees in one batch.</p><p>In the decoding process of an SD-NMT model, the score of each search path is the sum of log probabilities of target word sequence and transi- tion action sequence normalized by the sequence length:</p><formula xml:id="formula_16">score = 1 l l j=1 log P (a j |a &lt;j , X, ˆ Y &lt;j )+ 1 n l j=1 δ(SH, a j ) log P (ˆ y j |ˆy|ˆy &lt;j , X, A ≤j ) (15)</formula><p>where n is word sequence length and l is action sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The experiments are conducted on the Chinese- English task as well as the Japanese-English trans- lation tasks where the same data set from WAT 2016 ASPEC corpus (Nakazawa et al., 2016) 3 is used for a fair comparison with other work. In addition to evaluate translation performance, we also investigate the quality of dependency parsing as a by-product and the effect of parsing quality against translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>In the Chinese-English task, the bilingual training data consists of a set of LDC datasets, 4 which has around 2M sentence pairs. We use NIST2003 as the development set, and the testsets contain NIST2005, NIST2006, NIST2008 and NIST2012. All English words are lowercased.</p><p>In the Japanese-English task, we use top 1M sentence pairs from ASPEC Japanese-English cor- pus. The development data contains 1,790 sen- tences, and the test data contains 1,812 sentences with single reference per source sentence.</p><p>To train SD-NMT model, the target dependency tree references are needed. As there is no golden annotation of parse trees over the target training data, we use pseudo parsing results as the tar- get dependency references, which are got from an in-house developed arc-eager dependency parser based on work in ( <ref type="bibr" target="#b30">Zhang and Nivre, 2011</ref>  <ref type="table">Table 1</ref>: Evaluation results on Chinese-English translation task with BLEU% metric. The "Average" column is the averaged result of all test sets. The numbers in bold indicate statistically significant differ- ence (p &lt; 0.05) from baselines.</p><p>In the neural network training, the vocabulary size is limited to 30K high frequent words for both source and target languages. All low fre- quent words are normalized into a special token unk and post-processed by following the work in ( <ref type="bibr" target="#b17">Luong et al., 2015b</ref>). The size of word embed- ding and transition action embedding is set to 512. The dimensions of the hidden states for all RNNs are set to 1024. All model parameters are initial- ized randomly with Gaussian distribution <ref type="bibr" target="#b9">(Glorot and Bengio, 2010)</ref> and trained on a NVIDIA Tesla K40 GPU. The stochastic gradient descent (SGD) algorithm is used to tune parameters with a learn- ing rate of 1.0. The batch size is set to 96. In the update procedure, Adadelta <ref type="bibr" target="#b27">(Zeiler, 2012</ref>) algo- rithm is used to automatically adapt the learning rate. The beam sizes for both word prediction and transition action prediction are set to 12 in decod- ing.</p><p>The baselines in our experiments are a phrasal system and a neural translation system, denoted by HPSMT and RNNsearch respectively. HPSMT is an in-house implementation of the hierarchical phrase-based model <ref type="bibr" target="#b1">(Chiang, 2005)</ref>, where a 4- gram language model is trained using the mod- ified Kneser-Ney smoothing <ref type="bibr" target="#b13">(Kneser and Ney, 1995)</ref> algorism over the English Gigaword corpus (LDC2009T13) plus the target data from the bilin- gual corpus. RNNsearch is an in-house implemen- tation of the attention-based neural machine trans- lation model ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) using the same parameter settings as our SD-NMT model including word embedding size, hidden vector di- mension, beam size, as well as the same mecha- nism for OOV word processing.</p><p>The evaluation results are reported with the case-insensitive IBM BLEU-4 ( <ref type="bibr" target="#b21">Papineni et al., 2002)</ref>. A statistical significance test is performed using the bootstrap resampling method proposed by <ref type="bibr" target="#b14">Koehn (2004)</ref> with a 95% confidence level. For Japanese-English task, we use the official eval- uation procedure provided by WAT 2016. <ref type="bibr">5</ref> , where both BLEU and RIBES ( <ref type="bibr" target="#b11">Isozaki et al., 2010)</ref> are used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on Chinese-English Translation</head><p>We evaluate our method on the Chinese-English translation task. The evaluation results over all NIST test sets against baselines are listed in <ref type="table">Table  1</ref>. Generally, RNNsearch outperforms HPSMT by 3.78 BLEU points on average while SD-NMT surpasses RNNsearch 2.03 BLUE point gains on average, which shows that NMT models usually achieve better results than SMT models, and our proposed sequence-to-dependency NMT model performs much better than traditional sequence-to- sequence NMT model. We also investigate the effect of syntactic knowledge context by excluding its computation in Equation 12 and 13. The alternative model is denoted by SD-NMT\K. According to <ref type="table">Table  1</ref>, SD-NMT\K outperforms RNNsearch by 0.54 BLEU points but degrades SD-NMT by 1.49 BLEU points on average, which demonstrates that the long distance dependencies captured by the target syntactic knowledge context, such as left- most/rightmost children together with their depen- dency relationships, really bring strong positive effects on the prediction of target words.</p><p>In addition to translation quality, we compare the perplexity (PPL) changes on the development set in terms of numbers of training mini-batches for RNNsearch and SD-NMT in <ref type="figure" target="#fig_6">Figure 4</ref>. We can see that the PPL of SD-NMT is initially higher than that of RNNsearch, but decreased to be lower over time. This is mainly because the quality of parse tree is too poor at the beginning which degrades translation quality and leads to higher PPL. After some training iterations, the SD-NMT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU RIBES System Description SMT Hiero</head><p>18.72 0.6511 Moses' Hierarchical Phrase-based SMT SMT Phrase 18.45 0.6451 Moses' Phrase-based SMT SMT S2T</p><p>20.36 0.6782 Moses' String-to-Tree Syntax-based SMT <ref type="bibr" target="#b3">Cromieres (2016)</ref> <ref type="table">(Single model)</ref> 22.86 - Single-layer NMT model without ensemble Cromieres (2016)(Self-ensemble) 24.71 0.7508 Self-ensemble of 2-layer NMT model Cromieres (2016)(4-Ensemble)</p><p>26.22 0.7566 Ensemble of 4 single-layer NMT models RNNsearch 23.50 0.7459 Single-layer NMT model SD-NMT 25.93 0.7540 Single-layer SD-NMT model <ref type="table">Table 2</ref>: Evaluation results on Japanese-English translation task.</p><p>model learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL.   In our experiments, the time cost of SD-NMT is two times of that for RNNsearch due to a more complicated model structure. But we think it is a worthy trade to pursue high quality translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on Japanese-English Translation</head><p>In this section, we report results on the Japanese- English translation task. To ensure fair compar- isons, we use the same training data and follow the pre-processing steps recommended in WAT 2016 <ref type="bibr">6</ref> . <ref type="table">Table 2</ref> shows the comparison results from 8 sys- tems with the evaluation metrics of BLEU and RIBES. The results in the first 3 rows are pro- duced by SMT systems taken from the official WAT 2016. The remaining results are produced by NMT systems, among which the bottom two row results are taken from our in-house NMT systems and others refer to the work in <ref type="bibr" target="#b3">(Cromieres, 2016;</ref>) that are the competitive NMT results on WAT 2016. According to <ref type="table">Table  2</ref>, NMT results still outperform SMT results simi- lar to our Chinese-English evaluation results. The SD-NMT model significantly outperforms most other NMT models, which shows that our pro- posed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in <ref type="bibr" target="#b3">(Cromieres, 2016)</ref>. Note that our SD-NMT gets comparable results with the 4 single-layer ensem- ble model in <ref type="bibr" target="#b3">(Cromieres, 2016;</ref>). We believe SD-NMT can get more im- provements with an ensemble of multiple models in future experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of the Parsing Accuracy upon Translation Quality</head><p>The interaction effect between dependency tree conduction and target word generation is investi- gated in this section. The experiments are con- ducted on the Chinese-English task over multiple test sets. We evaluate how the quality of depen- dency trees affect the performance of translation.</p><p>In the decoding phase of SD-NMT, beam search is applied to the generations of both transition and actions as illustrated in Equation 15. Intuitively, the larger the beam size of action prediction is, the better the dependency tree quality is. We fix the beam size for generating target words to 12, and change the beam size for action prediction to see the difference. <ref type="figure">Figure 5</ref> shows the evaluation re- sults of all test sets. There is a tendency for BLEU scores to increase with the growth of action pre- diction beam size. The reason is that the transla- tion quality increases as the quality of dependency tree improves, which shows the construction of de- pendency trees can boost the generation of target Beam size of action prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIST2012</head><p>Figure 5: Translation performance against the beam size of action prediction.</p><p>words, and vice versa we believe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Quality Estimation of Dependency Tree Construction</head><p>As a by-product, the quality of dependency trees not only affects the performance of target word generation, but also influences the possible down- stream processors or tasks such as text analyses. The direct evaluation of tree quality is not feasible due to the unavailable golden references. So we resort to estimating the consistency between the by-products and the parsing results of our stand- alone dependency parser with state-of-the-art per- formance. The higher the consistency is, the closer the performance of by-product is to the stand- alone parser. To reduce the influence of ill-formed data as much as possible, we build the evaluation data set by heuristically selecting 360 SD-NMT translation results together with their dependency trees from NIST test sets where both source-and target-side do not contain unk and have a length of 20-30. We then take the parsing results of the stand-alone parser for these translations as ref- erences to indirectly estimate the quality of by- products. We get a UAS (unlabeled attachment score) of 94.96% and a LAS (labeled attachment score) of 93.92%, which demonstrates that the de- pendency trees produced by SD-NMT are much similar with the parsing results from the stand- alone parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Translation Example</head><p>In this section, we give a case study to explain how our method works. <ref type="figure" target="#fig_7">Figure 6</ref> shows a trans- lation example from the NIST testsets. SMT and RNNsearch refer to the translation results from the baselines HPSMT and NMT. For our SD-NMT model, we list both the generated translation and its corresponding dependency tree. We find that the translation of SMT is disfluent and ungram- matical, whereas RNNsearch is better than SMT. Although the translation of RNNsearch is locally fluent around word "have" in the rectangle, both its grammar is incorrect and its meaning is inaccu- rate from a global view. The word "have" should be in a singular form as its subject is "safety" rather than "workers". For our SD-NMT model, we can see that the translation is much better than baselines and the dependency tree is reasonable. The reason is that after generating the word "work- ers", the previous subtree in the gray region is transformed to the syntactic context which can guide the generation of the next word as illustrated by the dashed arrow. Thus our model is more likely to generate the correct verb "is" with sin- gular form. In addition, the global structure helps the model correctly identify the inverted sentence pattern of the former translated part and make bet- ter choices for the future translation ("only when .. can .." in our translation, "only when .. will .." in the reference), which remains a challenge for conventional NMT model.</p><p>[Source]</p><p>只有 施工 人员 的 安全 得到 了 保证 , 才能 继续 施 工 . <ref type="bibr">[Reference]</ref> only when the safety of the workers is guaranteed will they continue with the project . <ref type="bibr">[HPSMT]</ref> only safety is assured of construction personnel , to continue construction . <ref type="bibr">[RNNsearch]</ref> only when the safety of construction workers have been guaranteed to continue construction .</p><p>[SD-NMT] only when the safety of the workers is guaranteed can we continue to work . NMT is straightforward, because the source sen- tence is definitive and easy to attach extra informa- tion. However, it is non-trivial to add target syn- tax as target words are uncertain in decoding pro- cess. Up to now, there is few work that attempts to build and leverage target syntactic information for NMT.</p><p>There has been work that incorporates syntactic information into NLP tasks with neural networks. <ref type="bibr" target="#b6">Dyer et al. (2016)</ref> presented a RNN grammar for parsing and language modeling. They replaced SH with a set of generative actions to generate words under a Stack LSTM framework <ref type="bibr" target="#b5">(Dyer et al., 2015)</ref>, which achieves promising results for lan- guage modeling on the Penn Treebank data. In our work, we propose to involve target syntactic trees into NMT model to jointly learn target trans- lation and dependency parsing where target syn- tactic context over the parse tree is used to improve the translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose a novel string-to- dependency translation model over NMT. Our model jointly performs target word generation and arc-standard dependency parsing. Experimental results show that our method can boost the two procedures and achieve significant improvements on the translation quality of NMT systems.</p><p>In future work, along this research direction, we will try to integrate other prior knowledge, such as semantic information, into NMT systems. In addi- tion, we will apply our method to other sequence- to-sequence tasks, such as text summarization, to verify the effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>…</head><label></label><figDesc>Figure 1: Dependency trees help the prediction of the next target word. "NMT" refers to the translation result from a conventional NMT model, which fails to capture the long distance word relation denoted by the dashed arrow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Left-Reduce(LR(d)) : Link w 0 and w 1 with dependency label d as w 0 d − →w 1 , and reduce them to the head w 0 . • Right-Reduce(RR(d)) : Link w 0 and w 1 with dependency label d as w 0 d ← −w 1 , and reduce them to the head w 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Decoding example of our SD-NMT model for target sentence "who are you" with transition action sequence "SH SH LR SH RR". The ending symbol EOS is omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) is the overview of SD-NMT model. The dashed arrows mean copying previous recurrent state or word. The two RNNs use the same source context for prediction. a j ∈ {SH, RR(d), LR(d)}. The bidirection arrow refers to the interaction between two RNNs. (b) shows the construction of syntactic context. The gray box means the concatenation of vectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>are the weight matrices, E stands for the embedding matrix. Figure 2 (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Perplexity (PPL) changes in terms of numbers of training mini-batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Translation examples of SMT, RNNsearch and our SD-NMT on Chinese-English translation task. The italic words on the arrows are dependency labels. The ending symbol EOS is omitted. RNNsearch fails to capture the long dependency which leads to an ungrammatical result. Whereas with the help of the syntactic tree, our SD-NMT can get a much better translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Settings 

NIST 2005 NIST 2006 NIST 2008 NIST 2012 Average 
HPSMT 
35.34 
33.56 
26.06 
27.47 
30.61 
RNNsearch 
38.07 
38.95 
31.61 
28.95 
34.39 
SD-NMT\K 
38.83 
39.23 
31.92 
29.72 
34.93 
SD-NMT 
39.38 
41.81 
33.06 
31.43 
36.42 

</table></figure>

			<note place="foot" n="1"> In the rest of this paper, aj represents the transition action, rather than the attention weight in Equation 4. 2 RR(d) refers to a set of RR actions augmented with dependency labels so as to LR(d).</note>

			<note place="foot" n="3"> http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ 4 LDC2003E14, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2003E07, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07, LDC2004T08, LDC2005T06</note>

			<note place="foot" n="5"> http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index .html</note>

			<note place="foot" n="6"> http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/data PreparationJE.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Incorporating linguistic knowledge into machine translation has been extensively studied in Statistic Machine Translation (SMT) ( <ref type="bibr" target="#b8">Galley et al., 2006;</ref><ref type="bibr" target="#b22">Shen et al., 2008;</ref><ref type="bibr" target="#b15">Liu et al., 2006</ref>). <ref type="bibr" target="#b15">Liu et al. (2006)</ref> proposed a tree-to-string alignment tem-plate for SMT to leverage source side syntactic in-formation. <ref type="bibr" target="#b22">Shen et al. (2008)</ref> proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT.</p><p>Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs ( <ref type="bibr" target="#b16">Luong et al., 2015a;</ref><ref type="bibr" target="#b28">Zhang et al., 2016;</ref><ref type="bibr" target="#b23">Shen et al., 2016;</ref><ref type="bibr" target="#b19">Neubig, 2016)</ref>. In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incor-porate source syntax into NMT. <ref type="bibr" target="#b7">Eriguchi et al. (2016)</ref> proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. In-tuitively, adding source syntactic information to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers for their insightful comments. We also thank Shujie Liu and Zhirui Zhang for the helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ENMLP</title>
		<meeting>ENMLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kyoto-nmt: a neural machine translation implementation in chainer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Cromieres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kyoto university participation to wat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Cromieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Asian Translation (WAT2016)</title>
		<meeting>the 3rd Workshop on Asian Translation (WAT2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="166" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL</title>
		<meeting>the NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable inference and training of context-rich syntactic translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks. In Aistats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic evaluation of translation quality for distant language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>ICASSP-95</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aspec: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara ; Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grobelnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</title>
		<editor>Nicoletta Calzolari</editor>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)<address><addrLine>Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk; Portoroz, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2204" to="2208" />
		</imprint>
	</monogr>
	<note>and Stelios Piperidis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lexicons and minimum risk training for neural machine translation: NAISTCMU at WAT2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3nd Workshop on Asian Translation (WAT2016)</title>
		<meeting>the 3nd Workshop on Asian Translation (WAT2016)<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new string-to-dependency machine translation algorithm with a target dependency language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
