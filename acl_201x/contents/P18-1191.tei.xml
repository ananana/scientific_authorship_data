<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Scale QA-SRL Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2051</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Luheng He</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Luheng He</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Luheng He</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Luheng He</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Scale QA-SRL Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2051" to="2060"/>
							<date type="published">July 15-20, 2018. 2018. 2051</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our corpus , QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost. We also present neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship. The best models achieve question accuracy of 82.6% and span-level accuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL prediction task. They can also, as we show, be used to gather additional annotations at low cost.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning semantic parsers to predict the predicate- argument structures of a sentence is a long standing, open challenge ( <ref type="bibr" target="#b18">Palmer et al., 2005;</ref><ref type="bibr" target="#b1">Baker et al., 1998</ref>). Such systems are typically trained from datasets that are difficult to gather, 1 but recent research has explored training non- experts to provide this style of semantic supervi- sion ( <ref type="bibr" target="#b0">Abend and Rappoport, 2013;</ref><ref type="bibr" target="#b3">Basile et al., 2012;</ref><ref type="bibr" target="#b19">Reisinger et al., 2015;</ref><ref type="bibr" target="#b11">He et al., 2015)</ref>. In this paper, we show for the first time that it is pos- sible to go even further by crowdsourcing a large * Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence. <ref type="bibr">1</ref> The PropBank ( <ref type="bibr" target="#b4">Bonial et al., 2010)</ref> and FrameNet <ref type="bibr" target="#b20">(Ruppenhofer et al., 2016</ref>) annotation guides are 89 and 119 pages, respectively.</p><p>In 1950 Alan M. Turing published "Computing machinery and intelligence" in Mind, in which he proposed that machines could be tested for intelligence using questions and answers. What did someone propose? that machines could be tested for intelligent using questions and answers 6 When did someone propose something?</p><p>In <ref type="bibr">1950</ref> tested 7</p><p>What can be tested? machines</p><p>8 What can something be tested for? intelligence 9</p><p>How can something be tested? using questions and answers using 10</p><p>What was being used? questions and answers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head><p>Why was something being used? tested for intelligence <ref type="figure">Figure 1</ref>: An annotated sentence from our dataset. Question 6 was not produced by crowd workers in the initial collection, but was produced by our parser as part of Data Expansion (see Section 5.) scale dataset that can be used to train high quality parsers at modest cost. We adopt the Question-Answer-driven Seman- tic Role Labeling (QA-SRL) ( <ref type="bibr" target="#b11">He et al., 2015</ref>) annotation scheme. QA-SRL is appealing be- cause it is intuitive to non-experts, has been shown to closely match the structure of tra- ditional predicate-argument structure annotation schemes ( <ref type="bibr" target="#b11">He et al., 2015)</ref>, and has been used for end tasks such as Open IE <ref type="bibr" target="#b22">(Stanovsky and Dagan, 2016)</ref>. In QA-SRL, each predicate-argument re- lationship is labeled with a question-answer pair (see <ref type="figure">Figure 1)</ref>. <ref type="bibr" target="#b11">He et al. (2015)</ref> showed that high precision QA-SRL annotations can be gathered with limited training but that high recall is chal- lenging to achieve; it is relatively easy to gather answerable questions, but difficult to ensure that every possible question is labeled for every verb. For this reason, they hired and trained hourly an- notators and only labeled a relatively small dataset (3000 sentences).</p><p>Our first contribution is a new, scalable ap- proach for crowdsourcing QA-SRL. We introduce a streamlined web interface (including an auto- suggest mechanism and automatic quality control to boost recall) and use a validation stage to en-sure high precision (i.e. all the questions must be answerable). With this approach, we produce QA-SRL Bank 2.0, a dataset with 133,479 verbs from 64,018 sentences across 3 domains, total- ing 265,140 question-answer pairs, in just 9 days. Our analysis shows that the data has high preci- sion with good recall, although it does not cover every possible question. <ref type="figure">Figure 1</ref> shows example annotations.</p><p>Using this data, our second contribution is a comparison of several new models for learning a QA-SRL parser. We follow a pipeline approach where the parser does (1) unlabeled span detection to determine the arguments of a given verb, and (2) question generation to label the relationship be- tween the predicate and each detected span. Our best model uses a span-based representation sim- ilar to that introduced by <ref type="bibr" target="#b15">Lee et al. (2016)</ref> and a custom LSTM to decode questions from a learned span encoding. Our model does not require syn- tactic information and can be trained directly from the crowdsourced span labels.</p><p>Experiments demonstrate that the model does well on our new data, achieving up to 82.2% span- detection F1 and 47.2% exact-match question ac- curacy relative to the human annotations. We also demonstrate the utility of learning to predict easily interpretable QA-SRL structures, using a simple data bootstrapping approach to expand our dataset further. By tuning our model to favor recall, we over-generate questions which can be validated us- ing our annotation pipeline, allowing for greater recall without requiring costly redundant annota- tions in the question writing step. Performing this procedure on the training and development sets grows them by 20% and leads to improvements when retraining our models. Our final parser is highly accurate, achieving 82.6% question accu- racy and 77.6% span-level precision in an human evaluation. Our data, code, and trained models will be made publicly available. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Annotation</head><p>A QA-SRL annotation consists of a set of question-answer pairs for each verbal predicate in a sentence, where each answer is a set of contigu- ous spans from the sentence. QA-SRL questions are defined by a 7-slot template shown in <ref type="table">Table 1</ref>. We introduce a crowdsourcing pipeline to collect annotations rapidly, cheaply, and at large scale.  Pipeline Our crowdsourcing pipeline consists of a generation and validation step. In the genera- tion step, a sentence with one of its verbs marked is shown to a single worker, who must write QA- SRL questions for the verb and highlight their an- swers in the sentence. The questions are passed to the validation step, where n workers answer each question or mark it as invalid. In each step, no two answers to distinct questions may overlap with each other, to prevent redundancy.</p><p>Instructions Workers are instructed that a valid question-answer pair must satisfy three criteria: 1) the question is grammatical, 2) the question- answer pair is asking about the time, place, par- ticipants, etc., of the target verb, and 3) all correct answers to each question are given.</p><p>Autocomplete We provide an autocomplete drop-down to streamline question writing. Auto- complete is implemented as a Non-deterministic Finite Automaton (NFA) whose states correspond to the 7 QA-SRL slots paired with a partial rep- resentation of the question's syntax. We use the NFA to make the menu more compact by dis- allowing obviously ungrammatical combinations (e.g., What did been appeared?), and the syntactic representation to auto-suggest complete questions about arguments that have not yet been covered (see <ref type="figure" target="#fig_2">Figure 2</ref>). The auto-suggest feature signifi- cantly reduces the number of keystrokes required to enter new questions after the first one, speeding up the annotation process and making it easier for annotators to provide higher recall.  To allow for more careful evaluation, we vali- dated 5,205 sentences at a higher density (up to 1,000 for each domain in dev and test), re-running the generated questions through validation with n = 3 for a total of 6 answer annotations for each question.</p><p>Quality Judgments of question validity had moderate agreement. About 89.5% of validator judgments rated a question as valid, and the agree- ment rate between judgments of the same ques- tion on whether the question is invalid is 90.9%. This gives a Fleiss's Kappa of 0.51. In the higher- density re-run, validators were primed to be more critical: 76.5% of judgments considered a ques- tion valid, and agreement was at 83.7%, giving a Fleiss's Kappa of 0.55.</p><p>Despite being more critical in the denser anno- tation round, questions marked valid in the origi- nal dataset were marked valid by the new annota- tors in 86% of cases, showing our data's relatively high precision. The high precision of our annota- tion pipeline is also backed up by our small-scale manual evaluation (see Coverage below).</p><p>Answer spans for each question also exhibit P R F <ref type="bibr" target="#b11">He et al. (2015)</ref> 97.5 86.6 91.7 This work 95.7 72.4 82.4 This work (unfiltered) 94.9 85.4 89.9 <ref type="table">Table 3</ref>: Precision and recall of our annotation pipeline on a merged and validated subset of 100 verbs. The unfiltered number represents relaxing the restriction that none of 2 validators marked the question as invalid.</p><p>good agreement. On the original dataset, each an- swer span has a 74.8% chance to exactly match one provided by another annotator (up to two), and on the densely annotated subset, each answer span has an 83.1% chance to exactly match one pro- vided by another annotator (up to five).</p><p>Coverage Accurately measuring recall for QA- SRL annotations is an open challenge. For exam- ple, question 6 in <ref type="figure">Figure 1</ref> reveals an inferred tem- poral relation that would not be annotated as part of traditional SRL. Exhaustively enumerating the full set of such questions is difficult, even for ex- perts.</p><p>However, we can compare to the original QA- SRL dataset <ref type="bibr" target="#b11">(He et al., 2015)</ref>, where Wikipedia sentences were annotated with 2.43 questions per verb.</p><p>Our data has lower-but loosely comparable-recall, with 2.05 questions per verb in Wikipedia.</p><p>In order to further analyze the quality of our annotations relative to <ref type="bibr" target="#b11">(He et al., 2015)</ref>, we rean- notate a 100-verb subset of their data both manu- ally (aiming for exhaustivity) and with our crowd- sourcing pipeline. We merge the three sets of annotations, manually remove bad questions (and their answers), and calculate the precision and re- call of the crowdsourced annotations and those of <ref type="bibr" target="#b11">He et al. (2015)</ref> against this pooled, filtered dataset (using the span detection metrics described in Sec- tion 4). Results, shown in <ref type="table">Table 3</ref>, show that our pipeline produces comparable precision with only a modest decrease in recall. Interestingly, re- adding the questions rejected in the validation step greatly increases recall with only a small decrease in precision, showing that validators sometimes rejected questions considered valid by the authors. However, we use the filtered dataset for our ex- periments, and in Section 5, we show how another crowdsourcing step can further improve recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>Given a sentence X = x 0 , . . . , x n , the goal of a QA-SRL parser is to produce a set of tuples (v i , Q i , S i ), where v ∈ {0, . . . , n} is the index of a verbal predicate, Q i is a question, and S i ∈ {(i, j) | i, j ∈ [0, n], j ≥ i} is a set of spans which are valid answers. Our proposed parsers construct these tuples in a three-step pipeline:</p><p>1. Verbal predicates are identified using the same POS-tags and heuristics as in data col- lection (see Section</p><note type="other">2). 2. Unlabeled span detection selects a set S v of spans as arguments for a given verb v. 3. Question generation predicts a question for each span in S v . Spans are then grouped by question, giving each question a set of an- swers. We describe two models for unlabeled span de- tection in section 3.1, followed by question gen- eration in section 3.2. All models are built on an LSTM encoding of the sentence. Like He et al. (2017), we start with an input X</note><formula xml:id="formula_0">v = {x 0 . . . x n },</formula><p>where the representation x i at each time step is a concatenation of the token w i 's embedding and an embedded binary feature (i = v) which indi- cates whether w i is the predicate under consid- eration. We then compute the output representa- tion H v = BILSTM(X v ) using a stacked alter- nating LSTM (Zhou and Xu, 2015) with highway connections ( <ref type="bibr" target="#b21">Srivastava et al., 2015</ref>) and recur- rent dropout ( <ref type="bibr" target="#b8">Gal and Ghahramani, 2016)</ref>. Since the span detection and question generation mod- els both use an LSTM encoding, this component could in principle be shared between them. How- ever, in preliminary experiments we found that sharing hurt performance, so for the remainder of this work each model is trained independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Span Detection</head><p>Given an encoded sentence H v , the goal of span detection is to select the spans S v that correspond to arguments of the given predicate. We explore two models: a sequence-tagging model with BIO encoding, and a span-based model which assigns a probability to every possible span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">BIO Sequence Model</head><p>Our BIO model predicts a set of spans via a se- quence y where each y i ∈ {B, I, O}, represent- ing a token at the beginning, interior, or outside of any span, respectively. Similar to , we make independent predictions for each token at training time, and use Viterbi decoding to enforce hard BIO-constraints 5 at test time. The resulting sequences are in one-to-one correspon- dence with sets S v of spans which are pairwise non-overlapping. The locally-normalized BIO-tag distributions are computed from the BiLSTM out- puts H v = {h v0 , . . . , h vn }:</p><formula xml:id="formula_1">p(y t | x) ∝ exp(w tag MLP(h vt ) + b tag ) (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Span-based Model</head><p>Our span-based model makes independent binary decisions for all O(n 2 ) spans in the sentence. Fol- lowing <ref type="bibr" target="#b15">Lee et al. (2016)</ref>, the representation of a span (i, j) is the concatenation of the BiLSTM output at each endpoint:</p><formula xml:id="formula_2">s vij = [h vi , h vj ].<label>(2)</label></formula><p>The probability that the span is an argument of predicate v is computed by the sigmoid function:</p><formula xml:id="formula_3">p(y ij | X v ) = σ(w span MLP(s vij ) + b span ) (3)</formula><p>At training time, we minimize the binary cross en- tropy summed over all n 2 possible spans, counting a span as a positive example if it appears as an an- swer to any question. At test time, we choose a threshold τ and se- lect every span that the model assigns probability greater than τ , allowing us to trade off precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Generation</head><p>We introduce two question generation models. Given a span representation s vij defined in sub- subsection 3.1.2, our models generate questions by picking a word for each question slot (see Sec- tion 2). Each model calculates a joint distribution p(y | X v , s vij ) over values y = (y 1 , . . . , y 7 ) for the question slots given a span s vij , and is trained to minimize the negative log-likelihood of gold slot values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Local Model</head><p>The local model predicts the words for each slot independently:</p><formula xml:id="formula_4">p(y k | X v , s vij ) ∝ exp(w k MLP(s vij ) + b k ).<label>(4)</label></formula><p>5 E.g., an I-tag should only follow a B-tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sequence Model</head><p>The sequence model uses the machinery of an RNN to share information between slots. At each slot k, we apply a multiple layers of LSTM cells:</p><formula xml:id="formula_5">h l,k , c l,k = LSTMCELL l,k (h l−1,k , h l,k−1 , c l,k−1 )<label>(5)</label></formula><p>where the initial input at each slot is a concate- nation of the span representation and the embed- ding of the previous word of the question: h 0,k = [s vij ; y k−1 ]. Since each question slot predicts from a different set of words, we found it bene- ficial to use separate weights for the LSTM cells at each slot k. During training, we feed in the gold token at the previous slot, while at test time, we use the predicted token. The output distribution at slot k is computed via the final layers' output vector h Lk :</p><formula xml:id="formula_6">p(y k | X v , s vij ) ∝ exp(w k MLP(h Lk ) + b k )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Initial Results</head><p>Automatic evaluation for QA-SRL parsing presents multiple challenges. In this section, we introduce automatic metrics that can help us compare models. In Section 6, we will report human evaluation results for our final system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Span Detection</head><p>Metrics We evaluate span detection using a modified notion of precision and recall. We count predicted spans as correct if they match any of the labeled spans in the dataset. Since each pre- dicted span could potentially be a match to multi- ple questions (due to overlapping annotations) we map each predicted span to one matching question in the way that maximizes measured recall using maximum bipartite matching. We use both exact match and intersection-over-union (IOU) greater than 0.5 as matching criteria.</p><p>Results <ref type="table">Table 4</ref> shows span detection results on the development set. We report results for the span-based models at two threshold values τ : τ = 0.5, and τ = τ * maximizing F1. The span-based model significantly improves over the BIO model in both precision and recall, although the differ- ence is less pronounced under IOU matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Generation</head><p>Metrics Like all generation tasks, evaluation metrics for question generation must contend with Exact Match P R F BIO 69.0 75.9 72.2 Span (τ = 0.5) 81.7 80.9 81.3 Span (τ = τ * ) 80.0 84.7 82. <ref type="bibr">2</ref> IOU ≥ 0.5 P R F BIO 80.4 86.0 83.1 Span (τ = 0.5) 87.5 84.2 85.8 Span (τ = τ * ) 83.8 93.0 88.1 <ref type="table">Table 4</ref>: Results for Span Detection on the dense development dataset. Span detection results are given with the cutoff threshold τ at 0.5, and at the value which maximizes F-score. The top chart lists precision, recall and F-score with exact span match, while the bottom reports matches where the intersection over union (IOU) is ≥ 0.5.  <ref type="table">Table 5</ref>: Question Generation results on the dense development set. EM -Exact Match accuracy, PM -Partial Match Accuracy, SA -Slot-level accuracy the fact that there are in general multiple possi- ble valid questions for a given predicate-argument pair. For instance, the question "Who did some- one blame something on?" may be rephrased as "Who was blamed for something?" However, due to the constrained space of possible questions de- fined by QA-SRL's slot format, accuracy-based metrics can still be informative. In particular, we report the rate at which the predicted question ex- actly matches the gold question, as well as a re- laxed match where we only count the question word (WH), subject (SBJ), object (OBJ) and Mis- cellaneous (Misc) slots (see <ref type="table">Table 1</ref>). Finally, we report average slot-level accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EM PM</head><p>Results <ref type="table">Table 5</ref> shows the results for question generation on the development set. The sequen- tial model's exact match accuracy is significantly higher, while word-level accuracy is roughly com- parable, reflecting the fact that the local model learns the slot-level posteriors. <ref type="table">Table 6</ref> shows precision and recall for joint span detection and question generation, using exact P R F Span + Local 37.8 43.7 40.6 Span + Seq. (τ = 0.5) 39.6 45.8 42.4 <ref type="table">Table 6</ref>: Joint span detection and question gener- ation results on the dense development set, using exact-match for both spans and questions. match for both. This metric is exceedingly hard, but it shows that almost 40% of predictions are exactly correct in both span and question. In Sec- tion 6, we use human evaluation to get a more ac- curate assessment of our model's accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Expansion</head><p>Since our trained parser can produce full QA- SRL annotations, its predictions can be validated by the same process as in our original annotation pipeline, allowing us to focus annotation efforts towards filling potential data gaps.</p><p>By detecting spans at a low probability cutoff, we over-generate QA pairs for already-annotated sentences. Then, we filter out QA pairs whose answers overlap with answer spans in the exist- ing annotations, or whose questions match exist- ing questions. What remains are candidate QA pairs which fill gaps in the original annotation. We pass these questions to the validation step of our crowdsourcing pipeline with n = 3 validators, re- sulting in new labels.</p><p>We run this process on the training and devel- opment partitions of our dataset. For the develop- ment set, we use the trained model described in the previous section. For the training set, we use a relaxed version of jackknifing, training 5 models over 5 different folds. We generate 92,080 ques- tions at a threshold of τ = 0.2. Since in this case many sentences have only one question, we re- structure the pay to a 2c base rate with a 2c bonus per question after the first (still paying no less than 2c per question).</p><p>Data statistics 46,017 (50%) of questions run through the expansion step were considered valid by all three annotators. In total, after filtering, the expansion step increased the number of valid questions in the train and dev partitions by 20%. However, for evaluation, since our recall metric identifies a single question for each answer span (via bipartite matching), we filter out likely ques- tion paraphrases by removing questions in the ex-  <ref type="table">Table 7</ref>: Results on the expanded development set comparing the full model trained on the original data, and with the expanded data. panded development set whose answer spans have two overlaps with the answer spans of one ques- tion in the original annotations. After this filtering, the expanded development set we use for evalua- tion has 11.5% more questions than the original development set.</p><p>The total cost including MTurk fees was $8,210.66, for a cost of 8.9c per question, or 17.8c per valid question. While the cost per valid ques- tion was comparable to the initial annotation, we gathered many more negative examples (which may serve useful in future work), and this method allowed us to focus on questions that were missed in the first round and improve the exhaustiveness of the annotation (whereas it is not obvious how to make fully crowdsourced annotation more ex- haustive at a comparable cost per question).</p><p>Retrained model We retrained our final model on the training set extended with the new valid questions, yielding modest improvements on both span detection and question generation in the de- velopment set (see <ref type="table">Table 7</ref>). The span detection numbers are higher than on the original dataset, because the expanded development data captures true positives produced by the original model (and the resulting increase in precision can be traded off for recall as well).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Final Evaluation</head><p>We use the crowdsourced validation step to do a final human evaluation of our models. We test 3 parsers: the span-based span detection model paired with each of the local and sequential question generation models trained on the initial dataset, and our final model (span-based span de- tection and sequential question generation) trained with the expanded data.</p><p>Methodology On the 5,205 sentence densely annotated subset of dev and test, we generate QA- SRL labels with all of the models using a span detection threshold of τ = 0.2 and combine the questions with the existing data. We filter out questions that fail the autocomplete grammatical- ity check (counting them invalid) and pass the data into the validation step, annotating each question to a total of 6 validator judgments. We then com- pute question and span accuracy as follows: A question is considered correct if 5 out of 6 anno- tators consider it valid, and a span is considered correct if its generated question is correct and the span is among those selected for the question by validators. We rank all questions and spans by the threshold at which they are generated, which al- lows us to compute accuracy at different levels of recall.</p><p>Results <ref type="figure" target="#fig_3">Figure 3</ref> shows the results. As expected, the sequence-based question generation models are much more accurate than the local model; this is largely because the local model generated many questions that failed the grammaticality check. Furthermore, training with our expanded data re- sults in more questions and spans generated at the same threshold. If we choose a threshold value which gives a similar number of questions per sen- tence as were labeled in the original data annota- tion (2 questions / verb), question and span accu- racy are 82.64% and 77.61%, respectively. <ref type="table">Table 8</ref> shows the output of our best system on 3 randomly selected sentences from our develop- ment set (one from each domain). The model was overall highly accurate-only one question and 3 spans are considered incorrect, and each mistake is nearly correct, 6 even when the sentence contains a negation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Resources and formalisms for semantics often require expert annotation and underlying syntax <ref type="bibr" target="#b18">(Palmer et al., 2005;</ref><ref type="bibr" target="#b1">Baker et al., 1998;</ref><ref type="bibr" target="#b2">Banarescu et al., 2013)</ref>. Some more recent semantic re- sources require less annotator training, or can be crowdsourced <ref type="bibr" target="#b0">(Abend and Rappoport, 2013;</ref><ref type="bibr" target="#b19">Reisinger et al., 2015;</ref><ref type="bibr" target="#b3">Basile et al., 2012;</ref><ref type="bibr" target="#b17">Michael et al., 2018</ref>). In particular, the original QA-SRL (He et al., 2015) dataset is annotated by free- lancers, while we developed streamlined crowd- sourcing approaches for more scalable annotation. Crowdsourcing has also been used for indirectly annotating syntax <ref type="bibr" target="#b12">(He et al., 2016;</ref><ref type="bibr" target="#b6">Duan et al., 2016)</ref>, and to complement expert annotation of SRL ( <ref type="bibr" target="#b25">Wang et al., 2018)</ref>. Our crowdsourcing ap- proach draws heavily on that of <ref type="bibr" target="#b17">Michael et al. (2018)</ref>, with automatic two-stage validation for the collected question-answer pairs.</p><p>More recently, models have been developed for these newer semantic resources, such as UCCA ( <ref type="bibr" target="#b24">Teichert et al., 2017)</ref> and Semantic Proto-Roles ( <ref type="bibr" target="#b26">White et al., 2017)</ref>. Our work is the first high- quality parser for QA-SRL, which has several unique modeling challenges, such as its highly structured nature and the noise in crowdsourcing.</p><p>Several recent works have explored neural mod- els for SRL tasks <ref type="bibr" target="#b5">(Collobert and Weston, 2007;</ref><ref type="bibr" target="#b7">FitzGerald et al., 2015;</ref><ref type="bibr" target="#b23">Swayamdipta et al., 2017;</ref><ref type="bibr" target="#b27">Yang and Mitchell, 2017</ref>), many of which em- ploy a BIO encoding ( <ref type="bibr" target="#b28">Zhou and Xu, 2015;</ref>. Recently, span-based models have proven to be useful for question answering ( <ref type="bibr" target="#b15">Lee et al., 2016</ref>) and coreference resolution ( , and PropBank SRL ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Produced</head><p>What produced something?</p><p>A much larger super eruption</p><p>Where did something produce something? in Colorado</p><p>What did something produce? over 5,000 cubic kilometers of material A much larger super eruption in Colorado produced over 5,000 cubic kilometers of material.   <ref type="table">8</ref>: System output on 3 randomly sampled sentences from the development set (1 from each of the 3 domains). Spans were selected with τ = 0.5. Questions and spans with a red background were marked incorrect during human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we demonstrated that QA-SRL can be scaled to large datasets, enabling a new methodology for labeling and producing predicate-argument structures at a large scale. We presented a new, scalable approach for crowd- sourcing QA-SRL, which allowed us to collect QA-SRL Bank 2.0, a new dataset covering over 250,000 question-answer pairs from over 64,000 sentences, in just 9 days. We demonstrated the utility of this data by training the first parser which is able to produce high-quality QA-SRL struc- tures. Finally, we demonstrated that the validation stage of our crowdsourcing pipeline, in combina- tion with our parser tuned for recall, can be used to add new annotations to the dataset, increasing recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2</head><label></label><figDesc>http://qasrl.org</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interface for the generation step. Autocomplete shows completions of the current QASRL slot, and auto-suggest shows fully-formed questions (highlighted green) based on the previous questions.</figDesc><graphic url="image-1.png" coords="2,307.28,70.89,218.27,152.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Human evaluation accuracy for questions and spans, as each model's span detection threshold is varied. Questions are considered correct if 5 out of 6 annotators consider it valid. Spans are considered correct if their question was valid, and the span was among those labeled by human annotators for that question. The vertical line indicates a threshold value where the number of questions per sentence matches that of the original labeled data (2 questions / verb).</figDesc><graphic url="image-4.png" coords="8,79.14,243.44,218.27,163.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics for the dataset with questions 
written by workers across three domains. 

Payment and quality control Generation pays 
5c for the first QA pair (required), plus 5c, 6c, etc. 
for each successive QA pair (optional), to boost 
recall. The validation step pays 8c per verb, plus 
a 2c bonus per question beyond four. Generation 
workers must write at least 2 questions per verb 
and have 85% of their questions counted valid, and 
validators must maintain 85% answer span agree-
ment with others, or they are disqualified from 
further work. A validator's answer is considered 
to agree with others if their answer span overlaps 
with answer spans provided by a majority of work-
ers. 

Preprocessing We use the Stanford CoreNLP 
tools (Manning et al., 2014) for sentence segmen-
tation, tokenizing, and POS-tagging. We identify 
verbs by POS tag, with heuristics to filter out aux-
iliary verbs while retaining non-auxiliary uses of 
"have" and "do." We identify conjugated forms 
of each verb for the QA-SRL templates by finding 
them in Wiktionary. 3 

Dataset We gathered annotations for 133,479 
verb mentions in 64,018 sentences (1.27M tokens) 
across 3 domains: Wikipedia, Wikinews, and sci-
ence textbook text from the Textbook Question 
Answering (TQA) dataset (Kembhavi et al., 2017). 
We partitioned the source documents into train, 
dev, and test, sampled paragraph-wise from each 
document with an 80/10/10 split by sentence. 
Annotation in our pipeline with n = 2 valida-tors took 9 days on Amazon Mechanical Turk. 4 
1,165 unique workers participated, annotating a 
total of 299,308 questions. Of these, 265,140 (or 
89%) were considered valid by both validators, for 
an average of 1.99 valid questions per verb and 
4.14 valid questions per sentence. See Table 2 for 
a breakdown of dataset statistics by domain. The 
total cost was $43,647.33, for an average of 32.7c 
per verb mention, 14.6c per question, or 16.5c per 
valid question. For comparison, He et al. (2015) 
interviewed and hired contractors to annotate data 
at much smaller scale for a cost of about 50c per 
verb. Our annotation scheme is cheaper, far more 
scalable, and provides more (though noisier) su-
pervision for answer spans. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> www.wiktionary.org</note>

			<note place="foot" n="4"> www.mturk.com</note>

			<note place="foot" n="6"> The incorrect question &quot;When did someone appear?&quot; would be correct if the Prep and Misc slots were corrected to read &quot;When did someone appear to do something?&quot;</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The crowdsourcing funds for QA-SRL Bank 2.0 was provided by the Allen Institute for Artificial Intelligence. This research was supported in part by the ARO (W911NF-16-1-0121) the NSF (IIS-1252835, IIS-1562364), a gift from Amazon, and an Allen Distinguished Investigator Award. We would like to thank Gabriel Stanovsky and Mark Yatskar for their helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal conceptual cognitive annotation (UCCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2013</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Berkeley Framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Developing a large semantically annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noortje</forename><surname>Venhuizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2012</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Propbank annotation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Babko-Malaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jena</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast semantic extraction using a novel neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating disambiguating paraphrases for structurally ambiguous sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Linguistic Annotation Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semantic role labeling with neural network factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Jointly predicting predicates and arguments in neural semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Question-answer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Human-in-the-loop parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Crowdsourcing question-answer meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Rawlins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<title level="m">Semantic proto-roles. TACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">FrameNet II: Extended theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Miriam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Christopher R Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scheffczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Bibliothek</pubPlace>
		</imprint>
	</monogr>
	<note>Institut für Deutsche Sprache</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Creating a large benchmark for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Frame-semantic parsing with softmax-margin segmental rnns and a syntactic scaffold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09528</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic proto-role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Adam R Teichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gormley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4459" to="4466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Crowd-inthe-loop: A hybrid approach for annotating semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Chiticariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The semantic proto-role linking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Rawlins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A joint sequential and relational model for frame-semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
