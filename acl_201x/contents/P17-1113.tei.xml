<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1227" to="1236"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1113</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What&apos;s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Joint extraction of entities and relations is to de- tect entity mentions and recognize their semantic relations simultaneously from unstructured text, as <ref type="figure">Figure 1</ref> shows. Different from open information extraction (Open IE) ( <ref type="bibr" target="#b0">Banko et al., 2007</ref>) whose relation words are extracted from the given sen- tence, in this task, relation words are extracted from a predefined relation set which may not ap- pear in the given sentence. It is an important issue in knowledge extraction and automatic construc- tion of knowledge base.</p><p>Traditional methods handle this task in a pipelined manner, i.e., extracting the entities ( <ref type="bibr" target="#b17">Nadeau and Sekine, 2007</ref>) first and then recog- nizing their relations <ref type="bibr" target="#b20">(Rink, 2010)</ref>. This separated framework makes the task easy to deal with, and each component can be more flexible. But it ne- glects the relevance between these two sub-tasks and each subtask is an independent model. The results of entity recognition may affect the perfor- mance of relation classification and lead to erro- neous delivery ( <ref type="bibr" target="#b12">Li and Ji, 2014</ref>).</p><p>Different from the pipelined methods, join- t learning framework is to extract entities togeth- er with relations using a single model. It can ef- fectively integrate the information of entities and relations, and it has been shown to achieve bet- ter results in this task. However, most existing joint methods are feature-based structured system- s ( <ref type="bibr" target="#b12">Li and Ji, 2014;</ref><ref type="bibr" target="#b16">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b29">Yu and Lam, 2010;</ref><ref type="bibr" target="#b19">Ren et al., 2017)</ref>. They need complicated feature engineering and heavily re- ly on the other NLP toolkits, which might also lead to error propagation. In order to reduce the manual work in feature extraction, recently, <ref type="bibr">(Miwa and Bansal, 2016</ref>) presents a neural network- based method for the end-to-end entities and rela- tions extraction. Although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce re- dundant information. For instance, the sentence in <ref type="figure">Figure 1</ref> contains three entities: "United States", "Trump" and "Apple Inc". But only "United S- tates" and "Trump" hold a fix relation "Country- President". Entity "Apple Inc" has no obvious relationship with the other entities in this sen-tence. Hence, the extracted result from this sen- tence is {United States e1 , Country-President r , Trump e2 }, which called triplet here.</p><p>In this paper, we focus on the extraction of triplets that are composed of two entities and one relation between these two entities. Therefore, we can model the triplets directly, rather than extract- ing the entities and relations separately. Based on the motivations, we propose a tagging scheme ac- companied with the end-to-end model to settle this problem. We design a kind of novel tags which contain the information of entities and the rela- tionships they hold. Based on this tagging scheme, the joint extraction of entities and relations can be transformed into a tagging problem. In this way, we can also easily use neural networks to model the task without complicated feature engineering.</p><p>Recently, end-to-end models based on LSTM <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997)</ref> have been successfully applied to various tagging tasks: Named Entity Recognition ( <ref type="bibr" target="#b11">Lample et al., 2016)</ref>, CCG Supertagging ( <ref type="bibr" target="#b25">Vaswani et al., 2016)</ref>, Chunk- ing ( <ref type="bibr" target="#b31">Zhai et al., 2017)</ref> et al. LSTM is capable of learning long-term dependencies, which is benefi- cial to sequence modeling tasks. Therefore, based on our tagging scheme, we investigate different kinds of LSTM-based end-to-end models to joint- ly extract the entities and relations. We also modi- fy the decoding method by adding a biased loss to make it more suitable for our special tags.</p><p>The method we proposed is a supervised learn- ing algorithm. In reality, however, the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone. Therefore, we conduct experiments on a public dataset 1 which is produced by distant supervision method <ref type="bibr" target="#b19">(Ren et al., 2017</ref>) to validate our approach. The experimental results show that our tagging scheme is effective in this task. In ad- dition, our end-to-end model can achieve the best results on the public dataset.</p><p>The major contributions of this paper are: (1) A novel tagging scheme is proposed to jointly extrac- t entities and relations, which can easily transfor- m the extraction problem into a tagging task. (2) Based on our tagging scheme, we study different kinds of end-to-end models to settle the problem. The tagging-based methods are better than most of the existing pipelined and joint learning meth- ods. (3) Furthermore, we also develop an end-to-end model with biased loss function to suit for the novel tags. It can enhance the association between related entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Entities and relations extraction is an importan- t step to construct a knowledge base, which can be benefit for many NLP tasks. Two main frame- works have been widely used to solve the problem of extracting entity and their relationships. One is the pipelined method and the other is the joint learning method.</p><p>The pipelined method treats this task as two sep- arated tasks, i.e., named entity recognition (NER) ( <ref type="bibr" target="#b17">Nadeau and Sekine, 2007)</ref> and relation classifica- tion (RC) <ref type="bibr" target="#b20">(Rink, 2010)</ref>. Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) ( <ref type="bibr" target="#b18">Passos et al., 2014;</ref><ref type="bibr" target="#b13">Luo et al., 2015)</ref>. Re- cently, several neural network architectures <ref type="bibr" target="#b1">(Chiu and Nichols, 2015;</ref><ref type="bibr" target="#b11">Lample et al., 2016)</ref> have been successfully applied to NER, which is regarded as a sequential to- ken tagging task. Existing methods for relation classification can also be divided into handcraft- ed feature based methods <ref type="bibr" target="#b20">(Rink, 2010;</ref><ref type="bibr" target="#b8">Kambhatla, 2004</ref>) and neural network based methods <ref type="bibr" target="#b26">(Xu, 2015a;</ref><ref type="bibr" target="#b32">Zheng et al., 2016;</ref><ref type="bibr" target="#b30">Zeng, 2014;</ref><ref type="bibr" target="#b27">Xu, 2015b;</ref><ref type="bibr" target="#b2">dos Santos, 2015)</ref>.</p><p>While joint models extract entities and relations using a single model. Most of the joint method- s are feature-based structured systems <ref type="bibr" target="#b19">(Ren et al., 2017;</ref><ref type="bibr" target="#b28">Yang and Cardie, 2013;</ref><ref type="bibr" target="#b21">Singh et al., 2013;</ref><ref type="bibr" target="#b16">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b12">Li and Ji, 2014)</ref>. Recent- ly, <ref type="bibr">(Miwa and Bansal, 2016</ref>) uses a LSTM-based model to extract entities and relations, which can reduce the manual work.</p><p>Different from the above methods, the method proposed in this paper is based on a special tag- ging manner, so that we can easily use end-to- end model to extract results without NER and RC. end-to-end method is to map the input sentence into meaningful vectors and then back to produce a sequence. It is widely used in machine transla- tion ( <ref type="bibr" target="#b7">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b22">Sutskever et al., 2014</ref>) and sequence tagging tasks ( <ref type="bibr" target="#b11">Lample et al., 2016;</ref><ref type="bibr" target="#b25">Vaswani et al., 2016</ref>). Most meth- ods apply bidirectional LSTM to encode the input sentences, but the decoding methods are always d- ifferent. For examples, ( <ref type="bibr" target="#b11">Lample et al., 2016</ref>) use a CRF layers to decode the tag sequence, while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Sentence: The United States President Trump will visit the Apple Inc founded by Steven Paul Jobs</head><p>{Apple Inc, Company-Founder, Steven Paul Jobs} Final Results:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tags: O B-CP-1 E-CP-1 O S-CP-2 O O O B-CF-1 E-CF-1 O O B-CF-2 I-CF-2 E-CF-2</head><p>{United States, Country-President, Trump} <ref type="figure">Figure 2</ref>: Gold standard annotation for an example sentence based on our tagging scheme, where "CP" is short for "Country-President" and "CF" is short for "Company-Founder".</p><p>( <ref type="bibr" target="#b25">Vaswani et al., 2016;</ref><ref type="bibr" target="#b9">Katiyar and Cardie, 2016)</ref> apply LSTM layer to produce the tag sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose a novel tagging scheme and an end-to- end model with biased objective function to jointly extract entities and their relations. In this section, we firstly introduce how to change the extraction problem to a tagging problem based on our tag- ging method. Then we detail the model we used to extract results. <ref type="figure">Figure 2</ref> is an example of how the results are tagged. Each word is assigned a label that con- tributes to extract the results. Tag "O" represents the "Other" tag, which means that the correspond- ing word is independent of the extracted result- s. In addition to "O", the other tags consist of three parts: the word position in the entity, the relation type, and the relation role. We use the "BIES" (Begin, Inside, End,Single) signs to rep- resent the position information of a word in the entity. The relation type information is obtained from a predefined set of relations and the relation role information is represented by the numbers "1" and "2". An extracted result is represented by a triplet: (Entity 1 , RelationT ype, Entity 2 ). "1" means that the word belongs to the first entity in the triplet, while "2" belongs to second entity that behind the relation type. Thus, the total number of tags is N t = 2 * 4 * |R| + 1, where |R| is the size of the predefined relation set. <ref type="figure">Figure 2</ref> is an example illustrating our tag- ging method. The input sentence contains t- wo triplets: {United States, Country-President, Trump} and {Apple Inc, Company-Founder, Steven Paul Jobs}, where "Country-President" and "Company-Founder" are the predefined re- lation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Tagging Scheme</head><p>The words "United","States"," Trump","Apple","Inc" ,"Steven", "Paul" and "Jobs" are all related to the final extracted result- s. Thus they are tagged based on our special tags. For example, the word of "United" is the first word of entity "United States" and is related to the rela- tion "Country-President", so its tag is "B-CP-1". The other entity " Trump", which is correspond- ing to "United States", is labeled as "S-CP-2". Be- sides, the other words irrelevant to the final result are labeled as "O".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">From Tag Sequence To Extracted Results</head><p>From the tag sequence in <ref type="figure">Figure 2</ref>, we know that " Trump" and "United States" share the same re- lation type "Country-President", "Apple Inc" and "Steven Paul Jobs" share the same relation type "Company-Founder". We combine entities with the same relation type into a triplet to get the fi- nal result. Accordingly, " Trump" and "United S- tates" can be combined into a triplet whose rela- tion type is "Country-President". Because, the re- lation role of " Trump" is "2" and "United States" is "1", the final result is {United States, Country- President, Trump}. The same applies to {Apple Inc, Company-Founder, Steven Paul Jobs}.</p><p>Besides, if a sentence contains two or more triplets with the same relation type, we combine every two entities into a triplet based on the n- earest principle. For example, if the relation type "Country-President" in <ref type="figure">Figure 2</ref> is "Company- Founder", then there will be four entities in the given sentence with the same relation type. "U- nited States" is closest to entity " Trump" and the "Apple Inc" is closest to "Jobs", so the re- sults will be {United States, Company-Founder, Trump} and {Apple Inc, Company-Founder, Steven Paul Jobs}.</p><p>In this paper, we only consider the situation where an entity belongs to a triplet, and we leave identification of overlapping relations for future work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The End-to-end Model</head><p>In recent years, end-to-end model based on neural network is been widely used in sequence tagging task. In this paper, we investigate an end-to-end model to produce the tags sequence as <ref type="figure" target="#fig_2">Figure 3</ref> shows. It contains a bi-directional Long Short Ter- m Memory (Bi-LSTM) layer to encode the input sentence and a LSTM-based decoding layer with biased loss. The biased loss can enhance the rele- vance of entity tags.</p><p>The Bi-LSTM Encoding Layer. In sequence tag- ging problems, the Bi-LSTM encoding layer has been shown the effectiveness to capture the se- mantic information of each word. It contains for- ward lstm layer, backward lstm layer and the con- catenate layer. The word embedding layer con- verts the word with 1-hot representation to an em- bedding vector. Hence, a sequence of words can be represented as W = {w 1 , ...w t , w t+1 ...w n }, where w t ∈ R d is the d-dimensional word vector corresponding to the t-th word in the sentence and n is the length of the given sentence. After word embedding layer, there are two parallel LSTM lay- ers: forward LSTM layer and backward LSTM layer. The LSTM architecture consists of a set of recurrently connected subnets, known as memory blocks. Each time-step is a LSTM memory block. The LSTM memory block in Bi-LSTM encoding layer is used to compute current hidden vector h t based on the previous hidden vector h t−1 , the pre- vious cell vector c t−1 and the current input word embedding w t . Its structure diagram is shown in <ref type="figure" target="#fig_2">Figure 3</ref> (b), and detail operations are defined as follows:</p><formula xml:id="formula_0">i t = δ(W wi w t + W hi h t−1 + W ci c t−1 + b i ),<label>(1)</label></formula><formula xml:id="formula_1">f t = δ(W wf w t +W hf h t−1 +W cf c t−1 +b f ),<label>(2)</label></formula><formula xml:id="formula_2">z t = tanh(W wc w t + W hc h t−1 + b c ),<label>(3)</label></formula><formula xml:id="formula_3">c t = f t c t−1 + i t z t ,<label>(4)</label></formula><formula xml:id="formula_4">o t = δ(W wo w t + W ho h t−1 + W co c t + b o ),<label>(5)</label></formula><formula xml:id="formula_5">h t = o t tanh(c t ),<label>(6)</label></formula><p>where i, f and o are the input gate, forget gate and output gate respectively, b is the bias term, c is the cell memory, and W (.) are the parameters. For each word w t , the forward LSTM layer will encode w t by considering the contextual informa- tion from word w 1 to w t , which is marked as − → h t . In the similar way, the backward LSTM layer will en- code w t based on the contextual information from w n to w t , which is marked as ← − h t . Finally, we con- catenate ← − h t and − → h t to represent word t's encoding information, denoted as</p><formula xml:id="formula_6">h t = [ − → h t , ← − h t ].</formula><p>The LSTM Decoding Layer. We also adopt a L- STM structure to produce the tag sequence. When detecting the tag of word w t , the inputs of decod- ing layer are: h t obtained from Bi-LSTM encod- ing layer, former predicted tag embedding T t−1 , former cell value c </p><formula xml:id="formula_7">i (2) t = δ(W (2) wi h t + W (2) hi h (2) t−1 + W ti T t−1 + b (2) i ),<label>(7)</label></formula><formula xml:id="formula_8">f (2) t = δ(W<label>(2)</label></formula><formula xml:id="formula_9">wf h t + W (2) hf h (2) t−1 + W tf T t−1 + b (2) f ),<label>(8)</label></formula><formula xml:id="formula_10">z (2) t = tanh(W (2) wc h t +W (2) hc h (2) t−1 +W tc T t−1 +b (2) c ),<label>(9)</label></formula><formula xml:id="formula_11">c (2) t = f (2) t c<label>(2)</label></formula><formula xml:id="formula_12">t−1 + i (2) t z (2) t ,<label>(10)</label></formula><formula xml:id="formula_13">o (2) t = δ(W (2) wo h t + W (2) ho h (2) t−1 + W (2) co c t + b (2) o ),<label>(11)</label></formula><formula xml:id="formula_14">h (2) t = o (2) t tanh(c (2) t ),<label>(12)</label></formula><formula xml:id="formula_15">T t = W ts h (2) t + b ts .<label>(13)</label></formula><p>The final softmax layer computes normalized enti- ty tag probabilities based on the tag predicted vec- tor T t :</p><formula xml:id="formula_16">y t = W y T t + b y ,<label>(14)</label></formula><formula xml:id="formula_17">p i t = exp(y i t ) Nt j=1 exp(y j t ) ,<label>(15)</label></formula><p>where W y is the softmax matrix, N t is the total number of tags. Because T is similar to tag em- bedding and LSTM is capable of learning long- term dependencies, the decoding manner can mod- el tag interactions. The Bias Objective Function. We train our mod- el to maximize the log-likelihood of the data and the optimization method we used is RMSprop pro- posed by Hinton in <ref type="bibr" target="#b24">(Tieleman and Hinton, 2012</ref>). The objective function can be defined as:</p><formula xml:id="formula_18">L =max |D| j=1 L j t=1 (log(p (j) t = y (j) t |x j , Θ) · I(O) +α · log(p (j) t = y (j) t |x j , Θ) · (1 − I(O))),</formula><p>where |D| is the size of training set, L j is the length of sentence x j , y (j) t is the label of word t in sentence x j and p </p><formula xml:id="formula_19">I(O) = 1, if tag = O 0, if tag = O .</formula><p>α is the bias weight. The larger α is, the greater influence of relational tags on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setting</head><p>Dataset To evaluate the performance of our meth- ods, we use the public dataset NYT 2 which is pro- duced by distant supervision method ( <ref type="bibr" target="#b19">Ren et al., 2017)</ref>. There are three data sets in the public resource and we only use the NYT dataset. Because more than 50% of the data in BioInfer has overlapping relations which is beyond the scope of this paper. As for dataset Wiki-KBP, the number of relation type in the test set is more than that of the train set, which is also not suitable for a supervised training method. Details of the data can be found in Ren's( <ref type="bibr" target="#b19">Ren et al., 2017</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.463 LSTM-CRF</head><p>0.693 ± 0.008 0.310 ± 0.007 0.428 ± 0.008 LSTM-LSTM 0.682 ± 0.007 0.320 ± 0.006 0.436 ± 0.006 LSTM-LSTM-Bias 0.615 ± 0.008 0.414 ± 0.005 0.495 ± 0.006 <ref type="table">Table 1</ref>: The predicted results of different methods on extracting both entities and their relations. The first part (from row 1 to row 3) is the pipelined methods and the second part (row 4 to 6) is the jointly extracting methods. Our tagging methods are shown in part three (row 7 to 9). In this part, we not only report the results of precision, recall and F1, we also compute their standard deviation.</p><p>Baselines We compare our method with sever- al classical triplet extraction methods, which can be divided into the following categories: the pipelined methods, the jointly extracting method- s and the end-to-end methods based our tagging scheme.</p><p>For the pipelined methods, we follow (Ren et al., 2017)'s settings: The NER results are ob- tained by <ref type="bibr">CoType (Ren et al., 2017</ref>) then sever- al classical relation classification methods are ap- plied to detect the relations. These methods are: (1) DS-logistic ( <ref type="bibr" target="#b15">Mintz et al., 2009</ref>) is a distant su- pervised and feature based method, which com- bines the advantages of supervised IE and unsu- pervised IE features; (2) LINE ( <ref type="bibr" target="#b23">Tang et al., 2015</ref>) is a network embedding method, which is suit- able for arbitrary types of information networks; (3) FCM ( <ref type="bibr" target="#b3">Gormley et al., 2015</ref>) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction.</p><p>The jointly extracting methods used in this pa- per are listed as follows: (4) DS-Joint ( <ref type="bibr" target="#b12">Li and Ji, 2014</ref>) is a supervised method, which jointly ex- tracts entities and relations using structured per- ceptron on human-annotated dataset; (5) MultiR ( <ref type="bibr" target="#b5">Hoffmann et al., 2011</ref>) is a typical distant super- vised method based on multi-instance learning al- gorithms to combat the noisy training data; (6) Co- Type ( <ref type="bibr" target="#b19">Ren et al., 2017</ref>) is a domain independent framework by jointly embedding entity mention- s, relation mentions, text features and type labels into meaningful representations.</p><p>In addition, we also compare our method with two classical end-to-end tagging models: LSTM- CRF ( <ref type="bibr" target="#b11">Lample et al., 2016)</ref> and LSTM-LSTM ( <ref type="bibr" target="#b25">Vaswani et al., 2016)</ref>. LSTM-CRF is proposed for entity recognition by using a bidirectional L- STM to encode input sentence and a conditional random fields to predict the entity tag sequence. Different from LSTM-CRF, LSTM-LSTM uses a LSTM layer to decode the tag sequence instead of CRF. They are used for the first time to jointly extract entities and relations based on our tagging scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>We report the results of different methods as shown in <ref type="table">Table 1</ref>. It can be seen that our method, LSTM-LSTM-Bias, outperforms all other meth- ods in F1 score and achieves a 3% improvement in F 1 over the best method CoType ( <ref type="bibr" target="#b19">Ren et al., 2017)</ref>. It shows the effectiveness of our proposed method. Furthermore, from <ref type="table">Table 1</ref>, we also can see that the jointly extracting methods are better than pipelined methods, and the tagging methods are better than most of the jointly extracting meth- ods. It also validates the validity of our tagging scheme for the task of jointly extracting entities and relations.</p><p>When compared with the traditional methods, the precisions of the end-to-end models are signifi- cantly improved. But only LSTM-LSTM-Bias can be better to balance the precision and recall. The reason may be that these end-to-end models all use a Bi-LSTM encoding input sentence and different neural networks to decode the results. The meth- ods based on neural networks can well fit the da- ta. Therefore, they can learn the common features of the training set well and may lead to the lower expansibility. We also find that the LSTM-LSTM 458 LSTM-LSTM-Bias 0.590 0.479 0.529 0.597 0.451 0.514 0.645 0.437 0.520 <ref type="table">Table 2</ref>: The predicted results of triplet's elements based on our tagging scheme. model is better than LSTM-CRF model based on our tagging scheme. Because, LSTM is capable of learning long-term dependencies and CRF <ref type="bibr" target="#b10">(Lafferty et al., 2001</ref>) is good at capturing the joint probability of the entire sequence of labels. The related tags may have a long distance from each other. Hence, LSTM decoding manner is a little better than CRF. LSTM-LSTM-Bias adds a bias weight to enhance the effect of entity tags and weaken the effect of invalid tag. Therefore, in this tagging scheme, our method can be better than the common LSTM-decoding methods.</p><note type="other">Elements E1 E2 (E1,E2) PRF Prec.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Error Analysis</head><p>In this paper, we focus on extracting triplets com- posed of two entities and a relation. <ref type="table">Table 1 has</ref> shown the predict results of the task. It treats an triplet is correct only when the relation type and the head offsets of two corresponding entities are both correct. In order to find out the factors that af- fect the results of end-to-end models, we analyze the performance on predicting each element in the triplet as <ref type="table">Table 2</ref> shows. E1 and E2 represent the performance on predicting each entity, respective- ly. If the head offset of the first entity is correct, then the instance of E1 is correct, the same to E2. Regardless of relation type, if the head offsets of two corresponding entities are both correct, the in- stance of (E1, E2) is correct.</p><p>As shown in <ref type="table">Table 2</ref>, (E1, E2) has higher pre- cision when compared with E1 and E2. But its recall result is lower than E1 and E2. It means that some of the predicted entities do not form a pair. They only obtain E1 and do not find its cor- responding E2, or obtain E2 and do not find its corresponding E1. Thus it leads to the prediction of more single E and less (E1, E2) pairs. There- fore, entity pair (E1, E2) has higher precision and lower recall than single E. Besides, the predict- ed results of (E1, E2) in <ref type="table">Table 2</ref> have about 3% improvement when compared predicted results in <ref type="table">Table 1</ref>, which means that 3% of the test data is predicted to be wrong because the relation type is predicted to be wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Biased Loss</head><p>Different from LSTM-CRF and LSTM-LSTM, our approach is biased towards relational labels to enhance links between entities. In order to further analyze the effect of the bias objective function, we visualize the ratio of predicted single entities for each end-to-end method as <ref type="figure" target="#fig_5">Figure 4</ref>. The s- ingle entities refer to those who cannot find their corresponding entities. <ref type="figure" target="#fig_5">Figure 4</ref> shows whether it is E1 or E2, our method can get a relatively low ra- tio on the single entities. It means that our method can effectively associate two entities when com- pared LSTM-CRF and LSTM-LSTM which pay little attention to the relational tags. Besides, we also change the Bias Parameter α from 1 to 20, and the predicted results are shown in <ref type="figure" target="#fig_6">Figure 5</ref>. If α is too large, it will affect the accuracy of prediction and if α is too small, the recall will decline. When α = 10, LSTM-LSTM- Bias can balance the precision and recall, and can achieve the best F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard S1</head><p>[Panama City Beach] E2contain has condos , but the area was one of only two in <ref type="bibr">[Florida]</ref> E1contain where sales rose in March , compared with a year earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM-LSTM</head><p>Panama City Beach has condos , but the area was one of only two in <ref type="bibr">[Florida]</ref> E1contain where sales rose in March , compared with a year earlier.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM-LSTM-Bias</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head><p>In this section, we observe the prediction results of end-to-end methods, and then select several repre- sentative examples to illustrate the advantages and disadvantages of the methods as <ref type="table" target="#tab_5">Table 3</ref> shows. Each example contains three row, the first row is the gold standard, the second and the third rows are the extracted results of model LSTM-LSTM and LSTM-LSTM-Bias respectively. S1 represents the situation that the distance be- tween the two interrelated entities is far away from each other, which is more difficult to detect their relationships. When compared with LSTM- LSTM, LSTM-LSTM-Bias uses a bias objective function which enhance the relevance between en- tities. Therefore, in this example, LSTM-LSTM- Bias can extract two related entities, while LSTM- LSTM can only extract one entity of "Florida" and can not detect entity "Panama City Beach". S2 is a negative example that shows these meth- ods may mistakenly predict one of the entity. There are no indicative words between entities N uremberg and Germany. Besides, the patten "a * of *" between Germany and M iddleAges may be easy to mislead the models that there ex- ists a relation of "Contains" between them. The problem can be solved by adding some samples of this kind of expression patterns to the training data.</p><p>S3 is a case that models can predict the enti- ties' head offset right, but the relational role is wrong. LSTM-LSTM treats both "Stephen A. Schwarzman" and "Blackstone Group" as entity E1, and can not find its corresponding E2. Al- though, LSTM-LSMT-Bias can find the entities pair (E1, E2), it reverses the roles of "Stephen A. Schwarzman" and "Blackstone Group". It shows that LSTM-LSTM-Bias is able to better on pre-dicting entities pair, but it remains to be improved in distinguishing the relationship between the two entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel tagging scheme and investigate the end-to-end models to joint- ly extract entities and relations. The experimen- tal results show the effectiveness of our proposed method. But it still has shortcoming on the identi- fication of the overlapping relations. In the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags. In this way, a word can ap- pear in multiple triplet results, which can solve the problem of overlapping relations. Although, our model can enhance the effect of entity tags, the as- sociation between two corresponding entities still requires refinement in next works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: A standard example sentence for the task. "Country-President" is a relation in the predefined relation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of our model. (a): The architecture of the end-to-end model, (b): The LSTM memory block in Bi-LSTM encoding layer, (c): The LSTM memory block in LSTM d decoding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, and the former hidden vec- tor in decoding layer h (2) t−1 . The structure diagram of the memory block in LSTM d is shown in Figure 3 (c), and detail operations are defined as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>t</head><label></label><figDesc>is the normalized probabil- ities of tags which defined in Formula 15. Besides, I(O) is a switching function to distinguish the loss of tag 'O' and relational tags that can indicate the results. It is defined as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The ratio of predicted single entities for each method. The higher of the ratio the more entities are left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The results predicted by LSTM-LSTMBias on different bias parameter α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>[Panama City Beach] E2contain has condos , but the area was one of only two in [Florida] E1contain where sales rose in March , compared with a year earlier.</head><label></label><figDesc></figDesc><table>Standard S2 
All came from [Nuremberg] E2contain , [Germany] E1contain , a center of brass 
production since the Middle Ages. 

LSTM-LSTM 
All came from Nuremberg , [Germany] E1contain , a center of brass production 
since the [Middle Ages] E2contain . 

LSTM-LSTM-Bias 
All came from Nuremberg , [Germany] E1contain , a center of brass production 
since the [Middle Ages] E2contain . 

Standard S3 
[Stephen A.] E2CF , the co-founder of the [Blackstone Group] E1CF , which 
is in the process of going public , made $ 400 million last year. 

LSTM-LSTM 
[Stephen A.] E1CF , the co-founder of the [Blackstone Group] E1CF , which 
is in the process of going public , made $ 400 million last year. 

LSTM-LSTM-Bias 
[Stephen A.] E1CF , the co-founder of the [Blackstone Group] E2CF , which 
is in the process of going public , made $ 400 million last year. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Output from different models. Standard S i represents the gold standard of sentence i. The 
blue part is the correct result, and the red one is the wrong one. E1CF in case '3' is short for 
E1 Company−F ounder . 

</table></figure>

			<note place="foot" n="1"> https://github.com/shanzhenren/CoType</note>

			<note place="foot">Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using lstms on sequences and tree structures. In Proceedings of the 54rd Annual Meeting of the Association for Computational Linguistics.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Xiang Ren for dataset details and help-ful discussions. This work is also supported by the National High Technology Research and Devel-opment Program of China <ref type="formula">(863 Program</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processings of Transactions of the Association for Computational Linguistics</title>
		<meeting>essings of Transactions of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th ACL international conference</title>
		<meeting>the 53th ACL international conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">413</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43th ACL international conference</title>
		<meeting>the 43th ACL international conference</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Investigating lstms for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL international conference</title>
		<meeting>the 54th ACL international conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on machine learning</title>
		<meeting>the eighteenth international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL</title>
		<meeting>the NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>international conference</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
	<note>Chin-Yew Lin, and Zaiqing Nie</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqiu</forename><surname>Xiang Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th WWW international conference</title>
		<meeting>the 26th WWW international conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint inference of entities, relations, and coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 workshop on Automated</title>
		<meeting>the 2013 workshop on Automated</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural networks for machine learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supertagging with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL international conference</title>
		<meeting>the NAACL international conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP international conference</title>
		<meeting>EMNLP international conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th COLING international conference</title>
		<meeting>the 21th COLING international conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daojian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th COLING international conference</title>
		<meeting>the 25th COLING international conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural models for sequence chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>international conference</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A neural network framework for relation extraction: Learning entity semantic and relation pattern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KnowledgeBased Systems</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
