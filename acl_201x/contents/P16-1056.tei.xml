<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><forename type="middle">Vlad</forename><surname>Serban</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Université de Technologie deCompì egne CNRS Rue du Dr Schweitzer</orgName>
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal, Compigne</settlement>
									<region>QC</region>
									<country>Canada, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>2920 chemin de la Tour</addrLine>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="588" to="598"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
					<note>* First authors. • † CIFAR Senior Fellow</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to trans-duce facts into natural language questions. The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be comparable in quality to real human-generated questions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A major obstacle for training question-answering (QA) systems has been due to the lack of labeled data. The question answering field has focused on building QA systems based on traditional in- formation retrieval procedures ( <ref type="bibr">Lopez et al., 2011;</ref><ref type="bibr" target="#b13">Dumais et al., 2002;</ref><ref type="bibr" target="#b26">Voorhees and Tice, 2000</ref>). More recently, researchers have started to utilize large-scale knowledge bases (KBs) ( <ref type="bibr">Lopez et al., 2011</ref>), such as Freebase ( <ref type="bibr" target="#b4">Bollacker et al., 2008)</ref>, <ref type="bibr">WikiData (Vrandeči´Vrandeči´c and Krötzsch, 2014</ref>) and Cyc ( <ref type="bibr" target="#b17">Lenat and Guha, 1989)</ref>. <ref type="bibr">1</ref> Bootstrapping QA systems with such structured knowledge is clearly beneficial, but it is unlikely alone to overcome the lack of labeled data. To take into account the rich and complex nature of human language, such as paraphrases and ambiguity, it would appear that labeled question and answer pairs are necessary. The need for such labeled pairs is even more criti- cal for training neural network-based QA systems, where researchers until now have relied mainly on hand-crafted rules and heuristics to synthesize ar- tificial QA corpora ( <ref type="bibr" target="#b7">Bordes et al., 2014;</ref><ref type="bibr" target="#b8">Bordes et al., 2015)</ref>.</p><p>Motivated by these recent developments, in this paper we focus on generating questions based on the Freebase KB. We frame question generation as a transduction problem starting from a Freebase fact, represented by a triple consisting of a sub- ject, a relationship and an object, which is trans-duced into a question about the subject, where the object is the correct answer ( <ref type="bibr" target="#b8">Bordes et al., 2015)</ref>. We propose several models, largely in- spired by recent neural machine translation mod- els ( <ref type="bibr">Cho et al., 2014;</ref><ref type="bibr" target="#b25">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, and we use an approach sim- ilar to <ref type="bibr" target="#b18">Luong et al. (2015)</ref> for dealing with the problem of rare-words. We evaluate the produced questions in a human-based experiment as well as with respect to automatic evaluation metrics, in- cluding the well-established machine translation metrics BLEU and METEOR and a sentence simi- larity metric. We find that the question-generation model outperforms the competing template-based baseline, and, when presented to untrained human evaluators, the produced questions appear to be in- distinguishable from real human-generated ques- tions. This suggests that the produced question- answer pairs are of high quality and therefore that they will be useful for training QA systems. Fi- nally, we use the best performing model to con- struct a new factoid question-answer corpus -The 30M Factoid Question-Answer Corpus -which is made freely available to the research community. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Question generation has attracted interest in recent years with notable work by <ref type="bibr" target="#b24">Rus et al. (2010)</ref>, fol- lowed by the increasing interest from the Natural Language Generation (NLG) community. A sim- ple rule-based approach was proposed in different studies as wh-fronting or wh-inversion ( <ref type="bibr" target="#b16">Kalady et al., 2010;</ref><ref type="bibr" target="#b0">Ali et al., 2010)</ref>. This comes at the disadvantage of not making use of the semantic content of words apart from their syntactic role. The problem of determining the question type (e.g. that a Where-question should be triggered for loca- tions), which requires knowledge of the category type of the elements involved in the sentence, has been addressed in two different ways: by using named entity recognizers <ref type="bibr" target="#b19">(Mannem et al., 2010;</ref><ref type="bibr" target="#b28">Yao and Zhang, 2010)</ref> or semantic role labelers <ref type="bibr" target="#b9">(Chen et al., 2009)</ref>. In <ref type="bibr" target="#b11">Curto et al. (2012)</ref> ques- tions are split into classes according to their syn- tactic structure, prefix of the question and the cat- egory of the answer, and then a pattern is learned to generate questions for that class of questions. After the identification of key points, <ref type="bibr" target="#b9">Chen et al. (2009)</ref> apply handcrafted-templates to generate questions framed in the right target expression by following the analysis of <ref type="bibr" target="#b14">Graesser et al. (1992)</ref>, who classify questions according to a taxonomy consisting of 18 categories.</p><p>The works discussed so far propose ways to map unstructured text to questions. This implies a two-step process: first, transform a text into a symbolic representation (e.g. a syntactic represen- tation of the sentence), and second, transform the symbolic representation of the text into the ques- tion ( <ref type="bibr" target="#b29">Yao et al., 2012</ref>). On the other hand, go- ing from a symbolic representation (structured in- formation) to a question, as we will describe in the next section, only involves the second step. Closer to our approach is the work by <ref type="bibr" target="#b21">Olney et al. (2012)</ref>. They take triples as input, where the edge relation defines the question template and the head of the triple replaces the placeholder token in the selected question template. In the same spirit, <ref type="bibr" target="#b12">Duma et al. (2013)</ref> generate short descriptions from triples by using templates defined by the rela- tionship and replacing accordingly the placeholder tokens for the subject and object.</p><p>Our baseline is similar to that of <ref type="bibr" target="#b21">Olney et al. (2012)</ref>, where a set of relationship-specific tem- plates are defined. These templates include place- holders to replace the string of the subject. The main difference with respect to their work is that our baseline does not explicitly define these tem- plates. Instead, each relationship has as many templates as there are different ways of framing a question with that relationship in the training set. This yields more diverse and semantically richer questions by effectively taking advantage of the fact-question pairs, which Olney et al. did not have access to in their experiments.</p><p>Unlike the work by <ref type="bibr">Berant and Liang (2014)</ref>, which addresses the problem of deterministically generating a set of candidate logical forms with a canonical realization in natural language for each, our work addresses the inverse problem: given a logical form (fact) it outputs the associated ques- tion.</p><p>It should also be noted that recent work in ques- tion answering have used simpler rule-based and template-based approaches to generate synthetic questions to address the lack of question-answer pairs to train their models ( <ref type="bibr" target="#b7">Bordes et al., 2014;</ref><ref type="bibr" target="#b8">Bordes et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge Bases</head><p>In general, a KB can be viewed as a multi- relational graph, which consists of a set of nodes (entities) and a set of edges (relation- ships) linking nodes together. In Freebase ( <ref type="bibr" target="#b4">Bollacker et al., 2008</ref>) these relationships are di- rected and always connect exactly two enti- ties. For example, in Freebase the two enti- ties fires creek and nantahala national forest are linked together by the relationship contained by. Since the triple {fires creek, contained by, nan- tahala national forest} represents a complete and self-contained piece of information, it is also called a fact where fires creek is the subject (head of the edge), contained by is the relationship and nantahala national forest is the object (tail of the edge).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transducing Facts to Questions</head><p>We aim to transduce a fact into a question, such that:</p><p>1. The question is concerned with the subject and relationship of the fact, and 2. The object of the fact represents a valid an- swer to the generated question.</p><p>We model this in a probabilistic framework as a directed graphical model:</p><formula xml:id="formula_0">P (Q|F ) = N n=1 P (w n |w &lt;n , F ),<label>(1)</label></formula><p>where F = (subject, relationship, object) rep- resents the fact, Q = (w 1 , . . . , w N ) represents the question as a sequence of tokens w 1 , . . . , w N , and w &lt;n represents all the tokens generated before to- ken w n . In particular, w N represents the question mark symbol '?'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset</head><p>We use the SimpleQuestions dataset ( <ref type="bibr" target="#b8">Bordes et al., 2015</ref>) in order to train our models. This is by far the largest dataset of question-answer pairs created by humans based on a KB. It contains over 100K question-answer pairs created by users on Amazon Mechanical Turk 3 in English based on the Free- base KB. In order to create the questions, human participants were shown one whole Freebase fact <ref type="table">Table 1</ref>: Statistics of SimpleQuestions at a time and they were asked to phrase a ques- tion such that the object of the presented fact be- comes the answer of the question. <ref type="bibr">4</ref> Consequently, both the subject and the relationship are explic- itly given in each question. But indirectly char- acteristics of the object may also be given since the humans have an access to it as well. Often when phrasing a question the annotators tend to be more informative about the target object by giving specific information about it in the question pro- duced. For example, in the question What city is the American actress X from? the city name given in the object informs the human participant that it was in America -information, which was not pro- vided by either the subject or relationship of the fact. We have also observed that the questions are often ambiguous: that is, one can easily come up with several possible answers that may fit the spec- ifications of the question. <ref type="table">Table 1</ref> shows statistics of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>We propose to attack the problem with the models inspired by the recent success of neural machine translation models <ref type="bibr" target="#b25">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>). Intuitively, one can think of the transduction task as a "lossy translation" from structured knowledge (facts) to human language (questions in natural language), where certain as- pects of the structured knowledge is intentionally left out (e.g. the name of the object). These models typically consist of two components: an encoder, which encodes the source phrase into one or sev- eral fixed-size vectors, and a decoder, which de- codes the target phrase based on the results of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoder</head><p>In contrast to the neural machine translation framework, our source language is not a proper language but instead a sequence of three vari- ables making up a fact. We propose an encoder sub-model, which encodes each atom of the fact into an embedding. Each atom {s, r, o}, may stand for subject, relationship and object, respec- tively, of a fact F = (s, r, o) is represented as a 1-of-K vector x atom , whose embedding is ob- tained as e atom = E in x atom , where E in ∈ R D Enc ×K is the embedding matrix of the input vocabulary and K is the size of that vocabulary. The en- coder transforms this embedding into Enc(F ) atom ∈ R H Dec as Enc(F ) atom = W Enc e atom , where W Enc ∈ R H Dec ×D Enc . This embedding matrix, E in , could be another parameter of the model to be learned, however, as discussed later (see Section 4.3), we have learned it separately and beforehand with <ref type="bibr">TransE (Bordes et al., 2013</ref>), a model aimed at modeling this kind of multi-relational data. We fix it and do not allow the encoder to tune it during training.</p><p>We call fact embedding</p><formula xml:id="formula_1">Enc(F ) ∈ R 3H Dec the concatenation [Enc(F ) s , Enc(F ) r , Enc(F ) o ]</formula><p>of the atom embeddings, which is the input for the next module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoder</head><p>For the decoder, we use a GRU recurrent neu- ral network (RNN) ( <ref type="bibr">Cho et al., 2014</ref>) with an attention-mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) on the encoder representation to generate the associ- ated question Q to that fact F . Recently, it has been shown that the GRU RNN performs equally well across a range of tasks compared to other RNN architectures, such as the LSTM RNN ( <ref type="bibr" target="#b15">Greff et al., 2015</ref>). The hidden state of the decoder RNN is computed at each time step n as:</p><formula xml:id="formula_2">g r n = σ(W r E out w n−1 + C r c(F, h n−1 ) + U r h n−1 ) (2) g u n = σ(W u E out w n−1 + C u c(F, h n−1 ) + U u h n−1 ) (3) ˜ h = tanh(W E out w n−1 + Cc(F, h n−1 ) (4) + U (g r n • h n−1 )) h n = g u n • h n−1 + (1 − g u n ) • ˜ h,<label>(5)</label></formula><p>where σ is the sigmoid function, s.t. σ(x) ∈ [0, 1], and the circle, •, represents element-wise mul- tiplication. The initial state h 0 of this RNN is given by the output of a feedforward neural net- work fed with the fact embedding. The product E out w n ∈ R D Dec is the decoder embedding vec- tor corresponding to the word w n (coded as a 1- of-V vector, with V being the size of the output vocabulary), the variables</p><formula xml:id="formula_3">U r , U u , U, C r , C u , C ∈ R H Dec ×H Dec , W r , W u , W ∈ R H Dec ×D Dec are the pa- Figure 1:</formula><p>The computational graph of the question-generation model, where Enc(F ) is the fact embedding produced by the encoder model, and c(F, h n−1 ) for n = 1, . . . , N is the fact rep- resentation weighed according to the attention- mechanism, which depends on both the fact F and the previous hidden state of the decoder RNN h n−1 . For the sake of simplicity, the attention- mechanism is not shown explicitly.</p><p>rameters of the GRU and c(F, h n−1 ) is the con- text vector (defined below Eq. 6). The vector g r is called the reset gate, g u as the update gate and˜hand˜ and˜h the candidate activation. By adjusting g r and g u appropriately, the model is able to cre- ate linear skip-connections between distant hid- den states, which in turn makes the credit as- signment problem easier and the gradient signal stronger to earlier hidden states. Then, at each time step n the set of probabilities over word to- kens is given by applying a softmax layer over</p><formula xml:id="formula_4">V o tanh(V h h n + V w E out w n−1 + V c c(F, h n−1 )), where V o ∈ R V ×H Dec , V h , V c ∈ R H Dec ×H Dec and V w ∈ R H Dec ×D Dec .</formula><p>Lastly, the function c(F, h n−1 ) is computed using an attention-mechanism:</p><formula xml:id="formula_5">c(F, h n−1 ) = α s,n−1 Enc(F ) s + α r,n−1 Enc(F ) r + α o,n−1 Enc(F ) o ,<label>(6)</label></formula><p>where α s,n−1 , α r,n−1 , α r,n−1 are real-valued scalars, which weigh the contribution of the subject, relationship and object representations.</p><p>They correspond to the attention of the model, and are computed by applying a one-layer neural network with tanh-activation function on the encoder representations of the fact, Enc(F ), and the previous hidden state of the RNN, h n−1 , followed by the sigmoid function to restrict the attention values to be between zero and one. The need for the attention-mechanism is motivated by the intuition that the model needs to attend to the subject only once during the generation process while attending to the relationship at all other times during the generation process. The model is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modeling the Source Language</head><p>A particular problem with the model presented above is related to the embeddings for the enti- ties, relationships and tokens, which all have to be learned in one way or another. If we learn these naively on the SimpleQuestions training set, the model will perform poorly when it encoun- ters previously unseen entities, relationships or to- kens. Furthermore, the multi-relational graph de- fined by the facts in SimpleQuestions is extremely sparse, i.e. each node has very few edges to other nodes, as can be expected due to high ratio of unique entities over number of examples. There- fore, even for many of the entities in SimpleQues- tions, the model may perform poorly if the embed- ding is learned solely based on the SimpleQues- tions dataset alone.</p><p>On the source side, we can resolve this is- sue by initializing the subject, relationship and object embeddings to those learned by apply- ing multi-relational embedding-based models to the knowledge base. Multi-relational embedding- based models <ref type="bibr" target="#b5">(Bordes et al., 2011</ref>) have recently become popular to learn distributed vector embed- dings for knowledge bases, and have shown to scale well and yield good performance. Due to its simplicity and good performance, we choose to use <ref type="bibr">TransE (Bordes et al., 2013</ref>) to learn such embeddings. TransE is a translation-based model, whose energy function is trained to output low val- ues when the fact expresses true information, i.e. a fact which exists in the knowledge base, and other- wise high values. Formally, the energy function is defined as f (s, r, o) = ||e s + e r − e o || 2 , where e s , e r and e o are the real-valued embedding vectors for the subject, relationship and object of a fact. Further details are given by <ref type="bibr" target="#b6">Bordes et al. (2013)</ref>.</p><p>Embeddings for entities with few connections are easy to learn, yet the quality of these embed- dings depends on how inter-connected they are. In the extreme case where the subject and object of a triple only appears once in the dataset, the learned embeddings of the subject and object will be se- mantically meaningless. This happens very often in SimpleQuestions, since only around 5% of the entities have more than 2 connections in the graph. Thus, by applying TransE directly over this set of triples, we would eventually end up with a lay- out of entities that does not contain clusters of se- mantically close concepts. In order to guarantee an effective semantic representation of the embed- dings, we have to learn them together with addi- tional triples extracted from the whole Freebase graph to complement the SimpleQuestions graph with relevant information for this task.</p><p>We need a coarse representation for the entities contained in SimpleQuestions, capturing the ba- sic information, like the profession or nationality, the annotators tend to use when phrasing the ques- tions, and accordingly we have ensured the em- beddings contain this information by taking triples coming from the Freebase graph 5 regarding:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Category information:</head><p>given by the type/instance relationship, this ensures that all the entities of the same semantic category are close to each other. Although one might think that the expected category of the subject/object could be inferred directly from the relationship, there are fine-grained differences in the expected types that be extracted only directly by observing this category information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Geographical information:</head><p>sometimes the annotators have included information about nationality (e.g.</p><p>Which French president. . . ?) or location (e.g. Where in Germany. . . ?) of the subject and/or object.</p><p>This information is given by the relationships person/nationality and location/contained by. By including these facts in the learning, we ensure the existence of a fine-grained layout of the embeddings regarding this information within a same category.  To this end, we have included more than 300, 000 facts from Freebase in addition to the facts in SimpleQuestions for training. <ref type="table" target="#tab_1">Table 2</ref> shows the differences in the embeddings before and after adding additional facts for training the TransE representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generating Questions</head><p>To resolve the problem of data sparsity and previ- ously unseen words on the target side, we draw in- spiration from the placeholders proposed for han- dling rare words in neural machine translation by <ref type="bibr" target="#b18">Luong et al. (2015)</ref>. For every question and an- swer pair, we search for words in the question which overlap with words in the subject string of the fact. <ref type="bibr">6</ref> We heuristically estimate the sequence of most likely words in the question, which cor- respond to the subject string. These words are then replaced by the placeholder token &lt;place- holder&gt;. For example, given the fact {fires creek, contained by, nantahala national forest} the orig- inal question Which forest is Fires Creek in? is transformed into the question Which forest is &lt;placeholder&gt;in?. The model is trained on these modified questions, which means that model only has to learn decoder embeddings for tokens which are not in the subject string. At test time, after outputting a question, all placeholder tokens are replaced by the subject string and then the outputs are evaluated. We call this the Single-Placeholder (SP) model. The main difference with respect to that of <ref type="bibr" target="#b18">Luong et al. (2015)</ref> is that we do not use placeholder tokens in the input language, be-cause then the entities and relationships in the in- put would not be able to transmit semantic (e.g. topical) information to the decoder. If we had in- cluded placeholder tokens in the input language, the model would not be able to generate informa- tive words regarding the subject in the question (e.g. it would be impossible for the model to learn that the subject Paris may be accompanied by the words French city when generating a question, be- cause it would not see Paris but only a placeholder token).</p><p>A single placeholder token for all question types could unnecessarily limit the model. We there- fore also experiment with another model, called the Multi-Placeholder (MP) model, which uses 60 different placeholder tokens such that the place- holder for a given question is chosen based on the subject category extracted from the relation- ship (e.g. contained by is classified in the category location, and so the transformed question would be Which forest is &lt;location placeholder&gt; in?). This could make it easier for the model to learn to phrase questions about a diverse set of entities, but it also introduces additional parameters, since there are now 60 placeholder embeddings to be learned, and therefore the model may suffer from overfitting. This way of addressing the sparsity in the output reduces the vocabulary size to less than 7000 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Template-based Baseline</head><p>To compare our neural network models, we pro- pose a (non-parametric) template-based baseline model, which makes use of the entire training set when generating a question. The baseline oper- ates on questions modified with the placeholder as in the preceding section. Given a fact F as in- put, the baseline picks a candidate fact F c in the training set at uniformly random, where F c has the same relationship as F . Then the baseline consid- ers the questions corresponding to F c and as in the SP model, in the final step the placeholder token in the question is replaced by the subject string of the fact F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Procedure</head><p>All neural network models were implemented in <ref type="bibr">Theano (Theano Development Team, 2016)</ref>. To train the neural network models, we optimized the log-likelihood using the first-order gradient-based optimization algorithm Adam ( <ref type="bibr">Kingma and Ba, 2015)</ref>. To decide when to stop training we used early stopping with patience (Bengio, 2012) on the METEOR score obtained for the validation set.</p><p>In all experiments, we use the default split of the SimpleQuestions dataset into training, validation and test sets.</p><p>We trained TransE embeddings with embedding dimensionality 200 for each subject, relationship and object. Based on preliminary experiments, for all neural network models we fixed the learning rate to 0.00025 and clipped parameter gradients with norms larger than 0.1. We further fixed the embedding dimensionality of words to be 200, and the hidden state of the decoder RNN to have di- mensionality 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>To investigate the performance of our models, we make use of both automatic evaluation metrics and human evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Automatic Evaluation Metrics</head><p>BLEU ( <ref type="bibr" target="#b22">Papineni et al., 2002</ref>) and ME- TEOR ( <ref type="bibr" target="#b2">Banerjee and Lavie, 2005</ref>) are two widely used evaluation metrics in statistical machine translation and automatic image-caption generation <ref type="bibr" target="#b10">(Chen et al., 2015</ref>). Similar to sta- tistical machine translation, where a phrase in the source language is mapped to a phrase in the target language, in this task a KB fact is mapped to a natural language question. Both tasks are highly constrained, e.g. the set of valid outputs is limited. This is true in particular for short phrases, such as one sentence questions. Furthermore, in both tasks, the majority of valid outputs are paraphrases of each other, which BLEU and METEOR have been designed to capture. We therefore believe that BLEU and METEOR constitute reasonable performance metrics for evaluating the generated questions.</p><p>Although we believe that METEOR and BLEU are reasonable evaluation metrics, they may have not recognize certain paraphrases, in particular paraphrases of entities. We therefore also make use of a sentence similarity metric, as proposed by <ref type="bibr" target="#b23">Rus and Lintean (2012)</ref>, which we will denote Embedding Greedy (Emb. Greedy). The metric makes use of a word similarity score, which in our experiments is the cosine similarity between two Word2Vec word embeddings ( <ref type="bibr" target="#b20">Mikolov et al., 2013)</ref>. <ref type="bibr">7</ref> The metric finds a (non-exclusive) align- ment between words in the two questions, which maximizes the similarity between aligned words, and computes the sentence similarity as the mean over the word similarities between aligned words.</p><p>The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Exam- ple questions produced by the model with mul- tiple placeholders are shown in <ref type="table">Table 4</ref>. The neural network models outperform the template- based baseline by a clear margin across all met- rics. The template-based baseline is already a rel- atively strong model, because it makes use of a separate template for each relationship. Qualita- tively the neural networks outperform the base- line model in cases where they are able to levage additional knowledge about the entities (see first, third and fifth example in <ref type="table">Table 4</ref>). On the other hand, for rare relationships the baseline model ap- pears to perform better, because it is able to pro- duce a reasonable question if only a single exam- ple with the same relationship exists in the train- ing set (see eighth example in <ref type="table">Table 4</ref>). Given enough training data this suggests that neural net- works are generally better at the question genera- tion task compared to hand-crafted template-based procedures, and therefore that they may be useful for generating question answering corpora. Fur- thermore, it appears that the best performing mod- els are the models where TransE are trained on the largest set of triples (TransE++). This set con- tains, apart from the supporting triples described in Section 4.3, triples involving entities which are highly connected to the entities found in the Sim- pleQuestions facts. In total, around 30 millions of facts, which have been used to generate the 30M Factoid Question-Answer Corpus. Lastly, it is not clear whether the model with a single placeholder or the model with multiple placeholders performs best. This motivates the following human study.   <ref type="table">Table 4</ref>: Test examples and corresponding questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Human Evaluation Study</head><p>We carry out pairwise preference experiments on Amazon Mechanical Turk. Initially, we considered carrying out separate experiments for measuring relevancy and fluency respectively, since this is common practice in ma- chine translation. However, the relevancy of a question is determined solely by a single factor, i.e. the relationship, since by construction the sub- ject is always in the question. Measuring rel- evancy is therefore not very useful in our task. To verify this we carried out an internal pairwise preference experiment with human subjects, who were repeatedly shown a fact and two questions and asked to select the most relevant question. We found that 93% of the questions generated by the MP Triples TransE++ model were either judged better or at least as good as the human gen- erated questions w.r.t. relevancy. The remaining 7% questions of the MP Triples TransE++ model questions were also judged relevant questions, al- though less so compared to the human generated questions. In the next experiment, we therefore measure the holistic quality of the questions.</p><p>We setup experiments comparing: Human- Baseline (human and baseline questions), Human- MP (human and MP Triples TransE++ ques- tions) and Baseline-MP (baseline and MP Triples TransE++ questions). We show human evaluators a fact along with two questions, one question from each model for the corresponding fact, and ask the them to choose the question which is most relevant to the fact and most natural. The human evaluator also has the option of not choosing either question. This is important if both questions are equally good or if neither of the questions make sense. At the beginning of each experiment, we show the hu- man evaluators two examples of statements and a corresponding pair of questions, where we briefly explain the form of the statements and how ques- tions relate to those statements. Following the in- troductory examples, we present the facts and cor-  responding pair of questions one by one. To avoid presentation bias, we randomly shuffle the order of the examples and the order in which questions are shown by each model. During each experi- ment, we also show four check facts and corre- sponding check questions at random, which any attentive human annotator should be able to an- swer easily. We discard responses of human eval- uators who fail any of these four checks.</p><p>The preference of each example is defined as the question which is preferred by the majority of the evaluators. Examples where neither of the two questions are preferred by the majority of the eval- uators, i.e. when there is an equal number of eval- uators who prefer each question, are assigned to a separate preference class called "comparable". <ref type="bibr">8</ref> The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. In total, 3, 810 preferences were recorded by 63 indepen- dent human evaluators. The questions produced by each model model pair were evaluated in 5 batches (HITs). Each human evaluated 44-75 ex- amples (facts and corresponding question pairs) in each batch and each example was evaluated by 3-5 evaluators. In agreement with the automatic evaluation metrics, the human evaluators strongly prefer either the human or the neural network model over the template-based baseline. Further- more, it appears that humans cannot distinguish between the human-generated questions and the neural network questions, on average showing a preference towards the later over the former ones. We hypothesize this is because our model penal- izes uncommon and unnatural ways to frame ques- tionsand sometimes, includes specific information about the target object that the humans do not (see last example in <ref type="table">Table 4</ref>). This confirms our earlier assertion, that the neural network questions can be used for building question answering systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose new neural network models for map- ping knowledge base facts into corresponding nat- ural language questions. The neural networks combine ideas from recent neural network ar- chitectures for statistical machine translation, as well as multi-relational knowledge base embed- dings for overcoming sparsity issues and place- holder techniques for handling rare words. The produced question and answer pairs are evalu- ated using automatic evaluation metrics, includ- ing BLEU, METEOR and sentence similarity, and are found to outperform a template-based base- line model. When evaluated by untrained human subjects, the question and answer pairs produced by our best performing neural network appears to be comparable in quality to real human-generated questions. Finally, we use our best performing neural network model to generate a corpus of 30M question and answer pairs, which we hope will en- able future researchers to improve their question answering systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Closest neighbors to Warner Bros. Entertainment</head><label>Closest</label><figDesc></figDesc><table>Manchester 
hindi language 

SQ 

Billy Gibbons 
Ricky Anane 
nepali indian 
Jenny Lewis 
Lee Dixon 
Naseeb 
Lies of Love 
Jerri Bryne 
Ghar Ek Mandir 
Swordfish 
Greg Wood 
standard chinese 

SQ + FB 

Paramount Pictures 
Oxford 
dutch language 
Sony Pictures Entertainment 
Sale 
italian language 
Electronic Arts 
Liverpool 
danish language 
CBS 
Guildford 
bengali language 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of differences in the local structure of the vector space embeddings when adding more 
FB facts 

3. Gender: similarly, sometimes annotators 
have included information about gender (e.g. 
Which male audio engineer. . . ?). This in-
formation is given by the relationship per-
son/gender. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test performance for all models w.r.t. BLEU, METEOR and Emb. Greedy performance met-
rics, where SP indicates models with a single placeholder and MP models with multiple placeholders. 
TransE++ indicates models where the TransE embeddings have been pretrained on a larger set of triples. 
The best performance on each metric is marked in bold font. 

Fact 
Human 
Baseline 
MP Triples TransE++ 
bayuvi dupki 
-contained by -
europe 

where is bayuvi dupki? 
what state is the city 
of bayuvi dupki located 
in? 

what continent is bayuvi 
dupki in? 

illinois 
-contains -
ludlow township 

what is in illinois? 
what is a tributary 
found in illinois? 
what is the name of a place 
within illinois? 

neo contra 
-publisher -
konami 

who published 
neo contra? 
which company pub-
lished the game neo 
contra? 

who is the publisher for the 
computer videogame neo 
contra? 
fumihiko maki 
-structures designed -
makuhari messe 

fumihiko maki de-
signed what structure? 
what park did fumihiko 
maki help design? 
what's a structure designed 
by fumihiko maki? 

cheryl hickey 
-profession -
actor 

what is cheryl hickey's 
profession? 
what is cheryl hickey? 
what is cheryl hickey's pro-
fession in the entertainment 
industry? 
cherry 
-drugs with this flavor -
tussin expectorant for adults 
100 syrup 

name a cherry flavored 
drug? 
what is a cherry fla-
vored drug? 
what's a drug that cherry 
shaped like? 

pop music 
-artists -
nikki flores 

what artist is known for 
pop music? 
An example of pop music is 
what artist? 
who's an american 
singer that plays pop 
music? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Pairwise human evaluation preferences computed across evaluators with 95% confidence inter- vals. The preferred model in each experiment is marked in bold font. An asterisk next to the</head><label>5</label><figDesc></figDesc><table>preferred 
</table></figure>

			<note place="foot" n="1"> Freebase is now a part of WikiData.</note>

			<note place="foot" n="2"> www.agarciaduran.org</note>

			<note place="foot" n="3"> www.mturk.com Questions Entities Relationships Words 108,442 131,684 1,837 ∼77k</note>

			<note place="foot" n="4"> It is not necessary for the object to be the only answer, but it is required to be one of the possible answers.</note>

			<note place="foot" n="5"> Extracted from one of the latest Freebase dumps (downloaded in mid-August 2015) https://developers. google.com/freebase/data</note>

			<note place="foot" n="6"> We use the tool difflib: https://docs.python. org/2/library/difflib.html.</note>

			<note place="foot" n="7"> We use the Word2Vec embeddings pretrained on the Google News Corpus: https://code.google.com/ p/word2vec/.</note>

			<note place="foot" n="8"> The probabilities for the &quot;comparable&quot; class in Table 5 can be computed in each row as 100 minus the third and fourth column in the table.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors acknowledge IBM Research, NSERC, Canada Research Chairs and CIFAR for fund-ing. The authors thank Yang Yu, Bing Xiang, Bowen Zhou and Gerald Tesauro for constructive feedback, and Antoine Bordes, Nicolas Usunier, Sumit Chopra and Jason Weston for providing the SimpleQuestions dataset. This research was enabled in part by support provided by Calcul Qubec (www.calculquebec.ca) and Com-pute Canada (www.computecanada.ca).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automation of question generation from sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of QG2010: The Third Workshop on Question Generation</title>
		<meeting>QG2010: The Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lavie2005] Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>METEOR: An automatic metric for mt evaluation with improved correlation with human judgments</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Jonathan Berant and Percy Liang</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bollacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases-European Conference, (ECML PKDD)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<title level="m">Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Largescale simple question answering with memory networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating questions automatically from informational text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Question Generation (AIED 2009)</title>
		<meeting>the 2nd Workshop on Question Generation (AIED 2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Learning phrase representations using RNN encoder-decoder for statistical machine translation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Question generation based on lexicosyntactic patterns learned from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Curto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="175" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating natural language from linked data: Unsupervised template extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klein2013] Daniel</forename><surname>Duma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Duma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="83" to="94" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Web question answering: Is more always better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">QUEST: A model of question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Graesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="733" to="745" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Greff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04069</idno>
		<title level="m">Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber. 2015. LSTM: A search space odyssey</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural language question generation using syntax and keywords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kalady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">questiongeneration. org. [Kingma and Ba2015] Diederik Kingma and Jimmy Ba</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Building large knowledge-based systems; representation and inference in the Cyc project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lenat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guha</surname></persName>
		</author>
		<idno>Lopez et al.2011</idno>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="155" />
			<pubPlace>Lopez, Victoria Uren, Marta Sabou, and Enrico Motta</pubPlace>
		</imprint>
	</monogr>
	<note>Is question answering fit for the semantic web? a survey. Semantic Web</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Question generation from paragraphs at upenn: Qgstec system description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mannem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of QG2010: The Third Workshop on Question Generation</title>
		<meeting>QG2010: The Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Question generation from concept maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="99" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comparison of greedy and optimal assessment of natural language student input using wordto-word similarity metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Lintean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, NAACL</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP, NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
	<note>Rus and Lintean2012</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The first question generation shared task evaluation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference</title>
		<meeting>the 6th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tice</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Overview of the trec-9 question answering track</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krötzsch2014] Denny</forename><surname>Vrandeči´vrandeči´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vrandeči´vrandeči´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Question generation with minimal recursion semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang2010] Xuchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of QG2010: The Third Workshop on Question Generation</title>
		<meeting>QG2010: The Third Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantics-based question generation and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
