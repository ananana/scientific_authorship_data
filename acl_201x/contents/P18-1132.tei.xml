<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename><forename type="middle">♠</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>UK</roleName><surname>London</surname></persName>
						</author>
						<title level="a" type="main">LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1426" to="1436"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1426</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax-sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependencies-provided they have enough capacity. We then explore whether models that have access to explicit syntactic information learn agreement more effectively, and how the way in which this structural information is incorporated into the model impacts performance. We find that the mere presence of syntactic information does not improve accuracy , but when model architecture is determined by syntax, number agreement is improved. Further, we find that the choice of how syntactic structure is built affects how well number agreement is learned: top-down construction outperforms left-corner and bottom-up variants in capturing long-distance structural dependencies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNNs) are remarkably effective models of sequential data. Recent years have witnessed the widespread adoption of recur- rent architectures such as LSTMs <ref type="bibr" target="#b18">(Hochreiter and Schmidhuber, 1997</ref>) in various NLP tasks, with state of the art results in language modeling ( ) and conditional generation tasks like machine translation ( <ref type="bibr" target="#b6">Bahdanau et al., 2015</ref>) and text summarization ( <ref type="bibr" target="#b36">See et al., 2017)</ref>.</p><p>Here we revisit the question asked by <ref type="bibr" target="#b25">Linzen et al. (2016)</ref>: as RNNs model word sequences without explicit notions of hierarchical structure, <ref type="figure">Figure 1</ref>: An example of the number agreement task with two attractors and a subject-verb dis- tance of five.</p><p>to what extent are these models able to learn non-local syntactic dependencies in natural lan- guage? Identifying number agreement between subjects and verbs-especially in the presence of attractors-can be understood as a cognitively- motivated probe that seeks to distinguish hierar- chical theories from sequential ones, as models that rely on sequential cues like the most recent noun would favor the incorrect verb form. We provide an example of this task in <ref type="figure">Fig. 1</ref>, where the plural form of the verb have agrees with the distant subject parts, rather than the adjacent at- tractors (underlined) of the singular form.</p><p>Contrary to the findings of <ref type="bibr" target="#b25">Linzen et al. (2016)</ref>, our experiments suggest that sequential LSTMs are able to capture structural dependencies to a large extent, even for cases with multiple attrac- tors ( §2). Our finding suggests that network capac- ity plays a crucial role in capturing structural de- pendencies with multiple attractors. Nevertheless, we find that a strong character LSTM language model-which lacks explicit word representation and has to capture much longer sequential depen- dencies in order to learn non-local structural de- pendencies effectively-performs much worse in the number agreement task.</p><p>Given the strong performance of word-based LSTM language models, are there are any sub- stantial benefits, in terms of number agreement ac- curacy, to explicitly modeling hierarchical struc- tures as an inductive bias? We discover that a certain class of LSTM language models that ex- plicitly models syntactic structures, the recurrent neural network grammars <ref type="bibr">(Dyer et al., 2016, RNNGs)</ref>, considerably outperforms sequential LSTM language models for cases with multiple attrac- tors ( §3). We present experiments affirming that this gain is due to an explicit composition op- erator rather than the presence of predicted syn- tactic annotations. Rather surprisingly, syntactic LSTM language models without explicit compo- sition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures <ref type="bibr" target="#b10">(Choe and Charniak, 2016)</ref>.</p><p>Having established the importance of model- ing structures, we explore the hypothesis that how we build the structure affects the model's abil- ity to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model archi- tecture that enable left-corner <ref type="bibr" target="#b16">(Henderson, 2003</ref><ref type="bibr" target="#b17">(Henderson, , 2004</ref>) and bottom-up <ref type="bibr" target="#b9">(Chelba and Jelinek, 2000;</ref><ref type="bibr" target="#b12">Emami and Jelinek, 2005</ref>) generation orders ( §4).</p><p>Extensive prior work has characterized top- down, left-corner, and bottom-up parsing strate- gies in terms of cognitive plausibility <ref type="bibr" target="#b32">(Pulman, 1986;</ref><ref type="bibr" target="#b5">Abney and Johnson, 1991;</ref><ref type="bibr" target="#b33">Resnik, 1992)</ref> and neurophysiological evidence in human sen- tence processing <ref type="bibr" target="#b30">(Nelson et al., 2017</ref>). Here we move away from the realm of parsing and eval- uate the three strategies as models of generation instead, and address the following empirical ques- tion: which generation order is most appropri- ately biased to model structural dependencies in English, as indicated by number agreement accu- racy? Our key finding is that the top-down gener- ation outperforms left-corner and bottom-up vari- ants for difficult cases with multiple attractors.</p><p>In theory, the three traversal strategies approxi- mate the same chain rule that decompose the joint probability of words and phrase-structure trees, denoted as p(x, y), differently and as such will impose different biases on the learner. In §4.3, we show that the three variants achieve similar per- plexities on a held-out validation set. As we ob- serve different patterns in number agreement, this demonstrates that while perplexity can be a use- ful diagnostic tool, it may not be sensitive enough for comparing models in terms of how well they capture grammatical intuitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Number Agreement with LSTM Language Models</head><p>We revisit the number agreement task with LSTMs trained on language modeling objectives, as pro- posed by <ref type="bibr" target="#b25">Linzen et al. (2016)</ref>. Experimental Settings. We use the same parsed Wikipedia corpus, verb inflectors, prepro- cessing steps, and dataset split as <ref type="bibr" target="#b25">Linzen et al. (2016)</ref>. 1 Word types beyond the most frequent 10,000 are converted to their respective POS tags. We summarize the corpus statistics of the dataset, along with the test set distribution of the num- ber of attractors, in <ref type="table" target="#tab_1">Table 1</ref>. Similar to <ref type="bibr" target="#b25">Linzen et al. (2016)</ref>, we only include test cases where all intervening nouns are of the opposite number forms than the subject noun. All models are im- plemented using the DyNet library ( <ref type="bibr" target="#b23">Neubig et al., 2017)</ref>. </p><formula xml:id="formula_0"># Attractors # Instances % Instances n = 0 1,146,330</formula><p>94.7% n = 1 52,599 4.3% n = 2 9,380 0.77% n = 3 2,051 0.17% n = 4 561 0.05% n = 5 159 0.01% Training was done using a language modeling objective that predicts the next word given the pre- fix; at test time we compute agreement error rates by comparing the probability of the correct verb form with the incorrect one. We report perfor- mance of a few different LSTM hidden layer con- figurations, while other hyper-parameters are se- lected based on a grid search. <ref type="bibr">2</ref> Following Linzen n=0 n=1 n=2 n=3 n=4 Random 50.0 50.0 50.0 50.0 50.0 Majority 32.0 32.0 32.0 32.0 32.0 LSTM, H=50 † 6.8 32.6 ≈50 ≈65 ≈70   <ref type="bibr">4</ref> Hyper-parameter tun- ing is based on validation set perplexity. Discussion. <ref type="table" target="#tab_3">Table 2</ref> indicates that, given enough capacity, LSTM language models without explicit syntactic supervision are able to perform well in number agreement. For cases with mul- tiple attractors, we observe that the LSTM lan- guage model with 50 hidden units trails behind its larger counterparts by a substantial margin de- spite comparable performance for zero attractor cases, suggesting that network capacity plays an especially important role in propagating relevant structural information across a large number of steps. <ref type="bibr">5</ref> Our experiment independently derives the same finding as the recent work of <ref type="bibr" target="#b14">Gulordava et al. (2018)</ref>, who also find that LSTMs trained with lan- guage modeling objectives are able to learn num- ber agreement well; here we additionally identify model capacity as one of the reasons for the dis- crepancy with the <ref type="bibr" target="#b25">Linzen et al. (2016)</ref> results.</p><p>While the pretrained large-scale language model of <ref type="bibr" target="#b22">Jozefowicz et al. (2016)</ref> has certain ad- vantages in terms of model capacity, more train- ing data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set.</p><p>Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infre- quent instances that require deeper language un- derstanding abilities ( <ref type="bibr" target="#b34">Rimell et al., 2009;</ref><ref type="bibr" target="#b20">Jia and Liang, 2017)</ref>. In the vast majority of cases, struc- tural dependencies between subjects and verbs highly overlap with sequential dependencies (Ta- ble 1). Nevertheless, the fact that number agree- ment accuracy gets worse as the number of attrac- tors increases is consistent with a sequential re- cency bias in LSTMs: under this conjecture, iden- tifying the correct structural dependency becomes harder when there are more adjacent nouns of dif- ferent number forms than the true subject.</p><p>If the sequential recency conjecture is correct, then LSTMs would perform worse when the struc- tural dependency is more distant in the sequences, compared to cases where the structural depen- dency is more adjacent. We empirically test this conjecture by running a strong character-based LSTM language model of  that achieved state of the art results on EnWiki8 from the Hutter Prize dataset <ref type="bibr" target="#b19">(Hutter, 2012)</ref>, with 1,800 hidden units and 10 million parameters. The char- acter LSTM is trained, validated, and tested 6 on the same split of the <ref type="bibr" target="#b25">Linzen et al. (2016)</ref> number agreement dataset.</p><p>A priori, we expect that number agreement is harder for character LSTMs for two reasons. First, character LSTMs lack explicit word representa- forms much better for cases with multiple attractors.tions, thus succeeding in this task requires iden- tifying structural dependencies between two se- quences of character tokens, while word-based LSTMs only need to resolve dependencies be- tween word tokens. Second, by nature of model- ing characters, non-local structural dependencies are sequentially further apart than in the word- based language model. On the other hand, char- acter LSTMs have the ability to exploit and share informative morphological cues, such as the fact that plural nouns in English tend to end with 's'.</p><p>As demonstrated on the last row of <ref type="table" target="#tab_3">Table 2</ref>, we find that the character LSTM language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts. This finding is consistent with that of Sennrich (2017), who find that character-level decoders in neural machine translation perform worse than subword models in capturing mor- phosyntactic agreement. To some extent, our finding demonstrates the limitations that character LSTMs face in learning structure from language modeling objectives, despite earlier evidence that character LSTM language models are able to im- plicitly acquire a lexicon <ref type="bibr" target="#b24">(Le Godais et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Number Agreement with RNNGs</head><p>Given the strong performance of sequential LSTMs in number agreement, is there any further benefit to explicitly modeling hierarchical struc- tures? We focus on recurrent neural network grammars ( <ref type="bibr">Dyer et al., 2016, RNNGs)</ref>, which jointly model the probability of phrase-structure trees and strings, p(x, y), through structure- building actions and explicit compositions for rep- resenting completed constituents.</p><p>Our choice of RNNGs is motivated by the find- ings of <ref type="bibr" target="#b23">Kuncoro et al. (2017)</ref>, who find evidence for syntactic headedness in RNNG phrasal repre- sentations. Intuitively, the ability to learn heads is beneficial for this task, as the representation for the noun phrase "The flowers in the vase" would be similar to the syntactic head flowers rather than vase. In some sense, the composition operator can be understood as injecting a structural recency bias into the model design, as subjects and verbs that are sequentially apart are encouraged to be close together in the RNNGs' representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Neural Network Grammars</head><p>RNNGs ( <ref type="bibr" target="#b11">Dyer et al., 2016</ref>) are language models that estimate the joint probability of string termi- nals and phrase-structure tree nonterminals. Here we use stack-only RNNGs that achieve better per- plexity and parsing performance ( <ref type="bibr" target="#b23">Kuncoro et al., 2017)</ref>. Given the current stack configuration, the objective function of RNNGs is to predict the correct structure-building operation according to a top-down, left-to-right traversal of the phrase- structure tree; a partial traversal for the input sen- tence "The flowers in the vase are blooming" is illustrated in <ref type="figure" target="#fig_6">Fig. 3(a)</ref>. <ref type="bibr" target="#b1">7</ref> The structural inductive bias of RNNGs derives from an explicit composition operator that rep- resents completed constituents; for instance, the constituent (NP The flowers) is represented by a single composite element on the stack, rather than as four separate symbols. During each REDUCE action, the topmost stack elements that belong to the new constituent are popped from the stack and then composed by the composition function; the composed symbol is then pushed back into the stack. The model is trained in an end-to-end man- ner by minimizing the cross-entropy loss relative to a sample of gold trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>Here we summarize the experimental settings of running RNNGs on the number agreement dataset and discuss the empirical findings.</p><p>Experimental settings. We obtain phrase- structure trees for the <ref type="bibr" target="#b25">Linzen et al. (2016)</ref> dataset using a publicly available discriminative model 8 trained on the Penn Treebank ( <ref type="bibr" target="#b27">Marcus et al., 1993)</ref>. At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences. <ref type="bibr" target="#b3">9</ref> At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols. An example of the stack contents (i.e. the prefix) when predicting the verb is provided in <ref type="figure" target="#fig_6">Fig. 3(a)</ref>. We similarly run a grid search over the same hyper-parameter range as the sequential LSTM and compare the results with the strongest sequential LSTM baseline from §2. Discussion. <ref type="figure" target="#fig_0">Fig. 2</ref> shows that RNNGs (right- most) achieve much better number agreement ac- curacy compared to LSTM language models (left- most) for difficult cases with four and five at- tractors, with around 30% error rate reductions, along with a 13% error rate reduction (from 9% to 7.8%) for three attractors. We attribute the slightly worse performance of RNNGs on cases with zero and one attractor to the presence of inter- vening structure-building actions that separate the subject and the verb, such as a REDUCE (step 6 in <ref type="figure" target="#fig_6">Fig. 3</ref>(a)) action to complete the noun phrase and at least one action to predict a verb phrase (step 15 in <ref type="figure" target="#fig_6">Fig. 3(a)</ref>) before the verb itself is introduced, while LSTM language models benefit from shorter dependencies for zero and one attractor cases.</p><p>The performance gain of RNNGs might arise from two potential causes. First, RNNGs have access to predicted syntactic annotations, while LSTM language models operate solely on word sequences. Second, RNNGs incorporate explicit compositions, which encourage hierarhical repre- sentations and potentially the discovery of syntac- tic (rather than sequential) dependencies.</p><p>Would LSTMs that have access to syntactic annotations, but without the explicit composition function, benefit from the same performance gain as RNNGs? To answer this question, we run se- quential LSTMs over the same phrase-structure trees <ref type="bibr" target="#b10">(Choe and Charniak, 2016)</ref>, similarly es- timating the joint probability of phrase-structure nonterminals and string terminals but without an explicit composition operator. Taking the example in <ref type="figure" target="#fig_6">Fig. 3(a)</ref>, the sequential syntactic LSTM would have fifteen 10 symbols on the LSTM when pre- dicting the verb, as opposed to three symbols in the case of RNNGs' stack LSTM. In theory, the sequential LSTM over the phrase-structure trees <ref type="bibr" target="#b10">(Choe and Charniak, 2016)</ref> may be able to incor- porate a similar, albeit implicit, composition pro- cess as RNNGs and consequently derive similarly syntactic heads, although there is no inductive bias that explicitly encourages such process. <ref type="figure" target="#fig_0">Fig. 2</ref> suggests that the sequential syntactic LSTMs (center) perform comparably with sequen- tial LSTMs without syntax for multiple attractor cases, and worse than RNNGs for nearly all at- tractors; the gap is highest for multiple attractors. This result showcases the importance of an ex- plicit composition operator and hierarchical repre- sentations in identifying structural dependencies, as indicated by number agreement accuracy. Our finding is consistent with the recent work of <ref type="bibr" target="#b40">Yogatama et al. (2018)</ref>, who find that introducing el- ements of hierarchical modeling through a stack- structured memory is beneficial for number agree- ment, outperforming LSTM language models and attention-augmented variants by increasing mar- gins as the number of attractor grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Further Analysis</head><p>In order to better interpret the results, we con- duct further analysis into the perplexities of each model, followed by a discussion on the effect of incrementality constraints on the RNNG when predicting number agreement.</p><p>Perplexity. To what extent does the success of RNNGs in the number agreement task with mul- tiple attractors correlate with better performance under the perplexity metric? We answer this ques- tion by using an importance sampling marginal- ization procedure <ref type="bibr" target="#b11">(Dyer et al., 2016)</ref> to obtain an estimate of p(x) under both RNNGs and the se- quential syntactic LSTM model. Following <ref type="bibr" target="#b11">Dyer et al. (2016)</ref>, for each sentence on the validation set we sample 100 candidate trees from a dis- criminative model 11 as our proposal distribution. As demonstrated in <ref type="table">Table 3</ref>, the LSTM language model has the lowest validation set perplexity de- spite substantially worse performance than RN- NGs in number agreement with multiple attrac- tors, suggesting that lower perplexity is not neces-sarily correlated with number agreement success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation ppl. LSTM LM</head><p>72.6 Seq. Syntactic LSTM 79.2 RNNGs 77.9 <ref type="table">Table 3</ref>: Validation set perplexity of LSTM lan- guage model, sequential syntactic LSTM, and RN- NGs.</p><p>Incrementality constraints. As the syntactic prefix was derived from a discriminative model that has access to unprocessed words, one poten- tial concern is that this prefix might violate the incrementality constraints and benefit the RNNG over the LSTM language model. To address this concern, we remark that the empirical evidence from <ref type="figure" target="#fig_0">Fig. 2</ref> and <ref type="table">Table 3</ref> indicates that the LSTM language model without syntactic annotation out- performs the sequential LSTM with syntactic an- notation in terms of both perplexity and number agreement throughout nearly all attractor settings, suggesting that the predicted syntactic prefix does not give any unfair advantage to the syntactic mod- els.</p><p>Furthermore, we run an experiment where the syntactic prefix is instead derived from an in- cremental beam search procedure of <ref type="bibr" target="#b13">Fried et al. (2017)</ref>. <ref type="bibr" target="#b4">12</ref> To this end, we take the highest scoring beam entry at the time that the verb is generated to be the syntactic prefix; this procedure is applied to both the correct and incorrect verb forms. <ref type="bibr">13</ref> We then similarly compare the probabilities of the cor- rect and incorrect verb form given each respective syntactic prefix to obtain number agreement accu- racy. Our finding suggests that using the fully in- cremental tree prefix leads to even better RNNG number agreement performance for four and five attractors, achieving 7.1% and 8.2% error rates, respectively, compared to 9.4% and 12% for the RNNG error rates in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Top-Down, Left-Corner, and Bottom-Up Traversals</head><p>In this section, we propose two new variants of RNNGs that construct trees using a different con- <ref type="bibr" target="#b4">12</ref> As the beam search procedure is time-consuming, we randomly sample 500 cases for each attractor and compute the number agreement accuracy on these samples. <ref type="bibr">13</ref> Consequently, the correct and incorrect forms of the sen- tence might have different partial trees, as the highest scoring beam entries may be different for each alternative. struction order than the top-down, left-to-right or- der used above. These are a bottom-up construc- tion order ( §4.1) and a left-corner construction order ( §4.2), analogous to the well-known pars- ing strategies (e.g. Hale, 2014, chapter 3). They differ from these classic strategies insofar as they do not announce the phrase-structural content of an entire branch at the same time, adopting in- stead a node-by-node enumeration reminescent of Markov Grammars <ref type="bibr" target="#b7">(Charniak, 1997</ref>). This step- by-step arrangement extends to the derived string as well; since all variants generate words from left to right, the models can be compared using number agreement as a diagnostic. <ref type="bibr">14</ref> Here we state our hypothesis on why the build order matters. The three generation strategies rep- resent different chain rule decompositions of the joint probability of strings and phrase-structure trees, thereby imposing different biases on the learner. Earlier work in parsing has character- ized the plausibility of top-down, left-corner, and bottom-up strategies as viable candidates of hu- man sentence processing, especially in terms of memory constraints and human difficulties with center embedding constructions <ref type="bibr" target="#b21">(Johnson-Laird, 1983;</ref><ref type="bibr" target="#b32">Pulman, 1986;</ref><ref type="bibr" target="#b5">Abney and Johnson, 1991;</ref><ref type="bibr">Resnik, 1992, inter alia)</ref>, along with neurophys- iological evidence in human sentence processing <ref type="bibr" target="#b30">(Nelson et al., 2017</ref>). Here we cast the three strate- gies as models of language generation <ref type="bibr" target="#b26">(Manning and Carpenter, 1997)</ref>, and focus on the empirical question: which generation order has the most ap- propriate bias in modeling non-local structural de- pendencies in English?</p><p>These alternative orders organize the learn- ing problem so as to yield intermediate states in generation that condition on different aspects of the grammatical structure. In number agreement, this amounts to making an agreement controller, such as the word flowers in <ref type="figure" target="#fig_6">Fig. 3</ref>, more or less salient. If it is more salient, the model should be better-able to inflect the main verb in agreement with this controller, without getting distracted by the attractors. The three proposed build orders are compared in <ref type="figure" target="#fig_6">Fig. 3</ref>, showing the respective config- urations (i.e. the prefix) when generating the main verb in a sentence with a single attractor. <ref type="bibr">15</ref> In ad-dition, we show concrete action sequences for a simpler sentence in each section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bottom-Up Traversal</head><p>In bottom-up traversals, phrases are recursively constructed and labeled with the nonterminal type once all their daughters have been built, as illus- trated in <ref type="figure">Fig. 4</ref>. Bottom-up traversals benefit from shorter stack depths compared to top-down due to the lack of incomplete nonterminals. As the com- mitment to label the nonterminal type of a phrase is delayed until its constituents are complete, this means that the generation of a child node cannot condition on the label of its parent node.</p><p>In n-ary branching trees, bottom-up completion of constituents requires a procedure for determin- ing how many of the most recent elements on the stack should be daughters of the node that is be- ing constructed. <ref type="bibr">16</ref> Conceptually, rather than hav- ing a single REDUCE operation as we have before, we have a complex REDUCE(X, n) operation that must determine the type of the constituent (i.e., X) as well as the number of daughters (i.e., n).</p><p>In step 5 of <ref type="figure">Fig. 4</ref>, the newly formed NP con- stituent only covers the terminal worms, and nei- ther the unattached terminal eats nor the con- stituent (NP The fox) is part of the new noun phrase. We implement this extent decision us- ing a stick-breaking construction-using the stack LSTM encoding, a single-layer feedforward net- work, and a logistic output layer-which decides whether the top element on the stack should be the leftmost child of the new constituent (i.e. whether or not the new constituent is complete after pop- ping the current topmost stack element), as illus- trated in <ref type="figure">Fig. 5</ref>. If not, the process is then repeated after the topmost stack element is popped. Once the extent of the new nonterminal has been de- cided, we parameterize the decision of the nonter- minal label type; in <ref type="figure">Fig. 5</ref> this is an NP. A second difference to top-down generation is that when a single constituent remains on the stack, the sen- tence is not necessarily complete (see step 3 of <ref type="figure">Fig. 4</ref> for examples where this happens). We thus introduce an explicit STOP action (step 8, <ref type="figure">Fig. 4</ref>), indicating the tree is complete, which is only as- signed non-zero probability when the stack has a during the history of the full generation process vary consid- erably in the invariances and the kinds of actions they predict. <ref type="bibr">16</ref> This mechanism is not necessary with strictly binary branching trees, since each new nonterminal always consists of the two children at the top of the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg. stack depth</head><p>Ppl.  <ref type="table" target="#tab_5">Table 4</ref>: Average stack depth and validation set perplexity for top-down (TD), left-corner (LC), and bottom-up (BU) RNNGs.</p><p>single complete constituent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Left-Corner Traversal</head><p>Left-corner traversals combine some aspects of top-down and bottom-up processing. As illus- trated in <ref type="figure">Fig. 6</ref>, this works by first generating the leftmost terminal of the tree, The (step 0), be- fore proceeding bottom-up to predict its parent NP (step 1) and then top-down to predict the rest of its children (step 2). A REDUCE action similarly calls the composition operator once the phrase is com- plete (e.g. step 3). The complete constituent (NP The fox) is the leftmost child of its parent node, thus an NT SW(S) action is done next (step 4). The NT SW(X) action is similar to the NT(X) from the top-down generator, in that it introduces an open nonterminal node and must be matched later by a corresponding REDUCE operation, but, in addition, swaps the two topmost elements at the top of the stack. This is necessary because the parent nonterminal node is not built until af- ter its left-most child has been constructed. In step 1 of <ref type="figure">Fig. 6</ref>, with a single element The on the stack, the action NT SW(NP) adds the open nonterminal symbol NP to become the topmost stack element, but after applying the swap operator the stack now contains (NP | The (step 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>We optimize the hyper-parameters of each RNNG variant using grid searches based on validation set perplexity.  top (S (NP (NP The flowers) (PP in (NP the vase)))      <ref type="table">Table 5</ref>: Number agreement error rates for top- down (TD), left-corner (LC), and bottom-up (BU) RNNGs, broken down by the number of attractors. LM indicates the best sequential language model baseline ( §2). We report the mean, standard devia- tion, and minimum/maximum of 10 different ran- dom seeds of each model. action adds the open nonterminal symbol (X to the stack, followed by a deterministic swap operator that swaps the top two elements on the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(VP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U O y F 7 n q T Z Q e j W k T + z 8 q 4 h H 0 0 J / 4 = " &gt; A A A C Z H i c b V D f S x t B E N 4 7 t a a p t l H x q S C L o S W B E i 5 F s L 4 J v v g k V 0 x U y I W w t 5 l L F n d v j 9 0 5 b T j v n / T N 1 7 7 0 3 3 D z o x A T B w a + / e a b 2 Z k v z q S w G A Q v n r + x u f V h u / K x + m l n 9 / O X 2 t 7 + j d W 5 4 d D l W m p z F z M L U q T Q R Y E S 7 j I D T M U S b u P 7 i 2 n 9 9 g G M F T r t 4 C S D v m K j V C S C M 3 T U o P b 0 P U L 4 g 5 Y X q L O S R l F 1 T h h V N K 5 X 3 l c h n W Y E K h s X n T H Q R O p H N 7 t s 0 k b 4 n x d p u a R C p 3 p w 6 5 X N Z n N l 2 k 1 Y D m r 1 o B X M g q 6 D 9 g L U y S L C Q e 0 5 G m q e K 0 i R S 2 Z t r x 1 k 2 C + Y Q c E l l N U o t 5 A x f s 9 G 0 H M w Z Q p s v 5 i 5 V N J v j h n S R B u X K d I Z u 9 x R M G X t R M V O q R i O 7 W p t S r 5 X 6 + W Y / O q 7 w 7 M c I e X z j 5 J c U t R 0 a j k d C g M c 5 c Q B x o 1 w u 1 I + Z o Z x d O Z V n Q n t 1 Z P X Q f d n 6 6 w V / D 6 p n / 9 Y u F E h X 8 k x a Z A 2 O S X n 5 J K E p E s 4 + e t V v D 1 v 3 / v n 7 / o H / u F c 6 n u L n g P y J v y j V y k 1 t Z o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U O y F 7 n q T Z Q e j W k T + z 8 q 4 h H 0 0 J / 4 = " &gt; A A A C Z H i c b V D f S x t B E N 4 7 t a a p t l H x q S C L o S W B E i 5 F s L 4 J v v g k V 0 x U y I W w t 5 l L F n d v j 9 0 5 b T j v n / T N 1 7 7 0 3 3 D z o x A T B w a + / e a b 2 Z k v z q S w G A Q v n r + x u f V h u / K x + m l n 9 / O X 2 t 7 + j d W 5 4 d D l W m p z F z M L U q T Q R Y E S 7 j I D T M U S b u P 7 i 2 n 9 9 g G M F T r t 4 C S D v m K j V C S C M 3 T U o P b 0 P U L 4 g 5 Y X q L O S R l F 1 T h h V N K 5 X 3 l c h n W Y E K h s X n T H Q R O p H N 7 t s 0 k b 4 n x d p u a R C p 3 p w 6 5 X N Z n N l 2 k 1 Y D m r 1 o B X M g q 6 D 9 g L U y S L C Q e 0 5 G m q e K 0 i R S 2 Z t r x 1 k 2 C + Y Q c E l l N U o t 5 A x f s 9 G 0 H M w Z Q p s v 5 i 5 V N J v j h n S R B u X K d I Z u 9 x R M G X t R M V O q R i O 7 W p t S r 5 X 6 + W Y / O q 7 w 7 M c I e X z j 5 J c U t R 0 a j k d C g M c 5 c Q B x o 1 w u 1 I + Z o Z x d O Z V n Q n t 1 Z P X Q f d n 6 6 w V / D 6 p n / 9 Y u F E h X 8 k x a Z A 2 O S X n 5 J K E p E s 4 + e t V v D 1 v 3 / v n 7 / o H / u F c 6 n u L n g P y J v y j V y k 1 t Z o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U O y F 7 n q T Z Q e j W k T + z 8 q 4 h H 0 0 J / 4 = " &gt; A A A C Z H i c b V D f S x t B E N 4 7 t a a p t l H x q S C L o S W B E i 5 F s L 4 J v v g k V 0 x U y I W w t 5 l L F n d v j 9 0 5 b T j v n / T N 1 7 7 0 3 3 D z o x A T B w a + / e a b 2 Z k v z q S w G A Q v n r + x u f V h u / K x + m l n 9 / O X 2 t 7 + j d W 5 4 d D l W m p z F z M L U q T Q R Y E S 7 j I D T M U S b u P 7 i 2 n 9 9 g G M F T r t 4 C S D v m K j V C S C M 3 T U o P b 0 P U L 4 g 5 Y X q L O S R l F 1 T h h V N K 5 X 3 l c h n W Y E K h s X n T H Q R O p H N 7 t s 0 k b 4 n x d p u a R C p 3 p w 6 5 X N Z n N l 2 k 1 Y D m r 1 o B X M g q 6 D 9 g L U y S L C Q e 0 5 G m q e K 0 i R S 2 Z t r x 1 k 2 C + Y Q c E l l N U o t 5 A x f s 9 G 0 H M w Z Q p s v 5 i 5 V N J v j h n S R B u X K d I Z u 9 x R M G X t R M V O q R i O 7 W p t S r 5 X 6 + W Y / O q 7 w 7 M c I e X z j 5 J c U t R 0 a j k d C g M c 5 c Q B x o 1 w u 1 I + Z o Z x d O Z V n Q n t 1 Z P X Q f d n 6 6 w V / D 6 p n / 9 Y u F E h X 8 k x a Z A 2 O S X n 5 J K E p E s 4 + e t V v D 1 v 3 / v n 7 / o H /</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U H g a A u q w B F H O + w 3 + P M Z 4 h g E g 2 p g = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B a h g p R E B P V W 8 O K x g r G F J p T N d t M u 3 d 2 E 3 Y l Y Q v + G F w 8 q X v 0 1 3 v w 3 b t s c t P X B w O O 9 G W b m R a n g B l z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D B 5 N k m j K f J i L R n Y g Y J r h i P n A Q r J N q R m Q k W D s a 3 U z 9 9 i P T h i f q H s Y p C y U Z K B 5 z S s B K Q Q D s C b T M 6 + R 0 0 q v W 3 I Y 7 A 1 4 m X k F q q E C r V / 0 K + g n N J F N A B T G m 6 7 k p h D n R w K l g k 0 q Q G Z Y S O i I D 1 r V U E c l M m M 9 u n u A T q / R x n G h b C v B M / T 2 R E 2 n M W E a 2 U x I Y m k V v K v 7 n d T O I r 8 K c q z Q D p u h 8 U Z w J D A m e B o D 7 X D M K Y m w J o Z r b W z E d E k 0 o 2 J g q N g R v 8 e V l 4 p 8 3 r h v u 3 U W t e V a k U U Z H 6 B j V k Y c u U R P d o h b y E U U p e k a v 6 M 3 J n B f n 3 f m Y t 5 a c Y u Y Q / Y H z + Q N W / Z F Z &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U H g a A u q w B F H O + w 3 + P M Z 4 h g E g 2 p g = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B a h g p R E B P V W 8 O K x g r G F J p T N d t M u 3 d 2 E 3 Y l Y Q v + G F w 8 q X v 0 1 3 v w 3 b t s c t P X B w O O 9 G W b m R a n g B l z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D B 5 N k m j K f J i L R n Y g Y J r h i P n A Q r J N q R m Q k W D s a 3 U z 9 9 i P T h i f q H s Y p C y U Z K B 5 z S s B K Q Q D s C b T M 6 + R 0 0 q v W 3 I Y 7 A 1 4 m X k F q q E C r V / 0 K + g n N J F N A B T G m 6 7 k p h D n R w K l g k 0 q Q G Z Y S O i I D 1 r V U E c l M m M 9 u n u A T q / R x n G h b C v B M / T 2 R E 2 n M W E a 2 U x I Y m k V v K v 7 n d T O I r 8 K c q z Q D p u h 8 U Z w J D A m e B o D 7 X D M K Y m w J o Z r b W z E d E k 0 o 2 J g q N g R v 8 e V l 4 p 8 3 r h v u 3 U W t e V a k U U Z H 6 B j V k Y c u U R P d o h b y E U U p e k a v 6 M 3 J n B f n 3 f m Y t 5 a c Y u Y Q / Y H z + Q N W / Z F Z &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U H g a A u q w B F H O + w 3 + P M Z 4 h g E g 2 p g = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B a h g p R E B P V W 8 O K x g r G F J p T N d t M u 3 d 2 E 3 Y l Y Q v + G F w 8 q X v 0 1 3 v w 3 b t s c t P X B w O O 9 G W b m R a n g B l z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D B 5 N k m j K f J i L R n Y g Y J r h i P n A Q r J N q R m Q k W D s a 3 U z 9 9 i P T h i f q H s Y p C y U Z K B 5 z S s B K Q Q D s C b T M 6 + R 0 0 q v W 3 I Y 7 A 1 4 m X k F q q E C r V / 0 K + g n N J F N A B T G m 6 7 k p h D n R w K l g k 0 q Q G Z Y S O i I D 1 r V U E c l M m M 9 u n u A T q / R x n G h b C v B M / T 2 R E 2 n M W E a 2 U x I Y m k V v K v 7 n d T O I r 8 K c q z Q D p u h 8 U Z w J D A m e B o D 7 X D M K Y m w J o Z r b W z E d E k 0 o 2 J g q N g R v 8 e V l 4 p 8 3 r h v u 3 U W t e V a k U U Z H 6 B j V k Y c u U R P d o h b y E U U p e k a v 6 M 3 J n B f n 3 f m Y t 5 a c Y u Y Q / Y H z + Q N W / Z F Z &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Stack contents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B p g v 0 W 9 y r x M b I L 8 a f j L i N O 9 a k m k = " &gt; A A A C R n i c l V A 7 S w N B E J 6 L r x h f U U u b x S B Y S L i I o H Y B G 8 u I x g R y M e z t b Z L F v U d 2 Z 8 V w 5 N / Z 2 N r 5 F 2 w s V G z d P B B N b B z Y 4 e O b + W Z 2 P j + R Q q P r P j u Z u f m F x a X s c m 5 l d W 1 9 I 7 + 5 d a 1 j o x i v s l j G q u 5 T z a W I e B U F S l 5 P F K e h L 3 n N v z 0 b 1 m t 3 X G k R R 1 f Y T 3 g z p J 1 I t A W j a K l W / s Z D f o 8 q T C 9 R G Y Z G 8 Y H X 6 x k a / C c P 0 / c Y y m 4 J i y P k E e p B K 1 9 w i + 4 o y C w o T U A B J l F p 5 Z + 8 I G Y m t G o m q d a N k p t g M 6 U K B Z N 8 k P O M 5 o l d Q T u 8 Y W F E Q 6 6 b 6 c i H A d m z T E D a s b I v Q j J i f y p S G m r d D 3 3 b G V L s 6 u n a k P y r 1 j D Y P m m m I k q M P Y u N F 7 W N J B i T o a k k E I o z l H 0 L K F P C / p W w L l W U o b U + Z 0 0 o T Z 8 8 C 6 q H x d O i e 3 F U K B 9 M 3 M j C D u z C P p T g G M p w D h W o A o M H e I E 3 e H c e n V f n w / k c t 2 a c i W Y b f k U G v g D Q s 7 Y h &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B p g v 0 W 9 y r x M b I L 8 a f j L i N O 9 a k m k = " &gt; A A A C R n i c l V A 7 S w N B E J 6 L r x h f U U u b x S B Y S L i I o H Y B G 8 u I x g R y M e z t b Z L F v U d 2 Z 8 V w 5 N / Z 2 N r 5 F 2 w s V G z d P B B N b B z Y 4 e O b + W Z 2 P j + R Q q P r P j u Z u f m F x a X s c m 5 l d W 1 9 I 7 + 5 d a 1 j o x i v s l j G q u 5 T z a W I e B U F S l 5 P F K e h L 3 n N v z 0 b 1 m t 3 X G k R R 1 f Y T 3 g z p J 1 I t A W j a K l W / s Z D f o 8 q T C 9 R G Y Z G 8 Y H X 6 x k a / C c P 0 / c Y y m 4 J i y P k E e p B K 1 9 w i + 4 o y C w o T U A B J l F p 5 Z + 8 I G Y m t G o m q d a N k p t g M 6 U K B Z N 8 k P O M 5 o l d Q T u 8 Y W F E Q 6 6 b 6 c i H A d m z T E D a s b I v Q j J i f y p S G m r d D 3 3 b G V L s 6 u n a k P y r 1 j D Y P m m m I k q M P Y u N F 7 W N J B i T o a k k E I o z l H 0 L K F P C / p W w L l W U o b U + Z 0 0 o T Z 8 8 C 6 q H x d O i e 3 F U K B 9 M 3 M j C D u z C P p T g G M p w D h W o A o M H e I E 3 e H c e n V f n w / k c t 2 a c i W Y b f k U G v g D Q s 7 Y h &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B p g v 0 W 9 y r x M b I L 8 a f j L i N O 9 a k m k = " &gt; A A A C R n i c l V A 7 S w N B E J 6 L r x h f U U u b x S B Y S L i I o H Y B G 8 u I x g R y M e z t b Z L F v U d 2 Z 8 V w 5 N / Z 2 N r 5 F 2 w s V G z d P B B N b B z Y 4 e O b + W Z 2 P j + R Q q P r P j u Z u f m F x a X s c m 5 l d W 1 9 I 7 + 5 d a 1 j o x i v s l j G q u 5 T z a W I e B U F S l 5 P F K e h L 3 n N v z 0 b 1 m t 3 X G k R R 1 f Y T 3 g z p J 1 I t A W j a K l W / s Z D f o 8 q T C 9 R G Y Z G 8 Y H X 6 x k a / C c P 0 / c Y y m 4 J i y P k E e p B K 1 9 w i + 4 o y C w o T U A B J l F p 5 Z + 8 I G Y m t G o m q d a N k p t g M 6 U K B Z N 8 k P O M 5 o l d Q T u 8 Y W F E Q 6 6 b 6 c i H A d m z T E D a s b I v Q j J i f y p S G m r d D 3 3 b G V L s 6 u n a k P y r 1 j D Y P m m m I k q M P Y u N F 7 W N J B i T o a k k E I o z l H 0 L K F P C / p W w L l W U o b U + Z 0 0 o T Z 8 8 C 6 q H x d O i e 3 F U K B 9 M 3 M j C D u z C P p T g G M p w D h W o A o M H e I E 3 e H c e n V f n w / k c t 2 a c i W Y b f k U G v g D Q s 7 Y h &lt; / l a t e x i t &gt;</head><p>Discussion. In <ref type="table">Table 5</ref>, we focus on empiri- cal results for cases where the structural depen- dencies matter the most, corresponding to cases with two, three, and four attractors. All three RNNG variants outperform the sequential LSTM language model baseline for these cases. Nev- ertheless, the top-down variant outperforms both left-corner and bottom-up strategies for difficult cases with three or more attractors, suggesting that the top-down strategy is most appropriately biased to model difficult number agreement dependencies in English. We run an approximate randomization test by stratifying the output and permuting within each stratum <ref type="bibr" target="#b39">(Yeh, 2000</ref>) and find that, for four attractors, the performance difference between the top-down RNNG and the other variants is statisti- cally significant at p &lt; 0.05. The success of the top-down traversal in the do- main of number-agreement prediction is consis- tent with a classical view in parsing that argues top-down parsing is the most human-like pars- ing strategy since it is the most anticipatory. Only anticipatory representations, it is said, could ex- plain the rapid, incremental processing that hu- mans seem to exhibit <ref type="bibr" target="#b28">(Marslen-Wilson, 1973;</ref><ref type="bibr" target="#b38">Tanenhaus et al., 1995)</ref>; this line of thinking sim- ilarly motivates Charniak (2010), among others. While most work in this domain has been con- cerned with the parsing problem, our findings sug- gest that anticipatory mechanisms are also bene- ficial in capturing structural dependencies in lan- guage modeling. We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of <ref type="bibr" target="#b35">Schuler et al. (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Given enough capacity, LSTMs trained on lan- guage modeling objectives are able to learn syntax-sensitive dependencies, as evidenced by accurate number agreement accuracy with multi- ple attractors. Despite this strong performance, we discover explicit modeling of structure does improve the model's ability to discover non-local structural dependencies when determining the dis- tribution over subsequent word generation. Recur- rent neural network grammars (RNNGs), which jointly model phrase-structure trees and strings and employ an explicit composition operator, sub- stantially outperform LSTM language models and syntactic language models without explicit com- positions; this highlights the importance of a hier- archical inductive bias in capturing structural de- pendencies. We explore the possibility that how the structure is built affects number agreement per- formance. Through novel extensions to RNNGs that enable the use of left-corner and bottom-up generation strategies, we discover that this is in- deed the case: the three RNNG variants have dif- ferent generalization properties for number agree- ment, with the top-down traversal strategy per- forming best for cases with multiple attractors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number agreement error rates for sequential LSTM language models (left), sequential syntactic LSTM language models (Choe and Charniak, 2016, center), and RNNGs (right).</figDesc><graphic url="image-2.png" coords="5,80.28,103.67,201.71,120.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>NP (NP The flowers) (PP in (NP the vase)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>NP (NP The flowers) (PP in (NP the vase)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The (a) top-down, (b) bottom-up, and (c) left-corner build order variants showing in black the structure that exists as well as the generator's stack contents when the word are is generated during the derivation of the sentence The flowers in the vase are blooming. Structure in grey indicates material that will be generated subsequent to this. Circled numbers indicate the time when the corresponding structure/word is constructed. In (a) and (c), nonterminals are generated by a matched pair of NT and REDUCE operations, while in (b) they are introduced by a single complex REDUCE operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Architecture to determine type and span of new constituents during bottom-up generation.</figDesc><graphic url="image-3.png" coords="9,96.92,62.81,168.43,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Corpus statistics of the Linzen et al. 
(2016) number agreement dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Number agreement error rates for vari-
ous LSTM language models, broken down by the 
number of attractors. The top two rows represent 
the random and majority class baselines, while the 
next row (  † ) is the reported result from Linzen 
et al. (2016) for an LSTM language model with 
50 hidden units (some entries, denoted by ≈, are 
approximately derived from a chart, since Linzen 
et al. (2016) did not provide a full table of results). 
We report results of our LSTM implementations of 
various hidden layer sizes, along with our re-run of 
the Jozefowicz et al. (2016) language model, in the 
next five rows. We lastly report the performance of 
a state of the art character LSTM baseline with a 
large model capacity (Melis et al., 2018). 

et al. (2016), we include the results of our repli-
cation 3 of the large-scale language model of Joze-
fowicz et al. (2016) that was trained on the One 
Billion Word Benchmark. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 summarizes</head><label>4</label><figDesc>average stack depths and perplexities 17 on the Linzen et al. (2016) validation set. We evaluate each of the vari- ants in terms of number agreement accuracy as an evidence of its suitability to model structural de- pendencies in English, presented in Table 5. To account for randomness in training, we report the error rate summary statistics of ten different runs.</figDesc><table></table></figure>

			<note place="foot" n="1"> The dataset and scripts are obtained from https:// github.com/TalLinzen/rnn_agreement. 2 Based on the grid search results, we used the following hyper-parameters that work well across different hidden layer sizes: 1-layer LSTM, SGD optimizers with an initial learning rate of 0.2, a learning rate decay of 0.10 after 10 epochs, LSTM dropout rates of 0.2, an input embedding dimension of 50, and a batch size of 10 sentences. Our use of singlelayer LSTMs and 50-dimensional word embedding (learned from scratch) as one of the baselines is consistent with the experimental settings of Linzen et al. (2016).</note>

			<note place="foot" n="3"> When evaluating the large-scale language model, the primary difference is that we do not map infrequent word types to their POS tags and that we subsample to obtain 500 test instances of each number of attractor due to computation cost; both preprocessing were also done by Linzen et al. (2016). 4 The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/ tree/master/research/lm_1b. 5 This trend is also observed by comparing results with H=150 and H=250. While both models achieve near-identical performance for zero attractor, the model with H=250 per</note>

			<note place="foot" n="6"> For testing, we similarly evaluate number agreement accuracy by comparing the probability of the correct and incorrect verb form given the prefix, as represented by the respective character sequences.</note>

			<note place="foot" n="7"> For a complete example of action sequences, we refer the reader to the example provided by Dyer et al. (2016). 8 https://github.com/clab/rnng 9 Earlier work on RNNGs (Dyer et al., 2016; Kuncoro et al., 2017) train the model on gold phrase-structure trees on the Penn Treebank, while here we train the RNNG on the number agreement dataset based on predicted trees from another parser.</note>

			<note place="foot" n="10"> In the model of Choe and Charniak (2016), each nonterminal, terminal, and closed parenthesis symbol is represented as an element on the LSTM sequence. 11 https://github.com/clab/rnng</note>

			<note place="foot" n="14"> Only the order in which these models build the nonterminal symbols is different, while the terminal symbols are still generated in a left-to-right manner in all variants. 15 Although the stack configuration at the time of verb generation varies only slightly, the configurations encountered</note>

			<note place="foot" n="17"> Here we measure perplexity over p(x, y), where y is the presumptive gold tree on the Linzen et al. (2016) dataset. Dyer et al. (2016) instead used an importance sampling procedure to marginalize and obtain an estimate of p(x).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Tal Linzen for his help in data preparation and answering various ques-tions. We also thank Laura Rimell, Nando de Fre-itas, and the three anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">NP The fox) | eats NT SW(VP)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(s |</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">NP The fox) | (VP | eats GEN(worms)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">NP The fox) | (VP | eats | worms NT SW(NP)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fox) | (VP | eats | (NP | worms REDUCE 10 (S | (NP The fox) | (VP | eats | (NP worms) REDUCE 11 (S | (NP The fox) | (VP eats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S | (</forename><surname>Np</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>NP worms</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">S (NP The fox) (VP eats (NP worms)))</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory requirements and local ambiguities for parsing strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psycholinguistic Research</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Statistical techniques for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Top-down nearly-contextsensitive parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D10-1066" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="674" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neural syntactic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="195" to="227" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving neural parsing by disentangling model combination and reranking effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="161" to="166" />
		</imprint>
	</monogr>
	<note>Canada</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Colorless green recurrent networks dream hierarchically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automaton theories of human sentence comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John T Hale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CSLI Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inducing history representations for broad coverage statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The human knowledge compression contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson-Laird</surname></persName>
		</author>
		<title level="m">Mental Models</title>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comparing character-level neural language models using a lexical decision task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Gaël Le Godais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic parsing using left corner language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT</title>
		<meeting>of IWPT</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linguistic structure and speech shadowing at very short latencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Marslen-Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="522" to="523" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neurophysiological dynamics of phrase-structure building during sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><forename type="middle">El</forename><surname>Karoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Giber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilda</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sydney</forename><forename type="middle">S</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Naccache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Pallier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Dehaene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<meeting><address><addrLine>Yusuke Oda, Matthew Richardson, Naomi Saphra</addrLine></address></meeting>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Grammars, parsers, and memory limitations. Language and Cognitive Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen Pulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Left-corner parsing and psychological plausibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unbounded dependency recovery for parser evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Broad-coverage parsing using human-like memory constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How grammatical is characterlevel neural machine translation? assessing mt quality with contrastive translation pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integration of visual and linguistic information in spoken language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="1632" to="1634" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Memory architectures in recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
