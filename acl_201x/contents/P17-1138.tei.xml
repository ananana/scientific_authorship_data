<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bandit Structured Prediction for Neural Sequence-to-Sequence Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
						</author>
						<title level="a" type="main">Bandit Structured Prediction for Neural Sequence-to-Sequence Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1503" to="1513"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1138</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many NLP tasks involve learning to predict a structured output such as a sequence, a tree or a graph. Sequence-to-sequence learning with neu- ral networks has recently become a popular ap- proach that allows tackling structured prediction as a mapping problem between variable-length se- quences, e.g., from foreign language sentences into target-language sentences ), or from natural language input sentences into linearized versions of syntactic ( <ref type="bibr" target="#b47">Vinyals et al., 2015)</ref> or semantic parses <ref type="bibr" target="#b19">(Jia and Liang, 2016)</ref>. A known bottleneck in structured prediction is the requirement of large amounts of gold-standard structures for supervised learning of model pa- rameters, especially for data-hungry neural net- work models. <ref type="bibr">Sokolov et al. (2016a,b)</ref> presented a framework for stochastic structured prediction under bandit feedback that alleviates the need for labeled output structures in learning: Following an online learning protocol, on each iteration the learner receives an input, predicts an output struc- ture, and receives partial feedback in form of a task loss evaluation of the predicted structure. <ref type="bibr">1</ref> They "banditize" several objective functions for linear structured predictions, and evaluate the resulting algorithms with simulated bandit feedback on var- ious NLP tasks.</p><p>We show how to lift linear structured predic- tion under bandit feedback to non-linear models for sequence-to-sequence learning with attention- based recurrent neural networks ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. Our framework is applicable to sequence- to-sequence learning from various types of weak feedback. For example, extracting learning signals from the execution of structured outputs against databases has been established in the communi- ties of semantic parsing and grounded language learning since more than a decade <ref type="bibr" target="#b51">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b6">Clarke et al., 2010;</ref><ref type="bibr" target="#b21">Liang et al., 2011</ref>). Our work can build the basis for neural se- mantic parsing from weak feedback.</p><p>In this paper, we focus on the application of ma- chine translation via neural sequence-to-sequence learning. The standard procedure of training neu- ral machine translation (NMT) models is to com- pare their output to human-generated translations and to infer model updates from this comparison. However, the creation of reference translations or post-edits requires professional expertise of users. Our framework allows NMT models to learn from feedback that is weaker than human references or post-edits. One could imagine a scenario of per- sonalized machine translation where translations have to be adapted to the user's specific purpose and domain. The feedback required by our meth- ods can be provided by laymen users or can even be implicit, e.g., inferred from user interactions with the translated content on a web page.</p><p>Starting from the work of <ref type="bibr">Sokolov et al. (2016a,b)</ref>, we lift their objectives to neural sequence-to-sequence learning. We evaluate the resulting algorithms on the task of French-to- English translation domain adaptation where a seed model trained on Europarl data is adapted to the NewsCommentary and the TED talks do- main with simulated weak feedback. By learn- ing from this feedback, we find 4.08 BLEU points improvements on NewsCommentary, and 5.89 BLEU points improvement on TED. Furthermore, we show how control variates can be integrated in our algorithms, yielding faster learning and im- proved generalization in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>NMT models are most commonly trained un- der a word-level maximum likelihood objective. Even though this objective has successfully been applied to many sequence-to-sequence learning tasks, the resulting models suffer from exposure bias, since they learn to generate output words based on the history of given reference words, not on their own predictions. <ref type="bibr" target="#b32">Ranzato et al. (2016)</ref> ap- ply techniques from reinforcement learning <ref type="bibr" target="#b45">(Sutton and Barto, 1998;</ref><ref type="bibr" target="#b46">Sutton et al., 2000</ref>) and im- itation learning <ref type="bibr" target="#b35">(Schaal, 1999;</ref><ref type="bibr" target="#b34">Ross et al., 2011;</ref><ref type="bibr" target="#b8">Daumé et al., 2009</ref>) to learn from feedback to the model's own predictions. Furthermore, they ad- dress the mismatch between word-level loss and sequence-level evaluation metric by using a mix- ture of the REINFORCE <ref type="bibr" target="#b48">(Williams, 1992)</ref> algo- rithm and the standard maximum likelihood train- ing to directly optimize a sequence-level loss. Similarly, <ref type="bibr" target="#b38">Shen et al. (2016)</ref> lift minimum risk training <ref type="bibr" target="#b29">(Och, 2003;</ref><ref type="bibr" target="#b39">Smith and Eisner, 2006;</ref><ref type="bibr" target="#b11">Gimpel and Smith, 2010;</ref><ref type="bibr" target="#b50">Yuille and He, 2012;</ref><ref type="bibr" target="#b16">He and Deng, 2012</ref>) from linear models for machine translation to NMT. These works are closely re- lated to ours in that they use the technique of score function gradient estimators <ref type="bibr" target="#b10">(Fu, 2006;</ref><ref type="bibr" target="#b36">Schulman et al., 2015</ref>) for stochastic learning. However, the learning environment of <ref type="bibr" target="#b38">Shen et al. (2016)</ref> is dif- ferent from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their rewards. In our bandit setting, feedback to only a single sam- ple per sentence is available, making the learning problem much harder. The approach by <ref type="bibr" target="#b32">Ranzato et al. (2016)</ref> approximates the expectation with single samples, but still requires reference trans- lations which are unavailable in the bandit setting.</p><p>To our knowledge, the only work on training NMT from weak feedback is the work by . They propose a dual-learning mechanism where two translation models are jointly trained on monolingual data. The feedback in this case is a reward signal from language models and a recon- struction error. This is attractive because the feed- back can automatically be generated from mono- lingual data and does not require any human ref- erences. However, we are interested in using esti- mates of human feedback on translation quality to directly adapt the model to the users' needs.</p><p>Our approach follows most closely the work of <ref type="bibr">Sokolov et al. (2016a,b)</ref>. They introduce ban- dit learning objectives for structured prediction and apply them to various NLP tasks, including machine translation with linear models. Their approach can be seen as an instantiation of re- inforcement learning to one-state Markov deci- sion processes under linear policy models. In this paper, we transfer their algorithms to non- linear sequence-to-sequence learning. <ref type="bibr" target="#b40">Sokolov et al. (2016a)</ref> showed applications of linear bandit learning to tasks such as multiclass-classification, OCR, and chunking, where learning can be done from scratch. We focus on lifting their linear ma- chine translation experiments to the more complex NMT that requires a warm start for training. This is done by training a seed model on one domain and adapting it to a new domain based on bandit feedback only. For this task we build on the work of <ref type="bibr" target="#b9">Freitag and Al-Onaizan (2016)</ref>, who investigate strategies to find the best of both worlds: models that adapt well to the new domain without deteri- orating on the old domain. In contrast to previous approaches to domain adaptation for NMT, we do not require in-domain parallel data, but consult di- rect feedback to the translations generated for the new domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Machine Translation</head><p>Neural models for machine translation are based on a sequence-to-sequence learning architecture consisting of an encoder and a decoder ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). An encoder Recurrent Neural Network (RNN) reads in the source sentence and a decoder RNN generates the target sentence conditioned on the encoded source.</p><p>The input to the encoder is a sequence of vec- tors x = (x 1 , . . . , x Tx ) representing a sequence of source words of length T x . In the approach of , they are encoded into a single vector c = q({h 1 , . . . , h Tx }), where h t = f (x t , h t−1 ) is the hidden state of the RNN at time t. Several choices are possible for the non-linear functions f and q: Here we are using a Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b5">Chung et al., 2014</ref>) for f , and for q an attention mechanism that de- fines the context vector as a weighted sum over encoder hidden states ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b24">Luong et al., 2015a</ref>).</p><p>The decoder RNN predicts the next target word y t at time t given the context vector c and the pre- vious target words y &lt;t = {y 1 , . . . , y t−1 } from a probability distribution over the target vocab- ulary V . This distribution is the result of a softmax transformation of the decoder outputs o = {o 1 , . . . , o Ty }, such that</p><formula xml:id="formula_0">p θ (y t = w i |y &lt;t , c) = exp(o w i ) V v=1 exp(o wv )</formula><p>.</p><p>The probability of a full sequence of outputs y = (y 1 , . . . , y Ty ) of length T y is defined as the product of the conditional word probabilities:</p><formula xml:id="formula_1">p θ (y|x) = Ty t=1 p θ (y t |y &lt;t , c).</formula><p>Since this encoder-decoder architecture is fully differentiable, it can be trained with gradient de- scent methods. Given a parallel training set of S source sentences and their reference translations</p><formula xml:id="formula_2">D = {(x (s) , y (s) )} S s=1</formula><p>, we can define a word- level Maximum Likelihood Estimation (MLE) ob- jective, which aims to find the parametersˆθ</p><formula xml:id="formula_3">parametersˆ parametersˆθ MLE = arg max θ L MLE (θ)</formula><p>of the following loss function:</p><formula xml:id="formula_4">L MLE (θ) = S s=1 log p θ (y (s) |x (s) ) = S s=1 Ty t=1 log p θ (y t |x (s) , y (s) &lt;t ).</formula><p>This loss function is non-convex for the case of neural networks. Clever initialization strategies,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Neural Bandit Structured Prediction</head><p>Input: Sequence of learning rates γ k Output: Optimal parametersˆθparametersˆ parametersˆθ 1: Initialize θ 0 2: for k = 0, . . . , K do 3:</p><formula xml:id="formula_5">Observe x k 4: Sample˜ySample˜ Sample˜y k ∼ p θ (y|x k ) 5:</formula><p>Obtain feedback ∆(˜ y k )</p><p>6:</p><formula xml:id="formula_6">θ k+1 = θ k − γ k s k 7:</formula><p>Choose a solutionˆθsolutionˆ solutionˆθ from the list {θ 0 , . . . , θ K } adaptive learning rates and momentum techniques are required to find good local maxima and to speed up convergence ( . Another trick of the trade is to ensemble several models with different random initializations to im- prove over single models ( <ref type="bibr" target="#b24">Luong et al., 2015a)</ref>.</p><p>At test time, we face a search problem to find the sequence of target words with the highest prob- ability. Beam search reduces the search error in comparison to greedy search, but also exponen- tially increases decoding time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Bandit Structured Prediction</head><p>Algorithm 1 is an adaptation of the Bandit Struc- tured Prediction algorithm of <ref type="bibr" target="#b41">Sokolov et al. (2016b)</ref> to neural models: For K rounds, a model with parameters θ receives an input, samples an output structure, and receives user feedback. Based on this feedback, a stochastic gradient s k is computed and the model parameters are updated. As a post-optimization step, a solutionˆθsolutionˆ solutionˆθ is se- lected from the iterates. This is done with online- to-batch conversion by choosing the model with optimal performance on held-out data.</p><p>The core of the algorithm is the sampling: if the model distribution is very peaked, the model ex- ploits, i.e., it presents the most probable outputs to the user. If the distribution is close to uniform, the model explores, i.e., it presents random out- puts to the user. The balance between exploitation and exploration is crucial to the learning process: in the beginning the model is rather uninformed and needs to explore in order to find outputs with high reward, while in the end it ideally converges towards a peaked distribution that exactly fits the user's needs. Pre-training the model, i.e. set- ting θ 0 wisely, ensures a reasonable exploitation- exploration trade-off.</p><p>This online learning algorithm can be applied to any objective L provided the stochastic gradi- ents s k are unbiased estimators of the true gradi- ent of the objective, i.e., we require</p><formula xml:id="formula_7">L = E[s k ].</formula><p>In the following, we will present objectives from <ref type="bibr" target="#b41">Sokolov et al. (2016b)</ref> transferred to neural mod- els, and explain how they can be enhanced by con- trol variates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Expected Loss (EL) Minimization</head><p>The first objective is defined as the expectation of a task loss ∆(˜ y), e.g. −BLEU(˜ y), over all input and output structures:</p><formula xml:id="formula_8">L EL (θ) =E p(x) p θ (˜ y|x) [∆(˜ y)] .<label>(1)</label></formula><p>In the case of full-information learning where ref- erence outputs are available, we could evaluate all possible outputs against the reference to obtain an exact estimation of the loss function. However, this is not feasible in our setting since we only re- ceive partial feedback for a single output structure per input. Instead, we use stochastic approxima- tion to optimize this loss. The stochastic gradient for this objective is computed as follows:</p><formula xml:id="formula_9">s EL k =∆(˜ y) ∂ log p θ (˜ y|x k ) ∂θ .<label>(2)</label></formula><p>Objective (1) is known from minimum risk train- ing <ref type="bibr" target="#b29">(Och, 2003)</ref> and has been lifted to NMT by Shen et al. (2016) -but not for learning from weak feedback. Equation <ref type="formula" target="#formula_9">(2)</ref> is an instance of the score function gradient estimator <ref type="bibr" target="#b10">(Fu, 2006</ref>) where</p><formula xml:id="formula_10">log p θ (˜ y|x k )<label>(3)</label></formula><p>denotes the score function. We give an algorithm to sample structures from an encoder-decoder model in Algorithm 2. It corresponds to the algo- rithm presented by <ref type="bibr" target="#b38">Shen et al. (2016)</ref> with the dif- ference that it samples single structures, does not assume a reference structure, and additionally re- turns the sample probabilities. A similar objective has also been used in the REINFORCE algorithm <ref type="bibr" target="#b48">(Williams, 1992)</ref> which has been adapted to NMT by <ref type="bibr" target="#b32">Ranzato et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pairwise Preference Ranking (PR)</head><p>The previous objective requires numerical feed- back as an estimate of translation quality. Alterna- tively, we can learn from pairwise preference judg- ments that are formalized in preference ranking objectives. Let P(x) = {{y i , y j |y i , y j ∈ Y(x)} denote the set of output pairs for an input x, and let ∆(y i , y j ) : P(x) → [0, 1] denote a task loss function that specifies a dispreference of y i over y j . In our experimental simulations we use two types of pairwise feedback. Firstly, continuous pairwise feedback 2 is computed as</p><formula xml:id="formula_11">∆(y i , y j ) = ∆(y j ) − ∆(y i ),</formula><p>and secondly, binary feedback is computed as</p><formula xml:id="formula_12">∆(y i , y j ) = 1 if ∆(y j ) &gt; ∆(y i ), 0 otherwise.</formula><p>Analogously to the sequence-level sampling for linear models <ref type="bibr" target="#b41">(Sokolov et al., 2016b</ref>), we define the following probabilities for word-level sam- pling:</p><formula xml:id="formula_13">p + θ (˜ y t = w i |x, ˆ y &lt;t ) = exp(o w i ) V v=1 exp(o wv ) , p − θ (˜ y t = w j |x, ˆ y &lt;t ) = exp(−o w j ) V v=1 exp(−o wv )</formula><p>.</p><p>The effect of the negation within the softmax is that the two distributions p + θ and p − θ rank the next candidate target words˜ywords˜ words˜y t (given the same his- tory, here the greedy outputˆyoutputˆ outputˆy &lt;t ) in opposite or- der. Globally normalized models as in the linear case, or LSTM-CRFs ( <ref type="bibr" target="#b17">Huang et al., 2015</ref>) for the non-linear case would allow sampling full struc- tures such that the ranking over full structures is reversed. But in the case of locally normalized RNNs we retrieve only locally reversed-rank sam- ples. Since we want the model to learn to rank˜y rank˜ rank˜y i over˜yover˜ over˜y j , we would have to sample˜ysample˜ sample˜y i word-by- word from p + θ and˜yand˜ and˜y j from p − θ . However, sam- pling all words of˜yof˜ of˜y j from p − θ leads to transla- tions that are neither fluent nor source-related, so we propose to randomly choose one position of˜y of˜ of˜y j where the next word is sampled from p − θ and sample the remaining words from p + θ . We found that this method produces suitable negative sam- ples, which are only slightly perturbed and still relatively fluent and source-related. A detailed al- gorithm is given in Algorithm 3.</p><p>In the same manner as for linear models, we de- fine the probability of a pair of sequences as</p><formula xml:id="formula_14">p θ (˜ y i , ˜ y j |x) = p + θ (˜ y i |x) × p − θ (˜ y j |x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Sampling Structures</head><p>Input: Model θ, target sequence length limit T y Output: Sequence of words w = (w 1 , . . . , w T y ) and log-probability p 1: w 0 = START, p 0 = 0 2: w = (w 0</p><note type="other">) 3: for t ← 1 . . . T y do 4: w t ∼ p θ (w|x, w &lt;t ) 5: p t = p t−1 + log p θ (w|x, w &lt;t ) 6: w = (w 1 , . . . , w t−1 , w t ) 7: end for 8: Return w and p T</note><p>Note that with the word-based sampling scheme described above, the sequence˜ysequence˜ sequence˜y j also includes words sampled from p + θ . The pairwise preference ranking objective ex- presses an expectation over losses over these pairs:</p><formula xml:id="formula_15">L PR (θ) =E p(x) p θ (˜ y i ,˜ y j |x) [∆(˜ y i , ˜ y j )] .<label>(4)</label></formula><p>The stochastic gradient for this objective is</p><formula xml:id="formula_16">s PR k =∆(˜ y i , ˜ y j )<label>(5)</label></formula><formula xml:id="formula_17">× ∂ log p + θ (˜ y i |x k ) ∂θ + ∂ log p − θ (˜ y j |x k ) ∂θ .</formula><p>This training procedure resembles well-known ap- proaches for noise contrastive estimation <ref type="bibr" target="#b12">(Gutmann and Hyvärinen, 2010</ref>) with negative sam- pling that are commonly used for neural language modeling <ref type="bibr" target="#b7">(Collobert et al., 2011;</ref><ref type="bibr" target="#b27">Mnih and Teh, 2012;</ref>. In these approaches, negative samples are drawn from a non-parametric noise distribution, whereas we draw them from the perturbed model distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Control Variates</head><p>The stochastic gradients defined in equations <ref type="formula" target="#formula_9">(2)</ref> and (5) can be used in stochastic gradient descent optimization ( <ref type="bibr" target="#b2">Bottou et al., 2016)</ref> where the full gradient is approximated using a minibatch or a single example in each update. The stochastic choice, in our case on inputs and outputs, intro- duces noise that leads to slower convergence and degrades performance. In the following, we ex- plain how antithetic and additive control variate techniques from Monte Carlo simulation <ref type="bibr" target="#b33">(Ross, 2013)</ref> can be used to remedy these problems. The idea of additive control variates is to aug- ment a random variable X whose expectation is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Sampling Pairs of Structures</head><p>Input: Model θ, target sequence length limit T y Output: Pair of sequences w, w and their log- probability p 1: p 0 = 0 2: w, w , ˆ w = (START)</p><note type="other">3: i ∼ U(1, T ) 4: for t ← 1 . . . T y do 5: ˆ w t = arg max w∈V p + θ (w|x, ˆ w &lt;t ) 6:</note><formula xml:id="formula_18">w t ∼ p + θ (w|x, ˆ w &lt;t )</formula><p>7:</p><formula xml:id="formula_19">p t = p t−1 + log p + θ (w t |x, ˆ w &lt;t )</formula><p>8:</p><p>if i = t then 9:</p><formula xml:id="formula_20">w t ∼ p − θ (w|x, ˆ w &lt;t )</formula><p>10:</p><formula xml:id="formula_21">p t = p t + log p − θ (w t |x, ˆ w &lt;t ) 11:</formula><p>else 12:</p><formula xml:id="formula_22">w t ∼ p + θ (w|x, ˆ w &lt;t )</formula><p>13:  <ref type="bibr">[X]</ref>. In our case, the random variable of interest is the noisy gradient X = s k from Equation (2). The variance reduction effect of control variates can be seen by computing the variance of this quantity:</p><formula xml:id="formula_23">p t = p t + log p + θ (w t |x, ˆ</formula><formula xml:id="formula_24">Var(X − ˆ c Y ) = Var(X) + ˆ c 2 Var(Y ) (6) − 2ˆc2ˆc Cov(X, Y ).</formula><p>Choosing a control variate such that Cov(X, Y ) is positive and high enough, the variance of the gradient estimate will be reduced.</p><p>An example is the average reward baseline known from reinforcement learning <ref type="bibr" target="#b48">(Williams, 1992)</ref>, yielding</p><formula xml:id="formula_25">Y k = log p θ (˜ y|x k ) 1 k k j=1 ∆(˜ y j ).<label>(7)</label></formula><p>The optimal scalarˆcscalarˆ scalarˆc can be derived easily by tak- ing the derivative of (6), leading tô c = Cov(X,Y ) Var(X) . This technique has been applied to using the score function (Equation <ref type="formula" target="#formula_10">(3)</ref>) as control variate in <ref type="bibr" target="#b31">Ranganath et al. (2014)</ref>, yielding the following control variate:</p><formula xml:id="formula_26">Y k = log p θ (˜ y|x k ).<label>(8)</label></formula><p>Note that for both types of control variates, (7) and (8), the expectation ¯ Y is zero, simplifying the im- plementation. However, the optimal scalarˆcscalarˆ scalarˆc has to be estimated for every entry of the gradient sep- arately for the score function control variate. We will explore both types of control variates for the stochastic gradient (2) in our experiments.</p><p>A further effect of control variates is to reduce the magnitude of the gradient, the more so the more the stochastic gradient and the control vari- ate covary. For L-Lipschitz continuous functions, a reduced gradient norm directly leads to a bound on L which appears in the algorithmic stability bounds of <ref type="bibr" target="#b13">Hardt et al. (2016)</ref>. This effect of im- proved generalization by control variates is empir- ically validated in our experiments.</p><p>A similar variance reduction effect can be ob- tained by antithetic control variates. Here E[X] is approximated by the estimator X 1 +X 2 2 whose vari- ance is</p><formula xml:id="formula_27">Var X 1 + X 2 2 = 1 4 Var(X 1 )<label>(9)</label></formula><p>+ Var(X 2 ) + 2Cov(X 1 , X 2 ) .</p><p>Choosing the variates X 1 and X 2 such that Cov(X 1 , X 2 ) is negative will reduce the variance of the gradient estimate. Under certain assump- tions, the stochastic gradient (5) of the pairwise preference objective can be interpreted as an an- tithetic estimator of the score function (3). The antithetic variates in this case would be</p><formula xml:id="formula_28">X 1 = log p + θ (˜ y i |x k ),<label>(10)</label></formula><formula xml:id="formula_29">X 2 = log p − θ (˜ y j |x k )</formula><p>, where an antithetic dependence of X 2 on X 1 can be achieved by construction of p + θ and p − θ (see <ref type="bibr" target="#b3">Capriotti (2008)</ref> which is loosely related to our approach). Similar to control variates, antithetic variates have the effect of shrinking the gradient norm, the more so the more the variates are anti- thetically correlated, leading to possible improve- ments in algorithmic stability <ref type="bibr" target="#b13">(Hardt et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In the following, we present an experimental eval- uation of the learning objectives presented above on machine translation domain adaptation. We compare how the presented neural bandit learn- ing objectives perform in comparison to linear models, then discuss the handling of unknown words and eventually investigate the impact of techniques for variance reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Data. We perform domain adaptation from Eu- roparl (EP) to News Commentary (NC) and TED talks (TED) for translations from French to En- glish. NMT Models. We choose a standard encoder- decoder architecture with single-layer GRU RNNs with 800 hidden units, a word embedding size of 300 and tanh activations. The encoder consists of a bidirectional RNN, where the hidden states of backward and forward RNN are concatenated. The decoder uses the attention mechanism pro- posed by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>. <ref type="bibr">3</ref> Source and target vocabularies contain the 30k most frequent words of the respective parts of the training cor- pus. We limit the maximum sentence length to 50. Dropout ( <ref type="bibr" target="#b42">Srivastava et al., 2014</ref>) with a prob- ability of 0.5 is applied to the network in several places: on the embedded inputs, before the output layer, and on the initial state of the decoder RNN. The gradient is clipped when its norms exceeds 1.0 to prevent exploding gradients and stabilize learning ( <ref type="bibr" target="#b30">Pascanu et al., 2013</ref>). All models are im- plemented and trained with the sequence learning framework Neural Monkey (Libovick`Libovick`y et al., <ref type="bibr" target="#b1">Bojar et al., 2016</ref>). <ref type="bibr">4</ref> They are trained with a minibatch size of 20, fitting onto single 8GB GPU machines. The training dataset is shuffled before each epoch. The gap between the performance of the out-of- domain model and the in-domain models defines the range of possible improvements for bandit learning. All models are evaluated with Neural Monkey's mteval. For statistical significance tests we used Approximate Randomization testing <ref type="bibr" target="#b28">(Noreen, 1989)</ref>.</p><p>Bandit Learning. Bandit learning starts with the parameters of the out-of-domain baseline. The bandit models are expected to improve over the out-of-domain baseline by receiving feedback from the new domain, but at most to reach the in- domain baseline since the feedback is weak. The models are trained with Adam on in-domain data for at most 20 epochs. Adam's step-size param- eter α was tuned on the validation set and was found to perform best when set to 1 × 10 −5 for non-pairwise, 1 × 10 −6 for pairwise objectives on NC, 1 × 10 −7 for pairwise objectives on TED. The best model parameters, selected with early stopping on the in-domain validation set, are eval- uated on the held-out in-domain test set. In the spirit of Freitag and Al-Onaizan (2016) they are additionally evaluated on the out-of-domain test set to investigate how much knowledge of the old domain the models lose while adapting to the new domain. Bandit learning experiments are repeated two times, with different random seeds, and mean BLEU scores with standard deviation are reported.</p><p>Feedback Simulation. Weak feedback is simu- lated from the target side of the parallel corpus, but references are never revealed to the learner. <ref type="bibr">Sokolov et al. (2016a,b)</ref> used a smoothed version of per-sentence BLEU for simulating the weak feedback for generated translations from the com- parison with reference translations. Here, we use gGLEU instead, which  recently introduced for learning from sentence-level re- ward signals correlating well with corpus BLEU. This metric is closely related to BLEU, but does not have a brevity penalty and considers the recall of matching n-grams. It is defined as the mini- mum of recall and precision over the total n-grams up to a certain n. Hence, for our experiments ∆(˜ y) = −gGLEU(˜ y, y), where˜ywhere˜ where˜y is a sample translation and y is the reference translation.</p><p>Unknown words. One drawback of NMT mod- els is their limitation to a fixed source-and target vocabulary. In a domain adaptation setting, this limitation has a critical impact to the translation quality. The larger the distance between old and new domain, the more words in the new domain are unknown to the models trained on the old do- main (represented with a special UNK token). We consider two strategies for this problem for our ex- periments: For UNK-Replace we use fast align to gen- erate lexical translations on the EP training data.</p><p>When an UNK token is generated, we look up the attention weights and find the source token that receives most attention in this step. If possible, we replace the UNK token by its lexical trans- lation. If it is not included in the lexical trans- lations, it is replaced by the source token. The main benefit of this technique is that it deals well  with unknown named entities that are just passed through from source to target. However, since it is a non-differentiable post-processing step, the NMT model cannot directly be trained for this be- havior. Therefore we also train sub-word level NMT with BPE. We apply 29,800 merge opera- tions to obtain a vocabulary of 29,908 sub-words. The procedure for training these models is exactly the same as for the word-based models. The ad- vantage of this method is that the model is in prin- ciple able to generate any word composing it from sub-word units. However, training sequences be- come longer and candidate translations are sam- pled on a sub-word level, which introduces the risk of sampling nonsense words.</p><p>Control variates. We implement the average baseline control variate as defined in Equation 7, which results in keeping an running average over previous losses. Intuitively, absolute gGLEU feed- back is turned into relative feedback that reflects the current state of the model. The sign of the up- date is switched when the gGLEU for the current sample is worse than the average gGLEU, so the model makes a step away from it, while in the case of absolute feedback it would still make a small step towards it. In addition, we implement the score function control variate with a running es-</p><formula xml:id="formula_30">timatê c k = 1 k k j=1 Cov(s j , log p θ (˜ y j |x j )) Var(s j )</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In the following, we discuss the results of the experimental evaluation of the models described above. The out-of-domain baseline results are given in <ref type="table" target="#tab_4">Table 2</ref>, those for the in-domain baselines in 3. The results for bandit learning on NC and TED are reported in <ref type="table" target="#tab_7">Table 4</ref>. For bandit learning we give mean improvements over the respective out-of-domain baselines in the Diff.-columns.</p><p>Baselines. The NMT out-of-domain baselines, reported in  NC, but the in-domain EP→NC <ref type="table" target="#tab_6">(Table 3)</ref> base- lines outperform the linear baseline by more than 3 BLEU points. Continuing training of a pre-trained out-of-domain model on a small amount of in do- main data is very hence effective, whilst the per- formance of the models solely trained on small in- domain data is highly dependent on the size of this training data set. For TED, the in-domain dataset is almost four times as big as the NC training set, so the in-domain baselines perform better. This ef- fect was previously observed by <ref type="bibr" target="#b23">Luong and Manning (2015)</ref> and <ref type="bibr" target="#b9">Freitag and Al-Onaizan (2016)</ref>.</p><p>Bandit Learning. The NMT bandit models that optimize the EL objective yield generally a much higher improvement over the out-of-domain mod- els than the corresponding linear models: As listed in  nique is a considerable speedup of training speed of 1 to 2 orders of magnitude compared to EL. A beneficial side-effect of NMT learning from weak feedback is that the knowledge from the out-domain training is not simply "overwritten". This happens to full-information in-domain tun- ing where more than 4 BLEU points are lost in an evaluation on the out-domain data. On the con- trary, the bandit learning models still achieve high results on the original domain. This is useful for conservative domain adaptation, where the perfor- mance of the models in the old domain is still rel- evant.</p><p>Unknown words. By handling unknown words with UNK-Replace or BPEs, we find consistent improvements over the plain word-based models for all baselines and bandit learning models. We observe that the models with UNK replacement essentially benefit from passing through source tokens, and only marginally from lexical trans- lations. Bandit learning models take particular advantage of UNK replacement when it is in- cluded already during training. The sub-word models achieve the overall highest improvement over the baselines, although sometimes generating nonsense words.</p><p>Control variates. Applying the score function control variate to EL optimization does not largely change learning speed or BLEU results. How- ever, the average reward control variate leads to improvements of around 1 BLEU over the EL op- timization without variance reduction on both do- mains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we showed how to lift structured pre- diction under bandit feedback from linear models to non-linear sequence-to-sequence learning us- ing recurrent neural networks with attention. We introduced algorithms to train these models un- der numerical feedback to single output structures or under preference rankings over pairs of struc- tures. In our experimental evaluation on the task of neural machine translation domain adaptation, we found relative improvements of up to 5.89 BLEU points over out-of-domain seed models, outper- forming also linear bandit models. Furthermore, we argued that pairwise ranking under bandit feed- back can be interpreted as a use of antithetic vari- ates, and we showed how to include average re- ward and score function baselines as control vari- ates for improved training speed and generaliza- tion. In future work, we would like to apply the presented non-linear bandit learners to other struc- tured prediction tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Baselines.</head><label></label><figDesc>The out-of-domain baseline is trained on the EP training set with standard MLE. For both NC and TED domains, we train two full- information in-domain baselines: The first in- domain baseline is trained on the relatively small in-domain training data. The second in-domain baseline starts from the out-of-domain model and is further trained on the in-domain data. All base- lines are trained with MLE and Adam (Kingma and Ba, 2014) (α = 1 × 10 −4 , β 1 = 0.9, β 2 = 0.999) until their performance stops in- creasing on respective held-out validation sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>UNK-Replace: Jean et al. (2015) and Luong et al. (2015b) replace generated UNK tokens with aligned source words or their lexical translations in a post-processing step. Fre- itag and Al-Onaizan (2016) and Hashimoto et al. (2016) demonstrated that this technique is beneficial for NMT domain adaptation. 2. BPE: Sennrich et al. (2016) introduce byte pair encoding (BPE) for word segmenta- tion to build translation models on sub-word units. Rare words are decomposed into sub- word units, while the most frequent words re- main single vocabulary items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 provides</head><label>1</label><figDesc>details about the datasets. For data pre-processing we follow the procedure of Sokolov et al. (2016a,b) using cdec tools for filtering, lowercasing and tokenization. The chal- lenge for the bandit learner is to adapt from the EP domain to NC or TED with weak feedback only.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Out-of-domain NMT baseline results 
(BLEU) on in-and out-of-domain test sets trained 
only on EP data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 ,</head><label>2</label><figDesc></figDesc><table>perform comparable to the 
linear baseline from Sokolov et al. (2016a,b) on 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>In-domain NMT baselines results 
(BLEU) on in-and out-of-domain test sets. The 
EP→NC is first trained on EP, then fine-tuned on 
NC. The EP→TED is first trained on EP, then fine-
tuned on TED. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 ,</head><label>4</label><figDesc></figDesc><table>we find improvements of between 2.33 
and 2.89 BLEU points on the NC domain, and be-
tween 4.18 and 5.18 BLEU points on the TED do-
main. In contrast, the linear models with sparse 
features and hypergraph re-decoding achieved a 
maximum improvement of 0.82 BLEU points on 
NC. 
Optimization of the PR objective shows im-
provements of up to 1.79 BLEU points on NC 
(compared to 0.6 BLEU points for linear mod-
els), but no significant improvement on TED. The 
biggest impact of this variance reduction tech-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Bandit NMT results (BLEU) on EP, NC and TED test sets. UNK* models involve UNK 
replacement only during testing, UNK** include UNK replacement already during training. For PR, 
either binary (bin) or continuous feedback (cont) was used. Control variates: average reward baseline 
(BL) and score function (SF). Results are averaged over two independent runs and standard deviation is 
given in subscripts. Improvements over respective out-of-domain models are given in the Diff.-columns. 

</table></figure>

			<note place="foot" n="1"> The name &quot;bandit feedback&quot; is inherited from the problem of maximizing the reward for a sequence of pulls of arms of so-called &quot;one-armed bandit&quot; slot machines.</note>

			<note place="foot" n="2"> Note that our definition of continuous feedback is slightly different from the one proposed in Sokolov et al. (2016b) where updates are only made for misrankings.</note>

			<note place="foot" n="3"> We do not use beam search nor ensembling, although we are aware that higher performance is almost guaranteed with these techniques. Our goal is to show relative differences between different models, so a simple setup is sufficient for the purpose of our experiments.</note>

			<note place="foot" n="4"> The Neural Monkey fork https://github. com/juliakreutzer/bandit-neuralmonkey contains bandit learning objectives and the configuration files for our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by the Ger-man research foundation (DFG), and in part by a research cooperation grant with the Amazon De-velopment Center Germany.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">UFAL submissions to the IWSLT 2016 MT track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Sudarikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Cıfka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04838v1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reducing the variance of likelihood ratio greeks in Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Capriotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WCS</title>
		<meeting><address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">ing. eprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wing-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2505" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast domain adaptation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06897</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook in Operations Research and Management Science</title>
		<editor>S.G. Henderson and B.L. Nelson</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="575" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Softmaxmargin training for structured log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno>CMU-LTI-10-008</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<meeting><address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation and attentionbased unknown word replacement in chinese-tojapanese neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING Workshop on Asian Translation</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum expected BLEU training of phrase and lexicon translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for WMT&apos;15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tlust`y, Pavel Pecina, and Ondřej Bojar. 2016. CUNI system for WMT16 automatic post-editing and multimodal translation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Libovick`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Tlust</surname></persName>
		</author>
		<imprint>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT. Da Nang</title>
		<meeting><address><addrLine>Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Lisbon</title>
		<meeting><address><addrLine>Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. Lake</title>
		<meeting><address><addrLine>Tahoe, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Computer Intensive Methods for Testing Hypotheses. An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<editor>HLT-NAACL. Edmonton, Canada</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<meeting><address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks. In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>San Juan, Puerto Rico</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note>fifth edition</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS. Ft</title>
		<meeting><address><addrLine>Lauderdale, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Is imitation learning the route to humanoid robots?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="233" to="242" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient estimation using stochastic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning structured predictors from bandit feedback for interactive NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stochastic structured prediction under bandit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Reinforcement Learning. An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">eprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Probabilistic models of vision and max-margin methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Electrical and Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="106" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
