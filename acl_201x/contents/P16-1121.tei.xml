<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Semantically and Additively Compositional Distributional Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Semantically and Additively Compositional Distributional Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1277" to="1287"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally , we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors , achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A major goal of semantic processing is to map nat- ural language utterances to representations that fa- cilitate calculation of meanings, execution of com- mands, and/or inference of knowledge. Formal semantics supports such representations by defin- ing words as some functional units and combining them via a specific logic. A simple and illustra- tive example is the Dependency-based Composi- tional Semantics (DCS) ( <ref type="bibr" target="#b34">Liang et al., 2013)</ref>. DCS composes meanings from denotations of words (i.e. sets of things to which the words apply); say, the denotations of the concept drug and the event ban is shown in <ref type="figure">Figure 1b</ref>, where drug is a list of drug names and ban is a list of the subject- complement pairs in any ban event; then, a list of banned drugs can be constructed by first taking the COMP column of all records in ban (projection "π COMP "), and then intersecting the results with drug (intersection "∩"). This procedure defined how words can be combined to form a meaning.</p><p>Better yet, the procedure can be concisely illus- trated by the DCS tree of "banned drugs" <ref type="figure">(Fig- ure 1a)</ref>, which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in ques- tion answering ( <ref type="bibr" target="#b34">Liang et al., 2013</ref>) and textual en- tailment recognition <ref type="bibr" target="#b52">(Tian et al., 2014</ref>).</p><p>Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words <ref type="bibr" target="#b54">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b31">Levy et al., 2015)</ref>, and progress is made in combining the word vectors to form meanings of phrases/sentences ( <ref type="bibr" target="#b40">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b3">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b17">Grefenstette and Sadrzadeh, 2011;</ref><ref type="bibr" target="#b50">Socher et al., 2012;</ref><ref type="bibr" target="#b44">Paperno et al., 2014;</ref><ref type="bibr" target="#b23">Hashimoto et al., 2014</ref>). However, less effort is devoted to finding a link between vector-based compositions and the com- position operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that realize such a link must conform to the logic of the formal semantics.</p><p>In this paper, we establish a link between DCS and certain vector compositions, achieving a vector-based DCS by replacing denotations of words with word vectors, and realizing the compo- sition operations such as intersection and projec- tion as addition and linear mapping, respectively. For example, to construct a vector for "banned drugs", one takes the word vector v ban and mul- tiply it by a matrix M COMP , corresponding to the projection π COMP ; then, one adds the result to the word vector v drug to realize the intersection opera- tion ( <ref type="figure">Figure 1c</ref>). We provide a method to train the answer vectors:</p><p>coarse-grained candidate list for "banned drugs" <ref type="figure">Figure 1</ref>: (a) The DCS tree of "banned drugs", which controls (b) the calculation of its denota- tion. In this paper, we learn word vectors and ma- trices such that (c) the same calculation is realized in distributional semantics. The constructed query vector can be used to (d) retrieve a list of coarse- grained candidate answers to that query.</p><formula xml:id="formula_0">ARG Aspirin Thalidomide … COMP alcohol Thalidomide … SUBJ government Canada … drug ban ∩ = π COMP projection intersection denotation of "banned drugs" (c) (d) (a) (b) alcohol Thalidomide … Thalidomide … u food u thalidomide u cannabis … v ban v drug + v ban M COMP</formula><p>word vectors and linear mappings (i.e. matrices) jointly from unlabeled corpora.</p><p>The rationale for our model is as follows. First, recent research has shown that additive composi- tion of word vectors is an approximation to the sit- uation where two words have overlapping context <ref type="bibr" target="#b53">(Tian et al., 2015)</ref>; therefore, it is suitable to im- plement an "and" or intersection operation (Sec- tion 3). We design our model such that the resulted distributional representations are expected to have additive compositionality. Second, when intersec- tion is realized as addition, it is natural to imple- ment projection as linear mapping, as suggested by the logical interactions between the two oper- ations (Section 3). Experimentally, we show that vectors and matrices learned by our model exhibit favorable characteristics as compared with vectors trained by <ref type="bibr">GloVe (Pennington et al., 2014</ref>) or those learned from syntactic dependencies (Section 5.1). Finally, additive composition brings our model a strong ability to calculate similar vectors for similar phrases, whereas syntactic-semantic roles (e.g. SUBJ, COMP) can be distinguished by dif- ferent projection matrices (e.g. M SUBJ , M COMP ). We achieve near state-of-the-art performance on a wide range of phrase similarity tasks (Section 5.2) and relation classification (Section 5.3).</p><p>Furthermore, we show that a vector as con- structed above for "banned drugs" can be used as a query vector to retrieve a coarse-grained candi-</p><formula xml:id="formula_1">ban COMP drug ARG A man sells banned drugs. sell man COMP SUBJ ARG ARG ARG John Mike … COMP Aspirin perfume … SUBJ John Mary … man sell</formula><p>Figure 2: DCS tree for a sentence date list of banned drugs, by sorting its dot prod- ucts with answer vectors that are also learned by our model <ref type="figure">(Figure 1d</ref>). This is due to the ability of our approach to provide a language model that can find likely words to fill in the blanks such as " is a banned drug" or "the drug is banned by . . . ". A highlight is the calculation being done as if a query is "executed" by the DCS tree of "banned drugs". We quantitatively evaluate this utility on sentence completion task ( <ref type="bibr" target="#b57">Zweig et al., 2012</ref>) and report a new state-of-the-art (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DCS Trees</head><p>DCS composes meanings from denotations, or sets of things to which words apply. A "thing" (i.e. element of a denotation) is represented by a tuple of features of the form Field=Value, with a fixed inventory of fields. For example, a denotation ban might be a set of tuples ban = {(SUBJ=Canada, COMP=Thalidomide), . . .}, in which each tuple records participants of a ban- ning event (e.g. Canada banning Thalidomide).</p><p>Operations are applied to sets of things to gener- ate new denotations, for modeling semantic com- position. An example is the intersection of pet and fish giving the denotation of "pet fish". An- other necessary operation is projection; by π N we mean a function mapping a tuple to its value of the field N. For example, π COMP (ban) is the value set of the COMP fields in ban, which consists of banned objects (i.e. {Thalidomide, . . .}). In this paper, we assume a field ARG to be names of things representing themselves, hence for example π ARG (drug) is the set of names of drugs.</p><p>For a value set V , we also consider inverse im-</p><formula xml:id="formula_2">age π −1 N (V ) := {x | π N (x) ∈ V }.</formula><p>For example,</p><formula xml:id="formula_3">D 1 := π −1 SUBJ (π ARG (man))</formula><p>consists of all tuples of the form <ref type="bibr">(SUBJ=x, . . .)</ref>, where x is a man's name (i.e. x ∈ π ARG (man)).</p><p>Thus, sell ∩ D 1 denotes men's selling events (i.e. {(SUBJ=John, COMP=Aspirin), . . .} as in <ref type="figure">Figure 2</ref>). Similarly, the denotation of "banned  <ref type="figure">Figure 3</ref>: DCS trees in this work drugs" as in <ref type="figure">Figure 1b</ref> is formally written as</p><formula xml:id="formula_4">D 2 := drug ∩ π −1 ARG (π COMP (ban)),</formula><p>Hence the following denotation</p><formula xml:id="formula_5">D 3 := sell ∩ D 1 ∩ π −1 COMP (π ARG (D 2 ))</formula><p>consists of selling events such that the SUBJ is a man and the COMP is a banned drug. The calculation above can proceed in a recur- sive manner controlled by DCS trees. The DCS tree for the sentence "a man sells banned drugs" is shown in <ref type="figure">Figure 2</ref>. Formally, a DCS tree is de- fined as a rooted tree in which nodes are denota- tions of content words and edges are labeled by fields at each ends. Assume a node x has children y 1 , . . . , y n , and the edges (x, y 1 ), . . . , (x, y n ) are labeled by (P 1 , L 1 ), . . . , (P n , L n ), respectively. Then, the denotation <ref type="bibr">[[x]</ref>] of the subtree rooted at x is recursively calculated as</p><formula xml:id="formula_6">[[x]] := x ∩ n i=1 π −1 P i (π L i ([[y i ]])).<label>(1)</label></formula><p>As a result, the denotation of the DCS tree in <ref type="figure">Fig- ure 2</ref> is the denotation D 3 of "a man sells banned drugs" as calculated above. DCS can be further extended to handle phenomena such as quantifiers or superlatives ( <ref type="bibr" target="#b34">Liang et al., 2013;</ref><ref type="bibr" target="#b52">Tian et al., 2014)</ref>. In this paper, we focus on the basic ver- sion, but note that it is already expressive enough to at least partially capture the meanings of a large portion of phrases and sentences. DCS trees can be learned from question-answer pairs and a given database of denotations ( <ref type="bibr" target="#b34">Liang et al., 2013</ref>), or they can be extracted from de- pendency trees if no database is specified, by tak- ing advantage of the observation that DCS trees are similar to dependency trees ( <ref type="bibr" target="#b52">Tian et al., 2014</ref>). We use the latter approach, obtaining DCS trees by rule-based conversion from universal dependency (UD) trees <ref type="bibr" target="#b36">(McDonald et al., 2013)</ref>. Therefore, nodes in a DCS tree are content words in a UD tree, which are in the form of lemma-POS pairs ( <ref type="figure">Figure 3</ref>). The inventory of fields is designed to be ARG, SUBJ, COMP, and all prepositions. Prepositions are unlike content words which de- note sets of things, but act as relations which we treat similarly as SUBJ and COMP. For example, a prepositional phrase attached to a verb (e.g. play on the grass) is treated as in <ref type="figure">Figure 3a</ref>. The pres- ence of two field labels on each edge of a DCS tree makes it convenient for modeling semantics in several cases, such as a relative clause <ref type="figure">(Figure 3b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Vector-based DCS</head><p>For any content word w, we use a query vector v w to model its denotation, and an answer vector u w to model a prototypical element in that denotation. Query vector v and answer vector u are learned such that exp(v · u) is proportional to the prob- ability of u answering the query v. The learning source is a collection of DCS trees, based on the idea that the DCS tree of a declarative sentence usually has non-empty denotation. For exam- ple, "kids play" means there exists some kid who plays. Consequently, some element in the play denotation belongs to π −1 SUBJ (π ARG (kid)), and some element in the kid denotation belongs to</p><formula xml:id="formula_7">π −1 ARG (π SUBJ (play))</formula><p>. This is a signal to increase the dot product of u play and the query vector of π −1 SUBJ (π ARG (kid)), as well as the dot product of u kid and the query vector of π −1 ARG (π SUBJ (play)). When optimized on a large corpus, the "typical" elements of play and kid should be learned by u play and u kid , respectively. In general, one has Theorem 1 Assume the denotation of a DCS tree is not empty. Given any path from node x to y, assume edges along the path are labeled by (P, L), . . . , (K, N). Then, an element in the deno- tation y belongs to π −1</p><formula xml:id="formula_8">N (π K (. . . (π −1 L (π P (x) . . .)</formula><p>. Therefore, for any two nodes in a DCS tree, the path from one to another forms a training exam- ple, which signals increasing the dot product of the corresponding query and answer vectors.</p><p>It is noteworthy that the above formalization happens to be closely related to the skip-gram model ( <ref type="bibr" target="#b38">Mikolov et al., 2013b</ref>). The skip-gram learns a target vector v w and a context vector u w for each word w. It assumes the probability of a word y co-occurring with a word x in a context window is proportional to exp(v x · u y ). Hence, if x and y co-occur within a context window, then one gets a signal to increase v x · u y . If the con- text window is taken as the same DCS tree, then the learning of skip-gram and vector-based DCS will be almost the same, except that the target vec- tor v x becomes the query vector v, which is no longer assigned to the word x but the path from x to y in the DCS tree (e.g. the query vector for</p><formula xml:id="formula_9">π −1 SUBJ (π ARG (kid)) instead of v kid )</formula><p>. Therefore, our model can also be regarded as extending skip- gram to take account of the changes of meanings caused by different syntactic-semantic roles.</p><p>Additive Composition Word vectors trained by skip-gram are known to be semantically additive, such as exhibited in word analogy tasks. An effect of adding up two skip-gram vectors is further ana- lyzed in <ref type="bibr" target="#b53">Tian et al. (2015)</ref>. Namely, the target vec- tor v w can be regarded as encoding the distribution of context words surrounding w. If another word x is given, v w can be decomposed into two parts, one encodes context words shared with x, and an- other encodes context words not shared. When v w and v x are added up, the non-shared part of each of them tend to cancel out, because non-shared parts have nearly independent distributions. As a result, the shared part gets reinforced. An error bound is derived to estimate how close 1 2 (v w + v x ) gets to the distribution of the shared part. We can see the same mechanism exists in vector-based DCS. In a DCS tree, two paths share a context word if they lead to a same node y; semantically, this means some element in the denotation y belongs to both denotations of the two paths (e.g. given the sentence "kids play balls", π −1 SUBJ (π ARG (kid)) and π −1 COMP (π ARG (ball)) both contain a playing event whose SUBJ is a kid and COMP is a ball). Therefore, addition of query vectors of two paths approximates their intersection because the shared context y gets reinforced.</p><p>Projection Generally, for any two denotations X 1 , X 2 and any projection π N , we have</p><formula xml:id="formula_10">π N (X 1 ∩ X 2 ) ⊆ π N (X 1 ) ∩ π N (X 2 ).<label>(2)</label></formula><p>And the "⊆" can often become "=", for example when π N is a one-to-one map or X 1 = π −1 N (V ) for some value set V . Therefore, if intersection is realized by addition, it will be natural to realize projection by linear mapping because</p><formula xml:id="formula_11">(v 1 + v 2 )M N = v 1 M N + v 2 M N<label>(3)</label></formula><p>holds for any vectors v 1 , v 2 and any matrix M N , which is parallel to <ref type="bibr">(2)</ref>. If π N is realized by a ma- trix M N , then π −1 N should correspond to the in-</p><formula xml:id="formula_12">verse matrix M −1 N , because π N (π −1 N (V )) = V for</formula><p>any value set V . So we have realized all composi- tion operations in DCS.</p><p>Query vector of a DCS tree Now, we can define the query vector of a DCS tree as parallel to <ref type="formula" target="#formula_6">(1)</ref>:</p><formula xml:id="formula_13">v [[x]] := v x + 1 n n i=1 v [[y i ]] M L i M −1 P i .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>As described in Section 3, vector-based DCS as- signs a query vector v w and an answer vector u w to each content word w. And for each field N, it assigns two matrices M N and M −1 N . For any path from node x to y sampled from a DCS tree, assume the edges along are labeled by (P, L), . . . , <ref type="figure">(K, N)</ref>.</p><formula xml:id="formula_14">Then, the dot product v x M P M −1 L . . . M K M −1 N · u y gets a signal to increase.</formula><p>Formally, we adopt the noise-contrastive esti- mation ( <ref type="bibr" target="#b21">Gutmann and Hyvärinen, 2012)</ref> as used in the skip-gram model, and mix the paths sam- pled from DCS trees with artificially generated noise. Then,</p><formula xml:id="formula_15">σ(v x M P M −1 L . . . M K M −1 N ·u y )</formula><p>mod- els the probability of a training example coming from DCS trees, where σ(θ) = 1/{1 + exp(−θ)} is the sigmoid function. The vectors and matri- ces are trained by maximizing the log-likelihood of the mixed data. We use stochastic gradient de- scent (Bottou, 2012) for training. Some important settings are discussed below.</p><formula xml:id="formula_16">Noise For any v x M 1 M −1 2 . . . M 2l−1 M −1 2l</formula><p>· u y obtained from a path of a DCS tree, we generate noise by randomly choosing an index i ∈ <ref type="bibr">[2, 2l]</ref>, and then replacing M j or M −1 j (∀j ≥ i) and u y by M N(j) or M −1 N(j) and u z , respectively, where N(j) and z are independently drawn from the marginal (i.e. unigram) distributions of fields and words.</p><p>Update For each data point, when i is the chosen index above for generating noise, we view indices j &lt; i as the "target" part, and j &gt;= i as the "con- text", which is completely replaced by the noise, as an analogous to the skip-gram model. Then, at each step we only update one vector and one matrix from each of the target, context, and noise part; more specifically, we only update</p><formula xml:id="formula_17">v x , M i−1 or M −1 i−1 , M i or M −1 i , M N(i) or M −1 N(i)</formula><p>, u y and u z , at the step. This is much faster than always updat- ing all matrices.</p><p>Initialization Matrices are initialized as 1 2 (I + G), where I is the identity matrix; and G and all Learning Rate We find that the initial learning rate for vectors can be set to 0.1. But for matrices, it should be less than 0.0005 otherwise the model diverges. For stable training, we rescale gradients when their norms exceed a threshold.</p><note type="other">GloVe no matrix vecDCS vecUD books essay/N novel/N essay/N author novel/N essay/N novel/N published memoir/N anthology/N article/N novel books/N publication/N anthology/N memoir autobiography/N memoir/N poem/N wrote non-fiction/J poem/N autobiography/N biography reprint/V autobiography/N publication/N autobiography publish/V story/N journal/N essay republish/V pamphlet/N memoir/N illustrated chapbook/N tale/N pamphlet/N</note><p>Regularizer During training, M N and M −1 N are treated as independent matrices. However, we use the regularizer γM −1</p><formula xml:id="formula_18">N M N − 1 d tr(M −1 N M N )I 2 to drive M −1 N close to the inverse of M N . 1 We also use κM ⊥ N M N − 1 d tr(M ⊥ N M N )I 2</formula><p>to prevent M N from having too different scales at different direc- tions (i.e., to drive M N close to orthogonal). We set γ = 0.001 and κ = 0.0001. Despite the rather weak regularizer, we find that M −1 N can be learned to be exactly the inverse of M N , and M N can ac- tually be an orthogonal matrix, showing some se- mantic regularity (Section 5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>For training vector-based DCS, we use Wikipedia Extractor 2 to extract texts from the 2015-12-01 dump of English Wikipedia <ref type="bibr">3</ref> . Then, we use Stan- ford Parser <ref type="bibr">4</ref> ( <ref type="bibr" target="#b27">Klein and Manning, 2003)</ref> to parse all sentences and convert the UD trees into DCS trees by handwritten rules. We assign a weight to each path of the DCS trees as follows. For any path P passing through k intermediate nodes of degrees n 1 , . . . , n k , respectively, we set</p><formula xml:id="formula_19">π −1 SUBJ (π ARG (house)) π −1 COMP (π ARG (house)) π −1 ARG (π in (house)) victorian/J build/V sit/V stand/V rent/V house/N vacant/J leave/V stand/V 18th-century/J burn down/V live/V historic/J remodel/V hang/V old/J demolish/V seat/N georgian/J restore/V stay/V local/J renovate/V serve/V 19th-century/J rebuild/V reside/V tenement/J construct/V hold/V π −1 ARG (π SUBJ (learn)) π −1 ARG (π COMP (learn)) π −1 about (π ARG (learn)) teacher/N skill/N otherness/N skill/N lesson/N intimacy/N he/P technique/N femininity/N she/P experience/N self-awareness/N therapist/N ability/N life/N student/N something/N self-expression/N they/P knowledge/N sadomasochism/N mother/N language/N emptiness/N lesson/N opportunity/N criminality/N father/N instruction/N masculinity/N</formula><formula xml:id="formula_20">Weight(P ) := k i=1 1 n i − 1 .<label>(5)</label></formula><p>Note that n i ≥ 2 because there is a path P passing through the node; and Weight(P ) = 1 if P con- sists of a single edge. The equation <ref type="formula" target="#formula_20">(5)</ref> is intended to degrade long paths which pass through several high-valency nodes. We use a random walk algo- rithm to sample paths such that the expected times a path is sampled equals its weight. As a result, the sampled path lengths range from 1 to 19, av- erage 2.1, with an exponential tail. We convert all words which are sampled less than 1000 times to * UNKNOWN * /POS, and all prepositions occurring less than 10000 times to an *UNKNOWN* field. As a result, we obtain a vocabulary of 109k words and 211 field names. Using the sampled paths, vectors and matrices are trained as in Section 4 (vecDCS). The vector dimension is set to d = 250. We compare with three baselines: (i) all matrices are fixed to identity ("no matrix"), in order to investigate the effects of meaning changes caused by syntactic-semantic roles and prepositions; (ii) the regularizer enforc- ing M −1 N to be actually the inverse matrix of M N is set to γ = 0 ("no inverse"), in order to investigate the effects of a semantically motivated constraint; and (iii) applying the same training scheme to UD trees directly, by modeling UD relations as matri- ces ("vecUD"). In this case, one edge is assigned one UD relation rel, so we implement the transfor-  <ref type="table">Table 3</ref>: Spearman's ρ on phrase similarity mation from child to parent by M rel , and from par- ent to child by M −1 rel . The same hyper-parameters are used to train vecUD. By comparing vecDCS with vecUD we investigate if applying the seman- tics framework of DCS makes any difference. Ad- ditionally, we compare with the GloVe (6B, 300d) vector <ref type="bibr">5 (Pennington et al., 2014</ref>). Norms of all word vectors are normalized to 1 and Frobenius norms of all matrices are normalized to √ d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Qualitative Analysis</head><p>We observe several special properties of the vec- tors and matrices trained by our model.</p><p>Words are clustered by POS In terms of cosine similarity, word vectors trained by vecDCS and vecUD are clustered by POS tags, probably due to their interactions with matrices during training. This is in contrast to the vectors trained by GloVe or "no matrix" <ref type="table" target="#tab_1">(Table 1)</ref>.</p><p>Matrices show semantic regularity Matrices learned for ARG, SUBJ and COMP are exactly orthogonal, and some most frequent prepositions 6 are remarkably close. For these matrices, the cor- responding M −1 also exactly converge to their inverse. It suggests regularities in the semantic space, especially because orthogonal matrices pre- serve cosine similarity -if M N is orthogonal, two words x, y and their projections π N (x), π N (y) will have the same similarity measure, which is seman- tically reasonable. In contrast, matrices trained by vecUD are only orthogonal for three UD relations, namely conj, dep and appos.</p><p>Words transformed by matrices To illustrate the matrices trained by vecDCS, we start from the query vectors of two words, house and learn, 5 http://nlp.stanford.edu/projects/ glove/ 6 of, in, to, for, with, on, as, at, from applying different matrices to them, and show the 10 answer vectors of the highest dot prod- ucts (Tabel 2). These are the lists of likely words which: take house as a subject, take house as a complement, fills into " in house", serve as a subject of learn, serve as a complement of learn, and fills into "learn about ", respectively. As the table shows, matrices in vecDCS are appropriately learned to map word vectors to their syntactic- semantic roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Phrase Similarity</head><p>To test if vecDCS has the composition ability to calculate similar things as similar vectors, we con- duct evaluation on a wide range of phrase similar- ity tasks. In these tasks, a system calculates sim- ilarity scores for pairs of phrases, and the perfor- mance is evaluated as its correlation with human annotators, measured by Spearman's ρ.</p><p>Datasets Mitchell and Lapata (2010) create datasets <ref type="bibr">7</ref> for pairs of three types of two-word phrases: adjective-nouns (AN) (e.g. "black hair" and "dark eye"), compound nouns (NN) (e.g. "tax charge" and "interest rate") and verb-objects (VO) (e.g. "fight war" and "win battle"). Each dataset consists of 108 pairs and each pair is annotated by 18 humans (i.e., 1,944 scores in total). Similarity scores are integers ranging from 1 to 7. Another dataset 8 is created by extending VO to Subject- Verb-Object (SVO), and then assessing similari- ties by crowd sourcing <ref type="bibr" target="#b26">(Kartsaklis and Sadrzadeh, 2014</ref>). The dataset GS11 created by Grefen- stette and Sadrzadeh (2011) (100 pairs, 25 an- notators) is also of the form SVO, but in each pair only the verbs are different (e.g. "man pro-</p><formula xml:id="formula_21">Message-Topic(e1, e2)</formula><p>It is a monthly <ref type="bibr">[report]</ref>1 providing [opinion]2 and advice on current United States government contract issues. Message-Topic(e1, e2)</p><p>The [report]1 gives an account of the silvicultural [work]2 done in Africa, Asia, Australia, South American and the Caribbean. <ref type="bibr">Message-Topic(e1, e2)</ref> NUS today responded to the Government's [announcement]1 of the long-awaited [review]2 of university funding. Component-Whole(e2, e1)</p><p>The [review]1 published political <ref type="bibr">[commentary]</ref>2 and opinion, but even more than that. Message-Topic(e1, e2)</p><p>It is a 2004 <ref type="bibr">[book]</ref>1 criticizing the political and linguistic <ref type="bibr">[writings]</ref>2 of Noam Chomsky.  vide/supply money"). The dataset GS12 described in Grefenstette (2013a) (194 pairs, 50 annotators) is of the form Adjective-Noun-Verb-Adjective- Noun (e.g. "local family run/move small hotel"), where only verbs are different in each pair.</p><p>Our method We calculate the cosine similarity of query vectors corresponding to phrases. For ex- ample, the query vector for "fight war" is calcu- lated as v war M ARG M −1 COMP + v fight . For vecUD we use M nsubj and M dobj instead of M SUBJ and M COMP , respectively. For GloVe we use additive compositions.</p><p>Results As shown in <ref type="table">Table 3</ref>, vecDCS is com- petitive on AN, NN, VO, SVO and GS12, con- sistently outperforming "no inverse", vecUD and GloVe, showing strong compositionality. The weakness of "no inverse" suggests that relaxing the constraint of inverse matrices may hurt com- positionaly, though our preliminary examination on word similarities did not find any difference. The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Wadd nl in <ref type="bibr" target="#b23">Hashimoto et al. (2014)</ref> and the en- tanglement model in <ref type="bibr" target="#b26">Kartsaklis and Sadrzadeh (2014)</ref>. However, these models do not show par- ticular advantages on other datasets. The recur- sive autoencoder (RAE) proposed in <ref type="bibr" target="#b49">Socher et al. (2011)</ref> shares an aspect with vecDCS as to con- struct meanings from parse trees. It is tested by <ref type="bibr" target="#b8">Blacoe and Lapata (2012)</ref>   <ref type="table">Table 5</ref>: F1 on relation classification less, we note that "no matrix" performs as good as vecDCS, suggesting that meaning changes caused by syntactic-semantic roles might not be major factors in these datasets, because the syntactic- semantic relations are all fixed in each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Relation Classification</head><p>In a relation classification task, the relation be- tween two words in a sentence needs to be clas- sified; we expect vecDCS to perform better than "no matrix" on this task because vecDCS can dis- tinguish the different syntactic-semantic roles of the two slots the two words fit in. We confirm this conjecture in this section.</p><p>Dataset We use the dataset of <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2009</ref>), in which 9 di- rected relations (e.g. Cause-Effect) and 1 undi- rected relation Other are annotated, 8,000 in- stances for training and 2,717 for test. Perfor- mance is measured by the 9-class direction-aware Macro-F1 score excluding Other class.</p><p>Our method For any sentence with two words marked as e 1 and e 2 , we construct the DCS tree of the sentence, and take the subtree T rooted at the common ancestor of e 1 and e 2 . We construct four vectors from T , namely: the query vector for the subtree rooted at e 1 (resp. e 2 ), and the query vector of the DCS tree obtained from T by re- rooting it at e 1 (resp. e 2 ) <ref type="figure" target="#fig_1">(Figure 4</ref>). The four vectors are normalized and concatenated to form the only feature used to train a classifier. For ve- cUD, we use the corresponding vectors calculated from UD trees. For GloVe, we use the word vec- tor of e 1 (resp. e 2 ), and the sum of vectors of all words within the span [e 1 , e 2 ) (resp. (e 1 , e 2 ]) as Results VecDCS outperforms baselines on rela- tion classification <ref type="table">(Table 5)</ref>. It makes 16 errors in misclassifying the direction of a relation, as com- pared to 144 such errors made by "no matrix", 23 by "no inverse", 30 by vecUD, and 161 by GloVe. This suggests that models with syntactic-semantic transformations (i.e. vecDCS, "no inverse", and vecUD) are indeed good at distinguishing the dif- ferent roles played by e 1 and e 2 . VecDCS scores moderately lower than the state-of-the-art ( <ref type="bibr" target="#b56">Xu et al., 2015</ref>), however we note that these results are achieved by adding additional features and train- ing task-specific neural networks (dos <ref type="bibr" target="#b15">Santos et al., 2015;</ref><ref type="bibr" target="#b56">Xu et al., 2015)</ref>. Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in <ref type="bibr" target="#b50">Socher et al. (2012)</ref>, and vecDCS actually does better. <ref type="table" target="#tab_4">Ta- ble 4</ref> shows an example of clustered training in- stances as assessed by cosine similarities between their features. It suggests that the features used in our method can actually cluster similar relations.</p><formula xml:id="formula_22">"banned drugs" "banned movies" "banned books" drug/N bratz/N publish/N marijuana/N porn/N unfair/N cannabis/N indecent/N obscene/N trafficking/N blockbuster/N samizdat/N thalidomide/N movie/N book/N smoking/N idiots/N responsum/N narcotic/N blacklist/N illegal/N botox/N grindhouse/N reclaiming/N doping/N doraemon/N redbook/N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sentence Completion</head><p>If vecDCS can compose query vectors of DCS trees, one should be able to "execute" the vec- tors to get a set of answers, as the original DCS trees can do. This is done by taking dot prod- ucts with answer vectors and then ranking the an- swers. Examples are shown in <ref type="table" target="#tab_6">Table 6</ref>. Since query vectors and answer vectors are trained from unlabeled corpora, we can only obtain a coarse- grained candidate list. However, it is noteworthy that despite a common word "banned" shared by the phrases, their answer lists are largely different, suggesting that composition actually can be done. Moreover, some words indeed answer the queries vecDCS 50 -no matrix 60 -no inverse 46 vecUD 31 N-gram (Various) 39-41 <ref type="bibr" target="#b57">Zweig et al. (2012)</ref> 52 <ref type="bibr" target="#b41">Mnih and Teh (2012)</ref> 55 <ref type="bibr" target="#b20">Gubbins and Vlachos (2013)</ref> 50 <ref type="bibr" target="#b37">Mikolov et al. (2013a)</ref> 55 <ref type="table">Table 7</ref>: Accuracy (%) on sentence completion (e.g. Thalidomide for "banned drugs" and Samiz- dat for "banned books").</p><p>Quantitatively, we evaluate this utility of exe- cuting queries on the sentence completion task. In this task, a sentence is presented with a blank that need to be filled in. Five possible words are given as options for each blank, and a system needs to choose the correct one. The task can be viewed as a coarse-grained question answering or an evalua- tion for language models ( <ref type="bibr" target="#b57">Zweig et al., 2012</ref>). We use the MSR sentence completion dataset 10 which consists of 1,040 test questions and a corpus for training language models. We train vecDCS on this corpus and use it for evaluation.</p><p>Results As shown in <ref type="table">Table 7</ref>, vecDCS scores better than the N-gram model and demonstrates promising performance. However, to our surprise, "no matrix" shows an even better result which is the new state-of-the-art. Here we might be fac- ing the same problem as in the phrase similar- ity task (Section 5.2); namely, all choices in a question fill into the same blank and the same syntactic-semantic role, so the transforming matri- ces in vecDCS might not be able to distinguish dif- ferent choices; on the other hand, vecDCS would suffer more from parsing and POS-tagging errors. Nonetheless, we believe the result by "no matrix" reveals a new horizon of sentence completion, and suggests that composing semantic vectors accord- ing to DCS trees could be a promising direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have demonstrated a way to link a vector com- position model to a formal semantics, combining the strength of vector representations to calculate phrase similarities, and the strength of formal se- mantics to build up structured queries. In this sec- tion, we discuss several lines of previous research related to this work.</p><p>Logic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a struc- tured and unambiguous way. In contrast, distri- butional semantics provides an elegant methodol- ogy for assessing semantic similarity and is well suited for learning from data. There have been re- peated calls for combining the strength of these two approaches ( <ref type="bibr" target="#b12">Coecke et al., 2010;</ref><ref type="bibr" target="#b33">Liang and Potts, 2015)</ref>, and several systems ( <ref type="bibr" target="#b32">Lewis and Steedman, 2013;</ref><ref type="bibr" target="#b5">Beltagy et al., 2014;</ref><ref type="bibr" target="#b52">Tian et al., 2014</ref>) have contributed to this direc- tion. In the remarkable work by <ref type="bibr">Beltagy et al. (to appear)</ref>, word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of en- gineering, including the generation of rule candi- dates (e.g. by aligning sentence fragments), con- verting distributional similarities to weights, and efficiently handling the rules and inference. What if the distributional representations are equipped with a logical interface, such that the inference can be realized by simple vector calculations? We have shown it possible to realize semantic com- position; we believe this may lead to significant simplification of the system design for combining logic and distributional semantics.</p><p>Compositional Distributional Models There has been active exploration on how to combine word vectors such that adequate phrase/sentence similarities can be assessed <ref type="bibr">(Mitchell and Lapata, 2010, inter alia)</ref>, and there is nothing new in us- ing matrices to model changes of meanings. How- ever, previous model designs mostly rely on lin- guistic intuitions ( <ref type="bibr">Paperno et al., 2014, inter alia)</ref>, whereas our model has an exact logic interpreta- tion. Furthermore, by using additive composition we enjoy a learning guarantee ( <ref type="bibr" target="#b53">Tian et al., 2015</ref>).</p><p>Vector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and <ref type="bibr" target="#b48">Rocktaeschel et al. (2014)</ref>, in exploring vector cal- culations that realize logic operations. However, the previous works did not specify how to inte- grate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic com- position, largely due to the simple framework of Dependency-based Compositional Semantics ( <ref type="bibr" target="#b34">Liang et al., 2013)</ref>. It fits in a long tradition of logic-based semantics <ref type="bibr" target="#b42">(Montague, 1970;</ref><ref type="bibr" target="#b16">Dowty et al., 1981;</ref><ref type="bibr" target="#b25">Kamp and Reyle, 1993)</ref>, with exten- sive studies on extracting semantics from syntactic representations such as HPSG ( <ref type="bibr" target="#b13">Copestake et al., 2001;</ref><ref type="bibr" target="#b14">Copestake et al., 2005</ref>) and CCG ( <ref type="bibr" target="#b1">Baldridge and Kruijff, 2002;</ref><ref type="bibr" target="#b9">Bos et al., 2004;</ref><ref type="bibr" target="#b51">Steedman, 2012;</ref><ref type="bibr" target="#b0">Artzi et al., 2015;</ref><ref type="bibr" target="#b39">Mineshima et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logic for Natural Language Inference</head><p>The pursue of a logic more suitable for natural lan- guage inference is also not new. For exam- ple, <ref type="bibr" target="#b35">MacCartney and Manning (2008)</ref> has imple- mented a model of natural logic <ref type="bibr" target="#b28">(Lakoff, 1970)</ref>. We would not reach the current formalization of logic of DCS without reading the work by <ref type="bibr" target="#b11">Calvanese et al. (1998)</ref>, which is an elegant formal- ization of database semantics in description logic.</p><p>Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For ex- ample, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess para- phrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for ex- ample our training scheme is similar to <ref type="bibr" target="#b22">Guu et al. (2015)</ref> in using paths and composition of matri- ces, and our method is similar to <ref type="bibr" target="#b47">Poon and Domingos (2009)</ref> in building structured knowledge from clustering syntactic parse of unlabeled data.</p><p>Further Applications Regarding the usability of distributional representations learned by our model, a strong point is that the representation takes into account syntactic/structural information of context. Unlike several previous models <ref type="bibr" target="#b43">(Padó and Lapata, 2007;</ref><ref type="bibr" target="#b30">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b46">Pham et al., 2015)</ref>, our approach learns matrices at the same time that can extract the information according to different syntactic-semantic roles. A related application is selectional preference <ref type="bibr" target="#b2">(Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b29">Lenci, 2011;</ref><ref type="bibr" target="#b55">Van de Cruys, 2014</ref>), wherein our model might has potential for smoothly handling composition.</p><p>Reproducibility Find our code at https:// github.com/tianran/vecdcs</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: For "[smoke] 1 cause flight [delay] 2 ", we construct (a)(b) from subtrees, and (c)(d) from rerooted trees, to form 4 query vectors as feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Top 10 similar words to "book/N" 

vectors are initialized with i.i.d. Gaussians of vari-
ance 1/d, where d is the vector dimension. We 
find that the diagonal component I is necessary to 
bring information from v x to u y , whereas the ran-
domness of G makes convergence faster. M −1 
N is 
initialized as the transpose of M N . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Top 10 answers of high dot products</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Similar training instances clustered by cosine similarities between features</head><label>4</label><figDesc></figDesc><table>flight 

ARG 

delay 

ARG 

smoke 

SUBJ 

cause 

ARG 
ARG 

COMP 

smoke 

flight 

ARG 

delay 

ARG 

smoke 

SUBJ 

cause 

ARG 

ARG 
COMP 

? 

COMP 

cause 

ARG 

ARG 
SUBJ 

flight 

ARG 

delay 

ARG 

(a) 
(b) 

(c) 
(d) 
? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Answers for composed query vectors 

the four vectors. Classifier is SVM 9 with RBF ker-
nel, C = 2 and Γ = 0.25. The hyper-parameters 
are selected by 5-fold cross validation. 

</table></figure>

			<note place="foot" n="1"> Problem with the naive regularizer M −1 M − I 2 is that, when the scale of M goes larger, it will drive M −1 smaller, which may lead to degeneration. So we scale I according to the trace of M −1 M. 2 http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor 3 https://dumps.wikimedia.org/enwiki/ 4 http://nlp.stanford.edu/software/ lex-parser.shtml</note>

			<note place="foot" n="7"> http://homepages.inf.ed.ac.uk/ s0453356/ 8 http://www.cs.ox.ac.uk/activities/ compdistmeaning/</note>

			<note place="foot" n="9"> https://www.csie.ntu.edu.tw/ ˜ cjlin/ libsvm/</note>

			<note place="foot" n="10"> http://research.microsoft.com/en-us/ projects/scc/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad-coverage ccg semantic parsing with amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coupling ccg and hybrid logic dependency semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert-Jan</forename><surname>Kruijff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpusbased semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic soft logic for semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Representing meaning with a combination of logical form and vectors. Computational Linguistics, special issue on formal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Widecoverage semantic representations from a ccg parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCL</title>
		<meeting>ICCL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent tricks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Grégoire Montavon,Genevì eve B. Orr, and Klaus-Robert Müller</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the decidability of query containment under constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Calvanese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">De</forename><surname>Giacomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Lenzerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS98)</title>
		<meeting>the 17th ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS98)</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Linguistic Analysis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algebra for semantic construction in constraint-based grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimal recursion semantics: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><forename type="middle">A</forename><surname>Sag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to Montague Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Dowty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<publisher>Springer</publisher>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards a formal distributional semantics: Simulating logical calculi with tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM</title>
		<meeting>*SEM</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency language models for sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gubbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jointly learning word representations and composition functions using predicate-argument structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>SEW-2009</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">From Discourse to Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Springer</publisher>
			<pubPlace>Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A study of entanglement in a categorical framework of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL)</title>
		<meeting>the 11th Workshop on Quantum Physics and Logic (QPL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast exact inference with a factored model for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linguistics and natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Composing and updating verb argument expectations: A distributional semantic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 2nd Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bringing machine learning and compositional semantics together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling semantic containment and exclusion in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Claudia Bedini, Núria Bertomeu Castelló, and Jungmee Lee</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Proceedings ACL</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Higher-order logical inference with compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Martínez-Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Universal grammar. Theoria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Montague</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Low-dimensional embeddings of logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktaeschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Semantic Parsing (SP&apos;14)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Taking Scope-The Natural Semantics of Quantifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Logical inference on dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08407</idno>
		<title level="m">The mechanism of additive composition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A neural network approach to selectional preference acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Computational approaches to sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
