<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><forename type="middle">Huu</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">New York University New York</orgName>
								<address>
									<postCode>10003</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
							<email>grishman@cs.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">New York University New York</orgName>
								<address>
									<postCode>10003</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="68" to="74"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Relation extraction suffers from a performance loss when a model is applied to out-of-domain data. This has fostered the development of domain adaptation techniques for relation extraction. This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employ- ment or Citizenship relationships. Recent research in this area, whether feature-based <ref type="bibr" target="#b17">(Kambhatla, 2004;</ref><ref type="bibr" target="#b5">Boschee et al., 2005;</ref><ref type="bibr" target="#b33">Zhou et al., 2005;</ref><ref type="bibr" target="#b13">Grishman et al., 2005;</ref><ref type="bibr" target="#b15">Jiang and Zhai, 2007a;</ref><ref type="bibr" target="#b9">Chan and Roth, 2010;</ref><ref type="bibr" target="#b28">Sun et al., 2011</ref>) or kernel- based ( <ref type="bibr" target="#b31">Zelenko et al., 2003;</ref><ref type="bibr" target="#b7">Bunescu and Mooney, 2005a;</ref><ref type="bibr" target="#b8">Bunescu and Mooney, 2005b;</ref><ref type="bibr" target="#b32">Zhang et al., 2006;</ref><ref type="bibr" target="#b25">Qian et al., 2008;</ref><ref type="bibr" target="#b23">Nguyen et al., 2009)</ref>, at- tempts to improve the RE performance by enrich- ing the feature sets from multiple sentence anal- yses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mis- match between data distributions, the RE perfor- mance of these systems tends to degrade dramat- ically <ref type="bibr" target="#b24">(Plank and Moschitti, 2013)</ref>. This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into a new model which can per- form well on new domains (the target domains).</p><p>The consequences of linguistic variation be- tween training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tag- ging ( <ref type="bibr" target="#b2">Blitzer et al., 2006;</ref><ref type="bibr" target="#b14">Huang and Yates, 2010;</ref><ref type="bibr" target="#b26">Schnabel and Schütze, 2014</ref>), named entity recog- nition <ref type="bibr" target="#b11">(Daumé III, 2007)</ref> and sentiment analysis <ref type="bibr" target="#b3">(Blitzer et al., 2007;</ref><ref type="bibr" target="#b11">Daumé III, 2007;</ref><ref type="bibr" target="#b12">Daumé III et al., 2010;</ref><ref type="bibr" target="#b4">Blitzer et al., 2011</ref>), etc. Un- fortunately, there is very little work on domain adaptation for RE. The only study explicitly tar- geting this problem so far is by <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref> who find that the out-of-domain per- formance of kernel-based relation extractors can be improved by embedding semantic similarity in- formation generated from word clustering and la- tent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suf- fers from two major limitations: + It does not incorporate word cluster informa- tion at different levels of granularity. In fact, <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref> only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can ben- efit significantly from the addition of word cluster features at various granularities.</p><p>+ It is unclear if this approach can encode real- valued features of words (such as word embed- dings ( <ref type="bibr" target="#b21">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b10">Collobert and Weston, 2008)</ref>) effectively. As the real-valued fea- tures are able to capture latent yet useful proper- ties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains.</p><p>In this work, we propose to avoid these limita- tions by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more natu-rally and effectively.</p><p>The application of word representations such as word clusters in domain adaptation of RE <ref type="bibr" target="#b24">(Plank and Moschitti, 2013</ref>) is motivated by its successes in semi-supervised methods <ref type="bibr" target="#b9">(Chan and Roth, 2010;</ref><ref type="bibr" target="#b28">Sun et al., 2011</ref>) where word repre- sentations help to reduce data-sparseness of lexi- cal information in the training data. In DA terms, since the vocabularies of the source and target do- mains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source do- main can be compensated by these general fea- tures to improve RE performance on the target do- mains.</p><p>We extend this motivation by further evaluat- ing word embeddings ( <ref type="bibr" target="#b0">Bengio et al., 2001;</ref><ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b21">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b10">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b29">Turian et al., 2010</ref>) on feature-based methods to adapt RE systems to new domains. We explore the embedding-based fea- tures in a principled way and demonstrate that word embedding itself is also an effective repre- sentation for domain adaptation of RE. More im- portantly, we show empirically that word embed- dings and word clusters capture different informa- tion and their combination would further improve the adaptability of relation extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Regularization</head><p>Given the more general representations provided by word representations above, how can we learn a relation extractor from the labeled source domain data that generalizes well to new domains? In tra- ditional machine learning where the challenge is to utilize the training data to make predictions on unseen data points (generated from the same dis- tribution as the training data), the classifier with a good generalization performance is the one that not only fits the training data, but also avoids ov- efitting over it. This is often obtained via regular- ization methods to penalize complexity of classi- fiers. Exploiting the shared interest in generaliza- tion performance with traditional machine learn- ing, in domain adaptation for RE, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain 1 so that it could general- ize well on new domains. Eventually, regulariza- tion methods can be considered naturally as a sim- ple yet general technique to cope with DA prob- lems.</p><p>Following <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref>, we as- sume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data. Moreover, we consider the single- system DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains). This setting differs from most previous studies <ref type="bibr" target="#b2">(Blitzer et al., 2006</ref>) on DA which have attempted to de- sign a specialized system for every specific tar- get domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization <ref type="bibr" target="#b16">(Jiang and Zhai, 2007b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Although word embeddings have been success- fully employed in many NLP tasks <ref type="bibr" target="#b10">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b29">Turian et al., 2010;</ref><ref type="bibr" target="#b20">Maas and Ng, 2010)</ref>, the application of word embeddings in RE is very recent. <ref type="bibr" target="#b19">Kuksa et al. (2010)</ref> pro- pose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, <ref type="bibr" target="#b27">Socher et al. (2012)</ref> and <ref type="bibr" target="#b18">Khashabi (2013)</ref> use pre-trained word embed- dings as input for Matrix-Vector Recursive Neu- ral Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper.</p><p>Regarding domain adaptation, in representation learning, <ref type="bibr" target="#b2">Blitzer et al. (2006)</ref> propose structural correspondence learning (SCL) while <ref type="bibr" target="#b14">Huang and Yates (2010)</ref> attempt to learn a multi-dimensional feature representation. Unfortunately, these meth- ods require unlabeled target domain data which are unavailable in our single-system setting of DA. <ref type="bibr" target="#b11">Daumé III (2007)</ref> proposes an easy adaptation framework (EA) which is later extended to a semi- supervised version (EA++) to incorporate unla-beled data <ref type="bibr" target="#b12">(Daumé III et al., 2010)</ref>. In terms of word embeddings for DA, recently, Xiao and Guo (2013) present a log-bilinear language adaptation framework for sequential labeling tasks. However, these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA. Above all, we move one step further by evaluating the effectiveness of word em- beddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Word Representations</head><p>We consider two types of word representations and use them as additional features in our DA sys- tem, namely Brown word clustering <ref type="bibr" target="#b6">(Brown et al., 1992</ref>) and word embeddings ( <ref type="bibr" target="#b0">Bengio et al., 2001</ref>). While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, low- dimensional, and real-valued vectors (distributed representations). Each dimension of the word em- beddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities ( <ref type="bibr" target="#b29">Turian et al., 2010)</ref>. We investigate word embeddings induced by two typical language models: Collobert and Weston <ref type="formula">(2008)</ref>    <ref type="bibr" target="#b33">Zhou et al., 2005</ref>) plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the human-annotated (gold-standard) information on entity and mention types which is often miss- ing or noisy in reality <ref type="bibr" target="#b24">(Plank and Moschitti, 2013)</ref>. This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the set- tings of <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref>, we will only assume entity boundaries and not rely on the gold standard information in the experiments. We ap- ply the same feature set as Sun et al. (2011) but remove the entity and mention type information 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Lexical Feature Augmentation</head><p>While <ref type="bibr" target="#b28">Sun et al. (2011)</ref> show that adding word clusters to the heads of the two mentions is the most effective way to improve the generaliza- tion accuracy, the right lexical features into which word embeddings should be introduced to obtain the best adaptability improvement are unexplored. Also, which dimensionality of which word embed- ding should we use with which lexical features? In order to answer these questions, following <ref type="bibr" target="#b28">Sun et al. (2011)</ref>, we first group lexical features into 4 groups and rank their importance based on linguis- tic intuition and illustrations of the contributions of different lexical features from various feature- based RE systems. After that, we evaluate the ef- fectiveness of these lexical feature groups for word embedding augmentation individually and incre- mentally according to the rank of importance. For each of these group combinations, we assess the system performance with different numbers of di- mensions for both C&amp;W and HLBL word embed- dings. Let M1 and M2 be the first and second men- tions in the relation.  <ref type="table" target="#tab_0">Table 1</ref>: Lexical feature groups ordered by importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Tools and Data</head><p>Our relation extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al., 2011) and apply maximum entropy (MaxEnt) in the MALLET 3 toolkit as the machine learning tool. For Brown word clusters, we directly apply the clustering trained by <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref> In-domain (bn+nw)</p><p>Out-of-domain (bc development set) System C&amp;W,25 C&amp;W,50 C&amp;W,100 HLBL,50 HLBL,100 C&amp;W,25 C&amp;W,50 C&amp;W,100 HLBL,50 HLBL,100 1 Baseline 51.4 51.4 51.4 51.4 51.4 49.0 49.0 49.0 49.0 49.0 2 1+HM ED 54.0(+2.6) 54.1(+2.7) 55.7(+4.3) 53.7(+2.3) 55.2(+3.8) 51.5(+2.5) 52.7(+3.7) 52.5(+3.5) 50.2(+1.2) 50.6(+1.6) 3 1+BagWM ED 52.3(+0.9) 50.9(-0.5) 51.5(+0.1) 51.8(+0.4) 52.5(+1.1) 48.5(-0.5) 48.9(-0.1) 48.6(-0.4) 48.7(-0.3) 49.0(+0.0) 4 1+HC ED 51.3(-0.1) 50.9(-0.5) 48.3(-3.1) 50.8(-0.6) 49.8(-1.6) 44.9(-4.1) 45.8(-3.2) 45.8(-3.2) 48.7(-0.3) 47.3(-1.7) 5 1+BagWC ED 51.5(+0.1) 50.8(-0.6) 49.5(-1.9) 51.4(+0.0) 50.3(-1.1) 48.3(-0.7) 46.3(-2.7) 44.0(-5.0) 46.6(-2.4) 44.8(-4.2) 6 2+BagWM ED 54.3(+2.9) 53.2(+1.8) 53.2(+1.8) 54.0(+2.6) 53.8(+2.4) 52.5(+3.5) 51.4(+2.4) 50.6(+1.6) 50.0(+1.0) 48.6(-0.4) 7 6+HC ED 53.4(+2.0) 52.3(+0.9) 52.7(+1.3) 54.2(+2.8) 53.1(+1.7) 50.5(+1.5) 50.9(+1.9) 48.4(-0.6) 50.0(+1.0) 48.9(-0.1) 8 7+BagWC ED 53.4(+2.0) 52.2(+0.8) 50.8(-0.6) 53.5(+2.1) 53.6(+2.2) 49.2(+0.2) 50.7(+1.7) 49.2(+0.2) 47.9(-1.1) 49.5(+0.5) <ref type="table" target="#tab_1">Table 2</ref>: In-domain and Out-of-domain performance for different embedding features. The cells in bold are the best results.</p><p>to facilitate system comparison later. We evalu- ate C&amp;W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in <ref type="bibr" target="#b29">Turian et al. (2010)</ref> and can be downloaded here <ref type="bibr">4</ref> . The fact that we utilize the large, general and unbiased resources generated from the previ- ous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single-system DA.</p><p>We use the ACE 2005 corpus for DA experi- ments (as in <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref>). It in- volves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard prac- tices on ACE <ref type="bibr" target="#b24">(Plank and Moschitti, 2013)</ref> and use news (the union of bn and nw) as the source do- main and bc, cts and wl as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref>, the distributions of re- lations as well as the vocabularies of the domains are quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation of Word Embedding Features</head><p>We investigate the effectiveness of word embed- dings on lexical features by following the proce- dure described in Section 5.2. We test our system on two scenarios: In-domain: the system is trained and evaluated on the source domain (bn+nw, 5- fold cross validation); Out-of-domain: the system is trained on the source domain and evaluated on the target development set of bc (bc dev). suffix ED in lexical group names is to indicate the embedding features).</p><p>From the tables, we find that for C&amp;W and HLBL embeddings of 50 and 100 dimensions, the most effective way to introduce word embeddings is to add embeddings to the heads of the two men- tions (row 2; both in-domain and out-of-domain) although it is less pronounced for HLBL embed- ding with 50 dimensions. Interestingly, for C&amp;W embedding with 25 dimensions, adding the em- bedding to both heads and words of the two men- tions (row 6) performs the best for both in-domain and out-of-domain scenarios. This is new com- pared to the word cluster features where the heads of the two mentions are always the best places for augmentation <ref type="bibr" target="#b28">(Sun et al., 2011)</ref>. It suggests that a suitable amount of embeddings for words in the mentions might be useful for the augmentation of the heads and inspires further exploration. Intro- ducing embeddings to words of mentions alone has mild impact while it is generally a bad idea to augment chunk heads and words in the contexts.</p><p>Comparing C&amp;W and HLBL embeddings is somehow more complicated. For both in-domain and out-of-domain settings with different num- bers of dimensions, C&amp;W embedding outperforms HLBL embedding when only the heads of the mentions are augmented while the degree of neg- ative impact of HLBL embedding on chunk heads as well as context words seems less serious than C&amp;W's. Regarding the incremental addition of features (rows 6, 7, 8), C&amp;W is better for the out- of-domain performance when 50 dimensions are used, whereas HLBL (with both 50 and 100 di- mensions) is more effective for the in-domain set- ting. For the next experiments, we will apply the C&amp;W embedding of 50 dimensions to the heads of the mentions for its best out-of-domain perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Domain Adaptation with Word</head><p>Embeddings This section examines the effectiveness of word representations for RE across domains. We evalu- ate word cluster and embedding (denoted by ED) features by adding them individually as well as simultaneously into the baseline feature set. For word clusters, we experiment with two possibil- ities: (i) only using a single prefix length of 10 (as Plank and Moschitti (2013) did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC). <ref type="table" target="#tab_3">Table 3</ref> presents the system performance (F measures) for both in-domain and out-of-domain settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>In  The key observations from the table are:</p><p>(i): The baseline system achieves a performance of 51.4% within its own domain while the per- formance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively. Our base- line performance is worse than that of <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref> only on the target domain cts and better in the other cases. This might be explained by the difference between our baseline feature set and the feature set underlying their kernel-based system. However, the performance order across domains of the two baselines are the same. Be- sides, the baseline performance is improved over all target domains when the system is enriched with word cluster features of the 10 prefix length only (row 2).</p><p>(ii): Over all the target domains, the perfor- mance of the system augmented with word cluster features of various granularities (row 3) is supe- rior to that when only cluster features for the pre- fix length 10 are added (row 2). This is significant (at confidence level ≥ 95%) for domains bc and wl and verifies our assumption that various granu- larities for word cluster features are more effective than a single granularity for domain adaptation of RE.</p><p>(iii): Row 4 shows that word embedding itself is also very useful for domain adaptation in RE since it improves the baseline system for all the target domains.</p><p>(iv): In row 5, we see that the addition of both word cluster and word embedding features im- proves the system further and results in the best performance over all target domains (this is sig- nificant with confidence level ≥ 95% in domains bc and wl). The result suggests that word embed- dings seem to capture different information from word clusters and their combination would be ef- fective to generalize relation extractors across do- mains. However, in domain cts, the improvement that word embeddings provide for word clusters is modest. This is because the RCV1 corpus used to induce the word embeddings ( <ref type="bibr" target="#b29">Turian et al., 2010)</ref> does not cover spoken language words in cts very well.</p><p>(v): Finally, the in-domain performance is also improved consistently demonstrating the robust- ness of word representations <ref type="bibr" target="#b24">(Plank and Moschitti, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Domain Adaptation with Regularization</head><p>All the experiments we have conducted so far do not apply regularization for training. In this sec- tion, in order to evaluate the effect of regulariza- tion on the generalization capacity of relation ex- tractors across domains, we replicate all the ex- periments in Section 6.3 but apply regularization when relation extractors are trained <ref type="bibr">6</ref> .  All the improvements over the baseline in <ref type="table" target="#tab_4">Table 4</ref> are signif- icant at confidence level ≥ 95%.</p><p>For this experiment, every statement in (ii), (iii), (iv) and (v) of Section 6.3 also holds. More impor- tantly, the performance in every cell of <ref type="table" target="#tab_4">Table 4</ref> is significantly better than the corresponding cell in <ref type="table" target="#tab_3">Table 3</ref> (5% or better gain in F measure, a sig- nificant improvement at confidence level ≥ 95%). This demonstrates the effectiveness of regulariza- tion for RE in general and for domain adaptation of RE specifically.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>embeddings (C&amp;W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-bilinear embed- dings (HLBL) (Mnih and Hinton, 2007; Mnih and Hinton, 2009; Turian et al., 2010).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5</head><label></label><figDesc>Feature Set 5.1 Baseline Feature Set Sun et al. (2011) utilize the full feature set from (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 describes</head><label>1</label><figDesc>the lexical feature groups.</figDesc><table>Rank Group 
Lexical Features 
1 
HM 
HM1 (head of M1) 
HM2 (head of M2) 
2 
BagWM 
WM1 (words in M1) 
WM2 (words in M2) 
3 
HC 
heads of chunks in context 
4 
BagWC 
words of context 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 presents the F measures of this experiment 5 (the</head><label>2</label><figDesc></figDesc><table>4 http://metaoptimize.com/projects/ 
wordreprs/ 
5 All the in-domain improvement in rows 2, 6, 7 of Table 
2 are significant at confidence levels ≥ 95%. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Domain Adaptation Results with Word Represen- tations. All the improvements over the baseline in Table 3 are significant at confidence level ≥ 95%.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 presents the results.</head><label>4</label><figDesc></figDesc><table>System 
In-domain 
bc 
cts 
wl 
Baseline(B) 56.2 
55.5 
48.7 
42.2 
B+WC10 57.5(+1.3) 57.3(+1.8) 52.3(+3.6) 45.0(+2.8) 
B+WC 
58.9(+2.7) 58.4(+2.9) 52.8(+4.1) 47.3(+5.1) 
B+ED 
58.9(+2.7) 59.5(+4.0) 52.6(+3.9) 48.6(+6.4) 
B+WC+ED 59.4(+3.2) 59.8(+4.3) 52.9(+4.2) 49.7(+7.5) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Domain Adaptation Results with Regularization.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> domain overfitting (Jiang and Zhai, 2007b)</note>

			<note place="foot" n="2"> We have the same observation as Plank and Moschitti (2013) that when the gold-standard labels are used, the impact of word representations is limited since the goldstandard information seems to dominate. However, whenever the gold labels are not available or inaccurate, the word representations would be useful for improving adaptability performance. Moreover, in all the cases, regularization methods are still effective for domain adaptation of RE. 3 http://mallet.cs.umass.edu/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS&apos;13)</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="932" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Structural Correspondence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, Boom-boxes, and Blenders: Domain Adaptation for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Coupled Subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zamanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class-Based n-gram Models of Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Shortest Path Dependency Kenrel for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Subsequence Kernels for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting Background Knowledge for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="152" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Unied Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-regularization Based Semi-supervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<title level="m">NYU&apos;s English ACE 2005 System Description. ACE 2005 Evaluation Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring Representation-Learning Approaches to Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>the 2010 Workshop on Domain Adaptation for Natural Language Processing<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Systematic Exploration of the Feature Space for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT&apos;07)</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Two-stage Approach to Domain Adaptation for Statistical Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM 16th Conference on Information and Knowledge Management (CIKM&apos;07)</title>
		<meeting>the ACM 16th Conference on Information and Knowledge Management (CIKM&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-04</title>
		<meeting>ACL-04</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the Recursive Neural Networks for Relation Extraction and Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-05" />
			<publisher>UIUC</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-Supervised Abstraction-Augmented String Kernel for Multi-Level Bio-Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases, Part II (ECML PKDD&apos;10)</title>
		<meeting>the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases, Part II (ECML PKDD&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="128" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Probabilistic Model for Semantic Word Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Three new Graphical Models for Statistical Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML&apos;07</title>
		<meeting>ICML&apos;07<address><addrLine>Corvallis, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Scalable Hierarchical Distributed Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Truc-Vien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 09</title>
		<meeting>EMNLP 09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2013</title>
		<meeting>the ACL 2013<address><addrLine>Sofia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
<note type="report_type">Bulgaria</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploiting Constituent Ddependencies for Tree Kernel-based Semantic Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<publisher>Manchester</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
	<note>Qiaoming Zhu and Peide Qian</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FLORS: Fast and Simple Domain Adaptation for Part-ofSpeech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1526</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP-CoNLL&apos;12</title>
		<meeting>EMNLP-CoNLL&apos;12<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised Relation Extraction with Largescale Word Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLHLT</title>
		<meeting>ACLHLT<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL&apos;10)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL&apos;10)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kernel Methods for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10831106</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL-06</title>
		<meeting>COLING-ACL-06<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="825" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring various Knowledge in Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL&apos;05</title>
		<meeting>ACL&apos;05<address><addrLine>Ann Arbor, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
