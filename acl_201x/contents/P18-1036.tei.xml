<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Character-Level Models versus Morphology in Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözde</forename><forename type="middle">G ¨</forename><surname>Ul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">Technische Universität Darmstadt Darmstadt</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">¸</forename><surname>Ahin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">Technische Universität Darmstadt Darmstadt</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
							<email>steedman@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Character-Level Models versus Morphology in Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="386" to="396"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>386</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Encoding of words is perhaps the most impor- tant step towards a successful end-to-end natural language processing application. Although word embeddings have been shown to provide bene- fit to such models, they commonly treat words as the smallest meaning bearing unit and assume that each word type has its own vector repre- sentation. This assumption has two major short- comings especially for languages with rich mor- phology: (1) inability to handle unseen or out-of- vocabulary (OOV) word-forms (2) inability to ex- ploit the regularities among word parts.</p><p>The limitations of word embeddings are par- ticularly pronounced in sentence-level semantic tasks, especially in languages where word parts play a crucial role. Consider the Turkish sen- tences "Köy+ï u-ler (villagers) s ¸ehr+e (to town) geldi (came)" and "Sendika+lı-lar (union mem- bers) meclis+e (to council) geldi (came)". Here the stems köy (village) and sendika (union) func- tion similarly in semantic terms with respect to the verb come (as the origin of the agents of the verb), where s ¸ehir (town) and meclis (council) both func- tion as the end point. These semantic similar- ities are determined by the common word parts shown in bold. However ortographic similarity does not always correspond to semantic similar- ity. For instance the ortographically similar words knight and night have large semantic differences. Therefore, for a successful semantic application, the model should be able to capture both the regu- larities, i.e, morphological tags and the irregulari- ties, i.e, lemmas of the word.</p><p>Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Character- level models (CLM), being a cheaper and acces- sible alternative to morphology, have been re- ported as performing competitively on various NLP tasks ( <ref type="bibr" target="#b14">Ling et al., 2015;</ref><ref type="bibr" target="#b18">Plank et al., 2016;</ref>. However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underly- ing morphological structure of a word and their se- mantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of long- range dependencies, reaction to model complex- ity or performance on out-of-domain data are un- known. Analyzing such issues is a key to fully understanding the character-level models.</p><p>To achieve this, we perform a case study on semantic role labeling (SRL), a sentence- level semantic analysis task that aims to identify predicate-argument structures and assign mean- ingful labels to them as follows:</p><p>[Villagers] comers came [to town] end point We use a simple method based on bidirectional LSTMs to train three types of base semantic role labelers that employ (1) words (2) characters and character sequences and (3) gold morphological analysis. The gold morphology serves as the up- per bound for us to compare and analyze the per- formances of character-level models on languages of varying morphological typologies. We carry out an exhaustive error analysis for each language type and analyze the strengths and limitations of character-level models compared to morphology. In regard to the diversity hypothesis which states that diversity of systems in ensembles lead to further improvement, we combine character and morphology-level models and measure the perfor- mance of the ensemble to better understand how similar they are.</p><p>We experiment with several languages with varying degrees of morphological richness and ty- pology: Turkish, Finnish, Czech, German, Span- ish, Catalan and English. Our experiments and analysis reveal insights such as:</p><p>• CLMs provide great improvements over whole-word-level models despite not be- ing able to match the performance of morphology-level models (MLMs) for in- domain datasets. However their performance surpass all MLMs on out-of-domain data,</p><p>• Limitations and strengths differ by morpho- logical typology. Their limitations for agglu- tinative languages are related to rich deriva- tional morphology and high contextual am- biguity; whereas for fusional languages they are related to number of morphological tags (morpheme ambiguity) ,</p><p>• CLMs can handle long-range dependencies equally well as MLMs,</p><p>• In presence of more training data, CLM's performance is expected to improve faster than of MLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Neural SRL Methods: Neural networks have been first introduced to the SRL scene by <ref type="bibr" target="#b2">Collobert et al. (2011)</ref>, where they use a unified end-to-end convolutional network to perform vari- ous NLP tasks. Later, the combination of neural networks (LSTMs in particular) with traditional SRL features (categorical and binary) has been in- troduced ( <ref type="bibr" target="#b4">FitzGerald et al., 2015)</ref>. Recently, it has been shown that careful design and tuning of deep models can achieve state-of-the-art with no or minimal syntactic knowledge for English and Chinese SRL. Although the architectures vary slightly, they are mostly based on a variation of bi-LSTMs. <ref type="bibr">Zhou and Xu (2015)</ref>; <ref type="bibr" target="#b11">He et al. (2017)</ref> connect the layers of LSTM in an interleaving pat- tern where in ( <ref type="bibr" target="#b14">Wang et al., 2015;</ref>) regular bi-LSTM layers are used.</p><p>Commonly used features for the encoding layer are: pretrained word embeddings; distance from the predicate; predicate context; predicate region mark or flag; POS tag; and predicate lemma em- bedding. Only a few of the models  perform dependency-based SRL. Furthermore, all methods focus on languages with rich resources and less morphological complexity like English and Chinese.</p><p>Character-level Models: Character-level mod- els have proven themselves useful for many NLP tasks such as language modeling ( <ref type="bibr" target="#b14">Ling et al., 2015;</ref><ref type="bibr" target="#b12">Kim et al., 2016)</ref>, POS tagging ( <ref type="bibr" target="#b20">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b18">Plank et al., 2016)</ref>, dependency parsing ( <ref type="bibr" target="#b3">Dozat et al., 2017</ref>) and machine trans- lation ( . However the number of comparative studies that analyze their relation to morphology are rather limited. Recently, Va- nia and Lopez (2017) presented a unified frame- work, where they investigated the performances of different subword units, namely characters, mor- phemes and morphological analysis on language modeling task. They experimented with lan- guages of varying morphological typologies and concluded that the performance of character mod- els can not yet match the morphological models, albeit very close. Similarly, <ref type="bibr" target="#b1">Belinkov et al. (2017)</ref> analyzed how different word representations help learn better morphology and model rare words on a neural MT task and concluded that character- based representations are much better for learning morphology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Formally, we generate a label sequence l for each sentence and predicate pair: (s, p). Each l t ∈ l is chosen from L = {roles ∪ nonrole}, where roles are language-specific semantic roles (mostly con- sistent with PropBank) and nonrole is a symbol to present tokens that are not arguments. Given θ as model parameters and g t as gold label for t th token, we find the parameters that minimize the negative log likelihood of the sequence:</p><formula xml:id="formula_0">ˆ θ = arg min θ − n t=1 log(p(g t |θ, s, p))<label>(1)</label></formula><p>Label probabilities, p(l t |θ, s, p), are calculated with equations given below.First, the word encod- ing layer splits tokens into subwords via ρ func- tion.</p><formula xml:id="formula_1">ρ(w) = s 0 , s 1 , .., s n<label>(2)</label></formula><p>As proposed by <ref type="bibr" target="#b14">Ling et al. (2015)</ref>, we treat words as a sequence of subword units. Then, the sequence is fed to a simple bi-LSTM net- work ( <ref type="bibr" target="#b6">Graves and Schmidhuber, 2005;</ref><ref type="bibr" target="#b5">Gers et al., 2000</ref>) and hidden states from each direction are weighted with a set of parameters which are also learned during training. Finally, the weighted vec- tor is used as the word embedding given in Eq. 4.</p><formula xml:id="formula_2">hs f , hs b = bi-LSTM(s 0 , s 1 , .., s n ) (3) w = W f · hs f + W b · hs b + b<label>(4)</label></formula><p>There may be more than one predicate in the sen- tence so it is crucial to inform the network of which arguments we aim to label. In order to mark the predicate of interest, we concatenate a predi- cate flag pf t to the word embedding vector.</p><formula xml:id="formula_3">x t = [ w; pf t ]<label>(5)</label></formula><p>Final vector, x t serves as an input to another bi- LSTM unit.</p><formula xml:id="formula_4">h f , h b = bi-LSTM(x t )<label>(6)</label></formula><p>Finally, the label distribution is calculated via soft- max function over the concatenated hidden states from both directions.</p><formula xml:id="formula_5">p(l t |s, p) = sof tmax(W l · [ h f ; h b ] + b l ) (7)</formula><p>For simplicity, we assign the label with the highest probability to the input token. 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subword Units</head><p>We use three types of units: <ref type="formula" target="#formula_0">(1)</ref> words <ref type="formula" target="#formula_1">(2)</ref> char- acters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. <ref type="table">Table 1</ref> shows sample out- puts of various ρ functions. Here, char function  <ref type="table">Table 1</ref>: Sample outputs of different ρ functions simply splits the token into its characters. Similar to n-gram language models, char3 slides a char- acter window of width n = 3 over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this pa- per, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all lan- guages, morph outputs the lemma of the token fol- lowed by language specific morphological tags.</p><formula xml:id="formula_6">ρ word output char available &lt;-a-v-a-i-l-a-b-l-e-&gt; char3 available &lt;av-ava-vai-ail-ila-lab-abl-ble-le&gt; morph-DEU prächtiger [prächtig;</formula><p>As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfes- sor and Byte Pair Encoding (BPE) are other com- monly used subword units. Due to low scores ob- tained from our preliminary experiments and un- satisfactory results from previous studies (Vania and Lopez, 2017), we excluded these units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We use the datasets distributed by LDC for Cata- lan (CAT), Spanish (SPA), German (DEU), Czech (CZE) and English (ENG) (   <ref type="formula" target="#formula_0">(2015)</ref>; S ¸ ahin and Adalı (2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>To fit the requirements of the SRL task and of our model, we performed the following:</p><p>Spanish, Catalan: Multiword expressions (MWE) are represented as a single token, (e.g., Confederación Francesa del Trabajo), that causes notably long character sequences which are hard to handle by LSTMs. For the sake of memory efficiency and performance, we used an abbreviation (e.g., CFdT) for each MWE during training and testing.</p><p>Finnish: Original dataset defines its own format of semantic annotation, such as 17:PBArgM mod|19:PBArgM mod meaning the node is an argument of 17 th and 19 th tokens with ArgM-mod (temporary modifier) semantic role. They have been converted into CoNLL-09 tabular format, where each predicate's arguments are given in a specific column.</p><p>Turkish: Words are splitted from derivational boundaries in the original dataset, where each in- flectional group is represented as a separate token. We first merge boundaries of the same word, i.e, tokens of the word, then we use our own ρ func- tion to split words into subwords.</p><p>Training and Evaluation: We lowercase all to- kens beforehand and place special start and end of the token characters. For all experiments, we ini- tialized weight parameters orthogonally and used one layer bi-LSTMs both for subword composi- tion and argument labeling with hidden size of 200. Subword embedding size is chosen as 200. We used gradient clipping and early stopping to prevent overfitting. Stochastic gradient descent is used as the optimizer. The initial learning rate is set to 1 and reduced by half if scores on develop- ment set do not improve after 3 epochs. We use the provided splits and evaluate the results with the official evaluation script provided by CoNLL- 09 shared task. In this work (and in most of the recent SRL works), only the scores for argument labeling are reported, which may cause confusions for the readers while comparing with older SRL studies. Most of the early SRL work report com- bined scores (argument labeling with predicate sense disambiguation (PSD)). However, PSD is considered a simpler task with higher F1 scores 3 . Therefore, we believe omitting PSD helps us gain more useful insights on character level models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>Our main results on test and development sets for models that use words, characters (char), char- acter trigrams (char3) and morphological analy- ses (morph) are given in <ref type="table" target="#tab_4">Table 3</ref>. We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set.</p><p>The biggest improvement over the word base- line is achieved by the models that have access to morphology for all languages (except for English) as expected. Character trigrams consistently out- performed characters by a small margin. Same pattern is observed on the results of the develop- ment set. IOW has the values between 0% to 38% while IOC values range between 2%-10% depen- dending on the properties of the language and the dataset. We analyze the results separately for ag- glutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values.    Agglutinative languages have many mor- phemes attached to a word like beads on a string. This leads to high number of OOV words and cause word lookup models to fail. Hence, the highest IOWs by character models are achieved on these languages: Finnish and Turkish. This language family has one-to-one morpheme to meaning mapping with small orthographic differences (e.g., mıs¸,mıs¸, mis¸,mis¸, mus¸,mus¸, müsmüs¸müs¸for past perfect tense), that can be easily extracted from the data. Even though each morpheme has only one interpretation, each word (consisting of many morphemes) has usually more than one. For instance two pos- sible analyses for the Turkish word "dolar" are (1) "dol+Verb+Positive+Aorist+3sg" (it fills), (2) "dola+Verb+Positive+Aorist+3sg" (he/she wraps). For a syntactic task, models are not obliged to learn the difference between the two; whereas for a semantic task like SRL, they are. We will refer to this issue as contextual ambiguity. Another important linguistic issue for agglutinative languages is the complex interac- tion between morphology and syntax, which is usually achieved via derivational morphemes. In other words, unlike inflectional morphemes that only give information on word-level semantics, derivational morphemes provide more clues on sentence-level semantics. The effects of these two phenomena on model performances is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Scores given in <ref type="figure" target="#fig_1">Fig. 1</ref> are absolute F1 scores for each model. For the analysis in <ref type="figure" target="#fig_1">Fig. 1a</ref>, we separately calculated F1 scores of each model on words that have been observed with at least two different set of morphological features (ambiguous), and one set of features (non-ambiguous). Due to the low number of am- biguous words in Turkish dataset (≤100), it has been calculated for Finnish only. Similarly, for the derivational morphology analysis in <ref type="figure" target="#fig_1">Fig. 1b</ref>, we have separately calculated scores for sentences containing derived words (derivational), and simple sentences without any derivations. Both analyses show that access to gold morphological tags (oracle) provided big performance gains on arguments with contextual ambiguity and sentences with derived words. Moderate IOC signals that char and char3 learns to imitate the "beads" and their "predictable order" on the string (in the absence of the aforementioned issues). Fusional languages may have many mor- phemes in a word. Spanish and Catalan have relatively low morpheme per word ratio that re- sults with low OOV% (5.63 and 5.40 for Span- ish and Catalan respectively); whereas, German and Czech have OOV% of 7.93 and 7.98 ). We observe that IOW by character models are well aligned with OOV percentages of the datasets. Unlike agglutinative languages, sin- gle morpheme can serve multiple purposes in fu- sional languages. For instance, "o" (e.g., habl-o) may signal 1 st person singular present tense, or 3 rd person singular past tense. We count the num- ber of surface forms with at least two different fea- tures and use their ratio (#ambiguous forms/#total forms) as a proxy to morphological complexity of the language. The complexities are approximated as 22%, 16%, 15% for Czech, Spanish and Cata- lan respectively; which are aligned with the ob- served IOCs. Since there is no unique morpheme to meaning mapping, generally multiple morpho- logical tags are used to resolve the morpheme am- biguity. Therefore there is an indirect relation be- tween the number of morphological tags used and the ambiguity of the word. To demonstrate this phenomena, we calculate targeted F1 scores on arguments with varying number of morphologi- cal features. Results using feature bins of <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref>, <ref type="bibr">[3]</ref><ref type="bibr">[4]</ref> and <ref type="bibr">[5]</ref><ref type="bibr">[6]</ref> are given in <ref type="figure" target="#fig_2">Fig. 2</ref>. As the num- ber of features increase, the performance gap be- tween oracle and character models grows dramati- cally for Czech and Spanish, while it stays almost fixed for Finnish. This finding suggests that high number of morphological tags signal the vague- ness/complex cases in fusional languages where character models struggle; and also shows that the complexity can not be directly explained by num- ber of morphological tags for agglutinative lan- guages. German is known for having many com- pound words and compound lemmas that lead to high OOV% for lemma; and also is less ambigu- ous (9%). Therefore we would expect a lower IOC. However, the evaluation set consists only of 550 predicates and 1073 arguments, hence small changes in prediction lead to dramatic percentage changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Similarity between models</head><p>One way to infer similarity is to measure diver- sity. Consider a set of baseline models that are not diverse, i.e., making similar errors with sim- ilar inputs. In such a case, combination of these models would not be able to overcome the biases of the learners, hence the combination would not achieve a better result. In order to test if character and morphological models are similar, we com- bine them and measure the performance of the en- semble. Suppose that a prediction p i is generated for each token by a model m i , i ∈ n, then the final prediction is calculated from these predictions by:</p><formula xml:id="formula_7">p f inal = f (p 0 , p 1 , .., p n |φ) (8)</formula><p>where f is the combining function with parame- ter φ. The simplest global approach is averaging (AVG), where f is simply the mean function and p i s are the log probabilities. Mean function com- bines model outputs linearly, therefore ignores the nonlinear relation between base models/units. In order to exploit nonlinear connections, we learn the parameters φ of f via a simple linear layer fol- lowed by sigmoid activation. In other words, we train a new model that learns how to best combine the predictions from subword models. This en- semble technique is generally referred to as stack- ing or stacked generalization (SG). <ref type="bibr">4</ref> Although not guaranteed, diverse models can be achieved by altering the input representation,   <ref type="table">Table 4</ref>: Results of ensembling via averaging (Avg) and stack generalization (SG). IOB: Improvement Over Best of baseline models the learning algorithm, training data or the hyper- parameters. To ensure that the only factor con- tributing to the diversity of the learners is the input representation, all parameters, training data and model settings are left unchanged.</p><p>Our results are given in <ref type="table">Table 4</ref>. IOB shows the improvement over the best of the baseline models in the ensemble. Averaging and stack- ing methods gave similar results, meaning that there is no immediate nonlinear relations between units. We observe two language clusters: (1) Czech and agglutinative languages (2) Spanish, Catalan, German and English. The common prop- erty of that separate clusters are (1) high OOV% and (2) relatively low OOV%. Amongst the first set, we observe that the improvement gained by character-morphology ensembles is higher (shown with green) than ensembles between characters and character trigrams (shown with red), whereas the opposite is true for the second set of languages. It can be interpreted as character level models be- ing more similar to the morphology level mod- els for the first cluster, i.e., languages with high OOV%, and characters and morphology being more diverse for the second cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations and Strengths</head><p>To expand our understanding and reveal the limita- tions and strengths of the models, we analyze their ability to handle long range dependencies, their re- lation with training data and model size; and mea- sure their performances on out of domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Long Range Dependencies</head><p>Long range dependency is considered as an impor- tant linguistic issue that is hard to solve. Therefore the ability to handle it is a strong performance in- dicator. To gain insights on this issue, we mea- sure how models perform as the distance between the predicate and the argument increases. The unit of measure is number of tokens between the two; and argument is defined as the head of the argu- ment phrase in accordance with dependency-based SRL task. For that purpose, we created bins of <ref type="bibr">[0]</ref><ref type="bibr">[1]</ref><ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr">[4]</ref>, <ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr">[8]</ref><ref type="bibr">[9]</ref>, <ref type="bibr">[10]</ref><ref type="bibr">[11]</ref><ref type="bibr">[12]</ref><ref type="bibr">[13]</ref><ref type="bibr">[14]</ref> and <ref type="bibr">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref> distances. Then, we have calculate F1 scores for arguments in each bin. Due to low number of predicate-argument pairs in buckets, we could not analyze German and Turkish; and also the bin <ref type="bibr">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref> is only used for Czech. Our results are shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. We observe that either char or char3 closely follows the oracle for all languages. The gap between the two does not increase with the distance, suggest- ing that the performance gap is not related to long range dependencies. In other words, both charac- ters and the oracle handle long range dependencies equally well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training Data Size</head><p>We analyzed how char3 and oracle models per- form with respect to the training data size. For that purpose, we trained them on chunks of increas- ing size and evaluate on the provided test split. We used units of 2000 sentences for German and Czech; and 400 for Turkish. Results are shown in <ref type="figure">Fig. 4</ref>. Apparently as the data size increases, the performances of both models logarithmically increase -with a varying speed. To speak in statis- tical terms, we fit a logarithmic curve to the ob- served F1 scores (shown with transparent lines) and check the x coefficients, where x refers to the number of sentences. This coefficient can be con- sidered as an approximation to the speed of growth with data size. We observe that the coefficient is higher for char3 than oracle for all languages. It can be interpreted as: in the presence of more training data, char3 may surpass the oracle; i.e., char3 relies on data more than the oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Out-of-Domain (OOD) Data</head><p>As part of the CoNLL09 shared task ), out of domain test sets are provided for   three languages: Czech, German and English. We test our models trained on regular training dataset on these OOD data. The results are given in <ref type="table" target="#tab_7">Ta- ble 5</ref>. Here, we clearly see that the best model has shifted from oracle to character based models. The dramatic drop in German oracle model is due to the high lemma OOV rate which is a consequence of keeping compounds as a single lemma. Czech oracle model performs reasonably however is un- able to beat the generalization power of the char3 model. Furthermore, the scores of the character models in <ref type="table" target="#tab_7">Table 5</ref> are higher than the best OOD scores reported in the shared task ; even though our main results on evaluation set are not (except for Czech). This shows that character-level models have increased robustness to out-of-domain data due to their ability to learn regularities among data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Model Size</head><p>Throughout this paper, our aim was to gain in- sights on how models perform on different lan- guages rather than scoring the highest F1. For this reason, we used a model that can be consid- ered small when compared to recent neural SRL models and avoided parameter search. However,  we wonder how the models behave when given a larger network. To answer this question, we trained char3 and oracle models with more layers for two fusional languages (Spanish, Catalan), and two agglutinative languages (Finnish, Turkish). The results given in <ref type="table" target="#tab_9">Table 6</ref> clearly shows that model complexity provides relatively more benefit to morphological models. This indicates that mor- phological signals help to extract more complex linguistic features that have semantic clues.</p><formula xml:id="formula_8">char3 oracle F1 I (%) F1 I (%) Finnish = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Predicted Morphological Tags</head><p>Although models with access to gold morpho- logical tags achieve better F1 scores than char- acter models, they can be less useful a in real- life scenario since they require gold tags at test time. To predict the performance of morphology- level models in such a scenario, we train the same models with the same parameters with pre- dicted morphological features. Predicted tags were only available for German, Spanish, Catalan and Czech. Our results given in <ref type="figure" target="#fig_5">Fig. 5</ref>, show that (except for Czech), predicted morphological tags are not as useful as characters alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Character-level neural models are becoming the defacto standard for NLP problems due to their accessibility and ability to handle unseen data. In this work, we investigated how they compare to models with access to gold morphological analy- sis, on a sentence-level semantic task. We eval- uated their quality on semantic role labeling in a number of agglutinative and fusional languages.</p><p>Our results lead to the following conclusions:</p><p>• For in-domain data, character-level mod- els cannot yet match the performance of morphology-level models. However, they still provide considerable advantages over whole-word models,</p><p>• Their shortcomings depend on the morphol- ogy type. For agglutinative languages, their performance is limited on data with rich derivational morphology and high contextual ambiguity (morphological disambiguation); and for fusional languages, they struggle on tokens with high number of morphological tags,</p><p>• Similarity between character and morphology-level models is higher than the similarity within character-level (char and char-trigram) models on languages with high OOV%; and vice versa,</p><p>• Their ability to handle long-range dependen- cies is very similar to morphology-level mod- els,</p><p>• They rely relatively more on training data size. Therefore, given more training data their performance will improve faster than morphology-level models,</p><p>• They perform substantially well on out of do- main data, surpassing all morphology-level models. However, relatively less improve- ment is expected when model complexity is increased,</p><p>• They generally perform better than models that only have access to predicted/silver mor- phological tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>Gözde Gül S ¸ ahin was a PhD student at Istanbul Technical University and a visiting research stu- dent at University of Edinburgh during this study. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Differences in model performances on agglutinative languages</figDesc><graphic url="image-2.png" coords="5,77.46,244.11,207.36,128.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: x axis: Number of morphological features; y axis: Targeted F1 scores</figDesc><graphic url="image-3.png" coords="6,86.49,62.81,136.06,99.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: X axis: Distance between the predicate and the argument, Y axis: F1 scores on argument labels</figDesc><graphic url="image-6.png" coords="8,86.49,62.81,136.07,84.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: F1 scores for best-char (best of the CLMs) and model with predicted (predictedmorph) and gold morphological tags (goldmorph).</figDesc><graphic url="image-12.png" coords="9,79.88,62.81,202.50,125.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Training data statistics. A: Agglutinative, 
F: Fusional 

provided with syntactic dependency annotations 
and semantic roles of verbal predicates. In ad-
dition, English supplies nominal predicates anno-
tated with semantic roles and does not provide any 
morphological feature. Statistics for the training 
split for all languages are given in Table 2. Here, 
#pred is number of predicates, and #role refers 
to number distinct semantic roles that occur more 
than 10 times. More detailed statistics about the 
datasets can be found in Hajič et al. (2009); Haver-
inen et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F1 scores of word, character, character 
trigram and morphology models for argument la-
beling. Best F1 for each language is shown in 
bold. First row: results on test, Second row: re-
sults on development. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>F1 scores on out of domain data. Best scores are shown with bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Effect of layer size on model perfor-
mances. I: Improvement over model with one 
layer. 

</table></figure>

			<note place="foot" n="3"> For instance in English CoNLL-09 dataset, 87% of the predicates are annotated with their first sense, hence even a dummy classifier would achieve 87% accuracy. The best system from CoNLL-09 shared task reports 85.63 F1 on English evaluation dataset, however when the results of PSD are discarded, it drops down to 81.</note>

			<note place="foot" n="4"> To train the SG model, we have used one linear layer with 64 hidden units followed by sigmoid nonlinear activation. Weights are orthogonally initialized and optimized via adam algorithm with a learning rate of 0.02 for 25 epochs.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Annotation Process in the Turkish Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Nart Bedin Atalay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Say</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 4th International Workshop on Linguistically Interpreted Corpora, LINC at EACL</title>
		<meeting>4th International Workshop on Linguistically Interpreted Corpora, LINC at EACL<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-04-13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What do Neural Machine Translation Models Learn about Morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="page" from="861" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural Language Processing (almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2461" to="2505" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stanford&apos;s Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic Role Labeling with Neural Network Factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to Forget: Continual Prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><forename type="middle">A</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The CoNLL2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>CoNLL &apos;09</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<title level="m">CoNLL Shared Task</title>
		<editor>Pavel Straňák, Mihai Surdeanu, Nianwen Xue, and Yi Zhang</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Part 1 LDC2012T04. Web Download</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Straňák</surname></persName>
		</author>
		<title level="m">CoNLL Shared Task Part 1 LDC2012T03. Web Download</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Finnish Proposition Bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katri</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Missila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stina</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Viljanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="907" to="926" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and what&apos;s next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Character-Aware Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fully Character-Level Neural Machine Translation without Explicit Segmentation. TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="365" to="378" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1507" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a Turkish treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Say</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Zeynep Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gökhan</forename><surname>Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Treebanks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="261" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotation of semantic roles for the Turkish Proposition Bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözde</forename><surname>Gül</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Es¸refes¸ref</forename><surname>Adalı</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Implementing Universal Dependency, Morphology and Multiword Expression Annotation Standards for Turkish Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsgüls¸en</forename><surname>Eryi˘</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Turkish Journal of Electrical Engineering Computer Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">IMST: A Revisited Turkish Dependency Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsgüls¸en</forename><surname>Tu˘ Gba Pamay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eryi˘ Git</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Turkic Computational Linguistics (TurCLing) at CICLing</title>
		<meeting>the 1st International Conference on Turkic Computational Linguistics (TurCLing) at CICLing<address><addrLine>Konya, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From Characters to Words to in Between: Do We Capture Morphology?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="page" from="2016" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
