<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Metaphor Detection with Topic Transition, Emotion and Cognition in Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeju</forename><surname>Jang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlan</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">P</forename><surname>Rosé</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Metaphor Detection with Topic Transition, Emotion and Cognition in Context</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="216" to="225"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Metaphor is a common linguistic tool in communication, making its detection in discourse a crucial task for natural language understanding. One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text. While these methods are effective, they tend to overclassify target words as metaphorical when they deviate in meaning from its context. We present a new approach that (1) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and (2) captures the motivation of speakers to express emotions and abstract concepts metaphorically. Experiments on an on-line breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-the-art. These experimental results also reveal a tendency toward metaphor usage in personal topics and certain emotional contexts .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Figurative language is commonly used in human communication ranging from literature to every- day speech. One of the most common forms of non-literal language is metaphor, in which two dissimilar concepts are compared. In the ut- terance, "Time is money" <ref type="bibr" target="#b7">(Lakoff and Johnson, 1980)</ref>, for example, the concept of "time" is com- pared to "money" to emphasize that time is valu- able. Bringing in information from another do- main allows more effective ways of expressing thoughts, feelings, and ideas than only using lit- eral language.</p><p>Previous approaches to modeling metaphor have used either the semantic and syntactic in- formation in just the sentence that contains a metaphor <ref type="bibr" target="#b18">(Turney et al., 2011;</ref><ref type="bibr" target="#b17">Tsvetkov et al., 2014</ref>), or the context beyond a single sentence ( <ref type="bibr" target="#b12">Schulder and Hovy, 2014;</ref><ref type="bibr" target="#b6">Klebanov et al., 2015;</ref><ref type="bibr" target="#b4">Jang et al., 2015)</ref> to detect topical discrepancy between a candidate metaphor and the dominant theme (See Section 2 for more detailed literature review).</p><p>Although previous approaches were effective at capturing some aspects of the governing context of a metaphor, the space of how to best use the contextual information is still wide open. Previous context-based models tend to overclassify literal words as metaphorical if they find semantic con- trast with the governing context. These cases man- ifested in the work by <ref type="bibr" target="#b12">Schulder and Hovy (2014)</ref> and <ref type="bibr" target="#b4">Jang et al. (2015)</ref> as high recall but low preci- sion for metaphorical instances.</p><p>We present a new approach that uses lexical and topical context to resolve the problem of low pre- cision on metaphor detection. To better capture the relevant context surrounding a metaphor, we approach the problem in two directions. First, we hypothesize that topic transition patterns be- tween sentences containing metaphors and their contexts are different from that of literal sen- tences. To this end, we incorporate several indi- cators of sentence-level topic transitions as fea- tures, such as topic similarity between a sentence and its neighboring sentences, measured by Sen- tence LDA. Second, we observe that metaphor is often used to express speakers' emotional experi- ences; we therefore model a speaker's motivation in using metaphor by detecting emotion and cog- nition words in metaphorical and literal sentences and their contexts.</p><p>To demonstrate the efficacy of our approach, we evaluate our system on the metaphor detection task presented by <ref type="bibr" target="#b4">Jang et al. (2015)</ref> using a breast can- cer discussion forum dataset. This dataset is dis- tinct in that it features metaphors occurring in con- versational text, unlike news corpora or other for- mal texts typical in computational linguistics.</p><p>Our contributions are three-fold: (1) We ex- tend the previous approaches for contextually de- tecting metaphor by exploring topic transitions be- tween a metaphor and its context rather than only detecting lexical discrepancies. In addition, (2) we propose to capture emotional and cognitive con- tent to better uncover speakers' motivation for us- ing metaphors. Lastly, (3) through our empirical evaluation, we find that metaphor occurs more fre- quently around personal topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relation to Prior Work</head><p>Research in automatic metaphor detection has spanned from detecting metaphor in limited sets of syntactic constructions to studying the use of metaphor in discourse, with approaches ranging from rule-based methods using lexical resources to statistical machine learning models. Here, we focus in particular on approaches that use context wider than a sentence for metaphor detection. For a more thorough review of metaphor processing systems, refer to <ref type="bibr" target="#b13">Shutova (2015)</ref>.</p><p>The main idea behind using context in metaphor detection is that metaphorically used words tend to violate lexical cohesion in text. Different meth- ods, however, approach the problem of detecting semantic outliers in different ways. <ref type="bibr" target="#b14">Li and Sporleder (2009;</ref> identify metaphorical idioms using the idea that non-literal expressions break lexical cohesion of a text. <ref type="bibr" target="#b14">Li and Sporleder (2009)</ref> approached the problem by constructing a lexical cohesion graph. In the graph, content words in a text are represented as vertices, which are connected by edges repre- senting semantic relatedness. The intuition be- hind their approach was that non-literal expres- sions would lower the average semantic related- ness of the graph. To classify a word as literal or metaphorical, <ref type="bibr" target="#b8">Li and Sporleder (2010)</ref> use Gaus- sian Mixture Models with semantic similarity fea- tures, such as the relatedness between this target word and words in its context.  and  base their approach on the idea that metaphors are likely to be concrete words that are not semantically associated with the surrounding context.  implemented this idea using topic chains, which consist of noun phrases that are connected by pronominal men- tion, repetition, synonym, or hyponym relations.  build on this idea by taking nouns and adjectives around the target con- cept as candidate source relations. They filtered out candidate sources that were in the same topi- cal chain as the target concept or were not linked to the word being classified by a direct dependency path. <ref type="bibr" target="#b12">Schulder and Hovy (2014)</ref> also hypothesize that novel metaphors are marked by their unusualness in a given context. They use a domain-specific term relevance metric, which measures how typ- ical a term is for the domain associated with the literal usage of a word, and common relevance, which measures how common a word is across do- mains. If a term is neither typical for a text's do- main nor common, it is taken as a metaphor can- didate. A particular strength of this approach is its accommodation of common words without dis- criminative power, which often confuse context- based models. <ref type="bibr" target="#b4">Jang et al. (2015)</ref> model context by using both global context, the context of an entire post, and local context, the context within a sentence, in re- lationship to a word being classified as metaphor- ical or literal. They used word categories from FrameNet, topic distribution, and lexical chain in- formation (similar in concept to the topic chain in- formation in ) to model the contrast between a word and its global context. To model the contrast between a word and its local context, they used lexical concreteness, word cat- egories and semantic relatedness features. <ref type="bibr" target="#b11">Mohler et al. (2013)</ref> built a domain-aware se- mantic signature for a text to capture the con- text surrounding a metaphorical candidate. Un- like other approaches that try to discriminate metaphors from their context, their approach uses binary classifiers to compare the semantic signa- ture for a text with that of known metaphors.</p><p>The above approaches attempted to capture governing context in various ways and were ef- fective when applied to the problem of metaphor detection. However, these methods tend to over- classify literal instances as metaphorical when se- mantic cohesion is violated within their govern- ing contexts. Additionally, these methods could fail to detect extended metaphors, which span over wider contexts. In this paper, we specifically fo- cus on the problem of discriminating literal in- stances from metaphorical instances by expand- ing the scope of what is captured within a context. <ref type="bibr">Like (Mohler et al., 2013)</ref>, we share the intuition that there could be associations between specific metaphors and their contexts, but we relax the as- sumption that metaphors must be similar to known metaphors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>To better capture the distinctions between metaphorical and literal usages of the same word (target word), we approach the task in two directions. First, we model how topics in context change for both metaphorical and literal instances of a target word (Section 3.1). Second, we con- sider the situational context for why individuals choose to use metaphor (Section 3.2). We use multi-level modeling to combine these two types of features with the specific target word to model interactions between the features and a particular metaphor (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Transition</head><p>In writing, cohesion refers to the presence or ab- sence of explicit cues in the text that allow the reader to make connections between ideas <ref type="bibr" target="#b1">(Crossley and McNamara, 2010)</ref>. For example, over- lapping words and concepts between sentences indicate that the same ideas are being referred to across these sentences. Metaphorically used words tend to be semantically incohesive with the governing context. Therefore, determining seman- tic or topical cohesion is important for metaphor detection.</p><p>However, even if a text is literal and cohesive, not all words within the text are semantically re- lated. In example (1), a human could easily de- termine that "pillows", "music", "flickering can- dles", and "a foot massage" share the theme of relaxation. But it is difficult to define their re- latedness computationally -these terms are not synonyms, hypernyms, antonyms, or in any other well-defined lexical relation. Additionally, even if the whole sentence is correctly interpreted as ways of indulging oneself, it is still semantically contrasted with the surrounding sentences about medicine. In this example, the target word "can- dle" is used literally, but the contrast between the sentence containing the target word and its con- text makes it computationally difficult to deter- mine that it is not metaphorical:</p><p>(1) ... yet encouraged to hear you have a diagnosis and it's being treated.</p><p>Since you have to give up your scented stuff you'll just have to fig- ure out some very creative ways to indulge yourself. Soft pillows, relaxing music, flickering candles, maybe a foot massage. Let's hope your new pain relief strategy works and the Neulasta shot is not so bad . I never had Taxotere, but have read it can be much easier than AC for many people. ...</p><p>Example (2) also shows semantic inconsistency between the candidate metaphor "boat" and the surrounding sentences about medicine. However, in this example, "boat" is metaphorically used. Thus, it is difficult to determine whether a word is metaphorical or literal when there is semantic contrast because both example (1) and example <ref type="formula">(2)</ref> show semantic contrast.</p><p>(2) When my brain mets were discov- ered last year, I had to see a neu- rosurgeon. He asked if I under- stood that my treatment was palla- tive care. Boy, did it rock my boat to hear that phrase! I agree with Fitz, pallative treatment is to help with pain and alleviate symp- toms.....but definitely different than hospice care.</p><p>The primary difference between these two ex- amples is in the nature of the semantic contrast. In example (1), the topic of the sentence containing "candle" is relaxation, while the topic of the pre- vious and following sentences is medicine. The transition between medicine and relaxation tends to be more literal, whereas the transition between the topic in the sentence containing "boat" and the surrounding medical topic sentences tends to be more metaphorical.</p><p>We use these differences in the topic transition for metaphor detection. We consider topic transi- tions at the sentence level, rather than the word level, because people often represent an idea at or above the sentence level. Thus, topic is better- represented at the sentence level.</p><p>To model context at the sentence level, we first assign topics to each sentence using Sentence Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b5">Jo and Oh, 2011</ref>). Sentence LDA has two main advantages over standard LDA for our work. First, while stan- dard LDA assumes that each word is assigned a topic derived from the topic distribution of a doc- ument, Sentence LDA makes the constraint that all words in the same sentence must be assigned the same topic. Due to this property, the generated topics are better aligned with the role or purpose of a sentence, compared to topics generated from LDA. Additionally, having each sentence assigned to one topic helps us avoid using heuristics for rep- resenting the topic of each sentence. <ref type="bibr">1</ref> Using Sentence LDA, we modeled four features to capture how the topic changes around the sen- tence where a target word resides. We refer to this sentence as the target sentence.</p><p>Target Sentence Topic (TargetTopic): We hy- pothesize that sentences containing a metaphor may prefer topics that are different from those of sentences where the same word is used liter- ally. Hence, TargetTopic is a T -dimensional bi- nary feature, where T is the number of topics, that indicates the topic assigned to the sentence con- taining the target word.</p><p>Topic Difference (TopicDiff): We hypothesize that a metaphorical sentence is more likely to be different from its neighboring sentences, in terms of topic, than a literal sentence. Therefore, Top- icDiff is a two-dimensional binary feature that in- dicates whether the topic assigned to the target sentence is different from that of the previous and next sentences.</p><p>Topic Similarity (TopicSim): Under the same hypothesis as TopicDiff, TopicSim is a two- dimensional feature that represents the similarity between the topic of the target sentence and its previous and next sentences. Unlike TopicDiff, which is binary, TopicSim has continuous values between 0 and 1, as we use the cosine similarity between each topic's word distributions as topic similarity. Note that in Sentence LDA, all top- ics share the same vocabulary, but assign differ- ent probabilities to different words as in LDA al- though all tokens in a sentence are assigned to the same topic in Sentence LDA.</p><p>Topic Transition (TopicTrans): The topic of a metaphorical sentence may extend over mul- tiple sentences, so a topic transition may occur a few sentences ahead or behind the target sen- tence. TopicTrans looks for the nearest sentences with a different topic before and after the cur- rent target sentence and encodes the topics of the different-topic sentences. Hence, TopicTrans is a 2T -dimensional feature, where T is the number of topics, that indicates the topics of the nearest sen- tences that have a different topic from the target sentence.</p><p>Topic Transition Similarity (Topic- TransSim): The topics before and after a transition, even in the extended case for Topic- Trans, are still expected to be more different in metaphorical cases than in literal cases, as we as- sume for TopicSim. Therefore, TopicTransSim is a two-dimensional continuous feature that encodes the cosine similarity between the topic of the target sentence and the topics of the nearest sentences that have a different topic before and after the target sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Emotion and Cognition</head><p>Metaphors are often used to explain or describe abstract ideas, such as difficult concepts or emo- tions <ref type="bibr" target="#b10">(Meier and Robinson, 2005)</ref>. <ref type="bibr" target="#b3">(Fainsilber and Ortony, 1987)</ref> showed that descriptions of feelings contain more metaphorical language than descrip- tions of behavior.</p><p>In our domain, writers are searching for sup- port through the emotionally tumultuous experi- ence of breast cancer and often turn to metaphor to express this emotion. For example, the word "road" can be used as a metaphor to express the emotional experiences of waiting for or passing through steps in treatment. A similar phenomenon is that markers of cognition, such as "I think", can occur to introduce the abstract source of the metaphor. In example (3), one breast cancer pa- tient in our data describes her speculation about her condition metaphorically, writing, (3) i have such a long road i just won- der what to do with myself.</p><p>To encode these emotional and cognitive ele- ments as features, we use Linguistic Inquiry Word Count (LIWC) <ref type="bibr" target="#b16">(Tausczik and Pennebaker, 2010)</ref>. LIWC is a tool that counts word use in certain psychologically relevant categories. Focusing on emotional and cognitive processes, we use the LIWC term lists for categories seen in <ref type="table">Table 1</ref> We count the number of words that fall into each category within either an immediate or global con- text. For these LIWC features, we take the target sentence and its neighboring sentences as the im- mediate context and the entire post as the global context for a candidate metaphor instance. The counts for each category in either the immediate or global context are used as features encoded by what degree the immediate or global context ex- presses the emotional or cognitive category.</p><p>We expect words indicative of emotion and cog- nition to appear more frequently in metaphori- cal cases. Our preliminary statistical analysis on the development set revealed that this holds true within the target sentence and shows a tendency in the surrounding sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Level Modeling</head><p>Our topical and emotion and cognition context features are general across target words. How- ever, the specific features that are informative for metaphor identification may depend on the tar- get word. To account for the specificity of target words, we use multi-level modeling <ref type="bibr" target="#b2">(Daume III, 2007)</ref>. The idea of multi-level modeling is to pair each of our features with every target word while keeping one set of features independent of the tar- get words. There are then multiple copies of each topic transition and emotion/cognition feature, all paired with a different target word. Thus, if there are N target words, our feature space becomes N + 1 times larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our main experimental task is metaphor detec- tion or disambiguation -given a post containing a candidate metaphor word, we aim to determine whether the word is used literally or metaphori- cally in context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We conducted experiments on a dataset of posts from a public breast cancer support group discus- sion forum, annotated by <ref type="bibr" target="#b4">Jang et al. (2015)</ref>. We chose to work on this dataset because it features metaphors occurring in naturalistic language.</p><p>In this dataset, posts are restricted to those con- taining one of seven candidate metaphors that ap- pear either metaphorically or literally: "boat", "candle", "light", "ride", "road", "spice", and "train". We split the data randomly into a devel- opment set of 800 posts for preliminary analysis and a cross-validation set of 1,870 posts for clas- sification as in (Jang et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We report five evaluation metrics for every model: kappa, F1 score, precision, recall, and accuracy. Kappa, which corrects for agreement by chance, was calculated between predicted results and ac- tual results. Because the dataset is skewed towards metaphorical instances, we rely on the first four measures over accuracy for our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We use the following two baselines: the feature set of (Jang et al., 2015) and a context unigram model. <ref type="bibr" target="#b4">Jang et al. (2015)</ref>: We use the best configura- tion of features from <ref type="bibr" target="#b4">Jang et al. (2015)</ref>, the state- of-the-art model on our dataset, as a baseline. This feature set consists of all of their local context fea- tures (word category, semantic relatedness, con- creteness), all of their global context features ex- cept lexical chaining (word category, global topic distribution), and context unigrams.</p><p>Context Unigram Model: All the words in a post, including the target word, are used as context features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Settings</head><p>We ran Sentence LDA, setting the number of top- ics to 10, 20, 30, 50, and 100. α and β deter- mine the sparsity of the topic distribution of each document and the word distribution of each topic,   respectively; the lower the sparser. Following con- vention, we set these parameters to 0.1 and 0.001, respectively, to enforce sparsity. We also removed the 37 most frequent words in the corpus, draw- ing the threshold at the point where content words and pronouns started to appear in the ranked list.</p><formula xml:id="formula_0">Model κ F1 P-L R-L P-M R-M A</formula><p>The models with 10 topics performed the best on the development set, with performance degrading as the number of topics increased. We suspect that poorer performance on the models with more top- ics is due to feature sparsity. We used the support vector machine (SVM) classifier provided in the LightSIDE toolkit <ref type="bibr" target="#b9">(Mayfield and Rosé, 2010</ref>) with sequential mini- mal optimization (SMO) and a polynomial kernel of exponent 2. For each experiment, we performed 10-fold cross-validation. We also trained the base- lines with the same SVM settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>The results of our classification experiment are shown in <ref type="table">Table 2</ref>. We tested our topical and emo- tion and cognition features in combination with lexical features from our baselines: unigram and <ref type="bibr" target="#b4">Jang et al. (2015)</ref>.</p><p>Adding our topical and emotion/cognition fea- tures to the baselines improved performance in predicting metaphor detection. We see that our features combined with the unigram features im- proved over the Unigram baseline although they do not beat the <ref type="bibr" target="#b4">Jang et al. (2015)</ref> baseline. How- ever, when our features are combined with the fea- tures from <ref type="bibr" target="#b4">Jang et al. (2015)</ref>, we see large gains in performance. Additionally, our multi-level mod- eling significantly improved performance by tak- <ref type="table" target="#tab_4">T0  T1  T2  T3  T4  T5  T6  T7</ref> T8 T9 ing into account the effects of specific metaphors. The topical features added to the baseline led to a significant improvement in accuracy, while emo- tion and cognition features only slightly improved the accuracy without statistical significance. How- ever, the combination of these emotion and cogni- tion features with topical features (in the last row of <ref type="table">Table 2</ref>) leads to improvement. We performed a Student's t-test for calculating statistical signifi- cance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Distribution of Target Sentences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Metaphorical instances tend to have personal topics. An author was more likely to use target words metaphorically when the target sentence re- lates more closely to their own experience of dis- ease and treatment. Specifically, metaphors were relatively frequent when people shared their own disease experience (Topic 0, Topic 9) or sympa-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head><p>Top Words Example Sentences 0 Disease/ Treatment get, chemo, if, they, as, out, can, like, now, she, feel, did, up, know, think, been, good, time, or, when I'm scared of chemo and ct scans because it makes cancer come back and you become more resistance to treatment with drugs like these later. 1 Food good, they, gt, can, like, eat, fat, or, if, some, one, as, them, get, up, fiber, think, more, what *Martha's Way* Stuff a miniature marshmallow in the bottom of a sugar cone to prevent ice cream drips. 2 Emotions love, great, laura, good, hope, like, debbie, amy, up, happy, too, everyone, day, glad, look, fun, mary, what, kelly, how   <ref type="table" target="#tab_4">T0  T1  T2  T3  T4  T5  T6  T7  T8  T9</ref> Vs. Next Sentence thized with other people's experiences (Topic 7), but were more infrequent when they simply talked about other people in Topic 5 <ref type="figure" target="#fig_1">(Figure 1</ref>). Accord- ing to our closer examination of sample sentences, Topic 0 had many personal stories about disease and treatment, and Topic 7 was about learning and relating to other people's experiences. boat." (Topic 7). Analysis of our LIWC features also supports the reflective nature of metaphors: "insight" and "discrepancy" words such as "wish", "seem", and "feel" occur more frequently around metaphorical uses of target terms.</p><p>The topics of the surrounding context (TopicTrans) were also informative for metaphor detection <ref type="figure" target="#fig_0">(Figure 2)</ref>. However, the topics of the surrounding sentences followed an opposite pattern to the topics of the target sentence; talking q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Metaphorical Literal </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vs. Next Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Similarity Between Target Sentence and Context</head><p>Figure 4: Cosine similarity between the topic of a target sentence and the topic of its previous/next sentence, when target words were used metaphor- ically vs. literally. The means of the metaphorical and literal cases are different with statistical sig- nificance of p &lt; 0.01 by Welch's t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vs. Previous Sentence</head><p>Metaphorical Literal about other people (Topic 5) in the context of a target sentence led to more metaphorical usage of target words. Similarly, writers used target words more literally before or after they shared their personal stories (Topic 0). This pattern could be because the topic of the target sentence differs from the topics of the surrounding sentences in these instances, which would mean that the target sentence is a topic that is more likely to be literal. Topic 9, however, does not follow the same pattern. One possible reason is that Topic 9 and Topic 0 tend to frequently co-occur and be metaphorical. Thus, if a target word comes after or before Topic 9 and it is Topic 0, then this word may more likely be metaphorical. Topic transitions are effective indicators of metaphor. Metaphorical instances accompanied more drastic topic transitions than literal instances. This tendency, which matched our hypothesis, was shown in all our topic features. The immediately neighboring sentences of metaphorical instances were more likely to have a different topic from the target sentence than those of literal instances <ref type="figure">(Fig- ure 3)</ref>. Additionally, differences in topic between the target sentence and the neighboring sentences were greater for metaphorical instances <ref type="figure">(Figure 4)</ref>. The nearest sentences with topics different from the target sentence (TopicTransSim) also showed this pattern ( <ref type="figure" target="#fig_5">Figure 5</ref>). An interesting finding was that a topic transition after the target sentence was more indicative of metaphor than a transition be- fore.</p><p>Emotion and cognitive words are discrimina- tive depending on the metaphor. Emotion and cognition in the surrounding contexts, which were captured by the LIWC features, helped identify metaphors when combined with topical features. This result supports the claim in <ref type="bibr" target="#b3">(Fainsilber and Ortony, 1987</ref>) that descriptions of feelings contain more metaphorical language than descriptions of behavior.</p><p>This effect, however, was limited to specific tar- get words and emotions. For example, we saw a higher number of anxiety words in the immedi- ate and global contexts of metaphors, but the trend was the opposite for anger words. This may be be- cause our target words, "boat", "candle", "light", "ride", "road", "spice" and "train", relate more to anxiety in metaphors such as "bumpy road" and "rollercoaster ride", than to anger. On the other hand, cognitive words had more consistency, as words marking insight and discrepancy were seen significantly higher around metaphorical uses of the target words. These patterns, nevertheless, could be limited to our domain. It would be in- teresting to explore other patterns in different do- mains.</p><p>A multi-level model captures word-specific effects. Our features in context helped recog- nize metaphors in different ways for different tar- get words, captured by the multi-level model. The paucity of general trends across metaphori- cal terms does not mean a limited applicability of our method, though, as our features do not sup- pose any specific trends. Rather, our method only assumes the existence of a correlation between metaphors and the theme of their context, and our multi-level model effectively identifies the inter- action between metaphorical terms and their con- 223 texts as useful information.</p><p>For all the figures in this section, most target words have a similar pattern. See our supplemen- tal material for graphs by target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a new, effective method for metaphor detection using (1) sentence level topic transitions between target sentences and surrounding contexts and (2) emotion and cognition words. Both types of features showed significant improvement over the state-of-the-art. In particular, our system made significant gains in solving the problem of over- classification in metaphor detection.</p><p>We also find that personal topics are markers of metaphor, as well as certain patterns in topic tran- sition. Additionally, language expressing emotion and cognition relates to metaphor, but in ways spe- cific to particular candidate words. For our breast cancer forum dataset, we find more words related to anxiety around metaphors.</p><p>Our proposed features can be expanded to other domains. Though in other domains, the specific topic transition and emotion/cognition patterns would likely be different, these features would still be relevant to metaphor detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 :</head><label>2</label><figDesc>Performance on metaphor identification task. (Models) J: Jang et al. (2015), MM -Multilevel Modeling (Metrics) κ: Cohen's kappa, F1: average F1 score on M/L, P-L: precision on literals, R-L: recall on literals, P-M: precision on metaphors, R-M: recall on metaphors, A: accuracy, *: marginally sta- tistically significant (p &lt; 0.1), **: statistically significant (p &lt; 0.05), ***: highly statistically significant (p &lt; 0.01) improvement over corresponding baseline by Student's t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Proportions of topics assigned to target sentences, when target words were used metaphorically vs. literally. The proportions of metaphorical and literal cases are different with statistical significance of p &lt; 0.01 by Pearson's chi-square test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Topic Distribution of the Sentences Nearest to the Target Sentence and with a Different TopicFigure 2 :</head><label>2</label><figDesc>Figure 2: Proportions of the topics of the sentences that are nearest to the target sentence and have a different topic from the target sentence. The proportions of metaphorical and literal cases are different with statistical significance of p &lt; 0.01 by Pearson's chi-square test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: Proportions of target sentences whose topic is different from that of the previous/next sentence, when target words were used metaphorically vs. literally. The proportions of metaphorical and literal cases are different with statistical significance of p &lt; 0.01 by Pearson's chi-square test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Topic Similarity Between Target Sentence and Nearest Transitioning ContextFigure 5 :</head><label>5</label><figDesc>Figure 5: Cosine similarity of the topic of a target sentence and the topic of the sentences that are nearest to the target sentence and have a different topic from the target sentence. The means of metaphorical and literal cases are different with statistical significance only for the next sentence, with p &lt; 0.01 by Welch's t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>LIWC category 
Example Terms 
affect 
ache, like, sweet 
positive emotion 
passion, agree, giving 
negative emotion 
agony, annoy, miss 
anxiety 
embarrass, avoid 
anger 
assault, offend 
sadness 
despair, grim 
cognitive mechanisms if, could 
insight 
believe, aware 
cause 
make, pick 
discrep 
would, hope 
tentativeness 
anyone, suppose 
certainty 
never, true 

Table 1: Selected LIWC categories. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Topics learned by Sentence LDA. 

</table></figure>

			<note place="foot" n="1"> We also tried standard LDA for assigning topics to sentences, by representing each sentence as a topic distribution over its words. However, this representation was not as informative as Sentence LDA in our task, so we leave out the LDA topics in further discussion.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by NSF Grant IIS-1302522.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using imageability and topic chaining to locate metaphors in linguistic corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umit</forename><surname>George Aaron Broadwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Boz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomek</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Computing, Behavioral-Cultural Modeling and Prediction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cohesion, coherence, and expert evaluations of writing proficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">S</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd annual conference of the Cognitive Science Society</title>
		<meeting>the 32nd annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Metaphorical uses of language in the expression of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Fainsilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ortony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Metaphor and Symbolic Activity</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Metaphor detection in discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeju</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">Penstein</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">384</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aspect and Sentiment Unification Model for Online Review Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supervised word-level metaphor detection: Experiments with concreteness and reweighting of examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Wee</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2015 3rd Metaphor Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Metaphors we live by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<pubPlace>Chicago/London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using gaussian mixture models to detect figurative language in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An interactive tool for supporting error analysis for text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Demonstration Session</title>
		<meeting>the NAACL HLT 2010 Demonstration Session</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="25" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The metaphorical representation of affect. Metaphor and symbol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael D Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="239" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic signatures for example-based linguistic metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bracewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hinote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP</title>
		<meeting>the First Workshop on Metaphor in NLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Metaphor detection through term relevance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Design and evaluation of metaphor processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised recognition of literal and non-literal use of idiomatic expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robust extraction of metaphors from novel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Aaron</forename><surname>Broadwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Yamrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">67</biblScope>
			<pubPlace>Umit Boz, Ignacio Cases</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The psychological meaning of words: Liwc and computerized text analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Tausczik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of language and social psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="54" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Metaphor detection with cross-lingual model transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on the Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
