<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua National Laboratory for Information Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1623" to="1633"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1149</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disam-biguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambigua-tion method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Jointly learning text and knowledge representa- tions in a unified vector space greatly benefits many Natural Language Processing (NLP) tasks, such as knowledge graph completion ( <ref type="bibr" target="#b8">Han et al., 2016</ref>; <ref type="bibr" target="#b31">Wang and Li, 2016)</ref>, relation extraction ( <ref type="bibr" target="#b32">Weston et al., 2013)</ref>, word sense disambiguation ( <ref type="bibr" target="#b16">Mancini et al., 2016)</ref>, entity classification ( <ref type="bibr" target="#b12">Huang et al., 2017</ref>) and linking ( <ref type="bibr" target="#b11">Huang et al., 2015)</ref>.</p><p>Existing work can be roughly divided into two categories. One is encoding words and entities into a unified vector space using Deep Neural * Corresponding author.</p><p>Networks (DNN). These methods suffer from the problems of expensive training and great limita- tions on the size of word and entity vocabulary <ref type="bibr" target="#b8">(Han et al., 2016;</ref><ref type="bibr" target="#b28">Toutanova et al., 2015;</ref><ref type="bibr" target="#b33">Wu et al., 2016)</ref>. The other is to learn word and entity em- beddings separately, and then align similar words and entities into a common space with the help of Wikipedia hyperlinks, so that they share similar representations ( <ref type="bibr" target="#b29">Wang et al., 2014;</ref><ref type="bibr" target="#b34">Yamada et al., 2016)</ref>. However, there are two major problems arising from directly integrating word and entity embed- dings into a unified semantic space. First, men- tion phrases are highly ambiguous and can refer to multiple entities in the common space. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the same mention independence day (m 1 ) can either refer to a holiday: Independence Day (US) or a film: Independence Day (film). Sec- ond, an entity often has various aliases when men- tioned in various contexts, which implies a much larger size of mention vocabulary compared with entities. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the documents d 2 and d 3 describes the same entity Independence Day (US) (e 2 ) with distinct mentions: indepen- dence day and July 4th. We observe tens of mil- lions of mentions referring to 5 millions of entities in Wikipedia.</p><p>To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task <ref type="bibr" target="#b23">(Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b10">Huang et al., 2012;</ref><ref type="bibr" target="#b27">Tian et al., 2014;</ref><ref type="bibr" target="#b20">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b15">Li and Jurafsky, 2015)</ref>. The basic idea behind it is to con- sider entities in KBs that can provide a meaning repository of mentions (i.e. words or phrases) in texts. That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity. Furthermore, we assume that different mentions referring to the same en- tity express the same meaning and share a com- mon mention sense embedding, which largely re- duces the size of mention vocabulary to be learned. For example, the mentions Independence Day in d 2 and July 4th in d 3 have a common mention sense embedding during training since they refer to the same holiday. Thus, text and knowledge are bridged via mention sense.</p><p>In this paper, we propose a novel Multi- Prototype Mention Embedding (MPME) model, which jointly learns the representations of words, entities, and mentions at sense level. Different mention senses are distinguished by taking ad- vantage of both textual context information and knowledge of reference entities. Following the frameworks in ( <ref type="bibr" target="#b29">Wang et al., 2014;</ref><ref type="bibr" target="#b34">Yamada et al., 2016)</ref>, we use separate models to learn the rep- resentations for words, entities and mentions, and further align them by a unified optimization ob- jective. Extending from skip-gram model and CBOW model, our model can be trained effi- ciently ( <ref type="bibr" target="#b18">Mikolov et al., 2013a</ref>,b) from a large scale corpus. In addition, we also design a lan- guage model based approach to determine the sense for each mention in a document based on multi-prototype mention embeddings.</p><p>For evaluation, we first provide qualitative anal- ysis to verify the effectiveness of MPME to bridge text and knowledge representations at the sense level. Then, separate tasks for words and enti- ties show improvements by using our word, en- tity and mention representations. Finally, using entity linking as a case study, experimental results on the benchmark dataset demonstrate the effec- tiveness of our embedding model as well as the disambiguation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we formally define the input and output of multi-prototype mention embedding.</p><p>A knowledge base KB contains a set of entities E = {e j }, and their relations. We use Wikipedia as the given knowledge base, and organize it as a directed knowledge network: nodes denote enti- ties, and edges are outlinks from Wikipedia pages. In the directed network, we define the entities that point to e j as its neighbors N (e j ), but ignore those entities that e j points to, so that the repeated com- putations on the same edge would be avoided if edges were undirected.</p><p>A text corpus D is a set of sequential words D = {w 1 , · · · , w i , · · · , w |D| }, where w i is the ith word and |D| is the length of the word sequence.</p><p>Since an entity mention m l may consist of mul- tiple words, we define an annotated text corpus 1 as D = {x 1 , · · · , x i , · · · , x |D | }, where x i cor- responds to either a word w i or a mention m l . We define the words around x i within a predefined window as its context words C(x i ).</p><p>An Anchor is a Wikipedia hyperlink from a mention m l linking to its entity e j , and is repre- sented as a pair &lt; m h , e j &gt;∈ A. The anchors pro- vide mention boundaries as well as their reference entities from Wikipedia articles. These Wikipedia articles are used as an annotated text corpus D in this paper.</p><p>Multi-Prototype Mention Embedding . Given a KB, an annotated text corpus D and a set of anchors A, we aim to learn multi-prototype men- tion embedding, namely multiple sense embed- dings s j l ∈ R k for each mention m l as well as word embeddings w and entity embeddings e. We use M * l = {s l j } to denote the sense set of mention m l , where each s l j refers to an entity e j . Thus, the vocabulary size is reduced to a fixed number |{s * j }| = |E|. We use s * j to denote the shared sense of mentions referring to entity e j . Example As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, Independence Day (m 1 ) has two mention senses s 1 1 , s 1 2 , and July 4th (m 2 ) has one mention sense s 2 2 . Based on the assumption in Section 1, we have s * 2 = s 1 2 = s 2 2 referring to entity Independence Day (US) (e 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An Overview of Our Method</head><p>Given a knowledge base KB, an annotated text corpus D and a set of anchors A, we aim to jointly learn word, entity and mention sense representa- tions: w, e, s. As shown in <ref type="figure" target="#fig_20">Figure 2</ref>, our framework contains two key components: edge representation model, and enjoys their ad- vantages of different aspects in knowledge bases <ref type="bibr">2</ref> . This is reasonable because we output two sepa- rately semantic vector spaces for text and knowl- edge respectively, while we can still obtain the re- latedness between word and entity indirectly by computing similarity between word and mention embeddings referring to that entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present three main components in MPME: text model, knowledge model and joint model, and then introduce the detailed information on training process. Finally, we briefly introduce the framework for entity linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skip-gram model</head><p>capable of iterative learning; capable of learning more mention names; capable of tuning mention sense via text model; capable of NIL sense; 1. take pre-trained word and entity embeddings as input; 2. collect mention name to entity title mapping; use anchor to annotate each mention. each men- tion corresponds multiple sense; each sense relates 2 Thus, MPME only trains text model and joint model.  <ref type="table" target="#tab_21">4   323   324   325   326   327   328   329   330   331   332   333   334   335   336   337   338   339   340   341   342   343   344   345   346   347   348   349   373   374   375   376   377   378   379   380   381   382   383   384   385   386   387   388   389   390   391   392   393   394   395   396   397   398   399</ref> dings. Actually, MPME is flexible to utilize pre- trained entity embeddings from arbitrary knowl- edge representation model, and enjoys their ad- vantages of different aspects in knowledge bases 2 . This is reasonable because we output two sepa- rately semantic vector spaces for text and knowl- edge respectively, while we can still obtain the re- latedness between word and entity indirectly by computing similarity between word and mention embeddings referring to that entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint model</head><formula xml:id="formula_0">A X P (e j |w m t , s i ) + P (e j |w context ) e Independence Day (film)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present three main components in MPME: text model, knowledge model and joint model, and then introduce the detailed information on training process. Finally, we briefly introduce the framework for entity linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Skip-gram model</head><p>capable of iterative learning; capable of learning more mention names; capable of tuning mention sense via text model; capable of NIL sense; 1. take pre-trained word and entity embeddings as input; 2. collect mention name to entity title mapping; use anchor to annotate each mention. each men- tion corresponds multiple sense; each sense relates 2 Thus, MPME only trains text model and joint model.  <ref type="table" target="#tab_21">5   400   401   402   403   404   405   406   407   408   409   410   411   412   413   414   415   416   417   418   419   420   421   422   423   424   425   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449</ref> ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.</p><formula xml:id="formula_1">P (e neighbor |e i ) 3.4 Joint model A X P (e j |w m t , s i ) + P (e j |w context )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skip-gram model</head><formula xml:id="formula_2">g(Independence Day, ) P (N (e j )|e j ) P (e j |C(m h ), t s l ) P (C(w i )|w i )P (C(m h )|t s l , m h ) 3.3 Text model L w = T X t=1 log P (w t+j |w m t , s i )P (s i |w context ) + T X t=1 X cjc,j6 =0 log P (w t+j |w t ) (1) D X C X P (w t+j |w m t , s i )P (s i |w m t , w context ) 3.4 Knowledge model KB X N X P (e neighbor |e i )</formula><p>3.5 Joint model  <ref type="table" target="#tab_21">5   400   401   402   403   404   405   406   407   408   409   410   411   412   413   414   415   416   417   418   419   420   421   422   423   424   425   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   450   451   452   453   454   455   456   457   458   459   460   461   462   463   464   465   466   467   468   469   470   471   472   473   474   475   476   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE. </p><formula xml:id="formula_3">A X P (e j |w m t , s i ) + P (e j |w context ) 3.6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skip-gram model</head><formula xml:id="formula_4">g(Independence Day, ) P (N (e j )|e j ) P (e j |C(m h ), t s l ) e 1 e 2 P (C(w i )|w i ) · P (C(m h )|t s l , m h )<label>(1</label></formula><p>3.3 Text model  ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE. </p><formula xml:id="formula_6">L w = T X t=1 log P (w t+j |w m t , s i )P (s i |w context ) + T X t=1 X cjc,j6 =0 log P (w t+j |w t ) (4) D X C X P (w t+j |w m t , s i )P (s i |w m t , w context ) 3.4 Knowledge model KB X N X P (e neighbor |e i ) 3.5 Joint model A X P (e j |w m t , s i ) + P (e j |w context ) 3.6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Skip-gram model</head><formula xml:id="formula_7">g(Independence Day, ) P (N (e j )|e j ) P (e j |C(m h ), t s l ) e 1 e 2 P (C(w i )|w i ) · P (C(m h )|t s l , m h )<label>(1</label></formula><p>3.3 Text model embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word w i or a mention sense of entity title t s l :</p><formula xml:id="formula_9">L w = T X t=1 log P (w t+j |w m t , s i )P (s i |w context ) + T X t=1 X cjc,j6 =0 log P (w t+j |w t ) (4) D X C X P (w t+j |w m t , s i )P (s i |w m t , w context ) 3.4 Knowledge model KB X N X P (e neighbor |e i ) 3.5 Joint model A X P (e j |w m t , s i ) + P (e j |w context ) 3</formula><formula xml:id="formula_10">L w = X wi,t l 2D P (C(w i )|w i ) + P (C(t l )|t l , t s l )<label>(6)</label></formula><p>w i /t s l , , , w, , e j , e</p><p>on Software Engineering, 15(9):1066-1077.</p><p>Christopher J. C. Burges, Léon Bottou, Zoubin <ref type="bibr">Ghahramani, and Kilian Q. Weinberger, editors. 2013</ref>. Ad- vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meet- ing held December 5-8, 2013, Lake Tahoe, Nevada, United States.</p><p>Xu Han, Zhiyuan Liu, and Maosong Sun. 2016. Joint representation learning of text and knowl- edge for knowledge graph completion. CoRR, abs/1611.04125. tion sense has an embedding (sense vector) t l and a context cluster with center µ(t s l ). The repre- sentation of the context is defined as the aver- age of the word vectors in the context: C(wi) = 1 |C(wi)| P wj 2C(wi) wj. We predict t s l , the sense of entity title tl in the mention &lt; tl, C(tl) &gt;, when observed with con- text C(tl) as the context cluster membership. For- mally, we have:</p><formula xml:id="formula_12">t s l = ⇢ t s+1 l t max l &lt; t max l otherwise (5)</formula><p>where is a hyper-parameter and t max l = argmax t s l sim(µ(t s l ), C(tl)). We adopt an online non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments.</p><p>Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_13">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l )<label>(6)</label></formula><p>C(·)</p><p>before conducting the experiments on the tasks, we first give qualitative analysis of words, men- tions and entities. firstly, we give the phrase embedding by its nearest words and entities.</p><p>next, we give quantitative analysis on several tasks.  (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB sense. To be concrete, each men- tion sense has an embedding (sense vector) t s l and a context cluster with center µ(t s l ). The repre- sentation of the context is defined as the aver- age of the word vectors in the context: C(wi) = 1 |C(wi)| P wj 2C(wi) wj. We predict t s l , the sense of entity title tl in the mention &lt; tl, C(tl) &gt;, when observed with con- text C(tl) as the context cluster membership. For- mally, we have:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Entity Relatedness</head><formula xml:id="formula_15">t s l = ⇢ t s+1 l t max l &lt; t max l otherwise (5)</formula><p>where is a hyper-parameter and t max l = argmax t s l sim(µ(t s l ), C(tl)). We adopt an online non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments.</p><p>Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_16">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l ) (6) C(·)<label>(7)</label></formula><p>type model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>before conducting the experiments on the tasks, we first give qualitative analysis of words, men- tions and entities. firstly, we give the phrase embedding by its nearest words and entities.</p><p>next, we give quantitative analysis on several tasks.  When encounter an mention of entity title tl, in- spired by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB sense. To be concrete, each men- tion sense has an embedding (sense vector) t s l and a context cluster with center µ(t s l ). The repre- sentation of the context is defined as the aver- age of the word vectors in the context: C(wi) = 1 |C(wi)| P wj 2C(wi) wj. We predict t s l , the sense of entity title tl in the mention &lt; tl, C(tl) &gt;, when observed with con- text C(tl) as the context cluster membership. For- mally, we have:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Entity</head><formula xml:id="formula_17">t s l = ⇢ t s+1 l t max l &lt; t max l otherwise (5)</formula><p>where is a hyper-parameter and t max</p><formula xml:id="formula_18">l = argmax t s l sim(µ(t s l ), C(tl)).</formula><p>We adopt an online non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments.</p><p>Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_19">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l ) (6) C(·)<label>(7)</label></formula><p>1. directly align words with entity.</p><p>2. align mention with entity using single proto- type model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>before conducting the experiments on the tasks, we first give qualitative analysis of words, men- tions and entities.</p><p>firstly, we give the phrase embedding by its nearest words and entities.</p><p>next, we give quantitative analysis on several tasks. nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments. Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word w i or a mention sense of entity title t s l :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Entity</head><formula xml:id="formula_20">L w = X wi,t l 2D P (C(w i )|w i ) + P (C(t l )|t l , t s l ) (6) w i /t s l , , , w, , e j , e<label>(7)</label></formula><p>6 Conclusion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Alfred V Aho and Margaret J Corasick. 19 cient string matching: an aid to bibliograph Communications of the ACM, 18(6):333-3 J-I Aoe. 1989. An efficient digital search alg using a double-array structure. IEEE Tra on Software Engineering, 15 <ref type="formula" target="#formula_38">(9)</ref>  <ref type="table" target="#tab_21">424   425   426   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   472   473   474   475   476   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> text C(tl) as the context cluster membership. For- mally, we have:</p><formula xml:id="formula_21">t s l = ⇢ t s+1 l t max l &lt; t max l otherwise (5)</formula><p>where is a hyper-parameter and t max l = argmax t s l sim(µ(t s l ), C(tl)). We adopt an online non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments.</p><p>Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_22">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l ) (6) C(·)<label>(7)</label></formula><p>tasks.  <ref type="table" target="#tab_21">430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> where is a hyper-parameter and t max l = argmax t s l sim(µ(t s l ), C(tl)). We adopt an online non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Entity Relatedness</head><p>Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_23">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l ) (6) C(·)<label>(7)</label></formula><p>4.7.1 gbdt 4.7.2 unsupervised 5 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Alfred V <ref type="bibr">Aho and Margaret J Corasick. 1975</ref>. Effi- cient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333-340.</p><p>J-I Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15 <ref type="formula" target="#formula_38">(9)</ref>  <ref type="table" target="#tab_21">428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   476   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498</ref> 499</p><formula xml:id="formula_24">t s l = t s+1 l t max l &lt; t max l otherwise (5)</formula><p>where is a hyper-parameter and t max l = argmax t s l sim(µ(t s l ), C(tl)). We adopt an online non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments.</p><p>Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_25">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l )<label>(6)</label></formula><p>C(·)</p><p>4.6 Word Similarity <ref type="bibr">Corasick. 1975</ref>. Effi- cient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333-340.</p><note type="other">4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work 6 Conclusion References Alfred V Aho and Margaret J</note><p>J-I Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15 <ref type="formula" target="#formula_38">(9)</ref>   <ref type="table" target="#tab_21">438   439   440   441   442   443   444   445   446   447   448   449   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments. Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_27">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l )<label>(6)</label></formula><p>N (·)</p><p>Communications of the ACM, 18(6):333-340.</p><p>J-I Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15(9):1066-1077.</p><p>Christopher J. C. Burges, Léon Bottou, Zoubin <ref type="bibr">Ghahramani, and Kilian Q. Weinberger, editors. 2013</ref>. Ad- vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meet- ing held December 5-8, 2013, Lake Tahoe, Nevada, United States.</p><p>Xu Han, Zhiyuan Liu, and Maosong Sun. 2016. Joint representation learning of text and knowl- edge for knowledge graph completion. CoRR, abs/1611.04125. <ref type="table" target="#tab_21">5   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments. Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_29">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l )<label>(6)</label></formula><p>N (·)</p><p>References Alfred V <ref type="bibr">Aho and Margaret J Corasick. 1975</ref>. Effi- cient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333-340.</p><p>J-I Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15(9):1066-1077.</p><p>Christopher J. C. Burges, Léon Bottou, Zoubin <ref type="bibr">Ghahramani, and Kilian Q. Weinberger, editors. 2013</ref>. Ad- vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meet- ing held December 5-8, 2013, Lake Tahoe, Nevada, United States.</p><p>Xu Han, Zhiyuan Liu, and Maosong Sun. 2016. Joint representation learning of text and knowl- edge for knowledge graph completion. CoRR, abs/1611.04125. <ref type="table" target="#tab_21">5   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments. Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word wi or a mention sense of entity title t s l :</p><formula xml:id="formula_31">Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, t s l )<label>(6)</label></formula><p>N (·)</p><p>5 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Alfred V <ref type="bibr">Aho and Margaret J Corasick. 1975</ref>. Effi- cient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333-340.</p><p>J-I Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15(9):1066-1077.</p><p>Christopher J. C. Burges, Léon Bottou, Zoubin Ghahra- mani, and Kilian Q. Weinberger, editors. 2013.  <ref type="table" target="#tab_21">5   427   428   429   430   431   432   433   434   435   436   437   438   439   440   441   442   443   444   445   446   447   448   449   477   478   479   480   481   482   483   484   485   486   487   488   489   490   491   492   493   494   495   496   497   498   499</ref> t l otherwise where is a hyper-parameter and t max l = argmax t s l sim(µ(t s l ), C(t l )). We adopt an online non-parametric clustering procedure to learn out- of-KB mention senses, which means that if the nearest distance of the context vector to sense clus- ter center is larger than a threshold, we create a new context cluster and a new sense vector that doesn't belong to any entity-centric senses. The cluster center is the average of all the context vec- tors belonging to that cluster. For the similarity metric, we use cosine in our experiments.</p><p>Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the proba- bility of observing the context words given either a word w i or a mention sense of entity title t s l :</p><formula xml:id="formula_33">L w = X wi,tl2D P (C(w i )|w i ) + P (C(t l )|t l , t s l )<label>(6)</label></formula><p>g(July 4th, e 1 )</p><p>4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Alfred V <ref type="bibr">Aho and Margaret J Corasick. 1975</ref>. Effi- cient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333-340.</p><p>J-I Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on Software Engineering, 15 <ref type="formula" target="#formula_38">(9)</ref>  </p><note type="other">learn two mention senses s 1 1 , s 1 2 for m 1 , and one mention sense s 2 2 for m 2 . Clearly, these two men- tions share a common sense in the last two docu- ments: the United States holiday e 2 , so we have s ⇤ 2 = s 1 2 = s 2 2 . Note that w, m, s are naturally em- bedded into the same semantic space since they are basic units in texts, and e modeling the graph structure in KB is actually in another semantic space.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we firstly describe the framework of MPME, followed by the detailed information of each key component. Then, we introduce a well designed mention sense disambiguation method, which can also be used for entity linking in a un- supervised way.</p><formula xml:id="formula_35">e National Day s ⇤ Independence Day (film) , s ⇤ Independence Day (US)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>Given KB, D and A, we are to jointly learn word, entity and mention representations: w, e, m. Serving as basic units in texts, Word {w i } and entity title {t l } are naturally embedded into a unified semantic space, meanwhile entities {e j } are mapped to one of mention senses of its ti- tle: t s l . Thus, text and knowledge are com- bined via the bridge of mentions. We can eas- ily obtain the similarity between word and en- tity Similarity(w i , e j ) by computing the similar- ity between word and its corresponding mention sense: Similarity(w i , f(e j )).</p><p>As shown in <ref type="figure" target="#fig_20">Figure 2</ref>, our proposed MPME contains four key components: (1) Mention Sense Mapping: we map the anchor &lt; m h , e j &gt;2 A to the corresponding mention sense t s l to reduce the vocabulary to learn. (2) Entity Representation Learning given a knowledge base KB, we con- struct a knowledge network among entities, and </p><note type="other">given annotated we learn entity both contextual beddings in orde senses that has s sponding entity Representatio an iterative upd optimization ob beddings w i and own semantic sp the new learned inspires us to gl choosing mentio tion names in t mention sense c tion sense disam garded as linkin unsupervised wa tion ??. 3.2 Mention S There are two k mention senses, tion senses. The beginning. Give tract entity titles mention senses, on how many e latter is to find given mention n mention generat Conventional generally mainta and entity that knowledge base nizes the mentio matching. Or it fi names in texts u nition) tool, and didate entities vi Since this co paper, we adop</note><note type="other">s ⇤ 2 = s 1 2 = s 2 2 . Note that w, m, s are naturally e bedded into the same semantic space since t are basic units in texts, and e modeling the gra structure in KB is actually in another seman space.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we firstly describe the framew of MPME, followed by the detailed information each key component. Then, we introduce a w designed mention sense disambiguation meth which can also be used for entity linking in a supervised way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>Given KB, D and A, we are to jointly le word, entity and mention representations: w m. Serving as basic units in texts, Word {w and entity title {t l } are naturally embedded i a unified semantic space, meanwhile entities { are mapped to one of mention senses of its tle: t s l . Thus, text and knowledge are co bined via the bridge of mentions. We can e ily obtain the similarity between word and tity Similarity(w i , e j ) by computing the simi ity between word and its corresponding ment sense: Similarity(w i , f(e j )).</p><p>As shown in <ref type="figure" target="#fig_20">Figure 2</ref>, our proposed MPM contains four key components: <ref type="formula" target="#formula_4">(1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACL 2016 Submission ***. Confidential review copy. DO NOT DISTRIBUTE.</head><p>KB, a text corpus D and a set of anchors A, multi- prototype mention embedding is to learn multiple sense embeddings s j l 2 R k for each mention m l as well as word embeddings w and entity embed- dings e. Note that s l j 2 m ⇤ l denotes that mention sense of m l refers to entity e j , where m ⇤ l repre- sents the sense set of m l . Different mentions may share the same mention sense, denoted as s</p><note type="other">⇤ j . Example As shown in Figure 1, there are two different mentions "Independence Day" m 1 and "July 4th" m 2 in the documents. MPME is to learn two mention senses s 1 1 , s 1 2 for m 1 , and one mention sense s 2 2 for m 2 . Clearly, these two men- tions share a common sense in the last two docu- ments: the United States holiday e 2 , so we have s ⇤ 2 = s 1 2 = s 2 2 . Note that w, m, s are naturally em- bedded into the same semantic space since they are basic units in texts, and e modeling the graph structure in KB is actually in another semantic space.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we firstly describe the framework of MPME, followed by the detailed information of each key component. Then, we introduce a well designed mention sense disambiguation method, which can also be used for entity linking in a un- supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>Given knowledge base KB, text corpus D and a set of anchors A, we are to jointly learn word, entity and mention representations: w, e, m. As shown in <ref type="figure" target="#fig_20">Figure 2</ref>, our proposed MPME contains four key components: (1) Mention Sense Mapping: given an anchor &lt; m l , e j &gt;, we map it to the cor- responding mention sense to reduce the mention vocabulary to learn embeddings. If only a men- tion is given, we map it to several mention senses that requires disambiguation (Section 3.4). (2) Entity Representation Learning based on out- links in Wikipedia pages, we construct a knowl- edge network to represent the semantic relatedness among entities. And then learn entity embeddings so that similar entities on the graph have simi- lar representations. (3) Mention Representation Learning given mapped anchors in contexts, we learn mention sense embeddings by incorporating both textual context embeddings and entity em- beddings. (4) Text Representation Learning we extend skip-gram model to simultaneously learn word and mention sense embeddings on annotated text corpus D 0 . Following ( <ref type="bibr" target="#b34">Yamada et al., 2016)</ref>, we use wikipedia articles as text corpus, and the anchors provide annotated mentions <ref type="bibr">1</ref> .</p><p>We jointly train (2), (3) and (4) by using a uni- fied optimization objective. The outputs embed- dings of word and mention are naturally in the same semantic space since they are different units</p><note type="other">in annotated text corpus D 0 for text representation learning. Entity embeddings keep their own se- mantics in another vector space, because we only use them as answers to predict in mention repre- sentation learning by extending Continuous BOW model, which will be further discussed in Section ??. s ⇤</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memorial Day</head><p>word embeddings w i and entity embeddings e j keep their own semantic space and are naturally bridged via the new learned entity title embed- dings t l , which inspires us to globally optimize the probability of choosing mention senses of all the phrases of mention names in the given docu- ment. Since each mention sense corresponds to an entity, the mention sense disambiguation process can also be regarded as linking entities to knowl- edge base in a unsupervised way, which will be detailed in Section ??.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mention Sense Mapping</head><p>There are two kinds of mappings: from entities to mention senses, and from mention names to men- tion senses. The former is pre-defined at the very beginning. Given the knowledge Base KB, we ex- tract entity titles {t l } and initialize with multiple mention senses, where the sense number depends on how many entities share a common title. The latter is to find possible mention senses for the given mention name, which is similar to candidate mention generation in entity linking task.</p><p>Conventional candidate mention generation generally maintains a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mention name, and recog- nizes the mention name in text by accurate string <ref type="bibr">1</ref> We can also annotate text corpus by using NER tool like python nltk to recognize mentions, and disambiguating its mapped mention senses as described in Section 3.4. This is an ongoing work with the goal of learning additional out-of- KB senses by self-training. In this paper, we will focus on the effectiveness of our model and the quality of three kinds of learned embeddings. </p><note type="other">dings e. Note that s j 2 m l denotes that mention sense of m l refers to entity e j , where m ⇤ l repre- sents the sense set of m l . Different mentions may share the same mention sense, denoted as s ⇤ j . Example As shown in Figure 1, there are two different mentions "Independence Day" m 1 and "July 4th" m 2 in the documents. MPME is to learn two mention senses s 1 1 , s 1 2 for m 1 , and one mention sense s 2</note><p>2 for m 2 . Clearly, these two men- tions share a common sense in the last two docu- ments: the United States holiday e 2 , so we have s ⇤ 2 = s 1 2 = s 2 2 . Note that w, m, s are naturally em- bedded into the same semantic space since they are basic units in texts, and e modeling the graph structure in KB is actually in another semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we firstly describe the framework of MPME, followed by the detailed information of each key component. Then, we introduce a well designed mention sense disambiguation method, which can also be used for entity linking in a un- supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>Given knowledge base KB, text corpus D and a set of anchors A, we are to jointly learn word, entity and mention representations: w, e, m. As shown in <ref type="figure" target="#fig_20">Figure 2</ref>, our proposed MPME contains four key components: (1) Mention Sense Mapping: given an anchor &lt; m l , e j &gt;, we map it to the cor- responding mention sense to reduce the mention vocabulary to learn embeddings. If only a men- tion is given, we map it to several mention senses that requires disambiguation (Section 3.4 </p><note type="other">We jointly train (2), (3) and (4) by using a un fied optimization objective. The outputs embed dings of word and mention are naturally in th same semantic space since they are different uni in annotated text corpus D 0 for text representatio learning. Entity embeddings keep their own se mantics in another vector space, because we onl use them as answers to predict in mention repre sentation learning by</note><p>extending Continuous BOW model, which will be further discussed in Sectio 3.3.4. <ref type="figure" target="#fig_20">Figure 2</ref> shows a real example of "" e Memorial Day word embeddings w i and entity embeddings e keep their own semantic space and are naturall bridged via the new learned entity title embed dings t l , which inspires us to globally optimiz the probability of choosing mention senses of a the phrases of mention names in the given docu ment. Since each mention sense corresponds to a entity, the mention sense disambiguation proces can also be regarded as linking entities to know edge base in a unsupervised way, which will b detailed in Section ??.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mention Sense Mapping</head><p>There are two kinds of mappings: from entities t mention senses, and from mention names to men tion senses. The former is pre-defined at the ver beginning. Given the knowledge Base KB, we ex tract entity titles {t l } and initialize with multip mention senses, where the sense number depend on how many entities share a common title. Th latter is to find possible mention senses for th given mention name, which is similar to candida mention generation in entity linking task.</p><p>Conventional candidate mention generatio generally maintains a list of pairs of mention nam and entity that denotes a candidate reference i knowledge base for the mention name, and recog nizes the mention name in text by accurate strin matching. Or it firstly recognizes possible mentio <ref type="bibr">1</ref> We can also annotate text corpus by using NER tool lik python nltk to recognize mentions, and disambiguating i mapped mention senses as described in Section 3.4. This an ongoing work with the goal of learning additional out-o KB senses by self-training. In this paper, we will focus o the effectiveness of our model and the quality of three kin of learned embeddings.  cur in the same contexts.</p><p>Similar to WDS, we maintain a context cluster for each mention sense, which can be used for dis- ambiguation given the contexts (Section 5). For example, in d1 of <ref type="figure" target="#fig_20">Figure 2</ref>, the context cluster of s ⇤ consists of all context vectors When encounter- ing a mention, the context vector we also maintain a context cluster center µ ⇤ j for each mention sense s ⇤ j , which is computed by averaging all the context vectors belonging to the cluster. We define context vector as the average sum of context word embeddings 1 |C(wi)| P wj 2C(wi) w j . The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_36">d1, d2, d3, s ⇤ j , wi/s ⇤ j s ⇤ Independence Day (US) P (ej|C(m l ), s ⇤ j ) P (C(wi)|wi) · P (C(m l )|s ⇤ j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_37">L = Lw + Le + Lm<label>(8)</label></formula><p>Based on language model, identifying mention senses in a document can be regarded as maximiz- ing their joint probability. However, the global op- timum is expensive, in which each mention gets an optimum sense, to search over the space of all mention senses of all mentions in the document. Thus, we approximately assign each mention in- dependently:</p><formula xml:id="formula_38">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )<label>(9)</label></formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_39">P (s l j ) = ( |Ae j | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance to the cluster. We define context vector as the average sum of context word embeddings</p><formula xml:id="formula_40">1 |C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_41">d 1 , d 2 , d 3 , s ⇤ j , w i /s ⇤ j s ⇤ Independence Day (US) P (e j |C(m l ), s ⇤ j ) P (C(w i )|w i ) · P (C(m l )|s ⇤ j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_42">L = L w + L e + L m<label>(8)</label></formula><formula xml:id="formula_43">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )</formula><p>where P (C(m l )|s l j ) is proportional to cosine s ilarity between context vector and mention se cluster center µ l j to measure the mention's lo similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l occurring in a piece of text (e.g. a docume and P ( ˆ N (m l )|s l j ) is defined as global proba ity since it measures global coherence of neigh mentions. The underlying idea is to achieve c sistent semantics in a piece of text assuming t all mentions inside it are talking about the sa topic. In this paper, we regard the mention sen identified first as neighbors of the rest mention P (s l j ) denotes prior probability of a ment sense occurring in texts proportional to the quency of corresponding entity in Wikipedia chors:</p><formula xml:id="formula_44">P (s l j ) = ( |A ej | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth gaps between different entity frequencies, nam smoothing parameter. It controls the importa cur in the same contexts. Similar to WDS, we maintain a context cluster for each mention sense, which can be used for dis- ambiguation given the contexts (Section 5). For example, in d1 of <ref type="figure" target="#fig_20">Figure 2</ref>, the context cluster of s ⇤ consists of all context vectors When encounter- ing a mention, the context vector we also maintain a context cluster center µ ⇤ j for each mention sense s ⇤ j , which is computed by averaging all the context vectors belonging to the cluster. We define context vector as the average sum of context word embeddings</p><formula xml:id="formula_45">1 |C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_46">d1, d2, d3, s ⇤ j , wi/s ⇤ j s ⇤ Independence Day (US) P (ej|C(m l ), s ⇤ j ) P (C(wi)|wi) · P (C(m l )|s ⇤ j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_47">L = Lw + Le + Lm<label>(8)</label></formula><p>Based on language model, identifying mention senses in a document can be regarded as maximiz- ing their joint probability. However, the global op- timum is expensive, in which each mention gets an optimum sense, to search over the space of all mention senses of all mentions in the document. Thus, we approximately assign each mention in- dependently:</p><formula xml:id="formula_48">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )<label>(9)</label></formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_49">P (s l j ) = ( |Ae j | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance wi,ml2D 0</p><formula xml:id="formula_50">+ log P (C(m l )|s ⇤ j )<label>(6)</label></formula><p>where s ⇤ j = g(&lt; m l , e j &gt;) is obtained from an- chors in wikipedia articles.</p><p>Thus, similar words and mention senses will be closed in text space, such as w film and s ⇤ Independence Day (film) , or w celebrations and s ⇤ Independence Day (US) because they frequently oc- cur in the same contexts.</p><p>Similar to WDS, we maintain a context cluster for each mention sense, which can be used for dis- ambiguation given the contexts (Section 5). For example, in d 1 of <ref type="figure" target="#fig_20">Figure 2</ref>, the context cluster of s ⇤ consists of all context vectors When encounter- ing a mention, the context vector we also maintain a context cluster center µ ⇤ j for each mention sense s ⇤ j , which is computed by averaging all the context vectors belonging to the cluster. We define context vector as the average sum of context word embeddings</p><formula xml:id="formula_51">1 |C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_52">d 1 , d 2 , d 3 , s ⇤ j , w i /s ⇤ j s ⇤ Independence Day (US) P (e j |C(m l ), s ⇤ j ) P (C(w i )|w i ) · P (C(m l )|s ⇤ j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_53">L = L w + L e + L m<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mention Sense Disambiguation</head><p>MPME learns each mention with multiple sense embeddings, and each sense corresponds to a con- text cluster. Given an annotated document D 0 in- cluding M mentions, and their sense sets accord- ing to Section ??: M ⇤ l = {s l j |s l j 2 g(m l ), m l 2 M}. In this section, we describe how to determine the mention sense for each mention m l in the doc- ument.</p><p>Based on language model, identifying mention senses in a document can be regarded as maximiz- ing their joint probability. However, the global op- timum is expensive, in which each mention gets an optimum sense, to search over the space of all mention senses of all mentions in the document. Thus, we approximately assign each mention in- dependently:</p><formula xml:id="formula_54">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )<label>(9)</label></formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_55">P (s l j ) = ( |A ej | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance Similar to WDS, we maintain a context cluster for each mention sense, which can be used for dis- ambiguation given the contexts (Section 5). For example, in d 1 of <ref type="figure" target="#fig_20">Figure 2</ref>, the context cluster of s ⇤ consists of all context vectors When encounter- ing a mention, the context vector we also maintain a context cluster center µ ⇤ j for each mention sense s ⇤ j , which is computed by averaging all the context vectors belonging to the cluster. We define context vector as the average sum of context word embeddings</p><formula xml:id="formula_56">1 |C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_57">d 1 , d 2 , d 3 , s ⇤ j , w i /s ⇤ j s ⇤ Independence Day (US) P (e j |C(m l ), s ⇤ j ) P (C(w i )|w i ) · P (C(m l )|s ⇤ j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_58">L = L w + L e + L m<label>(8)</label></formula><p>Based on language model, identifying mention senses in a document can be regarded as maximiz- ing their joint probability. However, the global op- timum is expensive, in which each mention gets an optimum sense, to search over the space of all mention senses of all mentions in the document. Thus, we approximately assign each mention in- dependently:</p><formula xml:id="formula_59">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )<label>(9)</label></formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_60">P (s l j ) = ( |A ej | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance Independence <ref type="bibr">Day (film)</ref> s ⇤ Independence Day (US) because they frequently oc- cur in the same contexts.</p><p>Similar to WDS, we maintain a context cluster for each mention sense, which can be used for dis- ambiguation given the contexts (Section 5). For example, in d 1 of <ref type="figure" target="#fig_20">Figure 2</ref>, the context cluster of s ⇤ consists of all context vectors When encounter- ing a mention, the context vector we also maintain a context cluster center µ ⇤ j for each mention sense s ⇤ j , which is computed by averaging all the context vectors belonging to the cluster. We define context vector as the average sum of context word embeddings</p><formula xml:id="formula_61">1 |C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_62">d 1 , d 2 , d 3 , s ⇤ j , w i /s ⇤ j s ⇤ Independence Day (US) P (e j |C(m l ), s ⇤ j ) P (C(w i )|w i ) · P (C(m l )|s ⇤ j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_63">L = L w + L e + L m<label>(8)</label></formula><note type="other">l ument. Based on language model, identifying mention senses in a document can be regarded as maximiz- ing their joint probability. However, the global op- timum is expensive, in which each mention gets an optimum sense, to search over the space of all mention senses of all mentions in the document. Thus, we</note><p>approximately assign each mention in- dependently:</p><formula xml:id="formula_64">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )<label>(9)</label></formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_65">P (s l j ) = ( |A ej | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance </p><note type="other">Thus, similar words and mention senses will be closed in text space, such as w film and s ⇤ Independence Day (film) , or w celebrations and s ⇤ Independence Day (US) because they frequently oc- cur in the same contexts.</note><p>Similar to WDS, we maintain a context cluster for each mention sense, which can be used for dis- ambiguation given the contexts (Section 5). For example, in d 1 of <ref type="figure" target="#fig_20">Figure 2</ref>, the context cluster of s ⇤ consists of all context vectors When encounter- ing a mention, the context vector we also maintain a context cluster center µ ⇤ j for each mention sense s ⇤ j , which is computed by averaging all the context vectors belonging to the cluster. We define context vector as the average sum of context word embeddings</p><formula xml:id="formula_66">1 |C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_67">d 1 , d 2 , d 3 , s ⇤ j , w i /s ⇤ j s ⇤ Independence Day (US) P (e j |C(m l ), s ⇤ j ) P (C(w i )|w i ) · P (C(m l )|s ⇤ j )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_68">L = L w + L e + L m<label>(8)</label></formula><note type="other">ing to Section ??: M ⇤ l = {s l j |s l j 2 g(m l ), m l 2 M}. In this section, we describe how to determine the mention sense for each mention m l in the doc- ument. Based on language model, identifying mention senses in a document can be regarded as maximiz- ing their joint probability. However, the global</note><p>op- timum is expensive, in which each mention gets an optimum sense, to search over the space of all mention senses of all mentions in the document. Thus, we approximately assign each mention in- dependently:</p><formula xml:id="formula_69">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )<label>(9)</label></formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_70">P (s l j ) = ( |A ej | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance </p><note type="other">here s ⇤ j = g(&lt; m l , e j &gt;) is obtained from an- hors in wikipedia articles. Thus, similar words and mention senses ill be closed in text space, such as w film nd s ⇤ Independence Day (film) , or w celebrations and ⇤ Independence Day (US) because they frequently oc- ur in the same contexts.</note><p>Similar to WDS, we maintain a context cluster or each mention sense, which can be used for dis- mbiguation given the contexts (Section 5). For xample, in d 1 of <ref type="figure" target="#fig_20">Figure 2</ref>, the context cluster of ⇤ consists of all context vectors When encounter- ng a mention, the context vector we also maintain a context cluster center µ ⇤ j or each mention sense s ⇤ j , which is computed y averaging all the context vectors belonging o the cluster. We define context vector as he average sum of context word embeddings</p><formula xml:id="formula_71">1 C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ul for inducing mention sense in contexts. When ncounter a mention, we map it to a set of mention enses, and then find the nearest one according to he distance from its context vector to each men- ion sense cluster center, which will be discussed n Section 5.</p><formula xml:id="formula_72">d 1 , d 2 , d 3 , s ⇤ j , w i /s ⇤ j s ⇤ Independence Day (US) P (e j |C(m l ), s ⇤ j ) P (C(w i )|w i ) · P (C(m l )|s ⇤ j )<label>(7)</label></formula><p>.5 Joint Training onsidering all the above representation learning omponents, we define the overall objective func- ion as linear combinations:</p><formula xml:id="formula_73">L = L w + L e + L m<label>(8)</label></formula><note type="other">embeddings, and each sense corresponds to a con- text cluster. Given an annotated document D 0 in- cluding M mentions, and their sense sets accord- ing to Section ??: M ⇤ l = {s l j |s l j 2 g(m l ), m l 2 M}. In this section, we describe how to determine the mention sense for each mention m l in the doc- ument.</note><p>Based on language model, identifying mention senses in a document can be regarded as maximiz- ing their joint probability. However, the global op- timum is expensive, in which each mention gets an optimum sense, to search over the space of all mention senses of all mentions in the document. Thus, we approximately assign each mention in- dependently:</p><formula xml:id="formula_74">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j )<label>(9)</label></formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_75">P (s l j ) = ( |A ej | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance </p><note type="other">ing a mention, the context vector we also maintain a context cluster center µ ⇤ j for each mention sense s ⇤ j , which is computed by averaging all the context vectors belonging to the cluster. We define context vector as the average sum of context word embeddings</note><formula xml:id="formula_76">1 |C(wi)| P wj 2C(wi) w j .</formula><p>The cluster center is help- ful for inducing mention sense in contexts. When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each men- tion sense cluster center, which will be discussed in Section 5.</p><formula xml:id="formula_77">d 1 , d 2 , d 3 , s ⇤ j , w i /s ⇤ j , e 3 s ⇤ Independence Day (US) P (e j |C(m l ), s ⇤ j ) P (C(w i )|w i ) · P (C(m l )|s ⇤ j ) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all the above representation learning components, we define the overall objective func- tion as linear combinations:</p><formula xml:id="formula_78">L = L w + L e + L m (8)</formula><p>Thus, we approximately assign each mention in- dependently:</p><formula xml:id="formula_79">P (D 0 , . . . , s l j , . . . , ) ⇡ Y P (D 0 |s l j ) · P (s l j ) ⇡ Y P (C(m l )|s l j ) · P ( ˆ N (m l )|s l j ) · P (s l j ) (9)</formula><p>where P (C(m l )|s l j ) is proportional to cosine sim- ilarity between context vector and mention sense cluster center µ l j to measure the mention's local similarity, namely local probability. ˆ N (m l ) denotes neighbor mentions of m l co- occurring in a piece of text (e.g. a document), and P ( ˆ N (m l )|s l j ) is defined as global probabil- ity since it measures global coherence of neighbor mentions. The underlying idea is to achieve con- sistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic. In this paper, we regard the mention senses identified first as neighbors of the rest mentions.</p><p>P (s l j ) denotes prior probability of a mention sense occurring in texts proportional to the fre- quency of corresponding entity in Wikipedia an- chors:</p><formula xml:id="formula_80">P (s l j ) = ( |A ej | |A| ) 2 [0, 1]</formula><p>where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter. It controls the importance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Sense Mapping</head><p>To reduce the size of the mention vocabulary, each mention is mapped to a set of shared mention senses according to a predefined dictionary. We build the dictionary by collecting entity-mention pairs &lt; m l , e j &gt; from Wikipedia anchors and page titles, then cre- ate mention senses if there is a different entity. The sense number of a mention depends on how many different entity-mention pairs it is involved. Formally, we have:</p><formula xml:id="formula_81">M * l = g(m l ) = g(&lt; m l , e j &gt;) = {s * j },</formula><p>where g(·) denotes the map- ping function from an entity mention to its men- tion sense given an anchor. We directly use the anchors contained in the annotated text cor- pus D for training. As <ref type="figure" target="#fig_20">Figure 2</ref> shows, we re- place the anchor &lt;July 4th, Independence Day (US)&gt; with the corresponding mention sense: s * Independence Day (U S) . Representation Learning Using KB, A and D as input, we design three separate models and a unified optimization objective to jointly learn en- tity, word and mention sense representations into two semantic spaces. As shown in the knowledge space in <ref type="figure" target="#fig_20">Figure 2</ref>, entity embeddings can reflect their relatedness in the network. For example, Independence Day (US) (e 1 ) and Memorial Day (e 3 ) are close to each other because they share some common neighbors, such as United States and Public holidays in the United States.</p><p>Word and mention embeddings are learned in the same semantic space. As two basic units in D , their embeddings represent their distributed semantics in texts. For example, mention Inde- pendence Day and word celebrations co-occur fre- quently when it refers to the holiday: Indepen- dence Day (US), thus they have similar representa- tions. Without disambiguating the mention senses, some words, such as film will also share similar representations as Independence Day. Besides, by introducing entity embeddings into our MPME framework, the knowledge informa- tion will also be distilled into mention sense em- beddings, so that the mention sense Memorial Day will be similar as Independence Day (US).</p><p>Mention Sense Disambiguation According to our predefined dictionary, each mention has been mapped to more than one senses, and learned with multiple embedding vectors. Consequently, to in- duce the correct sense for a mention within a con- text is critical in the usage of the multiprototype embeddings, especially in an unsupervised way. Formally, given an annotated document D , we determine one sensê s * j ∈ M * l for each mention m l ∈ D , wherê s * j is the correct sense. Based on language model, we design a mention sense disambiguation method without using any supervision that takes into account three aspects: 1) sense prior denotes how dominant the sense is, 2) local context information reflects how seman- tically appropriate the sense is in the context, and 3) global mention information denotes how se- mantically consistent the sense is with the neigh- bor mentions. To better utilize the context infor- mation, we maintain a context cluster for each mention sense during training, which will be de- tailed in Section 4.4.</p><p>Since each mention sense corresponds to an en- tity in the given KB, the disambiguation method is equivalent to entity linking. Thus, text and knowl- edge base is bridged via the multiprototype men- tion embeddings. We will give more analysis in Section 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Representation Learning</head><p>Distributional representation learning plays an in- creasing important role in many fileds ( <ref type="bibr" target="#b1">Bengio et al., 2013;</ref><ref type="bibr" target="#b35">Zhang et al., 2017</ref><ref type="bibr" target="#b36">Zhang et al., , 2016</ref>) due to its effectiveness for dimensionality reduction and ad- dressing sparseness issue. For NLP tasks, this trends has been accelerated by the Skip-gram and CBOW models <ref type="bibr" target="#b18">(Mikolov et al., 2013a</ref>,b) due to its efficiency and remarkable semantic composition- ality of embedding vectors. In this section, we first briefly introduce the Skip-gram and CBOW mod- els, and then extend them to three variants for the word, mention and entity representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Skip-Gram and CBOW model</head><p>The basic idea of the Skip-gram and CBOW mod- els is to model the predictive relations among se- quential words. Given a sequence of words D, the optimization objective of Skip-gram model is to use the current word to predict its context words by maximizing the average log probability:</p><formula xml:id="formula_82">L = w i ∈D wo∈C(w i ) log P (w o |w i ) (1)</formula><p>In contrast, CBOW model aims to predict the current word given its context words:</p><formula xml:id="formula_83">L = w i ∈D log P (w i |C(w i )) (2)</formula><p>Formally, the conditional probability P (w o |w i ) is defined using a softmax function:</p><formula xml:id="formula_84">P (w o |w i ) = exp(w i · w o ) wo∈D exp(w i · w o ) (3)</formula><p>where w i , w o denote the input and output word vectors during training. Furthermore, these two models can be accelerated by using hierarchi- cal softmax or negative sampling ( <ref type="bibr" target="#b18">Mikolov et al., 2013a</ref>,b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Representation Learning</head><p>Given a knowledge base KB, we aim to learn entity embeddings by modeling "contextual" en- tities, so that the entities sharing more common neighbors tend to have similar representations. Therefore, we extend Skip-gram model to a net- work by maximizing the log probability of being a neighbor entity.</p><formula xml:id="formula_85">L e = e j ∈E log P (N (e j )|e j ) (4)</formula><p>Clearly, the neighbor entities serve a similar role as the context words in Skip-gram model. As shown in <ref type="figure" target="#fig_20">Figure 2</ref>, entity Memorial Day (e 3 ) also share two common neighbors of United States and Public holidays in the United States with entity In- dependence Day (US), thus their embeddings are close in the Knowledge Space. These entity em- beddings will be later used to learn mention repre- sentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mention Representation Learning</head><p>As mentioned above, the textual context informa- tion and reference entities are helpful to distin- guish different senses for a mention. Thus, given an anchor &lt; m l , e j &gt; and its context words C(m l ), we combine mention sense embeddings with its context word embeddings to predict the reference entity by extending CBOW model. The objective function is as follows:</p><formula xml:id="formula_86">L m = &lt;m l ,e j &gt;∈A log P (e j |C(m l ), s * j ) (5)</formula><p>where s * j = g(&lt; m l , e j &gt;). Thus, if two mentions refer to similar entities and share similar contexts, they tend to be close in semantic vector space. Take <ref type="figure" target="#fig_0">Figure 1</ref> as an example again, mentions Inde- pendence Day and Memorial Day refer to similar entities Independence Day (US) (e 1 ) and Memo- rial Day (e 2 ), they also share some similar context words, such as celebrations in documents d 2 , d 3 , so their sense embeddings are close to each other in the text space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Text Representation Learning</head><p>Instead of directly using a word or a mention to predict the context words, we incorporate mention sense to joint optimize word and sense represen- tations, which can avoid some noise introduced by ambiguous mentions. For example, in <ref type="figure" target="#fig_20">Fig- ure 2</ref>, without identifying the mention Indepen- dence Day as the holiday or the film, various dis- similar context words such as the words celebra- tions and film in documents d 1 , d 2 will share simi- lar semantics, which will further affect the perfor- mance of entity representations during joint train- ing.</p><p>Given the annotated corpus D , we use a word w i or a mention sense s * j to predict the con- text words by maximizing the following objective function:</p><formula xml:id="formula_87">L w = w i ,m l ∈D log P (C(w i )|w i ) + log P (C(m l )|s * j ) (6)</formula><p>where s * j = g(&lt; m l , e j &gt;) is obtained from an- chors in Wikipedia articles.</p><p>Thus, words and mention senses will share the same vector space, where similar words and men- tion senses are close to each other, such as cele- brations and Independence Day (US) because they frequently occur in the same contexts.</p><p>Similar to WDS, we maintain a context clus- ter for each mention sense, which can be used for mention sense disambiguation (Section 5). The context cluster of a mention sense s * j contains all the context vectors of its mention m l . We compute context vector of m l by averaging the sum of its context word embeddings:</p><formula xml:id="formula_88">1 |C(m l )| w j ∈C(m l ) w j .</formula><p>Further, the center of a context cluster µ * j is de- fined as the average of context vectors of all men- tions which refer to the sense. These context clus- ters will be later used to disambiguate the sense of a given mention with its contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Joint Training</head><p>Considering all of the above representation learn- ing components, we define the overall objective function as linear combinations:</p><formula xml:id="formula_89">L = L w + L e + L m<label>(7)</label></formula><p>The goal of training MPME is to maximize the above function, and iteratively update three types of embeddings. Also, we use negative sampling technique for efficiency <ref type="bibr" target="#b18">(Mikolov et al., 2013a)</ref>. MPME shares the same entity representation learning method with ( <ref type="bibr" target="#b34">Yamada et al., 2016)</ref>, but the role of entities in the entire framework as well as mention representation learning is different in three aspects. First, we focus on learning embed- dings for mentions, not merely words as in <ref type="bibr" target="#b34">(Yamada et al., 2016)</ref>. Clearly, MPME is more natu- ral to integrate text and knowledge base. Second, we propose to learn multiple embeddings for each mention denoting its different meanings. Third, we prefer to use both mentions and context words to predict entities, so that the distribution of en- tities will help improve word embeddings, mean- while, avoid being hurt if we force entity embed- dings to satisfy word embeddings during train- ing ( <ref type="bibr" target="#b29">Wang et al., 2014</ref>). We will give more analy- sis in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mention Sense Disambiguation</head><p>As mentioned in Section 3, we induce a correct sensê s * j ∈ M * l for each mention m l in an an- notated document D . We regard this problem from the perspective of language model that max- imizes a joint probability of all mention senses contained in the document. However, the global optimum is expensive with a time complexity of O(|M||M * l |). Thus, we approximately identify each mention sense independently:</p><formula xml:id="formula_90">P (D , . . . , s * j , . . . , ) ≈ P (D |s * j ) · P (s * j ) ≈ P (C(m l )|s * j ) · P ( ˆ N (m l )|s * j ) · P (s * j )<label>(8)</label></formula><p>where P (C(m l )|s * j ), local context information (Section 3), denotes the probability of the local contexts of m l given its mention sense s * j . we define it proportional to the cosine similarity be- tween the current context vector and the sense con- text cluster center µ * j as described in Section 4.4. It measures how likely a mention sense occurring together with current context words. For example, given the mention sense Independence Day (film), word film is more likely to appear within the con- text than the word celebrations. P ( ˆ N (m l )|s l j ), global mention information, de- notes the probability of the contextual mentions of m l given its sense s l j , wherê N (m l ) is the collec- tion of the neighbor mentions occurring together with m l in a predefined context window. We de- fine it proportional to the cosine similarity be- tween mention sense embeddings and the neigh- bor mention vector, which is computed similar to context vector:</p><formula xml:id="formula_91">1 | ˆ N (m l )| ˆ s l j , wherê s l j is the cor- rect sense for m l .</formula><p>Considering there are usually multiple mentions in a document to be disambiguated. The men- tions disambiguated first will be helpful for induc- ing the senses of the rest mentions. That is, how to choose the mentions disambiguated first will in- fluence the performance. Intuitively, we adopt two orders similar to ): 1) L2R (left to right) induces senses for all the mentions in the document following natural order that varies ac- cording to language, normally from left to right in the sequence. 2) S2C (simple to complex) denotes that we determine the correct sense for those men- tions with fewer senses, which makes the problem easier.</p><p>Global mention information assumes that there should be consistent semantics in a context win- dow, and measures whether all neighbor mentions are related. For instance, two mentions Memorial Day and Independence Day occur in the same doc- ument. If we already know that Memorial Day de- notes a holiday, then obviously Independence Day has higher probability of being a holiday than a film.</p><p>P (s * j ), sense prior, is a prior probability of sense s * j indicating how possible it occurs with- out considering any additional information. We define it proportional to the frequency of sense s * j in Wikipedia anchors:</p><formula xml:id="formula_92">P (s * j ) = ( |A s * j | |A| ) γ γ ∈ [0, 1]</formula><p>where A s * j is the set of anchors annotated with s * j , and γ is a smoothing hyper-parameter to con- trol the impact of prior on the overall probability, which is set by experiments (Section 6.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>Setup We choose Wikipedia, the March 2016 dump, as training corpus, which contains nearly 75 millions of anchors, 180 millions of edges among entities and 1.8 billions of tokens after pre- processing. We then train MPME 2 for 1.5 millions of words, 5 millions of entities and 1.7 millions of mentions. The entire training process in 10 iter- ations costs nearly 8 hours on the server with 64 core CPU and 188GB memory.</p><p>We use the default settings in word2vec 3 , and set our embedding dimension as 200 and context window size as 5. For each positive example, we sample 5 negative examples 4 .</p><p>Baseline Methods As far as we know, this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison. We use the method in ( <ref type="bibr" target="#b34">Yamada et al., 2016</ref>) as a base- line, marked as ALIGN 5 , because (1) this is the most similar work that directly aligns word and en- tity embeddings. (2) it achieves the state-of-the-art performance in entity linking task.</p><p>To investigate the effect of multi-prototype, we degrade our method to single-prototype as another baseline, which means to use one sense to repre- sent all mentions with the same phrase, namely Single-Prototype Mention Embedding (SPME). For example, SPME only learns one unique sense vector for Independence Day whatever it denotes a holiday or a film.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Qualitative Analysis</head><p>We use cosine similarity to measure the similar- ity of two vectors, and present the top 5 nearest words and entities for two most popular senses of the mention Independence Day. Because ALIGN is incapable of dealing with multiple words, we only present the results of SPME and MPME.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, without considering men- tion sense, the mention Independence Day can only show a dominant holiday sense based on SPME and ignore all other senses. Instead, MPME successfully learns two clear and distinct senses. For the sense Independence Day (US), all of its nearest words and entities, such as parades, cele- brations, and Memorial Day, are holiday related, while for another sense Independence Day (film), its nearest words and entities, like robocop and The Terminator, are all science fiction films. The results verify the effectiveness of our framework in learning mention embeddings at the sense level.  <ref type="table">Table 1</ref>: The nearest neighbors of mention Independence Day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Entity Relatedness</head><p>To evaluate the quality of entity embeddings, we conduct experiments using the dataset which is de- signed for measuring entity relatedness ( <ref type="bibr" target="#b3">Ceccarelli et al., 2013;</ref><ref type="bibr" target="#b11">Huang et al., 2015;</ref><ref type="bibr" target="#b34">Yamada et al., 2016</ref>). The dataset contains 3,314 entities, and each mention has 91 candidate entities on average with gold-standard labels indicating whether they are semantically related. We compute cosine similarity between entity embeddings to measure their relatedness, and rank them in a descending order. To evaluate the ranking quality, we use two standard metrics: normalized discounted cumulative gain (NDCG) <ref type="bibr" target="#b13">(Järvelin and Kekäläinen, 2002</ref>) and mean average precision (MAP) <ref type="bibr" target="#b24">(Schütze, 2008)</ref>.</p><p>We design another baseline method: En- tity2vec, which learns entity embeddings using the method described in Section 4.2, without joint training with word and mention sense embed- dings.  <ref type="table" target="#tab_21">Table 2</ref>, ALIGN achieves lower performance than Entity2vec, because it doesn't consider the mention phrase ambiguity and yields lots of noise when forcing entity embeddings to satisfy word embeddings and aligning them into the unified space. For example, the entity Gente (magazine) should be more relevant to the en- tity France, the place where its company lo- cates. However, ALIGN mixed various meanings of mention Gente (e.g., the song) and ranked some bands higher (e.g., entity Poolside (band)). SPME also doesn't consider the ambiguity of mentions but achieves comparative results with Entity2vec. We analyze the reasons and find that, it can avoid some noise by using word embed- dings to predict entities. MPME outperforms all the other methods, which demonstrates that the unambiguous textual information is helpful to re- fine the entity embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Word Analogical Reasoning</head><p>Following ( <ref type="bibr" target="#b18">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b29">Wang et al., 2014</ref>), we use the word analogical reasoning task to evaluate the quality of word embeddings. The dataset consists of 8,869 semantic questions ("Paris":"France"::"Rome":?), and 10,675 syn- tactic questions (e.g., "sit":"sitting"::"walk":?).</p><p>We solve it by finding the closest word vector w ? to w F rance −w P aris +w Rome according to cosine similarity. We compute accuracy for top 1 nearest word to measure the performance. We also adopt Word2vec 6 as an additional base- line method, which provides a standard to measure the impact from other components on word em- beddings. <ref type="table" target="#tab_22">Table 3</ref> shows the results. We can see that ALIGN, SPME and MPME, achieve higher performance in dealing with semantic ques- tions, because relations among entities (e.g., country-capital relation for entity France and Paris) enhance the semantics in word embeddings through jointly training. On the other hand, their performance for syntactic questions is weakened because more accurate semantics yields a bias to predict semantic relations even though given a syntactic query. For example, given the query "pleasant":"unpleasant"::"possibly":?, our model tends to return the word (e.g., probably) highly semantical related to query words, such as possibly, instead of the syntactical similar word impossibly. In this scenario, we are more concerned about semantic task to incorporate knowledge of reference entities into word embed- dings, and this issue could be tackled, to some extent, by using syntactic tool like stemming. The word embeddings of MPME achieve the best performance for semantic questions mainly because (1) text representation learning has bet- ter generalization ability due to the larger size of training examples than entities (e.g., 1.8b v.s. 0.18b) as well as relatively smaller size of vocab- ulary (e.g., 1.5m v.s. 5m). (2) unambiguous men- tion embeddings capture both textual context in- formation and knowledge, and thus enhance word and entity embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">A Case Study: Entity Linking</head><p>Entity linking is a core NLP task of identifying the reference entity for mentions in texts. The main difficulty lies in the ambiguity of various en- tities sharing the same mention phrase. Previous work addressed this issue by taking advantage of the similarity between words and entities (Francis- <ref type="bibr" target="#b7">Landau et al., 2016;</ref><ref type="bibr" target="#b25">Sun et al., 2015)</ref>, and/or the relations among entities <ref type="bibr">(Thien Huu Nguyen, 2016;</ref><ref type="bibr" target="#b2">Cao et al., 2015</ref>). Therefore, we use en- tity linking as a case study for a comprehensive measurement of the multi-prototype mention em- beddings. Given mentions in a text, entity linking aims to link them to a predefined knowledge base. One of the main challenges in this task is the am- biguity of entity mentions.</p><p>We use the public dataset AIDA created by <ref type="bibr" target="#b9">(Hoffart et al., 2011</ref>), which includes 1,393 docu- ments and 27,816 mentions referring to Wikipedia entries. The dataset has been divided into 946, 216 and 231 documents for the purpose of train- ing, developing and testing. <ref type="bibr">Following (Pershina et al., 2015;</ref><ref type="bibr" target="#b34">Yamada et al., 2016)</ref>, we use a pub- licly available dictionary to generate candidate en- tities and mention senses. For evaluation, we rank the candidate entities for each mention and report both standard micro (aggregates over all mentions) and macro (aggregates over all documents) preci- sion over top-ranked entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Entity Linking</head><p>Yamada et al. (2016) designed a list of fea- tures for each mention and candidate entity pair. By incorporating these features into a supervised learning-to-rank algorithm, Gradient Boosting Re- gression Tree (GBRT), each pair is assigned a relevance score indicating whether they should be linked to each other. Following their recom- mended parameters, we set the number of trees as 10,000, the learning rate as 0.02 and the maximum depth of the decision tree as 4.</p><p>Based on word and entity embeddings learned by ALIGN, the key features in ( <ref type="bibr" target="#b34">Yamada et al., 2016</ref>) are from two aspects: (1) the cosine simi- larity between context words and candidate entity, and (2) the coherence among "contextual" entities in the same document.</p><p>To evaluate the performance of multi-prototype mention embeddings, we incorporate the follow- ing features into GBDT for comparison: (1) the cosine similarity between the current context vec- tor and the sense context cluster center µ * j , which denotes how likely the mention sense refers to the candidate entity, (2) the cosine similarity between the current context vector and the mention sense embeddings.  <ref type="table" target="#tab_23">Table 4</ref>, we can see that ALIGN performs better than SPME. This is because SPME learns word embeddings and entity em- beddings in separate semantic spaces, and fails to measure the similarity between context words and candidate entities. However, MPME computes the similarity between context words with mention sense instead of entities, thus achieves the best per- formance, which also demonstrates the high qual- ity of the mention sense embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Entity Linking</head><p>Linking a mention to a specific entity equals to disambiguating mention senses since each candi- date entity corresponds to a mention sense. As de- scribed in Section 5, we disambiguate senses in two orders: (1) L2R (from left to right), and (2) S2C (from simple to complex).</p><p>We evaluate our unsupervised disambiguation methods on the entire AIDA dataset. To be fair, we choose the state-of-the-art unsupervised methods, which are proposed in <ref type="bibr" target="#b9">(Hoffart et al., 2011;</ref><ref type="bibr" target="#b0">Alhelbawy and Gaizauskas, 2014;</ref><ref type="bibr" target="#b5">Cucerzan, 2007;</ref><ref type="bibr"></ref>   <ref type="bibr" target="#b14">Kulkarni et al., 2009;</ref><ref type="bibr" target="#b17">Masumi Shirakawa and Nishio, 2011</ref>) using the same dataset. <ref type="table" target="#tab_24">Table 5</ref> shows the results. We can see that our two methods outperform all other methods. MPME (L2R) is more efficient and easy to ap- ply, while MPME (S2C) slightly outperforms it because the additional step of ranking mentions according to their candidates number guarantees a higher disambiguation performance for those simple mentions, which consequently help disam- biguate those complex mentions through global mention information in Equation 8.</p><p>We analyze the results and observe a disam- biguation bias to popular senses. For example, there are three mentions in the sentence "Japan began the defence of their Asian Cup I title with a lucky 2-1 win against Syria in a Group C cham- pionship match on Friday", where the country name Japan and Syria actually denote their na- tional football teams, while the football match name Asian Cup I has little ambiguity. Compared to the team, the sense of country occurs more fre- quently and has a dominant prior, which greatly affects the disambiguation. By incorporating lo- cal context information and global mention infor- mation, both the context words (e.g., defence or match) and the neighbor mentions (e.g., Asian Cup I) provide us enough clues to identify a soccer re- lated mention sense instead of the country.</p><p>Influence of Smoothing Parameter As men- tioned above, a mention sense may possess a dom- inant prior and greatly affect the disambiguation. So we introduce a smoothing parameter γ to con- trol its importance to the overall probability. <ref type="figure" target="#fig_21">Fig- ure 3</ref> shows the linking accuracy under different values of γ on the dataset of AIDA. γ = 0 indi- cates we don't use any prior knowledge, and γ = 1 indicates the case without smoothing parameter.</p><p>We can see that both micro and macro accu- racy decrease a lot if we don't use the parameter (γ = 1). Only using local and global probabilities for disambiguation (γ = 0) achieves a comparable performance when γ = 0.05, both accuracy reach their peaks, which is optimal and default value in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we propose a novel Multi-Prototype Mention Embedding model that jointly learns word, entity and mention sense embeddings. These mention senses capture both textual con- text information and knowledge from reference entities, and provide an efficient approach to dis- ambiguate mention sense in text. We conduct a series of experiments to demonstrate that multi- prototype mention embedding improves the qual- ity of both word and entity representations. Using entity linking as a study case, we apply our disam- biguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve the state-of-the-art.</p><p>In the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional ap- proaches to model the internal structures of multi- word mentions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>…</head><label></label><figDesc>holds annual [[Independence Day (US)| Independence Day]] celebrations and other festivals … … early Confederate [[Memorial Day]] celebrations were simple, somber occasions for veterans and their families to honor the dead …</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework of Multi-Prototype Mention Embedding model.</figDesc><graphic url="image-18.png" coords="3,447.79,192.21,79.72,79.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of Smoothing Parameter γ.</figDesc><graphic url="image-74.png" coords="9,314.36,135.72,204.09,153.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>This work is supported by NSFC Key Pro- gram (No. 61533018), 973 Program (No. 2014CB340504), Fund of Online Educa- tion Research Center, Ministry of Education (No. 2016ZD102), Key Technologies Re- search and Development Program of China (No. 2014BAK04B03), NSFC-NRF (No. 61661146007) and the U.S. DARPA LORELEI Program No. HR0011-15-C-0115.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="false"><head></head><label></label><figDesc>). (2) Entity Representation Learning based on out- links in Wikipedia pages, we construct a knowl- edge network to represent the semantic relatedness among entities. And then learn entity embeddings so that similar entities on the graph have simi- lar representations. (3) Mention Representation Learning given mapped anchors in contexts, we learn mention sense embeddings by incorporating both textual context embeddings and entity em- beddings. (4) Text Representation Learning we extend skip-gram model to simultaneously learn</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Entity Relatedness. 
NDCG 
MAP 
@1 
@5 
@10 
ALIGN 
0.416 0.432 0.472 0.410 
Entity2vec 0.593 0.595 0.636 0.566 
SPME 
0.593 0.594 0.636 0.566 
MPME 
0.613 0.613 0.654 0.582 

As shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" validated="false"><head>Table 3 : Word Analogical Reasoning.</head><label>3</label><figDesc></figDesc><table>Word2vec ALIGN SPME MPME 
Semantic 
66.78 
68.34 
71.65 
71.65 
Syntactic 
61.58 
59.73 
55.28 
54.75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of Supervised Method 
ALIGN SPME MPME 
Micro P@1 
0.828 
0.820 
0.851 
Macro P@1 
0.862 
0.844 
0.881 

As shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" validated="false"><head>Table 5 : Performance of Unsupervised Methods</head><label>5</label><figDesc></figDesc><table>Cucerzan Kulkarni Hoffart Shirakawa Alhelbawy MPME (L2R) MPME (S2C) 
Micro P@1 
0.510 
0.729 
0.818 
0.823 
0.842 
0.882 
0.885 
Macro P@1 
0.437 
0.767 
0.819 
0.830 
0.875 
0.875 
0.890 

</table></figure>

			<note place="foot" n="1"> Generally, the mention boundary can be obtained by using NER tools like Standford NER (Finkel et al., 2005). In this paper, we use Wikipedia anchors as annotations of Wikipedia text corpus for the concentration of our main purpose.</note>

			<note place="foot" n="2"> Our main code for MPME can be found in https://github.com/TaoMiner/bridgeGap.</note>

			<note place="foot" n="3"> https://code.google.com/archive/p/word2vec/ 4 We tested different parameters (e.g. window size of 10 and dimension of 500) which achieve similar results, and report the current settings considering program runtime efficiency. 5 We carefully re-implemented ALIGN and used the same shared parameters in our model for fairly comparison. However, we failed to fully reproduce the positive result in the original paper, meanwhile the authors are unable to release their code.</note>

			<note place="foot" n="6"> https://code.google.com/archive/p/word2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph ranking for collective named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayman</forename><surname>Alhelbawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Name list only? target entity disambiguation in short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuanhu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1077</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1077" />
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning relatedness measures for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ceccarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Trani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1110</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1110" />
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large-scale named entity disambiguation based on wikipedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silviu Cucerzan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capturing semantic similarity for entity linking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Francis-Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/N16-1150</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1256" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Joint representation learning of text and knowledge for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1611.04125</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07678</idno>
		<title level="m">Leveraging deep neural networks and knowledge graphs for entity disambiguation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liberal entity extraction: Rapid construction of fine-grained entity typing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James A</forename><surname>Hendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collective annotation of wikipedia entities in web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1200</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1200" />
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Embedding words and senses together via joint knowledgeenhanced training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno>CoRR abs/1612.02703</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Entity disambiguation based on a. technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song Zhongyuan Wang Kotaro Nakayama Takahiro Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masumi</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shojiro</forename><surname>Nishio</surname></persName>
		</author>
		<idno>MSR-TR- 2011-125</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Microsoft Research</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<idno type="doi">10.3115/v1/D14-1113</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1113" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Personalized page rank for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pershina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="238" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international communication of association for computing machinery conference</title>
		<meeting>the international communication of association for computing machinery conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling mention, context and entity with neural networks for entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1333" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint learning of local and global features for entity linking via neural networks</title>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<editor>Nicolas Fauceglia Mariano Rodriguez-Muro Oktie Hassanzadeh Alfio Massimiliano Gliozzo Mohammad Sadoghi Thien Huu Nguyen</editor>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="2310" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/D15-1174</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1174" />
	</analytic>
	<monogr>
		<title level="j">ACL Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="doi">10.3115/v1/D14-1167</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1167" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Text-enhanced representation learning for knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Zi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Knowledge representation via joint learning of sequential text and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/K16-1025</idno>
		<ptr target="https://doi.org/10.18653/v1/K16-1025" />
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zawlin</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08319</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online collaborative learning for open-vocabulary visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2809" to="2817" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
