<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transductive Adaptation of Black Box Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe 6 chemin Maupertuis</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe 6 chemin Maupertuis</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Chidlovskii</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe 6 chemin Maupertuis</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transductive Adaptation of Black Box Predictions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="326" to="331"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Access to data is critical to any machine learning component aimed at training an accurate predictive model. In reality, data is often a subject of technical and legal constraints. Data may contain sensitive topics and data owners are often reluctant to share them. Instead of access to data, they make available decision making procedures to enable predictions on new data. Under the black box classifier constraint, we build an effective domain adaptation technique which adapts classi-fier predictions in a transductive setting. We run experiments on text categorization datasets and show that significant gains can be achieved, especially in the unsuper-vised case where no labels are available in the target domain.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While huge volumes of unlabeled data are gener- ated and made available in various domains, the cost of acquiring data labels remains high. Do- main Adaptation problems arise each time when one leverage labeled data in one or more related source domains, to learn a classifier for unseen data in a target domain which is related, but not identical. The majority of domain adaptation methods makes an assumption of largely avail- able source collections; this allows to measure the discrepancy between distributions and either build representations common to both target and sources, or directly reuse source instances for a better target classification ( <ref type="bibr" target="#b21">Xu and Sun, 2012)</ref>.</p><p>Numerous approaches have been proposed to address domain adaptation for statistical machine translation ( <ref type="bibr" target="#b13">Koehn and Schroeder, 2007)</ref>, opin- ion mining, part of speech tagging and document ranking <ref type="bibr" target="#b8">(Daumé, 2009)</ref>, , ( <ref type="bibr" target="#b23">Zhou and Chang, 2014</ref>). Most effective tech- niques include feature replication <ref type="bibr" target="#b8">(Daumé, 2009)</ref>, pivot features ( <ref type="bibr" target="#b1">Blitzer et al., 2006</ref>), (  and finding topic models shared by source and target collections <ref type="bibr" target="#b5">(Chen and Liu, 2014</ref>). Do- main adaptation has equally received a lot of at- tention in computer vision ( <ref type="bibr" target="#b12">Gopalan et al., 2015)</ref> where domain shift is a consequence of changing conditions, such as background, location and pose, etc.</p><p>More recently, domain adaptation has been tackled with word embedding techniques or deep learning. ( <ref type="bibr" target="#b3">Bollegala et al., 2015)</ref> proposed an un- supervised method for learning domain-specific word embedding while <ref type="bibr" target="#b22">(Yang and Eisenstein, 2014</ref>) relied on word2vec models ( <ref type="bibr" target="#b15">Mikolov et al., 2013</ref>) to compute feature embedding. Deep learning has been considered as a generic solu- tion to domain adaptation ( <ref type="bibr" target="#b20">Vincent et al., 2008;</ref><ref type="bibr" target="#b11">Glorot et al., 2011</ref><ref type="bibr" target="#b7">), (Chopra et al., 2013</ref> and transfer learning problems ( <ref type="bibr" target="#b14">Long et al., 2015)</ref>. For instance, denoising autoencoders are success- ful models which find common features between source and target collection. They are trained to reconstruct input data from partial random corrup- tion and can be stacked into a multi-layered net- work where the weights are fine-tuned with back- propagation ( <ref type="bibr" target="#b20">Vincent et al., 2008</ref>) or marginalized out ( <ref type="bibr" target="#b6">Chen et al., 2012)</ref>.</p><p>Domain adaptation is also very attractive for service companies operating customer business processes as it can reduce annotation costs. For instance, opinion mining components deployed in a service solution can be customized to a new cus- tomer and adapted with few annotations in order to achieve a contractual performance.</p><p>But, in reality, the simplifying assumption of having access to source data rarely holds and lim- its therefore the application of existing domain adaptation methods. Source data are often a sub- ject of legal, technical and contractual constraints between data owners and data customers. Often, customers are reluctant to share their data. In- stead, they often put in place decision making pro- cedures. This allows to obtain predictions for new data under a black box scenario. Note that this scenario is different from the differential privacy setting <ref type="bibr" target="#b10">(Dwork and Roth, 2014</ref>) in the sense that no queries to the raw source database are allowed whereas, in our case, only requests for predict- ing labels of target documents are permitted. This makes privacy preserving machine learning meth- ods inapplicable here <ref type="bibr" target="#b4">(Chaudhuri and Monteleoni, 2008)</ref>, <ref type="bibr" target="#b0">(Agrawal and Srikant, 2000</ref>).</p><p>In addition, black boxes systems are frequent in natural language processing applications. For in- stance, Statistical Machine Translation (SMT) sys- tems are often used as black box to extract fea- tures ( <ref type="bibr">Specia et al., 2009)</ref>. Similarly, the prob- lem of adapting SMT systems for cross lingual retrieval has been addressed in ( <ref type="bibr" target="#b16">Nikoulina et al., 2012</ref>) where target document collections cannot be accessed and the retrieval engine works as a black box.</p><p>In this paper we address the problem of adapt- ing classifiers trained on the source data and avail- able as black boxes. The case of available source classifiers has been studied by <ref type="bibr" target="#b9">(Duan et al., 2009)</ref> to regularize supervised target classifiers, but we consider here a transductive setting, where the source classifiers are used to predict class scores for a set of available target instances.</p><p>We then apply the denoising principle <ref type="bibr" target="#b20">(Vincent et al., 2008)</ref> and consider these predictions on target instances as corrupted by the domain shift from the source to target. More precisely, we use the stacked Marginalized Denoising Au- toencoders ( <ref type="bibr" target="#b6">Chen et al., 2012</ref>) to reconstruct the predictions by exploiting the correlation between the target features and the predicted scores. This method has the advantage of coping with unsuper- vised cases where no labels in the target domain is available. We test the prediction denoising method on two benchmark text classification datasets and demonstrate its capacity to significantly improve the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transductive Prediction Adaptation</head><p>The domain adaptation problem consists of lever- aging the source labeled and target unlabeled data to derive a hypothesis performing well on the target domain. To achieve this goal, most DA methods compute correlation between features in source and target domains. With no access to source data, we argue that the above principle can be extended to the correlation between target fea- tures and the source class decisions. We tune an adaptation trick by considering predicted class scores as augmented features for target data. In other words, we use the source classifiers as a pivot to transfer knowledge from source to target. In addition, one can exploit relations between the predictions scores and the target feature distribu- tion to provide adapted predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Marginalized Denoising Autoencoder</head><p>The stacked Marginalized Denoising Autoencoder (sMDA) is a version of the multi-layer neural net- work trained to reconstruct input data from partial random corruption <ref type="bibr" target="#b20">(Vincent et al., 2008)</ref> proposed by <ref type="bibr" target="#b6">(Chen et al., 2012)</ref>, where the random corrup- tion is marginalized out yielding the optimal re- construction weights in the closed form.</p><p>The basic building block of the method is a one- layer linear denoising autoencoder where a set of N input documents x n are corrupted M times by random feature dropout with the probability p. It is then reconstructed with a linear mapping W : R d → R d by minimizing the squared recon- struction loss <ref type="bibr">1</ref> :</p><formula xml:id="formula_0">L(W) = N n=1 M m=1 ||x n − W˜xW˜x nm || 2 .<label>(1)</label></formula><p>Let ¯ X be the concatenation of M replicated ver- sion of the original data and˜Xand˜ and˜X be the matrix rep- resentation of the M corrupted versions.</p><p>Then, the solution of (1) can be expressed as the closed-form solution for ordinary least squares W = PQ −1 with Q = ˜ X ˜ X and P = ¯ X ˜ X , where the solution depends on the re-sampling of x 1 , . . . , x N and which features are randomly cor- rupted.</p><p>It is preferable to consider all possible corrup- tions of all possible inputs when the denoising transformation W is computed, i.e. letting m → ∞. By the weak law of large numbers, the ma- trices P and Q converge to their expected values E[Q], E <ref type="bibr">[P]</ref> as more copies of the corrupted data are created. In the limit, one can derive their ex- pectations and express the corresponding mapping for W in a closed form as W = E[P] E[Q] −1 , where:</p><formula xml:id="formula_1">E[Q] ij = S ij q i q j , if i = j, S ij q i , if i = j,</formula><p>and E[P] ij = S ij q j where q = [1 − p, . . . , 1 − p, 1] ∈ R d+1 and S = XX is the covariance ma- trix of the uncorrupted data. This closed form de- noising layer with a unique noise p is referred in the following as marginalized denoising autoen- coder (MDA). It was shown by <ref type="bibr" target="#b6">(Chen et al., 2012</ref>) that MDA can be applied with success to domain adaptation where the source set X s and target set X t are con- catenated to form X and the mapping W can ex- ploit the correlation between source and target fea- tures. The case of fully available source and target data is referred as a dream case in the evaluation section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prediction Adaptation</head><p>Without access to X s , MDA cannot be directly ap- plied to [X s ; X t ]. Instead, we augment the fea- ture set X t with the class predictions represented as vector f s (x t ) of class predictions P s (Y = y|x t n ), n = 1, . . . , N . Let u t n = [x t n ; f s (x t n )] be the target instance augmented with the source classifier predictions and U = [u t 1 u t 2 . . . u t N ] be the input to the MDA. Then we compute the op- timal mapping W * = min W ||U − W ˜ U|| 2 that takes into account the correlation between the tar- get features x t and class predictions f s (x t ). The reconstructed class predictions can be obtained as</p><formula xml:id="formula_2">W * [1:N,d+1:d+C] · f s (x t ),</formula><p>where C is the number of classes, and used to label the target data. Al- gorithm 1 summarizes all steps of the transductive prediction adaptation for a single source domain; the generalization to multiple sources is straight- forward 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>We test our approach on two standard domain adaptation datasets: the Amazon reviews (AMT) and the 20Newsgroups (NG). The AMT dataset consists of products reviews with 2 classes (posi- tive and negative) represented by tf-idf normalized Algorithm 1 Transductive prediction adaptation. Require: Unlabeled target dataset X t ∈ R N ×d . Require: Class predictions f s (x t ) = [P s (Y = 1|x t i ), . . . , P s (Y = C|x t n )] ∈ R C . 1: Compose U ∈ R N ×(d+C) with u t n = [x t n ; f s (x t n )]. 2: Use MDA with noise level p to estimate W * = min W ||U − W ˜ U|| 2 . 3: Get the denoised class predictions for x t as y t = W * [1:N,d+1:d+C] · f s (x t ). 4: Label x t with c * = argmax c {y t c |y t }. 5: return Labels for X t .</p><p>bag-of-words, used in previous studies on domain adaptation <ref type="bibr" target="#b2">(Blitzer et al., 2011</ref>). We consider the 10,000 most frequent features and four domains used in the studies: kitchen (k), dvd (d), books (b) and electronics (e) with roughly 5,000 documents per domain. We use all the source dataset as train- ing and test on the whole target dataset. We set the MDA noise level p to high values (e.g. 0.9), as document representations are sparse and adding low noise have no effect on the features already equal to zero.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we show the performance of the Transductive Prediction Adaptation (TPA) on 12 adaptation tasks in the AMT dataset. The first column shows the accuracies for the dream case where the standard MDA is applied to both source and target data. The second column shows the baseline results (f s (X t )) obtained directly as class predictions by the source classifier. The classifica- tion model is an l 2 regularized Logistic Regres- sion 3 cross-validated with regularized parameter C ∈ [0.0001, 0.001, 0.1, 1, 10, 50, 100].</p><p>The two last columns show the results obtained with two versions of TPA (results are underlined when improving over the baseline and in bold when yielding the highest values). In the first ver- sion, target instances x t n contains only features (words and bigrams) appearing in the source docu- ments and used to make the predictions f (x t n ). In the second version, denoted as TPAe, we extend TPA with words unseen in the source documents. If the extension part is denoted v t n , we obtain an augmented representation u t n = [x t n ; v t n ; f (x t n )] as input to MDA. </p><note type="other">k → d 80.76 77.58 79.16 81.92 b → e 80.32 76.44 78.54 81.81 d → e 83.70 78.65 80.75 82.89 k → e 89.05 87.55 88.38 88.50 b → k 84.00 79.46 81.44 85.21 d → k 86.08 80.83 83.15 86.14 e → k 90.76 89.97 91.10 90.86 Avg 83.4 79.85 81.44 83.73</note><p>As we can see, both TPA and TPAe signifi- cantly outperform the baseline f s (X t ) obtained with no adaptation. Furthermore, extending TPA with words present in target documents only al- lows to further improve the classification accuracy in most cases. Finally, TPAe often outperforms the dream case and also on average (note however that MDA * uses the features common to source and target documents as input).</p><p>To understand the effect of prediction adapta- tion we analyze the book → electronics adapta- tion task. In the mapping W, we sort the weights corresponding to the correlation between the posi- tive class and the target features. Features with the highest weights (up-weighted by TPA) are great, my, sound, easy, excellent, good, easy to, best, yo, a great, when, well, the best. On contrary, the words that got the smallest weight (down-weighted by TPA) are no, was, number, don't, after, money, if, work, bad, get, buy.</p><p>As TPA is totally unsupervised, we run addi- tional experiments to understand its practical use- fulness. We compare TPA to the case of weakly annotated target data, where few target examples are labelled and used for training a target classi- fier. Trained with 40, 100 and 200 target exam- ples, a logistic regression yields an average accu- racy of 64.63%, 68.01% and 75.13% over 12 tasks and a Multinomial Naives Bayes reports 65.82%, 71.49% and 76%, respectively. Even with 200 labeled target documents, the target versus tar- get classification results are significantly below the 79.8% average accuracy of the baseline source classifier.</p><p>All these values are therefore significantly be- low the 83.73% obtained with TPAe. This strongly supports the domain adaptation scenario, when a sentiment analysis classifier trained on a larger source set and adapted to target documents can do better than a classifier trained on a small set of labeled target documents. Furthermore, we have seen that the baseline can be significantly im- proved by TPA and even more by TPAe without the need of even a small amount of manual label- ing of the target set.</p><p>The second group of evaluation tests is on the 20Newsgroup dataset. It contains around 20,000 documents of 20 classes and represents a stan- dard testbed for text categorization. For the do- main adaptation, we follow the setting described in ( <ref type="bibr" target="#b19">Pan et al., 2012</ref>). We filter out rare words (ap- pearing less than 3 times) and keep at most 10,000 features for each task with a tf-idf termweight- ing. As all documents are organized as a hi- erarchy, the domain adaptation tasks are defined on category pairs with sources and targets cor- responding to subcategories. For example, for the 'comp vs sci' task, subcategories such as comp.sys.ibm.pc.hardware and sci.crypt are set as source domains and comp.sys.ibm.mac.hardware and sci.med as targets, respectively.</p><p>In our experiments we consider 5 adaptation tasks on category pairs ( 'comp vs sci','rec vs talk', 'rec vs sci', 'sci vs talk' and 'comp vs rec' as in <ref type="bibr" target="#b19">(Pan et al., 2012)</ref> ), and run the baseline, TPA and TPAe methods. For each category pair, we addi- tionally inverse the source and target roles; this explains two sets of experimental results for each pair. We show the evaluation results in <ref type="table" target="#tab_1">Table 2</ref>. It is easy to observe again the significant improve- ment over the baseline f s (x t n ) and the positive ef- fect of including the unseen words in the TPA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we address the domain adaptation scenario without access to source data and where source classifiers are available as black boxes. In the transductive setting, the source classifiers can predict class scores for target instances, and we consider these predictions as corrupted by domain shift. We use the Marginalized Denoising Autoen- coders ( <ref type="bibr" target="#b6">Chen et al., 2012</ref>) to reconstruct the pre- dictions by exploiting the "correlation" between the target features and the predicted scores. We test the transductive prediction adaptation on two known benchmarks and demonstrate that it can significantly improve the classification accuracy, comparing to the baseline and to the case of full access to source data. This is an encouraging re- sult because it demonstrates that domain adapta- tion can still be effective despite the absence of source data. Lastly, in the future, we would like to explore the adaptation of other language process- ing components, such as named entity recognition, with our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : TPA results on the AMT dataset.</head><label>1</label><figDesc></figDesc><table>S → T 
MDA  *  f s (X t ) 
TPA 
TPAe 
d → b 
84.59 
81.36 
82.61 83.19 
e → b 
78.07 
73.87 
75.93 79.95 
k → b 
78.75 
73.50 
75.02 78.39 
b → d 
85.07 
82.54 
83.56 84.32 
e → d 
79.99 
76.46 
77.67 81.60 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : TPA results on the 20Newsgroup dataset.</head><label>2</label><figDesc></figDesc><table>class pair 
f s (X t ) 
TPA 
TPAe 
'comp vs sci' 
71.06 
80.24 80.43 
65.4 
71.6 
71.98 
'rec vs talk' 
65.66 
68.01 70.18 
69.93 
75.84 
77.2 
'rec vs sci' 
76.02 
85.97 86.42 
74.17 
81.14 82.71 
'sci vs talk' 
76.1 
80.22 
81.3 
74.92 
80.07 80.19 
'comp vs rec' 
86.63 
91.56 92.06 
86.97 
92.67 93.34 
Avg 
74.69 
80.73 81.58 

</table></figure>

			<note place="foot" n="1"> A constant is added to the input, xn = [xn; 1], and an appropriate bias, never corrupted, is incorporated within W.</note>

			<note place="foot" n="2"> It requires concatenating the class predictions from different sources at step 1 and averaging the reconstructed predictions per class at step 3.</note>

			<note place="foot" n="3"> We also experimented with other classifiers, such as SVM , Multinomial Naive Bayes, and obtained similar improvement after applying TPA. Results are not shown due to the space limitation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Privacy-preserving data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishnan</forename><surname>Srikant2000] Rakesh Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="439" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation with coupled subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain word representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics(ACL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Privacy-preserving logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Monteleoni2008</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Topic modeling using topics from many domains, lifelong learning and big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu2014] Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="767" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DLID: Deep learning for domain adaptation by interpolating between domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning (WREPL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Sumit Chopra, Suhrid Balakrishnan, and Raghuraman Gopalan</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources via auxiliary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for largescale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Glorot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Experiments in domain adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Statistical Machine Translation (STAT-MT)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptation of statistical machine translation model for cross-lingual information retrieval in a service context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikoulina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
	<note>Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos, and Christof Monz</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Pan and Yang2010</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating the sentence-level quality of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erheng</forename><surname>] Weike Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the European Association for Machine Translation (EAMT)</title>
		<editor>Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello N. Cristianini</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
	<note>Mining Text Data</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-source transfer learning with multi-view adaboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun2012] Zhijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS), volume LNCS 7665</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised multi-domain adaptation with feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eisenstein2014] Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="672" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unifying learning to rank and domain adaptation: Enabling cross-task document scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang2014] Mianwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="781" to="790" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
