<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
						</author>
						<title level="a" type="main">Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
						<meeting>ACL 2017, Student Research Workshop <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="49" to="55"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-3009</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a convolutional neural network model for text-based speaker identification on multiparty dialogues extracted from the TV show, Friends. While most previous works on this task rely heavily on acoustic features, our approach attempts to identify speakers in dialogues using their speech patterns as captured by transcriptions to the TV show. It has been shown that different individual speakers exhibit distinct idiolec-tal styles. Several convolutional neural network models are developed to discriminate between differing speech patterns. Our results confirm the promise of text-based approaches , with the best performing model showing an accuracy improvement of over 6% upon the baseline CNN model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speakers verbalize their thoughts in different ways through dialogues. The differences in their expres- sions, be they striking or subtle, can serve as clues to the speakers' identities when they are withheld. This paper investigates the possibility of identify- ing speakers in anonymous multiparty dialogues.</p><p>Impressive advancements have been achieved in the field of speech recognition prior to this pa- per ( <ref type="bibr" target="#b15">Sadjadi et al., 2016;</ref><ref type="bibr" target="#b4">Fine et al., 2001;</ref><ref type="bibr" target="#b1">Campbell et al., 2006</ref>). Research on dialogue systems has also involved considerable efforts on speaker identi- fication, as it constitutes an important step in build- ing a more natural and human-like system <ref type="bibr" target="#b13">(Raux et al., 2006;</ref><ref type="bibr" target="#b5">Hazen et al., 2003)</ref>. Research in this area, however, has mostly been focused on acoustic features, which are absent in many situations (e.g., online chats, discussion forums, text messages). In addition, it is commonly acknowledged that nat- ural language texts themselves reflect the person- alities of speakers, in addition to their semantic content ( <ref type="bibr" target="#b11">Mairesse et al., 2007)</ref>.</p><p>Various experiments have demonstrated significant differences in the linguistic patterns generated by different participants, suggesting the possibility to perform speaker identification with text-based data. An increasing number of large unstructured dia- logue datasets are becoming available, although they comprise only the dialogue transcripts with- out speaker labels <ref type="bibr" target="#b18">(Tiedemann, 2012;</ref><ref type="bibr" target="#b10">Lowe et al., 2015)</ref>. This paper attempts to identify the six main characters in the dialogues occurring in the first 8 seasons of the TV show, Friends. The minor char- acters in the show are to be identified collectively as Other.</p><p>For each episode, we first withhold the identity of the speaker to each utterance in its transcript, and have prediction models label the speakers. The accuracy and the F1 score of the labeling against the gold labels are used to measure the model per- formance. Our best model using multi-document convolutional neural network shows an accuracy of 31.06% and a macro average F1 score of 29.72, ex- hibiting promising performance on the text-based speaker identification task. We believe that the application of text-based speaker identification is extensive since data collected from online chat- ting and social media contains no acoustic infor- mation. Building accurate speaker identification models will enable the prediction of speaker labels in such datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Reynolds and <ref type="bibr" target="#b14">Rose (1994)</ref> introduced the Gaussian Mixture Models (GMM) for robust text indepen- dent speaker identification. Since then, GMM has been applied to a number of datasets and achieved great results ( <ref type="bibr" target="#b4">Fine et al., 2001;</ref><ref type="bibr" target="#b1">Campbell et al., 2006</ref>). <ref type="bibr" target="#b7">Knyazeva et al. (2015)</ref> proposed to perform sequence labeling and structured prediction in TV show speaker identification, and achieved better performance on sequential data. Despite the poten- tial of text-based speaker identification in targeted</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utterance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monica</head><p>No . Not after what happened with Steve . Chandler What are you talking about ? We love Schhteve ! Schhteve was schhexy !.. Sorry . Monica</p><p>Look , I do n't even know how I feel about him yet . Just give me a chance to figure that out .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rachel</head><p>Well , then can we meet him ? Monica Nope . Schhorry .  <ref type="formula">(2015)</ref> proposed their text-based speaker identification approach us- ing Logistic Regression and Recurrent Neural Net- work (RNN) to learn the turn changes in movie dialogues. Their task is fundamentally different from the task of this paper, as their main focus is on the turn changes of dialogues instead of the iden- tities of speakers. To the best of our knowledge, it is the first time the multi-document CNN has been applied to the speaker identification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus</head><p>The Character Mining project provides transcripts to the TV show Friends; transcripts to the first 8 seasons of the show are publicly available in JSON format. Moreover, the first 2 seasons are annotated for the character identification task <ref type="bibr" target="#b2">(Chen and Choi, 2016)</ref>. Each season contains a number of episodes, and each episode is comprised of separate scenes. <ref type="bibr">1</ref> The scenes in an episode, in turn, are divided at the utterance level. An excerpt from the data is shown in <ref type="table" target="#tab_0">Table 1</ref>. In total, this corpus consists of 194 episodes, 2,579 scenes and 49,755 utterances. The utterance distribution by speaker is shown in <ref type="figure">Figure 1</ref>. The percentages for major speakers are fairly consistent. However, the Other speaker has a larger percentage in the dataset than any of the major speakers. The frequencies of interactions between pairs of speakers exhibit significant vari- ance. For instance, Monica talks with Chandler more often than any other speaker, whereas Phoebe does not talk with Rachel and Joey very frequently. It will be of interest to note whether the variance of interaction rates can affect the performance of our identification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monica</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phoebe</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rachel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ross</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joey</head><p>Chandler Others 13.52%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11.89%</head><p>14.73%</p><p>15.04% 13.2%</p><p>13.64%</p><p>17.96%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Data distribution</head><p>The first dataset is structured such that each utter- ance is considered as one discrete sample. To test the prediction performance for samples of greater lengths, all utterances of the same speaker in a scene are concatenated together as one single sam- ple in the second dataset. Additional summary of the dataset is presented in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">K Nearest Neighbor</head><p>In <ref type="bibr" target="#b8">Kundu et al. (2012)</ref>, the best result is reported using the K Nearest Neighbor algorithm (KNN), which is selected as the baseline approach for this paper, and implemented according to the original authors' specifications. Each utterance is treated as one sample, and 8 discrete stylistic features defined in the original feature template are extracted from each sample. Cosine similarity is used to locate the 15 nearest neighbors to each utterance. Ma- jority voting of the neighbors, weighted by cosine similarity, is used to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent Neural Network</head><p>A recurrent neural network (RNN) model is also considered in the course of the experiments, where each utterance in the transcripts is handled sepa- rately. The RNN model treats the speaker iden- tification task as a variant of sequence classifica- tion. For each instance, the concatenation of word embedding vectors is fed into the model, with a dense layer and softmax activation to model the probability for each speaker. The model is unable to demonstrate significantly above random accu- racy on labeling, achieving a maximal accuracy of 16.05% after training. We conclude that a simple RNN model is unable to perform speaker identifica- tion based on textual data. Variations on the hyper- parameters, including the dimension of the RNN, the dimension of word embeddings, and dropout rate, produced no appreciable improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Convolutional Neural Network</head><p>Widely utilized for computer vision, Convolutional Neural Network (CNN) models have recently been applied to natural language processing and showed great results for many tasks <ref type="bibr" target="#b19">(Yih et al., 2014;</ref><ref type="bibr" target="#b6">Kim, 2014;</ref><ref type="bibr" target="#b17">Shen et al., 2014</ref>). Speaker identification can be conceptualized as a variant of document classifi- cation. Therefore, we elected to use the traditional CNN for our task. The model is a minor modifica- tion to the proposal of <ref type="bibr" target="#b6">Kim (2014)</ref>, which consists of a 1-dimensional convolution layer with different filter sizes, a global max pooling layer, and a fully connected layer. Each utterance is treated as one sample and classified independently. One of the challenges is the large number of misspellings and colloquialisms in the dataset as a result of the mistakes in the human transcription process and the nature of human dialogues. It is unlikely for these forms to appear in pre-trained word embeddings. The bold instances in <ref type="table" target="#tab_0">Table 1</ref> provide a glimpse into these challenges. It should also be noted that these irregularities oftentimes only deviate slightly from the standard spellings. A character-aware word embedding model is ex- pected to produce similar vectors for the irregular forms and the standard spellings. Most of the col- loquialisms appear frequently in the dataset, and the challenge they pose can be resolved by a pre- trained character-aware word embedding model, such as fastText ( <ref type="bibr" target="#b0">Bojanowski et al., 2016</ref>). The word embeddings used in this paper are trained on a dataset consisting of the New York Times corpus, </p><formula xml:id="formula_0">U-#1 U-#2 U-#3 U-#4 Prediction Pooling Convolution</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CNN with Surrounding Utterance</head><p>Unlike other types of short documents such as movie reviews, where each sample is independent from the others, dialogues within a typical TV show are highly structured ( <ref type="bibr" target="#b7">Knyazeva et al., 2015)</ref>. Ev- ery utterance is highly related to its prior and sub- sequent utterances, and it is important to take se- quential information into account in predicting the speakers. However, contextual information is com- pletely ignored by the basic CNN model. Each batch of input to the model consists of discrete ut- terances from different episodes and seasons, as shown in <ref type="figure">Figure 2</ref>.</p><p>To remedy the loss of contextual information, the CNN model is modified in a manner similar to the one proposed by <ref type="bibr" target="#b9">Lee and Dernoncourt (2016)</ref>. After the global max pooling layer, each utterance vector is concatenated with both the previous two utterances and the subsequent utterance in the same scene. Then, the vector is fed into the fully con-  nected layer. In this model, some information on the original dialogue sequence is preserved. Each scene is padded to the maximal sequence length, and fed into the model as one batch for both train- ing and decoding. <ref type="figure">Figure 3</ref> illustrates the structure of the model. Although topics within the scene are closely related, any single utterance is usually only relevant to its surrounding utterances. Based on this observation, including additional utterances in the prediction process can result in noisy input to the prediction model. In a typical TV show, only a subset of characters are present in any particular scene. To further boost our model's ability to distinguish between speakers, the model optionally considers the set of speakers appearing in the scene. At the decoding stage, the Softmax probabilities for speakers absent from the scene are set to 0. The model benefits from the restrictions on its prediction search space. Such restrictions are applicable in the domain of targeted surveillance, where a vast number of speakers can be precluded from consideration during the identifi- cation process. For instance, speaker identification on a surveilled dialogue inside a criminal syndicate need only consider the members of the organiza- tion. In the majority of cases, however, the set of possible speakers may be difficult to ascertain. Therefore, we exclude this information in the deter- mination of the best performing model.</p><formula xml:id="formula_1">Dataset M P R 1 R 2 J C O Total Training</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5,017 4,349 5,308 5,527 4,738 5,268 7,006 37,213</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">CNN with Utterance Concatenation</head><p>Many individual utterances appearing in the dataset are fairly laconic and generic, as exemplified by the last utterance shown in <ref type="table" target="#tab_0">Table 1</ref>, rendering them challenging to classify even with the help of con- textual information. The proposed solution is to group multiple utterances together as one sample. Specifically, all of the utterances for each speaker in one scene are concatenated in the original dia- logue order. We assign consistent unknown labels to all speakers in this dataset so that all the utter- ances in a single scene maintain their trackable provenances from the same speakers. The concate- nated individual utterances can be fairly reasonable and consistent speech. As documents increase in length, it becomes easier for the CNN model to capture the speech pattern of each speaker. Once again, this model also optionally restricts its predic- tion search space to the set of speakers appearing in the scene for each batch of input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In the KNN experiment, the transcript to season 8 of Friends is used as evaluation data, and the first 7 seasons as training data. In the rest of the experiments, season 8 is used as evaluation data, and season 7 is used as the development set. The first 6 seasons are used as the training dataset. In each experiment, the F1 scores for the speakers, the average F1 score for major speakers, the average F1 score for all speakers, and the accuracy are reported in <ref type="table" target="#tab_4">Tables 3 and 4.</ref> In <ref type="bibr" target="#b8">Kundu et al. (2012)</ref>, the highest accuracy achieved by the KNN approach on the paper's film dialogue dataset was 30.39% , which is compa- rable to the best result of this paper. In contrast, the KNN approach did not perform well on the Friends dataset. Upon further examination of the KNN model's prediction process, we observe that the cosine similarities between any given utterance and its 15 nearest neighbors are consistently above 98%. The speaker labels are not linearly separable due to the low dimensionality of the feature space. The basic CNN model is able to outperform the baseline by almost 9% because the highly differ- ing n-grams frequencies in the dataset enabled the model to distinguish between speakers. It is also worth noting that when the surrounding utterances 52 Individual F1 Score   <ref type="table">Table 4</ref>: Model performance where the prediction labels are restricted to speakers present in each scene.</p><formula xml:id="formula_2">Model M P R 1 R 2 J C O MF1 F1</formula><p>are taken into account, identification accuracy in- creases significantly from that achieved by the sim- ple CNN. With more contextual information, the model is able to identify speakers with higher ac- curacy, as individual speakers react differently in comparable situations.</p><p>The experiment on the utterance concatenation dataset yields a relatively high identification accu- racy, corroborating our theory that the prediction model can better capture different speech patterns on longer documents. When prediction labels are restricted to the speakers present in a scene, accu- racy boosts of 10% and 12% are achieved on the two datasets, respectively. <ref type="table">Table 5</ref> shows the confusion matrix produced by the multi-document CNN, i.e., the best performing model. The speakers for whom the model produces higher accuracies (Ross and Other) are also con- fused by the model more often than other speakers. The cause can be accounted for by the model's overzealousness in assigning these two labels to utterances due to their relatively large percentages in the training data. In addition, Monica and Chan- dler are often confused with each other. Due to their romantic relationship, it is possible that there is a convergence between their idiolectal styles. On the other hand, the confusion rates between Phoebe and Rachel, and between Phoebe and Joey are both fairly low. Such results confirm the observation that the frequency of interactions between speaker pairs correlates with the rate of confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a neural network-based ap- proach to speaker identification in multiparty dia- logues relying only on textual transcription data. The promising experimental results confirm the value of textual features in speaker identification on multiparty dialogues. The improvements pro- duced by the consideration of neighboring utter- ances in the CNN's prediction process indicate that contextual information is essential to the perfor- mance of text-based speaker identification. Prior to this paper,  used scripted dialogues to identify turn-taking and differences in speakers, where the actual identities of the speak- ers are irrelevant. However, this paper enables an identification where the names of the speakers are associated with their own utterances, a novel at- tempt in text-based speaker identification. Because of the ability of the model to uncover speaker iden- tities in the absence of audio data, applications and interests in the intelligence and surveillance com- munity are expected.</p><p>Although speaker verification based on acoustic signals is a helpful tool, it can conceivably be de- feated by voice modulating algorithms. Whereas text-based speaker identification can discern the involuntary and unconscious cues of speakers. It is of interest to incorporate text-based features in a larger system of speaker identification to enhance its security. Several dialogue emotion recogni- tion systems have incorporated both acoustic and 53 S P  <ref type="table">Table 5</ref>: Confusion Matrix between speakers. S: true speaker, P: predicted speaker.</p><p>textual features, and resultant performances show improvements upon previous systems which rely only on one kind of features <ref type="bibr" target="#b3">(Chuang* and Wu, 2004;</ref><ref type="bibr" target="#b12">Polzehl et al., 2009</ref>). Similarly, integration of acoustic and textual information in the speaker identification task can result in improved perfor- mance in future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The baseline CNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>An excerpt from the transcripts to the TV show Friends. 

Internet surveillance, research into this area has 
been scant. So far, there have been only a handful 
of attempts at text-based speaker identification. 
Kundu et al. (2012) proposed to use the K Near-
est Neighbor Algorithm, Naive Bayes Classifier 
and Conditional Random Field to classify speak-
ers in the film dialogues based on discrete stylistic 
features. Although their classification accuracies 
increase significantly from the random assignment 
baseline, there remains significant room for im-
provement. Serban and Pineau </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset distribution by speakers. M: Monica, P: Phoebe, R 1 : Rachel, R 2 : Ross, J: Joey, C: 
Chandler, O: Other. Non-main speakers (all the others), are collectively grouped as the Other speaker. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Model performance. MF1: Average of F1 scores for major speakers, F1: Average of F1 scores 
for all speakers, ACC: Accuracy 

Individual F1 Score 

Model 
M 
P 
R 1 
R 2 
J 
C 
O 
MF1 
F1 
ACC 

Multi-Document-CNN-2 28.13 29.59 41.49 48.15 45.72 36.06 46.98 38.19 39.45 41.36 

CNN-Concatenation-2 
36.43 33.16 50.09 45.03 53.67 39.90 51.02 43.05 44.19 46.48 

</table></figure>

			<note place="foot" n="1"> http://nlp.mathcs.emory.edu/ character-mining</note>

			<note place="foot" n="2"> snap.stanford.edu/data/web-Amazon. html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector machines for speaker and language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Torres-Carrasquilloe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The speaker and Language Recognition Workshop</title>
		<meeting><address><addrLine>Odyssey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="210" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Character Identification on Multiparty Conversation: Identifying Mentions of Characters in TV Shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Yu-Hsin Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="90" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MultiModal Emotion Recognition from Speech and Text. The Association for Computational Linguistics and Chinese Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze-Jing</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Hsien</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hybrid gmm/svm approach to speaker identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirí</forename><surname>Navrátil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Integration of speaker recognition into conversational spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">C</forename><surname>Kukolich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>EUROSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Structured Prediction for Speaker Identification in TV Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Knyazeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Yvon</surname></persName>
		</author>
		<editor>LIMSI-CNRS-Rue John Von Neumann</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Orsay, France. Université Paris Sud</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speaker identification from film dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitava</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Proceedings of 4th International Conference on Intelligent Human Computer Interaction</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Conference</title>
		<meeting>the NAACL Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured MultiTurn Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL Conference</title>
		<meeting>the SIGDIAL Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using linguistic cues for the automatic recognition of personality in conversation and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="475" to="500" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotion classification in children&apos;s speech using fusion of acoustic and linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Polzehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiva</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Ketabdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. InterSpeech</title>
		<meeting>InterSpeech</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Doing research on a deployed spoken dialogue system: one year of let&apos;s go! experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Robust textindependent speaker identification using Gaussian mixture speaker models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The ibm speaker recognition system: Recent advances and error analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seyed Omid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Pelecanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganapathy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01635</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Text-Based Speaker Identification For Multi-Participant OpenDomain Dialogue System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Operations Research, Université de Montréal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Semantic Representations Using Convolutional Neural Networks for Web Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC pages</title>
		<meeting>LREC pages</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic Parsing for Single-Relation Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
