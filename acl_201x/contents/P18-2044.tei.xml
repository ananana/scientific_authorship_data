<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN for Text-Based Multiple Choice Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Chaturvedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision &amp; Pattern Recognition Unit</orgName>
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<addrLine>203, B. T. Road, Kolkata-700108</addrLine>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onkar</forename><surname>Pandit</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">INRIA</orgName>
								<address>
									<addrLine>40 Avenue Halley, Villeneuve-d&apos;Ascq 59650</addrLine>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Utpal Garain †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CNN for Text-Based Multiple Choice Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="272" to="277"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>272</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The task of Question Answering is at the very core of machine comprehension. In this paper, we propose a Convolutional Neural Network (CNN) model for text-based multiple choice question answering where questions are based on a particular article. Given an article and a multiple choice question, our model assigns a score to each question-option tuple and chooses the final option accordingly. We test our model on Textbook Question Answering (TQA) and SciQ dataset. Our model outperforms several LSTM-based baseline models on the two datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Answering questions based on a particular text re- quires a diverse skill set. It requires look-up abil- ity, ability to deduce, ability to perform simple mathematical operations (e.g. to answer questions like how many times did the following word oc- cur?), ability to merge information contained in multiple sentences. This diverse skill set makes question answering a challenging task.</p><p>Question Answering (QA) has seen a great surge of more challenging datasets and novel ar- chitectures in recent times. Question Answering task may require the system to reason over few sentences ( <ref type="bibr" target="#b16">Weston et al., 2015)</ref>, table <ref type="bibr" target="#b11">(Pasupat and Liang, 2015)</ref>, Wikipedia passage ( <ref type="bibr" target="#b12">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b19">Yang et al., 2015</ref>), lesson ( <ref type="bibr" target="#b7">Kembhavi et al., 2017)</ref>. Increase in the size of the datasets has allowed researchers to explore different neu- ral network architectures ( <ref type="bibr" target="#b3">Cui et al., 2016;</ref><ref type="bibr" target="#b18">Xiong et al., 2016;</ref><ref type="bibr" target="#b14">Trischler et al., 2016)</ref> for this task. Given a question based on a text, the model needs to attend to a specific portion of the text in order to answer the question. Hence, the use of attention mechanism ( ) is common in these architectures.</p><p>Convolutional Neural Networks (CNN) have been shown to be effective for various natural lan- guage processing tasks such as sentiment analysis, question classification etc. <ref type="bibr" target="#b8">(Kim, 2014</ref>). However for the task of question answering, Long Short Term Memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) based methods are the most common. In this paper we build a CNN based model for mul- tiple choice question answering 1 . We show the ef- fectiveness of the proposed model by comparing it with several LSTM-based baselines.</p><p>The main contributions of this paper are (i) The proposed CNN model performs comparatively or better than LSTM-based baselines on two differ- ent datasets. ( <ref type="bibr" target="#b7">Kembhavi et al., 2017;</ref><ref type="bibr" target="#b15">Welbl et al., 2017)</ref> (ii) Our model takes question-option tuple to generate a score for the concerned option. We argue that this is a better strategy than consider- ing questions and options separately for multiple choice question answering. For example, consider the question "The color of the ball is" with three options: red, green and yellow. If the model gen- erates a vector which is to be compared with the three option embeddings, then this might lead to error since the three option embeddings are close to each other. (iii) We have devised a simple but effective strategy to deal with questions having op- tions like none of the above, two of the above, all of the above, both (a) and (b) etc. which was not done before. (iv) Instead of attending on words present in the text, our model attends at sentence level. This helps the model for answering look-up questions since the necessary information required to answer such questions will often be contained in a single sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given a question based on an article, usually a small portion of article is needed to answer the concerned question. Hence it is not fruitful to give the entire article as input to the neural network. To select the most relevant paragraph in the ar- ticle, we take both the question and the options into consideration instead of taking just the ques- tion into account for the same. The rationale be- hind this approach is to get the most relevant para- graphs in cases where the question is very gen- eral in nature. For example, consider that the ar- ticle is about the topic carbon and the question is "Which of the following statements is true about carbon?". In such a scenario, it is not possible to choose the most relevant paragraph by just looking at the question. We select the most relevant para- graph by word2vec based query expansion (Kuzi et al., 2016) followed by tf-idf score (Foundation, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Network Architecture</head><p>We use word embeddings ( <ref type="bibr" target="#b10">Mikolov et al., 2013)</ref> to encode the words present in question, option and the most relevant paragraph. As a result, each word is assigned a fixed d-dimensional represen- tation. The proposed model architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Let q, o i denote the word embed- dings of words present in the question and the i th option respectively. Thus, q ∈ R d×lq and o i ∈ R d×lo where l q and l o represent the number of words in the question and option respectively. The question-option tuple (q, o i ) is embedded us- ing Convolutional Neural Network (CNN) with a convolution layer followed average pooling. The convolution layer has three types of filters of sizes f j ×d ∀j = 1, 2, 3 with size of output channel of k. Each filter type j produces a feature map of shape (l q + l o − f j + 1) × k which is average pooled to generate a k-dimensional vector. The three k- dimensional vectors are concatenated to form 3k- dimensional vector. Note that <ref type="bibr" target="#b8">Kim (2014)</ref> used max pooling but we use average pooling to ensure different embedding for different question-option tuples. Hence,</p><formula xml:id="formula_0">h i = CN N ([q; o i ]) ∀i = 1, 2, .., n q (1)</formula><p>where n q is the number of options, h i is the out- put of CNN and [q; o i ] denotes the concatenation of q and o i i.e. [q; o i ] ∈ R d×(lq+l 0 ) . The sentences in the most relevant paragraph are embedded us- ing the same CNN. Let s j denote the word em- beddings of words present in the j th sentence i.e. s j ∈ R d×ls where l s is the number of words in the sentence. Then,</p><formula xml:id="formula_1">d j = CN N (s j ) ∀j = 1, 2, .., n sents (2)</formula><p>where n sents is the number of sentences in the most relevant paragraph and d j is the out- put of CNN. The rationale behind using the same CNN for embedding question-option tuple and sentences in the most relevant paragraph is to ensure similar embeddings for similar question- option tuple and sentences. Next, we use h i to attend on the sentence embeddings. Formally,</p><formula xml:id="formula_2">a ij = h i · d j ||h i ||.||d j || (3) r ij = exp(a ij ) nsents j=1 exp(a ij )<label>(4)</label></formula><formula xml:id="formula_3">m i = nsents j=1 r ij d j<label>(5)</label></formula><p>where ||.|| signifies the l 2 norm, exp(x) = e x and h i · d j is the dot product between the two vec- tors. Since a ij is the cosine similarity between h i and d j , the attention weights r ij give more weight- ing to those sentences which are more relevant to the question. The attended vector m i can be thought of as the evidence in favor of the i th op- tion. Hence, to give a score to the i th option, we take the cosine similarity between h i and m i i.e.</p><formula xml:id="formula_4">score i = h i · m i ||h i ||.||m i ||<label>(6)</label></formula><p>Finally, the scores are normalized using softmax to get the final probability distribution.</p><formula xml:id="formula_5">p i = exp(score i ) nq i=1</formula><p>exp(score i )</p><p>where p i denotes the probability for the i th option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dealing with forbidden options</head><p>We refer to options like none of the above, two of the above, all of the above, both (a) and (b) as forbidden options. During training, the questions  having a forbidden option as the correct option were not considered. Furthermore, if a question had a forbidden option, that particular question- option tuple was not taken into consideration. Let S = [score i ∀i | i th option not in forbidden op- tions] and |S| = k. During prediction, the ques- tions having one of the forbidden options as an op- tion are dealt with as follows:</p><p>1. Questions with none of the above/ all of the above option: If the max(S) − min(S) &lt; threshold then the final option is the concerned forbidden option.</p><p>Else, the final option is argmax(p i ).</p><p>2. Questions with two of the above option: If the S (k) −S (k−1) &lt; threshold where S (n) denotes the n th order statistic, then the final option is the concerned forbidden option. Else, the final option is argmax(p i ).</p><p>3. Questions with both (a) and (b) type option: For these type of questions, let the corresponding scores for the two options be score i 1 and score i 2 . If the |score i 1 − score i 2 | &lt; threshold then the final option is the concerned forbidden option.</p><p>Else, the final option is argmax(p i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Questions with any of the above option:</head><p>Very few questions had this option. In this case, we always choose the concerned forbidden option.</p><p>We tried different threshold values ranging from 0.0 to 1.0. Finally, the threshold was set to a value gave the highest accuracy on the training set for these kind of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Details</head><p>We tried two different CNN models, one having f j 's equal to 3,4,5 and other having f j 's equal to 2,3,4. We refer to two models as CN N 3,4,5 and CN N 2,3,4 respectively. The values of hyperpa- rameters used are: d = 300, k = 100. The other hyperparamters vary from dataset to dataset. Since the number of options vary from question to ques- tion, our model generates the probability distribu- tion over the set of available options. Similarly, the number of sentences in the most relevant para- graph can vary from question to question, so we set a ij = −∞ whenever d j was a zero vector. Cross entropy loss function was minimized during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>The accuracy of our proposed model on validation set of TQA and SciQ dataset ( <ref type="bibr" target="#b7">Kembhavi et al., 2017;</ref><ref type="bibr" target="#b15">Welbl et al., 2017</ref>) is given in <ref type="table">Table 1 and  Table 2</ref>. GRU bl refers to the model where CNN is replaced by Gated Recurrent Unit (GRU) ( ) to embed question-option tuples and the sentences. The size of GRU cell was 100.</p><p>For SciQ dataset, we used the associated passage provided with the question. AS Reader ( <ref type="bibr" target="#b6">Kadlec et al., 2016</ref>) which models the question and the paragraph using GRU followed by attention mechanism got 74.1% accuracy on the SciQ test set. However, for a question, they used a different corpus to extract the text passage. Hence it is not judicious to compare the two models. As</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>True-False (Correct/Total) Multiple Choice (Correct/Total) GRU bl 536/994 (53.9%) 529/1530 (34.6%) CN N 3,4,5 531/994 (52.4%) 531/1530 (34.7%) CN N 2,3,4 537/994 (54.0%) 543/1530 (35.5%) <ref type="table">Table 1</ref>: Accuracy for true-false and multiple choice questions on validation set of TQA dataset.</p><p>can be seen from the <ref type="table">Tables 1 and 2</ref> <ref type="figure">, CN N 2,3,4</ref> gives the best performance on the validation set of both the datasets so we evaluate it on the test sets. Note that GRU bl highly overfits on the SciQ dataset which shows that CNN-based models work better for those datasets where long-term depen- dency is not a major concern. This rationale is also supported by the fact that CN N 2,3,4 performed better than CN N 3,4,5 on the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Accuracy</head><p>GRU bl 68. <ref type="figure">2%  CN N 3,4</ref>,5 87. <ref type="figure" target="#fig_0">1%  CN N 2,3,4</ref> 87. <ref type="figure">8%  CN N 2,3,4</ref> 84.7% (test-set) <ref type="table">Table 2</ref>: Accuracy of the models on SciQ dataset. The first three accuarcies are on validation set. The last accuracy is of CN N 2,3,4 model on the test set.</p><p>Baselines for TQA dataset: Three baselines models are mentioned in <ref type="bibr" target="#b7">Kembhavi (2017)</ref> . These baseline models rely on word-level attention and encoding question and options separately. The baseline models are random model, Text-Only model and BIDAF <ref type="bibr">Model (Seo et al., 2016)</ref>. Text- Only model is a variant of Memory network <ref type="bibr" target="#b17">(Weston et al., 2014)</ref> where the paragraph, question and options are embedded separately using LSTM followed by attention mechanism. In BIDAF Model, character and word level embedding is used to encode the question and the text followed by bidirectional attention mechanism. This model predicts the subtext within the text containing the answer. Hence, the predicted subtext is compared with each of the options to select the final option.</p><p>Note that the result of the baseline models given in <ref type="bibr" target="#b7">Kembhavi (2017)</ref> were on test set but the au- thors had used a different data split than the pub- licly released split. As per the suggestion of the authors, we evaluate CN N 2,3,4 model by combin- ing validation and test set. The comparison with the baseline models is given in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>True  <ref type="table">Table 3</ref>:</p><p>Accuracy of different models for true-false and multiple choice questions. Results marked with ( * ) are taken from <ref type="bibr" target="#b7">Kembhavi (2017)</ref> and are on test set obtained using a different data split. Result of our proposed model is on publicly released validation and test set combined.</p><p>As can be seen from <ref type="table">Table 3</ref>, CN N 2,3,4 model shows significant improvement over the baseline models. We argue that our proposed model out- performs the Text-Only model because of three reasons (i) sentence level attention, (ii) question- option tuple as input, and (iii) ability to tackle for- bidden options. Sentence level attention leads to better attention weights, especially in cases where a single sentence suffices to answer the question. If question is given as input to the model, then the model has to extract the embedding of the answer whereas giving question-option tuple as input sim- plifies the task to comparison between the two em- beddings.</p><p>SciQ dataset didn't have any questions with for- bidden options. However, in the validation set of TQA, 433 out of 1530 multiple choice ques- tions had forbidden options. Using the proposed threshold strategy for tackling forbidden options, CN N 2,3,4 gets 188 out of 433 questions cor- rect. Without using this strategy and giving every question-option tuple as input, CN N 2,3,4 gets 109 out of 433 questions correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this paper, we proposed a CNN based model for multiple choice question answering and showed its effectiveness in comparison with several LSTM- based baselines. We also proposed a strategy for dealing with forbidden options. Using question- option tuple as input gave significant advantage to our model. However, there is a lot of scope for future work. Our proposed model doesn't work well in cases where complex deductive reasoning is needed to answer the question. For example, suppose the question is "How much percent of par- ent isotope remains after two half-lives?" and the lesson is on carbon dating which contains the def- inition of half-life. Answering this question us- ing the definition requires understanding the def- inition and transforming the question into a nu- merical problem. Our proposed model lacks such skills and will have near random performance for such questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of our proposed model. Attention layer attends on sentence embeddings d j 's using question-option tuple embeddings h i 's. Score Calculation layer calculates the cosine similarity between m i and h i which is passed through softmax to get the final probability distribution.</figDesc></figure>

			<note place="foot" n="1"> The code is available at https://github.com/ akshay107/CNN-QA</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Attention-over-attention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR abs/1607.04423</idno>
		<ptr target="http://arxiv.org/abs/1607.04423" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Apache Software Foundation</title>
		<ptr target="https://lucene.apache.org/core/" />
	</analytic>
	<monogr>
		<title level="m">Letzter Zugriff: 20. Oktober</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Apache lucene-scoring</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P16/P16-1086.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1181.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Query expansion using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saar</forename><surname>Kuzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
		<idno type="doi">10.1145/2983323.2983876</idno>
		<ptr target="https://doi.org/10.1145/2983323.2983876" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA, CIKM</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1929" to="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P15/P15-1142.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D16/D16-1264.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>CoRR abs/1611.01603</idno>
		<ptr target="http://arxiv.org/abs/1611.01603" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural language comprehension with the epireader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="128" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno>CoRR abs/1707.06209</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards aicomplete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<ptr target="http://arxiv.org/abs/1410.3916" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR abs/1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Meek</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/" />
		<title level="m">Wikiqa: A challenge dataset for open-domain question answering. ACL Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
