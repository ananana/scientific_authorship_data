<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Deconvolution Saliency (TDS): a deep tool box for linguistic analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vanni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>BCL</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ducoffe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>I3S</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Mayaffre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>BCL</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Precioso</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>I3S</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Longrée</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Univ.Lì ege, L.A.S.L.A</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veeresh</forename><surname>Elango</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>I3S</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazly</forename><surname>Santos</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>I3S</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Gonzalez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>I3S</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Galdo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>I3S</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Aguilar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>BCL</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Deconvolution Saliency (TDS): a deep tool box for linguistic analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="548" to="557"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>548</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a new strategy , called Text Deconvolution Saliency (TDS), to visualize linguistic information detected by a CNN for text classification. We extend Deconvolution Networks to text in order to present a new perspective on text analysis to the linguistic community. We empirically demonstrated the efficiency of our Text Decon-volution Saliency on corpora from three different languages: English, French, and Latin. For every tested dataset, our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As in many other fields of data analysis, Natural Language Processing (NLP) has been strongly im- pacted by the recent advances in Machine Learn- ing, more particularly with the emergence of Deep Learning techniques. These techniques outper- form all other state-of-the-art approaches on a wide range of NLP tasks and so they have been quickly and intensively used in industrial systems. Such systems rely on end-to-end training on large amounts of data, making no prior assumptions about linguistic structure and focusing on stasti- cally frequent patterns. Thus, they somehow step away from computational linguistics as they learn implicit linguistic information automatically with- out aiming at explaining or even exhibiting classic linguistic structures underlying the decision. This is the question we raise in this article and that we intend to address by exhibiting classic lin- * L. Vanni and M. Ducoffe contributed equally to this work and should be considered as co-first authors.</p><p>guistic patterns which are indeed exploited im- plictly in deep architectures to lead to higher per- formances. Do neural networks make use of co- occurrences and other standard features, consid- ered in traditional Textual Data Analysis (TDA) (Textual Mining)? Do they also rely on comple- mentary linguistic structure which is invisible to traditional techniques? If so, projecting neural net- works features back onto the input space would highlight new linguistic structures and would lead to improving the analysis of a corpus and a bet- ter understanding on where the power of the Deep Learning techniques comes from.</p><p>Our hypothesis is that Deep Learning is sensi- tive to the linguistic units on which the computa- tion of the key statistical sentences is based as well as to phenomena other than frequency and com- plex linguistic observables. The TDA has more difficulty taking such elements into account -such as linguistic linguistic patterns. Our contribu- tion confronts Textual Data Analysis and Convolu- tional Neural Networks for text analysis. We take advantage of deconvolution networks for image analysis in order to present a new perspective on text analysis to the linguistic community that we call Text Deconvolution Saliency (TDS). Our de- convolution saliency corresponds to the sum over the word embedding of the deconvolution projec- tion of a given feature map. Such a score provides a heat-map of words in a sentence that highlights the pattern relevant for the classification decision. We examine z-test (see section 4.2) and TDS for three languages: English, French and Latin. For all our datasets, TDS highlights new linguistic ob- servables, invisible with z-test alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Convolutional Neural Networks (CNNs) are widely used in the computer vision community for a wide panel of tasks: ranging from image classifi- cation, object detection to semantic segmentation. It is a bottom-up approach where we applied an input image, stacked layers of convolutions, non- linearities and sub-sampling.</p><p>Encouraged by the success for vision tasks, researchers applied CNNs to text-related prob- lems <ref type="bibr" target="#b6">Kalchbrenner et al. (2014)</ref>; <ref type="bibr" target="#b9">Kim (2014)</ref>. The use of CNNs for sentence modeling traces back to <ref type="bibr" target="#b1">Collobert and Weston (2008)</ref>. Collobert adapted CNNs for various NLP problems includ- ing Part-of-Speech tagging, chunking, Named En- tity Recognition and semantic labeling. CNNs for NLP work as an analogy between an image and a text representation. Indeed each word is embed- ded in a vector representation, then several words build a matrix (concatenation of the vectors).</p><p>We first discuss our choice of architectures. If Recurrent Neural Networks (mostly GRU and LSTM) are known to perform well on a broad range of tasks for text, recent comparisons have confirmed the advantage of CNNs over RNNs when the task at hand is essentially a keyphrase recognition task <ref type="bibr" target="#b16">Yin et al. (2017)</ref>.</p><p>In Textual Mining, we aim at highlighting lin- guistics patterns in order to analyze their constrast: specificities and similarities in a corpus Feldman, R., and J. Sanger (2007); L. <ref type="bibr" target="#b10">Lebart, A. Salem and L. Berry (1998)</ref>. It mostly relies on frequential based methods such as z-test. However, such ex- isting methods have so far encountered difficulties in underlining more challenging linguistic knowl- edge, which up to now have not been empirically observed as for instance syntactical motifs <ref type="bibr" target="#b13">Mellet and Longrée (2009)</ref>.</p><p>In that context, supervised classification, espe- cially CNNs, may be exploited for corpus anal- ysis. Indeed, CNN learns automatically param- eters to cluster similar instances and drive away instances from different categories. Eventually, their prediction relies on features which inferred specificities and similarities in a corpus. Project- ing such features in the word embedding will re- veal relevant spots and may automatize the dis- covery of new linguistic structures as in the pre- viously cited syntactical motifs. Moreover, CNNs hold other advantages for linguistic analysis. They are static architectures that, according to specific settings are more robust to the vanishing gradi- ent problem, and thus can also model long-term dependency in a sentence <ref type="bibr" target="#b2">Dauphin et al. (2017)</ref>; <ref type="bibr" target="#b15">Wen et al. (2017)</ref>; <ref type="bibr" target="#b0">Adel and Schütze (2017)</ref>. Such a property may help to detect structures relying on different parts of a sentence.</p><p>All previous works converged to a shared as- sessment: both CNNs and RNNs provide relevant, but different kinds of information for text classifi- cation. However, though several works have stud- ied linguistic structures inherent in RNNs, to our knowledge, none of them have focused on CNNs. A first line of research has extensively studied the interpretability of word embeddings and their se- mantic representations <ref type="bibr" target="#b5">Ji and Eisenstein (2014)</ref>. When it comes to deep architectures, Karpathy et al. <ref type="bibr" target="#b8">Karpathy et al. (2015)</ref> used LSTMs on charac- ter level language as a testbed. They demonstrate the existence of long-range dependencies on real word data. Their analysis is based on gate activa- tion statistics and is thus global. On another side, Li et al.  provided new visualization tools for recurrent models. They use decoders, t- SNE and first derivative saliency, in order to shed light on how neural models work. Our perspec- tive differs from their line of research, as we do not intend to explain how CNNs work on textual data, but rather use their features to provide com- plementary information for linguistic analysis.</p><p>Although the usage of RNNs is more common, there are various visualization tools for CNNs analysis, inspired by the computer vision field. Such works may help us to highlight the linguis- tic features learned by a CNN. Consequently, our method takes inspiration from those works. Visu- alization models in computer vision mainly con- sist in inverting hidden layers in order to spot ac- tive regions or features that are relevant to the classification decision. One can either train a de- coder network or use backpropagation on the in- put instance to highlight its most relevant features. While those methods may hold accurate informa- tion in their input recovery, they have two main drawbacks: (i) they are computationally expen- sive: the first method requires training a model for each latent representation, and the second re- lies on backpropagation for each submitted sen- tence; (ii) they are highly hyperparameter depen- dent and may require some finetuning depending on the task at hand. On the other hand, Deconvolu- tion Networks, proposed by Zeiler et al <ref type="bibr">Zeiler and Fergus (2014)</ref>, provide an off-the-shelf method to project a feature map in the input space. It consists in inverting each convolutional layer iteratively, back to the input space. The inverse of a discrete convolution is computationally challenging. In re- sponse, a coarse approximation may be employed which consists of inverting channels and filter weights in a convolutional layer and then trans- posing their kernel matrix. More details of the deconvolution heuristic are provided in section 3. Deconvolution has several advantages. First, it in- duces minimal computational requirements com- pared to previous visualization methods. Also, it has been used with success for semantic segmen- tation on images: in Noh et al. <ref type="formula">(2015)</ref>; Noh et al demonstrate the efficiency of deconvolution net- works to predict segmentation masks to identify pixel-wise class labels. Thus deconvolution is able to localize meaningful structure in the input space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CNN for Text Classification</head><p>We propose a deep neural model to capture lin- guistics patterns in text. This model is based on Convolutional Neural Networks with an embed- ding layer for word representations, one convolu- tional with pooling layer and non-linearities. Fi- nally we have two fully-connected layers. The final output size corresponds to the number of classes. The model is trained by cross-entropy with an Adam optimizer. <ref type="figure">Figure 1</ref> shows the global structure of our architecture. The input is a sequence of words w 1 , w 2 ...w n and the out- put contains class probabilities (for text classifica- tion).</p><p>The embedding is built on top of a Word2Vec architecture, here we consider a Skip-gram model. This embedding is also finetuned by the model to to increase the accuracy. Notice that we do not use lemmatisation, as in <ref type="bibr" target="#b1">Collobert and Weston (2008)</ref>, thus the linguistic material which is automatically detected does not rely on any prior assumptions about the part of speech. In computer vision, we consider images as 2-dimensional isotropic sig- nals. A text representation may also be consid- ered as a matrix: each word is embedded in a fea- ture vector and their concatenation builds a ma- trix. However, we cannot assume both dimensions the sequence of words and their embedding repre- sentation are isotropic. Thus the filters of CNNs for text typically differ from their counterparts de- signed for images. Consequently in text, the width of the filter is usually equal to the dimension of the embedding, as illustrated with the red, yellow, blue and green filters in <ref type="figure">figure 1</ref> Using CNNs has another advantage in our con- text: due to the convolution operators involved, they can be easily parallelized and may also be easily used by the CPU, which is a practical so- lution for avoiding the use of GPUs at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: CNN for Text Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deconvolution</head><p>Extending Deconvolution Networks for text is not straightforward. Usually, in computer vision, the deconvolution is represented by a convolution whose weights depends on the filters of the CNN: we invert the weights of the channels and the filters and then transpose each kernel matrix. When con- sidering deconvolution for text, transposing the kernel matrices is not realistic since we are deal- ing with nonisotropic dimensions -the word se- quences and the filter dimension. Eventually, the kernel matrix is not transposed.</p><p>Another drawback concerns the dimension of the feature map. Here feature map means the out- put of the convolution before applying max pool- ing. Its shape is actually the tuple (# words, # fil- ters). Because the filters' width (red, yellow, blue and green in <ref type="figure">fig 1)</ref> matches the embedding dimen- sion, the feature maps cannot contain this informa- tion. To project the feature map in the embedding space, we need to convolve our feature map with the kernel matrices. To this aim, we upsample the feature map to obtain a 3-dimensional sample of size (# words, embedding dimension, # filters).</p><p>To analyze the relevance of a word in a sen- tence, we only keep one value per word which cor- responds to the sum along the embedding axis of the output of the deconvolution. We call this sum Text Deconvolution Saliency (TDS).</p><p>For the sake of consistency, we sum up our method in figure 2 Eventually, every word in a sentence has a unique TDS score whose value is related to the others. In the next section, we analyze the rel- evance of TDS. We thoroughly demonstrate em- pirically, that the TDS encodes complex linguis- tic patterns based on co-occurrences and possibly also on grammatical and syntaxic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In order to understand what the linguistic mark- ers found by the convolutional neural network ap- proach are, we conducted several tests on different languages and our model seems to get the same behavior in all of them. In order to perform all the linguistic statistical tests, we used our own simple linguistic toolbox Hyperbase, which allows the creation of databases from textual corpus, the analysis and the calculations such as z-test, co- occurrences, PCA, K-Means distance,... We use it to evaluate TDS against z-test scoring. We compel our analysis by only presenting cases on which z- test fail while TDS does not. Indeed TDS captures z-test, as we did not find any sentence on which z-test succeeds while TDS fails. Red words in the studied examples are the highest TDS.</p><p>The first dataset we used for our experiments is the well known IMDB movie review corpus for sentiment classification. It consists of 25,000 re- views labeled by positive or negative sentiment with around 230,000 words.</p><p>The second dataset targets French political dis- courses. It is a corpus of 2.5 millions of words of French Presidents from 1958 (with De Gaulle, the first President of the Fifth Republic) to 2018 with the first speeches by Macron. In this corpus we have removed Macron's speech from the 31st of December 2017, to use it as a test data set. The training task is to recognize each french president.</p><p>The last dataset we used is based on Latin. We assembled a contrastive corpus of 2 million words with 22 principle authors writting in clas- sical Latin. As with the French dataset, the learn- ing task here is to be able to</p><note type="other">predict each author according to new sequences of words. The next example is an excerpt of chapter 26 of the 23th book of Livy: [...] tutus tenebat se quoad multum ac diu obtestanti quattuor milia peditum et quingenti equites in supplementum missi ex Africa sunt . tum refecta tandem spe castra propius hostem mouit classem que et ipse instrui parari que iubet ad insulas maritimam que oram tutandam . in ipso impetu mouendarum de [...]</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Z-test Versus Text Deconvolution Saliency</head><p>Z-test is one of the standard metrics used in linguistic statistics, in particular to measure the occurrences of word collocations <ref type="bibr" target="#b12">Manning and Schütze (1999)</ref>. Indeed, the z-test provides a sta- tistical score of the co-occurrence of a sequence of words to appear more frequently than any other se- quence of words of the same length. This score re- sults from the comparison between the frequency of the observerd word sequence with the frequency expected in the case of a "Normal" distribution. In the context of constrative corpus analysis, this same calculation applied to single words can read- ily provide, for example, the most specific vocab- ulary of a given author. The highest z-test are the most specific words of this given author in this case. This is a simple but strong method for ana- lyzing features of text. It can also be used to clas- sify word sentences according to the global z-test (sum of the scores) of all the words in the given sentence. We can thus use this global z-test as a very simple metric for authorship classification.</p><p>The resulting authorship of a given sentence is for instance given by the author corresponding to the highest global z-test on that sentence compared to all other global z-test obtained by summing up the z-test of each word of the same sentence but with the vocabulary specificity of another author. The mean accuracy of assigning the right author to the right sentence, in our data set, is around 87%, which confirms that z-test is indeed meaningful for z-test Deep Learning Latin 84% 93% French 89% 91% English 90% 97% <ref type="table">Table 1</ref>: Test accuray with z-test and Deep Learn- ing contrast pattern analysis. On the other hand, most of the time CNN reaches an accuracy greater than 90% for text classification (as shown in <ref type="table">Table 1</ref>). This means that the CNN approaches can learn also on their own some of the linguistic specifici- ties useful in discriminating text categories. Pre- vious works on image classification have high- lighted the key role of convolutional layers which learn different level of abstractions of the data to make classification easier.</p><p>The question is: what is the nature of the ab- straction on text?</p><p>We show in this article that CNN approach de- tects automatically words with high z-test but ob- viously this is not the only linguistic structure de- tected.</p><p>To make the two values comparable, we nor- malize them. The values can be either positive or negative. And we distinguish between two thresh- olds 1 for the z-test: over 2 a word is considered as specific and over 5 it is strongly specific (and the oposite with negative values). For the TDS it is just a matter of activation strength.</p><p>The <ref type="figure" target="#fig_1">Figure 3</ref> shows us a comparison between z-test and TDS on a sentence extracted from our Latin corpora (Livy Book XXIII Chap. 26). This sentence is an example of specific words used by Livy 2 . As we can see, when the z-test is the high- est, the TDS is also the highest and the TDS val- ues are high also for the neighbor words (for ex- ample around the word castra). However, this is not always the case: for example small words as que or et are also high in z-test but they do not impact the network at the same level. We can see also on <ref type="figure" target="#fig_1">Figure 3</ref> that words like tenebat, multum or propius are totally uncorrelated. The Pearson cor-  <ref type="formula">38)</ref>. This example is one of the most correlated examples of our dataset, thus CNN seems to learn more than a simple z- test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dataset: English</head><p>For English, we used the IMDB movie review cor- pus for sentiment classification. With the default methods, we can easily show the specific vocabu- lary of each class (positive/negative), according to the z-test. There are for example the words too, bad, no or boring as most indicitive of negative sentiment, and the words and, performance, pow- erful or best for positive. Is it enough to detect automatically if a new review is positive or not? Let's see an example excerpted from a review from December 2017 (not in the training set) on the last American blockbuster:</p><p>[...] i enjoyed three moments in the film in total , and if i am being honest and the person next to me fell asleep in the middle and started snoring during the slow space chasescenes . the story failed to draw me in and entertain me the way <ref type="bibr">[...]</ref> In general the z-test is sufficient to predict the class of this kind of comment. But in this case, the CNN seems to do better, but why?</p><p>If we sum all the z-test (for negative and posi- tive), the positive class obtains a greater score than the negative. The words film, and, honest and en- tertain -with scores 5.38, 12.23, 4 and 2.4 -make this example positive. CNN has activated differ- ent parts of this sentence (as we show in bold/red in the example). If we take the sub-sequence and if i am being honest and, there are two occurences of and but the first one is followed by if and our toolbox gives us 0.84 for and if as a negative class. This is far from the 12.23 in the positive. And if we go further, we can do a co-occurrence analy- sis on and if on the training set. As we see with our co-occurrence analysis <ref type="bibr">4</ref>  <ref type="figure" target="#fig_2">(Figure 4)</ref>, honest is among the most specific adjectivals 5 associated with and if. Exactly what we found in our exam- ple. In addition, we have the same behavior with the verb fall. There is the word asleep next to it. Asleep alone is not really specific of nega- tive review (z-test of 1.13). But the association of both words become highly specific of negative sentences (see the co-occurrences analysis - <ref type="figure" target="#fig_3">Fig- ure 5</ref>). <ref type="bibr">4</ref> Those figures shows the major co-occurrences for a given word (or lemma or PartOfSpeech). There two layers of co-occurrences, the first one (on top) show the direct co- occurrence and the second (on bottom) show a second level of co-occurrence. This level is given by the context of two words (taken together). The colors and the dotted lines are only used to make it more readable (dotted lines are used for the first level). The width of each line is related to the z-test score (more the z-test is big, more the line is wide). <ref type="bibr">5</ref> With our toolbox, we can focus on different part of speech. The Text Deconvolution Saliency here confirms that the CNN seems to focus not only on high z- test but on more complex patterns and maybe de- tects the lemma or the part of speech linked to each word. We will see now that these observations are still valid for other languages and can even be gen- eralized between different TDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dataset: French</head><p>In this corpus we have removed Macron's speech from the 31st of December 2017, to use it as a test data set. In this speech, the CNN primarily recog- nizes Macron (the training task was to be able to predict the correct President). To achieve this task the CNN seems to succeed in finding really com- plex patterns specific to Macron. For example in this sequence:</p><p>[...] notre pays adviennè a l'´ ecole pour nos enfants, au travail pour l' ensem- ble de nos concitoyens pour le climat pour le quotidien de chacune et chacun d' entre vous . Ces transformations pro- fondes ont commencé et se poursuiv- ront avec la même force le même rythme la même intensité [...]</p><p>The z-test gives a result statistically closer to De Gaulle than to Macron. The error in the statistical attribution can be explained by a Gaullist phrase- ology and the multiplication of linguistic markers strongly indexed with De Gaulle: De Gaulle had the specificity of making long and literary sen- tences articulated around co-ordination conjunc- tions as in et (z-test = 28 for de Gaulle, two oc- currences in the excerpt). His speech was also more conceptual than average, and this resulted in an over-use of the articles defined le, la, l´, les) very numerous in the excerpt (7 occurrences); es- pecially in the feminine singular (la république, la liberté, la nation, la guerre, etc., here we have la même force, la même intensité.</p><p>The best results given by the CNN may be sur- prising for a linguist but match perfectly with what is known about the sociolinguistics of Macron's dynamic kind of speeches.</p><p>The part of the excerpt, which impacts most the CNN classification, is related to the nominal syntagm transformations profondes. Taken sepa- rately, neither of the phrase's two words are very Macronian from a statistical point of view (trans- formations = 1.9 profondes = 2.9). Better, the syntagm itself does not appear in the President's learning corpus (0 occurrence). However, it can be seen that the co-occurrence of transformation and profondes amounts to 4.81 at Macron: so it is not the occurrence of one word alone, or the other, which is Macronian but the simultaneous appear- ance of both in the same window. The second and complementary most impacting part of the excerpt thus is related to the two verbs advienne and pour- suivront. From a semantic point of view, the two verbs perfectly contribute, after the phrase trans- formations profondes, to give the necessary dy- namic to a discourse that advocates change. How- ever it is the verb tenses (carried by the morphol- ogy of the verbs) that appear to be the determining factor in the analysis. The calculation of the gram- matical codes co-occurring with the word trans- formations thus indicates that the verbs in the sub- junctive and the verbs in the future (and also the nouns) are the privileged codes for Macron <ref type="figure" target="#fig_5">(Fig- ure 7)</ref>. More precisely the algorithm indicates that, for Macron, when transformation is associated with a verb in the subjunctive (here advienne), then there is usually a verb in the future co-present (here poursuivront). transformations profondes, advi- enne to the subjunctive, poursuivront to the fu- ture: all these elements together form a speech promising action, from the mouth of a young and dynamic President. Finally, the graph indicates that transformations is especially associated with nouns in the President's speeches: in an extraor- dinary concentration, the excerpt lists 11 (pays, ´ ecole, enfants, travail, concitoyens, climat, quo- tidien, transformations, force, rythme, intensité).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Dataset: Latin</head><p>As with the French dataset, the learning task here is to be able to predict the identity of each author from a contrastive corpus of 2 million words with 22 principle authors writting in classical Latin.</p><p>The statistics here identify this sentence as Cae- sar 6 but Livy is not far off. As historians, Caesar and Livy share a number of specific words: for example tool words like se (reflexive pronoun) or que (a coordinator) and prepositions like in, ad, ex, of. There are also nouns like equites (cavalry) or castra (fortified camp).</p><p>The attribution of the sentence to Caesar cannot only rely only on z-test: que or in or castra, with differences thereof equivalent or inferior to Livy. On the other hand, the differences of se, ex, are greater, as is that of equites. Two very Caesarian terms undoubtedly make the difference iubet (he orders) and milia (thousands).</p><p>The greater score of quattuor (four), castra, hostem (the enemy), impetu (the assault) in Livy are not enough to switch the attribution to this au- thor.</p><p>On the other hand, CNN activates several zones appearing at the beginning of sentences and cor- responding to coherent syntactic structures (for Livy) -Tandem reflexes spe castra propius hostem mouit (then, hope having finally returned, he moved the camp closer to the camp of the enemy) -despite the fact that castra in hostem mouit is attested only by Tacitus <ref type="bibr">7</ref> .</p><p>There are also in ipso metu (in fear itself), while in followed by metu is counted one time with Cae- sar and one time also with Quinte-Curce 8 .</p><p>More complex structures are possibly also de- tected by the CNN: the structure tum + participates Ablative Absolute (tum refecta) is more character- istic of Livy (z-test 3.3 with 8 occurrences) than of Caesar (z-test 1.7 with 3 occurrences), even if it is even more specific of Tacitus (z-test 4.2 with 10 occurrences).</p><p>Finally and more likely, the co-occurrence be- tween castra, hostem and impetu may have played a major role: Figure 8 With Livy, impetu appears as a co-occurrent 7 Publius (or Gaius) Cornelius Tacitus, 56 BC -120 BC, was a senator and a historian of the Roman Empire.</p><p>8 Quintus Curtius Rufus was a Roman historian, probably of the 1st century, his only known and only surviving work being "Histories of Alexander the Great" with the lemmas hostis (z-test 9.42) and castra (z- test 6.75), while hostis only has a gap of 3.41 in Caesar and that castra does not appear in the list of co-occurrents.</p><p>For castra, the first co-occurent for Livy is hostis (z-test 22.72), before castra (z-test 10.18), ad (z-test 10.85), in (z-test 8.21), impetus (z-test 7.35), que (z-test 5.86) while in Caesar, impetus does not appear and the scores of all other lemmas are lower except castra (z-test <ref type="bibr">15.15), hostis (8), ad (10,35)</ref>, in (5,17), que (4.79).</p><p>Thus, our results suggest that CNNs manage to account for specificity, phrase structure, and co- occurence networks. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Preprocessings and hyperparameters</head><p>In order to make our experiments reproductible, we detail here all the hyperparameters used in our architecture. The neural network is written in python with the library Keras (an tensorflow as backend).</p><p>The embedding uses a Word2Vec implementa- tion given by the gensim Library. Here we use the SkipGram model with a window size of 10 words and output vectors of 128 values (embedding di- mension).</p><p>The textual datas are tokenized by a home- made tokensizer (which work on English, Latin and French). The corpus is splited into 50 length sequence of words (punctuation is keeped) and each word is converted inta an unique vector of 128 value.</p><p>The first layer of our model takes the text se- quence (as word vectors) and applies a weight corresponding to our WordToVec values. Those weights are still trainable during model training.</p><p>The second layer is the convolution, a Conv2D in Keras with 512 filters of size 3 * 128 (filter- ing three words at time), with a Relu activation method. Then, there is the Maxpooling (MaxPool- ing2D) (The deconvolution model is identical until here. We replace the rest of the classifica- tion model (Dense) by a transposed convolution (Conv2DTranspose).)</p><p>The last layers of the model are Dense layers. One hidden layer of 100 neurons with a Relu acti- vation and one final layer of size equal to the num- ber of classes with a softmax activation.</p><p>All experiments in this paper share the same architecture and the same hyperparameters, and are trained with a cross-entropy method (with an Adam optimizer) with 90% of the dataset for the training data and 10% for the validation. All the tests in this paper are done with new data not in- cluded in the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In a nutshell, Text Deconvolution Saliency is ef- ficient on a wide range of corpora. By crossing statistical approaches with neural networks, we propose a new strategy for automatically detect- ing complex linguistic observables, which up to now hardly detectable by frequency-based meth- ods. Recall that the linguistic matter and the topology recovered by our TDS cannot return to chance: the zones of activation make it possible to obtain recognition rates of more than 91% on the French political speech and 93% on the Latin corpus; both rates equivalent to or higher than the rates obtained by the statistical calculation of the key passages. Improving the model and under- standing all the mathematical and linguistic out- comes remains an import goal. In future work, we intend to thoroughly study the impact of TDS given morphosyntactic information. <ref type="bibr">Matthew D Zeiler and Rob Fergus. 2014</ref>. Visualizing and understanding convolutional networks. In Eu- ropean conference on computer vision, pages 818- 833. Springer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Textual Deconvolution Saliency (TDS)</figDesc><graphic url="image-2.png" coords="4,74.84,62.81,212.60,132.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: z-test versus Text Deconvolution Saliency (TDS)-Example on Livy Book XXIII Chap. 26</figDesc><graphic url="image-3.png" coords="5,307.28,63.81,221.09,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: co-occurrences analysis of and if (Hyperbase)</figDesc><graphic url="image-4.png" coords="6,74.84,308.78,212.59,174.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: co-occurrences analysis of fall (Hyperbase)</figDesc><graphic url="image-5.png" coords="6,310.11,62.81,212.60,176.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Deconvolution on Macron speech.</figDesc><graphic url="image-6.png" coords="7,72.00,63.80,221.10,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Main part-of-speech co-occurrences for transformations (Hyperbase)</figDesc><graphic url="image-7.png" coords="7,310.11,100.44,212.60,178.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Specific co-occurrences between impetu and castra (Hyperbase)</figDesc><graphic url="image-8.png" coords="8,72.00,481.21,221.10,155.90" type="bitmap" /></figure>

			<note place="foot" n="1"> The z-test can be approximated by a normal distribution. The score we obtain by the z-test is the standard deviation. A low standard deviation indicates that the data points tend to be close to the mean (the expected value). Over 2 this score means there is less than 2% of chance to have this distribution. Over 5 it&apos;s less than 0.1%. 2 Titus Livius Patavinus-(64 or 59 BC-AD 12 or 17)was a Roman historian.</note>

			<note place="foot" n="3"> Pearson correlation coefficient measures the linear relationship between two datasets. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative</note>

			<note place="foot" n="6"> Gaius Julius Caesar, 100 BC-44 BC, usually called Julius Caesar, was a Roman politician and general and a notable author of Latin prose.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been partly funded by the French Government (National Research Agency, ANR) through the grant ANR-16-CE23-0006 Deep in France, through the "Investments for the Future" Program ANR-11-LABX-0031-01, and through the UCAJEDI Investments in the Future project ANR-15-IDEX-01.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global normalization of convolutional neural networks for joint entity and relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1723" to="1729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Text Mining Handbook. Advanced Approaches in Analyzing Unstructured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Web based toolbox for linguistics analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyperbase</surname></persName>
		</author>
		<ptr target="http://hyperbase.unice.fr" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring Textual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01066</idno>
		<title level="m">Visualizing and understanding neural models in nlp</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Syntactical motifs and textual structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Longrée</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Belgian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina M Rojas</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01923</idno>
		<title level="m">Comparative study of cnn and rnn for natural language processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
