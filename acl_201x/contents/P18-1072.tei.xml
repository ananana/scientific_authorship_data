<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Limitations of Unsupervised Bilingual Dictionary Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<email>soegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Insight Research Centre</orgName>
								<orgName type="institution">National University of Ireland</orgName>
								<address>
									<settlement>Galway</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Aylien Ltd</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><forename type="middle">♣</forename><surname>Ivan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuli´c</forename><surname>Vuli´c</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Language Technology Lab</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Limitations of Unsupervised Bilingual Dictionary Induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="778" to="788"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>778</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised machine translation-i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations , or comparable corpora-seems impossible , but nevertheless, Lample et al. (2018a) recently proposed a fully unsu-pervised machine translation (MT) model. The model relies heavily on an adversar-ial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here. Our results identify the limitations of current unsupervised MT: un-supervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking , when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-lingual word representations enable us to reason about word meaning in multilingual con- texts and facilitate cross-lingual transfer <ref type="bibr" target="#b25">(Ruder et al., 2018)</ref>. Early cross-lingual word embedding models relied on large amounts of parallel data ( <ref type="bibr" target="#b16">Klementiev et al., 2012;</ref><ref type="bibr" target="#b22">Mikolov et al., 2013a</ref>), but more recent approaches have tried to minimize the amount of supervision necessary <ref type="bibr" target="#b20">Levy et al., 2017;</ref><ref type="bibr" target="#b1">Artetxe et al., 2017)</ref>. Some researchers have even presented un- supervised methods that do not rely on any form of cross-lingual supervision at all <ref type="bibr" target="#b3">(Barone, 2016;</ref><ref type="bibr" target="#b7">Conneau et al., 2018;</ref><ref type="bibr" target="#b34">Zhang et al., 2017)</ref>.</p><p>Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and ma- chine translation models in the absence of dictio- naries and translations <ref type="bibr" target="#b3">(Barone, 2016;</ref><ref type="bibr" target="#b34">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Lample et al., 2018a</ref>), and would therefore be a major step toward machine translation to, from, or even between low-resource languages.</p><p>Unsupervised approaches to learning cross- lingual word embeddings are based on the assump- tion that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) ( <ref type="bibr" target="#b23">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b3">Barone, 2016;</ref><ref type="bibr" target="#b34">Zhang et al., 2017;</ref><ref type="bibr" target="#b7">Conneau et al., 2018</ref>). In the words of Barone (2016):</p><p>. . . we hypothesize that, if languages are used to convey thematically similar information in similar contexts, these random processes should be approx- imately isomorphic between languages, and that this isomorphism can be learned from the statistics of the realizations of these processes, the mono- lingual corpora, in principle without any form of explicit alignment.</p><p>Our results indicate this assumption is not true in general, and that approaches based on this assump- tion have important limitations.</p><p>Contributions We focus on the recent state- of-the-art unsupervised model of <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>. <ref type="bibr">1</ref> Our contributions are: (a) In §2, we show that the monolingual word embeddings used in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> are not approximately iso- morphic, using the VF2 algorithm ( <ref type="bibr" target="#b8">Cordella et al., 2001</ref>) and we therefore introduce a metric for quan- tifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In §3, we identify cir- cumstances under which the unsupervised bilingual dictionary induction (BDI) algorithm proposed in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> does not lead to good perfor- mance. (c) We show that a simple trick, exploiting a weak supervision signal from words that are iden- tical across languages, makes the algorithm much more robust. Our main finding is that the perfor- mance of unsupervised BDI depends heavily on all three factors: the language pair, the comparability of the monolingual corpora, and the parameters of the word embedding algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">How similar are embeddings across languages?</head><p>As mentioned, recent work focused on unsuper- vised BDI assumes that monolingual word embed- ding spaces (or at least the subgraphs formed by the most frequent words) are approximately isomor- phic. In this section, we show, by investigating the nearest neighbor graphs of word embedding spaces, that word embeddings are far from isomorphic. We therefore introduce a method for computing the similarity of non-isomorphic graphs. In §4.7, we correlate our similarity metric with performance on unsupervised BDI.</p><p>Isomorphism To motivate our study, we first establish that word embeddings are far from graph isomorphic 2 -even for two closely re- 2 Two graphs that contain the same number of graph ver- tices connected in the same way are said to be isomorphic. In the context of weighted graphs such as word embeddings, a lated languages, English and German, and us- ing embeddings induced from comparable corpora (Wikipedia) with the same hyper-parameters.</p><p>If we take the top k most frequent words in En- glish, and the top k most frequent words in German, and build nearest neighbor graphs for English and German using the monolingual word embeddings used in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>, the graphs are of course very different. This is, among other things, due to German case and the fact that the translates into der, die, and das, but unsupervised alignment does not have access to this kind of information. Even if we consider the top k most frequent En- glish words and their translations into German, the nearest neighbor graphs are not isomorphic. <ref type="figure" target="#fig_0">Fig- ure 1a-b</ref> shows the nearest neighbor graphs of the top 10 most frequent English words on Wikipedia, and their German translations.</p><p>Word embeddings are particularly good at cap- turing relations between nouns, but even if we con- sider the top k most frequent English nouns and their translations, the graphs are not isomorphic; see <ref type="figure" target="#fig_0">Figure 1c</ref>-d. We take this as evidence that word embeddings are not approximately isomor- phic across languages. We also ran graph isomor- phism checks on 10 random samples of frequent English nouns and their translations into Spanish, and only in 1/10 of the samples were the corre- sponding nearest neighbor graphs isomorphic.</p><p>Eigenvector similarity Since the nearest neigh- bor graphs are not isomorphic, even for frequent translation pairs in neighboring languages, we want to quantify the potential for unsupervised BDI us- ing a metric that captures varying degrees of graph similarity. Eigenvalues are compact representations of global properties of graphs, and we introduce a spectral metric based on Laplacian eigenvalues ( <ref type="bibr" target="#b27">Shigehalli and Shettar, 2011</ref>) that quantifies the extent to which the nearest neighbor graphs are isospectral. Note that (approximately) isospectral graphs need not be (approximately) isomorphic, but (approximately) isomorphic graphs are always (approximately) isospectral <ref type="bibr" target="#b14">(Gordon et al., 1992</ref> compute the eigensimilarity of the Laplacians of the nearest neighbor graphs, L 1 and L 2 . For each graph, we find the smallest k such that the sum of the k largest Laplacian eigenvalues is &lt;90% of the Laplacian eigenvalues. We take the smallest k of the two, and use the sum of the squared differences between the largest k Laplacian eigenvalues ∆ as our similarity metric.</p><formula xml:id="formula_0">∆ = k i=1 (λ 1 i − λ 2 i ) 2</formula><p>where k is chosen s.t.</p><formula xml:id="formula_1">min j { k i=1 λ j i n i=1 λ ji &gt; 0.9}</formula><p>Note that ∆ = 0 means the graphs are isospec- tral, and the metric goes to infinite. Thus, the higher ∆ is, the less similar the graphs (i.e., their Lapla- cian spectra). We discuss the correlation between unsupervised BDI performance and approximate isospectrality or eigenvector similarity in §4.7.</p><p>3 Unsupervised cross-lingual learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning scenarios</head><p>Unsupervised neural machine translation relies on BDI using cross-lingual embeddings ( <ref type="bibr" target="#b18">Lample et al., 2018a;</ref><ref type="bibr" target="#b2">Artetxe et al., 2018)</ref>, which in turn relies on the assumption that word embedding graphs are approximately isomorphic. The work of <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>, which we focus on here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries.</p><p>We focus on the work of <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>, who present a fully unsupervised approach to align- ing monolingual word embeddings, induced using fastText ( <ref type="bibr" target="#b5">Bojanowski et al., 2017)</ref>. We describe the learning algorithm in §3.2. <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> consider a specific set of learning scenarios:</p><p>(a) The authors work with the following lan- guages: English-{French, German, Chinese, Russian, Spanish}. These languages, except</p><p>French, are dependent marking <ref type="table" target="#tab_1">(Table 1)</ref>. <ref type="bibr">3</ref> We evaluate <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> on (English to) Estonian (ET), Finnish (FI), Greek (EL), Hun- garian (HU), Polish (PL), and Turkish (TR) in §4.2, to test whether the selection of languages in the original study introduces a bias.</p><p>(b) The monolingual corpora in their experiments are comparable; Wikipedia corpora are used, except for an experiment in which they in- clude Google Gigawords. We evaluate across different domains, i.e., on all combinations of Wikipedia, EuroParl, and the EMEA medical corpus, in §4.3. We believe such scenarios are more realistic for low-resource languages.</p><p>(c) The monolingual embedding models are in- duced using the same algorithms with the same hyper-parameters. We evaluate Con- neau et al. <ref type="formula" target="#formula_2">(2018)</ref> on pairs of embeddings induced with different hyper-parameters in §4.4. While keeping hyper-parameters fixed is always possible, it is of practical interest to know whether the unsupervised methods work on any set of pre-trained word embeddings.</p><p>We also investigate the sensitivity of unsuper- vised BDI to the dimensionality of the monolin- gual word embeddings in §4.5. The motivation for this is that dimensionality reduction will alter the geometric shape and remove characteristics of the embedding graphs that are important for alignment; but on the other hand, lower dimensionality intro- duces regularization, which will make the graphs more similar. Finally, in §4.6, we investigate the impact of different types of query test words on performance, including how performance varies across part-of-speech word classes and on shared vocabulary items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Summary of Conneau et al. (2018)</head><p>We now introduce the method of <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>. <ref type="bibr">4</ref> The approach builds on existing work on learning a mapping between monolingual word em- beddings ( <ref type="bibr" target="#b23">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b33">Xing et al., 2015)</ref> and consists of the following steps: 1) Monolin- gual word embeddings: An off-the-shelf word embedding algorithm ( <ref type="bibr" target="#b5">Bojanowski et al., 2017</ref>) is used to learn source and target language spaces X and Y . 2) Adversarial mapping: A translation matrix W is learned between the spaces X and Y using adversarial techniques ( <ref type="bibr" target="#b11">Ganin et al., 2016)</ref>. A discriminator is trained to discriminate samples from the translated source space W X from the tar- get space Y , while W is trained to prevent this. This, again, is motivated by the assumption that source and target language word embeddings are approximately isomorphic. 3) Refinement (Pro- crustes analysis): W is used to build a small bilin- gual dictionary of frequent words, which is pruned such that only bidirectional translations are kept . A new translation matrix W that translates between the spaces X and Y of these frequent word pairs is then induced by solving the Orthogonal Procrustes problem:</p><formula xml:id="formula_2">W * = argmin W W X − Y F = U V s.t. U ΣV = SVD(Y X )<label>(1)</label></formula><p>This step can be used iteratively by using the new matrix W to create new seed translation pairs. It requires frequent words to serve as reliable anchors for learning a translation matrix. In the experiments in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>, as well as in ours, the iter- ative Procrustes refinement improves performance across the board. 4) Cross-domain similarity lo- cal scaling (CSLS) is used to expand high-density areas and condense low-density ones, for more ac- curate nearest neighbor calculation, CSLS reduces the hubness problem in high-dimensional spaces <ref type="bibr" target="#b24">(Radovanovi´cRadovanovi´c et al., 2010;</ref><ref type="bibr" target="#b9">Dinu et al., 2015)</ref>. It relies on the mean similarity of a source language embedding x to its K target language nearest neigh- bours (K = 10 suggested) nn 1 , . . . , nn K :</p><formula xml:id="formula_3">mnn T (x) = 1 K K i=1 cos(x, nn i )<label>(2)</label></formula><p>where cos is the cosine similarity. mnn S (y) is defined in an analogous manner for any target lan- guage embedding y. CSLS(x, y) is then calcu- lated as follows:</p><formula xml:id="formula_4">2cos(x, y) − mnn T (x) − mnn S (y)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A simple supervised method</head><p>Instead of learning cross-lingual embeddings com- pletely without supervision, we can extract inex- pensive supervision signals by harvesting identi- cally spelled words as in, e.g. ( <ref type="bibr" target="#b1">Artetxe et al., 2017;</ref><ref type="bibr" target="#b28">Smith et al., 2017)</ref>. Specifically, we use identi- cally spelled words that occur in the vocabularies of both languages as bilingual seeds, without em- ploying any additional transliteration or lemma- tization/normalization methods. Using this seed dictionary, we then run the refinement step using Procrustes analysis of <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the following experiments, we investigate the robustness of unsupervised cross-lingual word embedding learning, varying the language pairs, monolingual corpora, hyper-parameters, etc., to obtain a better understanding of when and why unsupervised BDI works.</p><p>Task: Bilingual dictionary induction After the shared cross-lingual space is induced, given a list of N source language words x u,1 , . . . , x u,N , the task is to find a target language word t for each query word x u relying on the representations in the space. t i is the target language word closest to the source language word x u,i in the induced cross-lingual space, also known as the cross-lingual nearest neighbor. The set of learned N (x u,i , t i ) pairs is then run against a gold standard dictionary. We use bilingual dictionaries compiled by Con- neau et al. (2018) as gold standard, and adopt their evaluation procedure: each test set in each language consists of 1500 gold translation pairs. We rely on CSLS for retrieving the nearest neighbors, as it con- sistently outperformed the cosine similarity in all our experiments. Following a standard evaluation practice <ref type="bibr" target="#b32">(Vuli´cVuli´c and Moens, 2013;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b7">Conneau et al., 2018)</ref>, we report Precision at 1 scores (P@1): how many times one of the correct translations of a source word w is retrieved as the nearest neighbor of w in the target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Our default experimental setup closely follows the setup of <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>. For each language we induce monolingual word embeddings for all languages from their respective tokenized and low- ercased Polyglot Wikipedias (Al-Rfou et al., 2013) using fastText ( <ref type="bibr" target="#b5">Bojanowski et al., 2017</ref>). Only words with more than 5 occurrences are retained for training. Our fastText setup relies on skip-gram with negative sampling ( <ref type="bibr" target="#b22">Mikolov et al., 2013a</ref>) with standard hyper-parameters: bag-of-words contexts with the window size 2, 15 negative samples, sub- sampling rate 10 −4 , and character n-gram length  <ref type="table" target="#tab_6">Table 2</ref>: Bilingual dictionary induction scores (P@1×100%) using a) the unsupervised method with adversarial training; b) the supervised method with a bilingual seed dictionary consisting of iden- tical words (shared between the two languages). The third columns lists eigenvector similarities be- tween 10 randomly sampled source language near- est neighbor subgraphs of 10 nodes and the sub- graphs of their translations, all from the benchmark dictionaries in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>.</p><note type="other">Marking Type # Cases English (EN) dependent isolating None French (FR) mixed fusional None German (DE) dependent fusional 4 Chinese (ZH) dependent isolating None Russian (RU) dependent fusional 6-7 Spanish (ES) dependent fusional None Estonian (ET) mixed agglutinative 10+ Finnish (FI) mixed agglutinative 10+ Greek (EL) double fusional 3 Hungarian (HU) dependent agglutinative 10+ Polish (PL) dependent fusional 6-7 Turkish (TR) dependent agglutinative 6-7</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-6. All embeddings are 300-dimensional.</head><p>As we analyze the impact of various modeling assumptions in the following sections (e.g., domain differences, algorithm choices, hyper-parameters), we also train monolingual word embeddings us- ing other corpora and different hyper-parameter choices. Quick summaries of each experimental setup are provided in the respective subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact of language similarity</head><p>Conneau et al. (2018) present results for several target languages: Spanish, French, German, Rus- sian, Chinese, and Esperanto. All languages but Es- peranto are isolating or exclusively concatenating languages from a morphological point of view. All languages but French are dependent-marking. Ta- ble 1 lists three important morphological properties of the languages involved in their/our experiments.</p><p>Agglutinative languages with mixed or double marking show more morphological variance with content words, and we speculate whether unsuper- vised BDI is challenged by this kind of morpholog- ical complexity. To evaluate this, we experiment with Estonian and Finnish, and we include Greek, Hungarian, Polish, and Turkish to see how their approach fares on combinations of these two mor- phological traits.</p><p>We show results in the left column of <ref type="table" target="#tab_6">Table 2</ref>. The results are quite dramatic. The approach achieves impressive performance for Spanish, one of the languages <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> include in their paper. For the languages we add here, perfor- mance is less impressive. For the languages with dependent marking (Hungarian, Polish, and Turk- ish), P@1 scores are still reasonable, with Turkish being slightly lower (0.327) than the others. How- ever, for Estonian and Finnish, the method fails completely. Only in less than 1/1000 cases does a nearest neighbor search in the induced embeddings return a correct translation of a query word. <ref type="bibr">5</ref> The sizes of Wikipedias naturally vary across languages: e.g., fastText trains on approximately 16M sentences and 363M word tokens for Spanish, while it trains on 1M sentences and 12M words for Finnish. However, the difference in performance cannot be explained by the difference in training data sizes. To verify that near-zero performance in Finnish is not a result of insufficient training data, we have conducted another experiment using the large Finnish WaC corpus <ref type="bibr" target="#b21">(Ljubeši´cLjubeši´c et al., 2016</ref>) containing 1.7B words in total (this is similar in size to the English Polyglot Wikipedia). However, even with this large Finnish corpus, the model does not induce anything useful: P@1 equals 0.0.</p><p>We note that while languages with mixed mark- ing may be harder to align, it seems unsupervised BDI is possible between similar, mixed marking languages. So while unsupervised learning fails for English-Finnish and English-Estonian, perfor- mance is reasonable and stable for the more similar Estonian-Finnish pair <ref type="table" target="#tab_6">(Table 2)</ref>. In general, un- supervised BDI, using the approach in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>, seems challenged when pairing En-glish with languages that are not isolating and do not have dependent marking. <ref type="bibr">6</ref> The promise of zero-supervision models is that we can learn cross-lingual embeddings even for low-resource languages. On the other hand, a simi- lar distribution of embeddings requires languages to be similar. This raises the question whether we need fully unsupervised methods at all. In fact, our supervised method that relies on very naive supervi- sion in the form of identically spelled words leads to competitive performance for similar language pairs and better results for dissimilar pairs. The fact that we can reach competitive and more robust per- formance with such a simple heuristic questions the true applicability of fully unsupervised approaches and suggests that it might often be better to rely on available weak supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of domain differences</head><p>Monolingual word embeddings used in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> are induced from Wikipedia, a near- parallel corpus. In order to assess the sensitivity of unsupervised BDI to the comparability and domain similarity of the monolingual corpora, we repli- cate the experiments in <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> using combinations of word embeddings extracted from three different domains: 1) parliamentary proceed- ings from EuroParl.v7 <ref type="bibr" target="#b17">(Koehn, 2005</ref>), 2) Wikipedia (Al-Rfou et al., 2013), and 3) the EMEA corpus in the medical domain <ref type="bibr" target="#b29">(Tiedemann, 2009)</ref>. We report experiments with three language pairs: English- {Spanish, Finnish, Hungarian}.</p><p>To control for the corpus size, we restrict each corpus in each language to 1.1M sentences in to- tal (i.e., the number of sentences in the smallest, EMEA corpus). 300-dim fastText vectors are in- duced as in §4.1, retaining all words with more than 5 occurrences in the training data. For each pair of monolingual corpora, we compute their domain (dis)similarity by calculating the Jensen-Shannon divergence <ref type="bibr" target="#b10">(El-Gamal, 1991)</ref>, based on term distri- butions. <ref type="bibr">7</ref> The domain similarities are displayed in Figures 2a-c. <ref type="bibr">8</ref> We show the results of unsupervised BDI in <ref type="figure">Fig- ures 2g</ref>-i. For Spanish, we see good performance in all three cases where the English and Spanish corpora are from the same domain. When the two corpora are from different domains, performance is close to zero. For Finnish and Hungarian, perfor- mance is always poor, suggesting that more data is needed, even when domains are similar. This is in sharp contrast with the results of our minimally supervised approach <ref type="figure">(Figures 2d-f)</ref> based on iden- tical words, which achieves decent performance in many set-ups.</p><p>We also observe a strong decrease in P@1 for English-Spanish (from 81.19% to 46.52%) when using the smaller Wikipedia corpora. This result indicates the importance of procuring large mono- lingual corpora from similar domains in order to enable unsupervised dictionary induction. How- ever, resource-lean languages, for which the unsu- pervised method was designed in the first place, cannot be guaranteed to have as large monolingual training corpora as available for English, Spanish or other major resource-rich languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of hyper-parameters</head><p>Conneau et al. <ref type="formula" target="#formula_2">(2018)</ref> use the same hyper- parameters for inducing embeddings for all lan- guages. This is of course always practically possi- ble, but we are interested in seeing whether their ap- proach works on pre-trained embeddings induced with possibly very different hyper-parameters. We focus on two hyper-parameters: context window- size (win) and the parameter controlling the num- ber of n-gram features in the fastText model (chn), while at the same time varying the underlying algo- rithm: skip-gram vs. cbow. The results for English- Spanish are listed in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>The small variations in the hyper-parameters with the same underlying algorithm (i.e., using skip- gram or cbow for both EN and ES) yield only slight drops in the final scores. Still, the best scores are obtained with the same configuration on both sides. Our main finding here is that unsupervised BDI fails (even) for EN-ES when the two monolingual embedding spaces are induced by two different al- gorithms (see the results of the entire Spanish cbow column). <ref type="bibr">9</ref> In sum, this means that the unsuper- vised approach is unlikely to work on pre-trained word embeddings unless they are induced on same-  or comparable-domain, reasonably-sized training data using the same underlying algorithm.</p><note type="other">EN:EP EN:Wiki EN:EMEA Training Corpus (English) 0.50 0.55 0.60 0.65 0.70 0.75 Jensen-Shannon Similarity EP Wiki EMEA (a) en-es: domain similarity EN:EP EN:Wiki EN:EMEA Training Corpus (English) 0.50 0.55 0.60 0.65 0.70 0.75 Jensen-Shannon Similarity EP Wiki EMEA (b) en-fi: domain similarity EN:EP EN:Wiki EN:EMEA Training Corpus (English) 0.50 0.55 0.60 0.65 0.70 0.75 Jensen-Shannon Similarity EP Wiki EMEA (c) en-hu: domain similarity EN:EP EN:Wiki EN:EMEA Training Corpus (English</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of dimensionality</head><p>We also perform an experiment on 40-dimensional monolingual word embeddings. This leads to re- duced expressivity, and can potentially make the geometric shapes of embedding spaces harder to align; on the other hand, reduced dimensionality may also lead to less overfitting. We generally see worse performance (P@1 is 50.33 for Spanish, 21.81 for Hungarian, 20.11 for Polish, and 22.03 for Turkish) -but, very interestingly, we obtain better performance for Estonian (13.53), Finnish (15.33), and Greek (24.17) than we did with 300 di- mensions. We hypothesize this indicates monolin- gual word embedding algorithms over-fit to some of the rarer peculiarities of these languages.</p><formula xml:id="formula_5">English (skipgram, win=2, chn=3-6) Spanish Spanish (skipgram) (cbow)</formula><p>== 81.89 00.00 = win=10</p><p>81.28 00.07 = chn=2-7 80.74 00.00 = win=10, chn=2-7 80.15 00.13  <ref type="table">Table 4</ref>: P@1 × 100% scores for query words with different parts-of-speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Impact of evaluation procedure</head><p>BDI models are evaluated on a held-out set of query words. Here, we analyze the performance of the unsupervised approach across different parts-of- speech, frequency bins, and with respect to query words that have orthographically identical coun- terparts in the target language with the same or a different meaning.</p><p>Part-of-speech We show the impact of the part- of-speech of the query words in <ref type="table">Table 4</ref>; again on a representative subset of our languages. The results indicate that performance on verbs is lowest across the board. This is consistent with research on dis- tributional semantics and verb meaning ( <ref type="bibr" target="#b26">Schwartz et al., 2015;</ref><ref type="bibr" target="#b12">Gerz et al., 2016)</ref>.</p><p>Frequency We also investigate the impact of the frequency of query words. We calculate the word frequency of English words based on Google's Tril- lion Word Corpus: query words are divided in groups based on their rank -i.e., the first group contains the top 100 most frequent words, the sec- ond one contains the 101th-1000th most frequent words, etc. -and plot performance (P@1) relative to rank in <ref type="figure">Figure 3</ref>. For EN-FI, P@1 was 0 across all frequency ranks. The plot shows sensitivity to frequency for HU, but less so for ES.</p><p>Homographs Since we use identical word forms (homographs) for supervision, we investigated   whether these are representative or harder to align than other words. <ref type="table" target="#tab_5">Table 5</ref> lists performance for three sets of query words: (a) source words that have homographs (words that are spelled the same way) with the same meaning (homonyms) in the target language, e.g., many proper names; (b) source words that have homographs that are not homonyms in the target language, e.g., many short words; and (c) other words. Somewhat surpris- ingly, words which have translations that are ho- mographs, are associated with lower precision than other words. This is probably due to loan words and proper names, but note that using homographs as supervision for alignment, we achieve high pre- cision for this part of the vocabulary for free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Evaluating eigenvector similarity</head><p>Finally, in order to get a better understanding of the limitations of unsupervised BDI, we correlate the graph similarity metric described in §2 (right column of  categorizations and the characteristics of the mono- lingual corpora would be more widely applicable. We plot the values in <ref type="table" target="#tab_6">Table 2</ref> in <ref type="figure" target="#fig_3">Figure 4</ref>. Recall that our graph similarity metric returns a value in the half-open interval [0, ∞). The correlation be- tween BDI performance and graph similarity is strong (ρ ∼ 0.89).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike <ref type="bibr" target="#b7">Conneau et al. (2018)</ref>, require aligned words, sentences, or documents ( <ref type="bibr" target="#b20">Levy et al., 2017</ref>). Most approaches based on word alignments learn an explicit map- ping between the two embedding spaces ( <ref type="bibr" target="#b23">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b33">Xing et al., 2015)</ref>. Recent approaches try to minimize the amount of supervision needed <ref type="bibr" target="#b1">Artetxe et al., 2017;</ref><ref type="bibr" target="#b28">Smith et al., 2017)</ref>. See <ref type="bibr" target="#b30">Upadhyay et al. (2016)</ref> and <ref type="bibr" target="#b25">Ruder et al. (2018)</ref> for surveys.</p><p>Unsupervised cross-lingual learning <ref type="bibr" target="#b15">Haghighi et al. (2008)</ref> were first to explore unsupervised BDI, using features such as context counts and or- thographic substrings, and canonical correlation analysis. Recent approaches use adversarial learn- ing ( <ref type="bibr" target="#b13">Goodfellow et al., 2014</ref>) and employ a discrim- inator, trained to distinguish between the translated source and the target language space, and a gener- ator learning a translation matrix <ref type="bibr" target="#b3">(Barone, 2016)</ref>. <ref type="bibr" target="#b34">Zhang et al. (2017)</ref>, in addition, use different forms of regularization for convergence, while <ref type="bibr" target="#b7">Conneau et al. (2018)</ref> uses additional steps to refine the in- duced embedding space.</p><p>Unsupervised machine translation Research on unsupervised machine translation ( <ref type="bibr" target="#b18">Lample et al., 2018a;</ref><ref type="bibr" target="#b2">Artetxe et al., 2018;</ref><ref type="bibr" target="#b19">Lample et al., 2018b)</ref> has generated a lot of interest recently with a promise to support the construction of MT systems for and between resource-poor languages. All unsu- pervised NMT methods critically rely on accurate unsupervised BDI and back-translation. Models are trained to reconstruct a corrupted version of the source sentence and to translate its translated version back to the source language. Since the cru- cial input to these systems are indeed cross-lingual word embedding spaces induced in an unsupervised fashion, in this paper we also implicitly investigate one core limitation of such unsupervised MT tech- niques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We investigated when unsupervised BDI ( <ref type="bibr" target="#b7">Conneau et al., 2018</ref>) is possible and found that differences in morphology, domains or word embedding algo- rithms may challenge this approach. Further, we found eigenvector similarity of sampled nearest neighbor subgraphs to be predictive of unsuper- vised BDI performance. We hope that this work will guide further developments in this new and exciting field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Nearest neighbor graphs.</figDesc><graphic url="image-3.png" coords="2,92.23,186.07,115.20,86.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Figure 2: Influence of language-pair and domain similarity on BLI performance, with three language pairs (en-es/fi/hu). Top row, (a)-(c): Domain similarity (higher is more similar) computed as dsim = 1 − JS, where JS is Jensen-Shannon divergence; Middle row, (d)-(f): baseline BLI model which learns a linear mapping between two monolingual spaces based on a set of identical (i.e., shared) words; Bottom row, (g)-(i): fully unsupervised BLI model relying on the distribution-level alignment and adversarial training. Both BLI models apply the Procrustes analysis and use CSLS to retrieve nearest neighbours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Strong correlation (ρ = 0.89) between BDI performance (x) and graph similarity (y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Languages in Conneau et al. (2018) and 
in our experiments (lower half) 

Unsupervised Supervised 
Similarity 
(Adversarial) (Identical) 
(Eigenvectors) 

EN-ES 

81.89 
82.62 
2.07 

EN-ET 

00.00 
31.45 
6.61 

EN-FI 

00.09 
28.01 
7.33 

EN-EL 

00.07 
42.96 
5.01 

EN-HU 

45.06 
46.56 
3.27 

EN-PL 

46.83 
52.63 
2.56 

EN-TR 

32.71 
39.22 
3.14 

ET-FI 

29.62 
24.35 
3.98 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Varying the underlying fastText algorithm 
and hyper-parameters. The first column lists differ-
ences in training configurations between English 
and Spanish monolingual embeddings. 

en-es 
en-hu 
en-fi 

Noun 
80.94 
26.87 
00.00 
Verb 
66.05 
25.44 
00.00 
Adjective 85.53 
53.28 
00.00 
Adverb 
80.00 
51.57 
00.00 
Other 
73.00 
53.40 
00.00 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Scores (P@1 × 100%) for query words 
with same and different spellings and meanings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 )</head><label>2</label><figDesc></figDesc><table>with performance across lan-
guages (left column). Since we already established 
that the monolingual word embeddings are far from 
isomorphic-in contrast with the intuitions motivat-
ing previous work (Mikolov et al., 2013b; Barone, 
2016; Zhang et al., 2017; Conneau et al., 2018)-
we would like to establish another diagnostic met-
ric that identifies embedding spaces for which the 
approach in Conneau et al. (2018) is likely to work. 
Differences in morphology, domain, or embedding 
parameters seem to be predictive of poor perfor-
mance, but a metric that is independent of linguistic </table></figure>

			<note place="foot" n="1"> Our motivation for this is that Artetxe et al. (2017) use small dictionary seeds for supervision, and Barone (2016) seems to obtain worse performance than Conneau et al. (2018). Our results should extend to Barone (2016) and Zhang et al. (2017), which rely on very similar methodology.</note>

			<note place="foot" n="3"> A dependent-marking language marks agreement and case more commonly on dependents than on heads. 4 https://github.com/facebookresearch/ MUSE</note>

			<note place="foot" n="5"> We note, though, that varying our random seed, performance for Estonian, Finnish, and Greek is sometimes (approximately 1 out of 10 runs) on par with Turkish. Detecting main causes and remedies for the inherent instability of adversarial training is one the most important avenues for future research.</note>

			<note place="foot" n="6"> One exception here is French, which they include in their paper, but French arguably has a relatively simple morphology. 7 In order to get comparable term distributions, we translate the source language to the target language using the bilingual dictionaries provided by Conneau et al. (2018). 8 We also computed A-distances (Blitzer et al., 2007) and confirmed that trends were similar.</note>

			<note place="foot" n="9"> We also checked if this result might be due to a lowerquality monolingual ES space. However, monolingual word similarity scores on available datasets in Spanish show performance comparable to that of Spanish skip-gram vectors: e.g., Spearman&apos;s ρ correlation is ≈ 0.7 on the ES evaluation set from SemEval-2017 Task 2 (Camacho-Collados et al., 2017).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, as well as Hin-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR (Conference Track)</title>
		<meeting>ICLR (Conference Track)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards crosslingual distributed representations without parallel text trained with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Valerio Miceli</forename><surname>Barone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="125" to="136" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SemEval2017 Task 2: Multilingual and cross-lingual semantic word similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SEMEVAL</title>
		<meeting>SEMEVAL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An improved algorithm for matching large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IAPR TC-15 Workshop on Graphbased Representations in Pattern Recognition</title>
		<meeting>the 3rd IAPR TC-15 Workshop on Graphbased Representations in Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR (Workshop Papers</title>
		<meeting>ICLR (Workshop Papers</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The role of priors in active Bayesian learning in the sequential statistical decision framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maximum Entropy and Bayesian Methods</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SimVerb-3500: A largescale evaluation set of verb similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2173" to="2182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">One cannot hear the shape of a drum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wolpert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Machine Translation Summit (MT SUMMIT)</title>
		<meeting>the 10th Machine Translation Summit (MT SUMMIT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR (Conference Papers)</title>
		<meeting>ICLR (Conference Papers)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Phrase-based &amp; neural unsupervised machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno>abs/1804.07755</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A strong baseline for learning cross-lingual word embeddings from sentence alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Ljubeši´cljubeši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<title level="m">Finnish Web corpus fiWaC 1.0. Slovenian language resource repository CLARIN.SI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Radovanovi´cradovanovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjana</forename><surname>Ivanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2487" to="2531" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey of cross-lingual word embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Symmetric pattern based word embeddings for improved word similarity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral technique using normalized adjacency matrices for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayalaxmi</forename><surname>Shigehalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidya</forename><surname>Shettar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Science and Mathematics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="371" to="378" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR (Conference Papers)</title>
		<meeting>ICLR (Conference Papers)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">News from OPUS-A collection of multilingual parallel corpora with tools and interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-lingual models of word embeddings: An empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1613" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1005" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
