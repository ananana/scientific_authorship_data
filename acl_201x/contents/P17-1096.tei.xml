<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised QA with Generative Domain-Adaptive Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
						</author>
						<title level="a" type="main">Semi-Supervised QA with Generative Domain-Adaptive Nets</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1040" to="1050"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1096</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of semi-supervised question answering-utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, various neural network models were proposed and successfully applied to the tasks of questions answering (QA) and/or reading com- prehension ( <ref type="bibr" target="#b38">Xiong et al., 2016;</ref><ref type="bibr" target="#b6">Dhingra et al., 2016;</ref><ref type="bibr" target="#b41">Yang et al., 2017)</ref>. While achieving state- of-the-art performance, these models rely on a large amount of labeled data. However, it is extremely difficult to collect large-scale question answering datasets. Historically, many of the question answering datasets have only thousands of question answering pairs, such as WebQues- tions ( <ref type="bibr" target="#b1">Berant et al., 2013</ref>), MCTest ( <ref type="bibr" target="#b24">Richardson et al., 2013</ref>), WikiQA ( <ref type="bibr" target="#b39">Yang et al., 2015)</ref>, and TREC-QA <ref type="bibr" target="#b33">(Voorhees and Tice, 2000</ref>). Although larger question answering datasets with hundreds of thousands of question-answer pairs have been collected, including <ref type="bibr">SQuAD (Rajpurkar et al., 2016</ref>), MSMARCO ( <ref type="bibr" target="#b20">Nguyen et al., 2016)</ref>, and NewsQA ( <ref type="bibr" target="#b29">Trischler et al., 2016a</ref>), the data collec- tion process is expensive and time-consuming in practice. This hinders real-world applications for domain-specific question answering.</p><p>Compared to obtaining labeled question answer pairs, it is trivial to obtain unlabeled text data. In this work, we study the following problem of semi-supervised question answering: is it possi- ble to leverage unlabeled text to boost the perfor- mance of question answering models, especially when only a small amount of labeled data is avail- able? The problem is challenging because con- ventional manifold-based semi-supervised learn- ing algorithms ( <ref type="bibr" target="#b44">Zhu and Ghahramani, 2002;</ref><ref type="bibr" target="#b40">Yang et al., 2016a</ref>) cannot be straightforwardly applied. Moreover, since the main foci of most question answering tasks are extraction rather than genera- tion, it is also not sensible to use unlabeled text to improve language modeling as in machine transla- tion ( <ref type="bibr">Gulcehre et al., 2015)</ref>.</p><p>To better leverage the unlabeled text, we pro- pose a novel neural framework called Genera- tive Domain-Adaptive Nets (GDANs). The start- ing point of our framework is to use linguistic tags to extract possible answer chunks in the un- labeled text, and then train a generative model to generate questions given the answer chunks and their contexts. The model-generated question- answer pairs and the human-generated question- answer pairs can then be combined to train a ques- tion answering model, referred to as a discrimina- tive model in the following text. However, there is discrepancy between the model-generated data distribution and the human-generated data distri- bution, which leads to suboptimal discriminative models. To address this issue, we further propose two domain adaptation techniques that treat the model-generated data distribution as a different domain. First, we use an additional domain tag to indicate whether a question-answer pair is model- generated or human-generated. We condition the discriminative model on the domain tags so that the discriminative model can learn to factor out domain-specific and domain-invariant representa- tions. Second, we employ a reinforcement learn- ing algorithm to fine-tune the generative model to minimize the loss of the discriminative model in an adversarial way.</p><p>In addition, we present a simple and effective baseline method for semi-supervised question an- swering. Although the baseline method performs worse than our GDAN approach, it is extremely easy to implement and can still lead to substan- tial improvement when only limited labeled data is available.</p><p>We experiment on the SQuAD dataset <ref type="bibr" target="#b22">(Rajpurkar et al., 2016</ref>) with various labeling rates and various amounts of unlabeled data. Experimen- tal results show that our GDAN framework con- sistently improves over both the supervised learn- ing setting and the baseline methods, including ad- versarial domain adaptation ( <ref type="bibr" target="#b8">Ganin and Lempitsky, 2014</ref>) and dual learning ( . More specifically, the GDAN model improves the F1 score by 9.87 points in F1 over the supervised learning setting when 8K labeled question-answer pairs are used.</p><p>Our contribution is four-fold. First, different from most of the previous neural network stud- ies on question answering, we study a critical but challenging problem, semi-supervised ques- tion answering. Second, we propose the Gener- ative Domain-Adaptive Nets that employ domain adaptation techniques on generative models with reinforcement learning algorithms. Third, we in- troduce a simple and effective baseline method. Fourth, we empirically show that our framework leads to substantial improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Semi-Supervised Question Answering</head><p>Let us first introduce the problem of semi- supervised question answering.</p><formula xml:id="formula_0">Let L = {q (i) , a (i) , p (i) } N i=1</formula><p>denote a question answering dataset of N instances, where q (i) , a (i) , and p (i) are the question, answer, and paragraph of the i-th instance respectively. The goal of ques- tion answering is to produce the answer a (i) given the question q (i) along with the paragraph p (i) . We will drop the superscript · (i) when the con- text is unambiguous. In our formulation, follow- ing the setting in SQuAD ( <ref type="bibr" target="#b22">Rajpurkar et al., 2016)</ref>, we specifically focus on extractive question an- swering, where a is always a consecutive chunk of text in p. More formally, let p = (p 1 , p 2 , · · · , p T ) be a sequence of word tokens with T being the length, then a can always be represented as a = (p j , p j+1 , · · · , p k−1 , p k ), where j and k are the start and end token indices respectively. The ques- tions can also be represented as a sequence of word tokens q = (q 1 , q 2 , · · · , q T ) with length T .</p><p>In addition to the labeled dataset L, in the semi- supervised setting, we are also given a set of unla- beled data, denoted as</p><formula xml:id="formula_1">U = {a (i) , p (i) } M i=1</formula><p>, where M is the number of unlabeled instances. Note that it is usually trivial to have access to an almost infi- nite number of paragraphs p from sources such as Wikipedia articles and other web pages. And since the answer a is always a consecutive chunk in p, we argue that it is also sensible to extract possible answer chunks from the unlabeled text using lin- guistic tags. We will discuss the technical details of answer chunk extraction in Section 4.1, and in the formulation of our framework, we assume that the answer chunks a are available.</p><p>Given both the labeled data L and the unlabeled data U , the goal of semi-supervised question an- swering is to learn a question answering model D that captures the probability distribution P(a|p, q). We refer to this question answering model D as the discriminative model, in contrast to the gener- ative model that we will present in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Simple Baseline</head><p>We now present a simple baseline for semi- supervised question answering.</p><p>Given a paragraph p = (p 1 , p 2 , · · · , p T ) and the an-</p><formula xml:id="formula_2">swer a = (p j , p j+1 , · · · , p k−1 , p k ), we extract (p j−W , p j−W +1 , · · · , p j−1 , p k+1 , p k+2 , p k+W )</formula><p>from the paragraph and treat it as the question. Here W is the window size and is set at 5 in our experiments so that the lengths of the questions are similar to human-generated questions. The context-based question-answer pairs on U are combined with human-generated pairs on L for training the discriminative model. Intuitively, this method extracts the contexts around the answer chunks to serve as hints for the question answering model. Surprisingly, this simple baseline method leads to substantial improvements when labeled data is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generative Domain-Adaptive Nets</head><p>Though the simple method described in Section 2.1 can lead to substantial improvement, we aim to design a learning-based model to move even fur- ther. In this section, we will describe the model architecture and the training algorithms for the GDANs. We will use a notation in the context of question answering following Section 2, but one should be able to extend the notion of GDANs to other applications as well.</p><p>The GDAN framework consists of two models, a discriminative model and a generative model. We will first discuss the two models in detail in the context of question answering, and then present an algorithm based on reinforcement learning to combine the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discriminative Model</head><p>The discriminative model learns the conditional probability of an answer chunk given the para- graph and the question, i.e., P(a|p, q). We em- ploy a gated-attention (GA) reader ( <ref type="bibr" target="#b6">Dhingra et al., 2016</ref>) as our base model in this work, but our framework does not make any assumptions about the base models being used. The discriminative model is referred to as D.</p><p>The GA model consists of K layers with K being a hyper-parameter. Let H k p be the inter- mediate paragraph representation at layer k, and H q be the question representation. The paragraph representation H k p is a T × d matrix, and the question representation H q is a T × d matrix, where d is the dimensionality of the representa- tions. Given the paragraph p, we apply a bidi- rectional Gated Recurrent Unit (GRU) network <ref type="bibr" target="#b4">(Chung et al., 2014</ref>) on top of the embeddings of the sequence (p 1 , p 2 , · · · , p T ), and obtain the ini- tial paragraph representation H 0 p . Given the ques- tion q, we also apply another bidirectional GRU to obtain the question representation H q .</p><p>The question and paragraph representations are combined with the gated-attention (GA) mecha- nism ( <ref type="bibr" target="#b6">Dhingra et al., 2016</ref>). More specifically, for each paragraph token p i , we compute</p><formula xml:id="formula_3">α j = exp h T q,j h k−1 p,i T j =1 exp h T q,j h k−1 p,i h k p,i = T j=1 α j h q,j h k−1 p,i</formula><p>where h k p,i is the i-th row of H k p and h q,j is the j-th row of H q .</p><p>Since the answer a is a sequence of consecutive word tokens in the paragraph p, we apply two soft- max layers on top of H K p to predict the start and end indices of a, following <ref type="bibr" target="#b41">Yang et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Domain Adaptation with Tags</head><p>We will train our discriminative model on both model-generated question-answer pairs and human-generated pairs. However, even a well- trained generative model will produce questions somewhat different from human-generated ones. Learning from both human-generated data and model-generated data can thus lead to a biased model. To alleviate this issue, we propose to view the model-generated data distribution and the human-generated data distribution as two different data domains and explicitly incorporate domain adaptation into the discriminative model. More specifically, we use a domain tag as an additional input to the discriminative model. We use the tag "d true" to represent the domain of human-generated data (i.e., the true data), and "d gen" for the domain of model-generated data. Following a practice in domain adaptation <ref type="bibr" target="#b16">(Johnson et al., 2016;</ref><ref type="bibr" target="#b3">Chu et al., 2017)</ref>, we append the domain tag to the end of both the questions and the paragraphs. By introducing the domain tags, we expect the discriminative model to factor out domain-specific and domain-invariant representa- tions. At test time, the tag "d true" is appended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Model</head><p>The generative model learns the conditional prob- ability of generating a question given the para- graph and the answer, i.e., P(q|p, a). We im- plement the generative model as a sequence-to- sequence model <ref type="bibr" target="#b27">(Sutskever et al., 2014</ref>) with a copy mechanism ( <ref type="bibr" target="#b13">Gu et al., 2016;</ref><ref type="bibr" target="#b14">Gulcehre et al., 2016</ref>).</p><p>The generative model consists of an encoder and a decoder. An encoder is a GRU that en- codes the input paragraph into a sequence of hid- den states H. We inject the answer information by appending an additional zero/one feature to the word embeddings of the paragraph tokens; i.e., if a word token appears in the answer, the feature is set at one, otherwise zero.</p><p>The decoder is another GRU with an attention mechanism over the encoder hidden states H. At each time step, the generation probabilities over all</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training Generative Domain- Adaptive Nets</head><p>Input: labeled data L, unlabeled data U , #iter- ations</p><formula xml:id="formula_4">T G and T D Initialize G by MLE training on L Randomly initialize D while not stopping do for t ← 1 to T D do Update D to maximize J(L, d true, D) + J(U G , d gen, D) with SGD end for for t ← 1 to T G do Update G to maximize J(U G , d true, D)</formula><p>with Reinforce and SGD end for end while return model D word types are defined with a copy mechanism:</p><formula xml:id="formula_5">p overall = g t p vocab + (1 − g t )p copy (1)</formula><p>where g t is the probability of generating the token from the vocabulary, while (1 − g t ) is the proba- bility of copying a token from the paragraph. The probability g t is computed based on the current hidden state h t :</p><formula xml:id="formula_6">g t = σ(w T g h t )</formula><p>where σ denotes the logistic function and w g is a vector of model parameters. The generation prob- abilities p vocab are defined as a softmax func- tion over the word types in the vocabulary, and the copying probabilities p copy are defined as a soft- max function over the word types in the paragraph. Both p vocab and p copy are defined as a function of the current hidden state h t and the attention re- sults (Gu et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Algorithm</head><p>We first define the objective function of the GDANs, and then present an algorithm to optimize the given objective function. Similar to the Gener- ative Adversarial Nets (GANs) ( <ref type="bibr" target="#b11">Goodfellow et al., 2014</ref>) and adversarial domain adaptation ( <ref type="bibr" target="#b8">Ganin and Lempitsky, 2014</ref>), the discriminative model and the generative model have different objectives in our framework. However, rather than formulat- ing the objective as an adversarial game between the two models ( <ref type="bibr" target="#b11">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b8">Ganin and Lempitsky, 2014</ref>), in our framework, the dis- criminative model relies on the data generated by the generative model, while the generative model aims to match the model-generated data distribu- tion with the human-generated data distribution using the signals from the discriminative model. Given a labeled dataset</p><formula xml:id="formula_7">L = {p (i) , q (i) , a (i) } N i=1</formula><p>, the objective function of a discriminative model D for a supervised learning setting can be written as</p><formula xml:id="formula_8">p (i) ,q (i) ,a (i) ∈L log P D (a (i) |p (i) , q (i) )</formula><p>, where P D is a probability distribution defined by the model D. Since we also incorporate domain tags into the model D, we denote the objective function as</p><formula xml:id="formula_9">J(L, tag, D) = 1 |L| p (i) ,q (i) ,a (i) ∈L log P D,tag (a (i) |p (i) , q (i) )</formula><p>meaning that the domain tag, "tag", is appended to the dataset L. We use |L| = N to denote the number of the instances in the dataset L. The ob- jective function is averaged over all instances such that we can balance labeled and unlabeled data. Let U G denote the dataset obtained by gener- ating questions on the unlabeled dataset U with the generative model G. The objective of the discriminative model is then to maximize J for both labeled and unlabeled data under the do- main adaptation notions, i.e.,</p><formula xml:id="formula_10">J(L, d true, D) + J(U G , d gen, D).</formula><p>Now we discuss the objective of the genera- tive model. Similar to the dual learning (Xia et al., 2016) framework, one can define an auto- encoder objective. In this case, the generative model aims to generate questions that can be re- constructed by the discriminative model, i.e., max- imizing J(U G , d gen, D). However, this objective function can lead to degenerate solutions because the questions can be thought of as an overcom- plete representation of the answers <ref type="bibr" target="#b32">(Vincent et al., 2010)</ref>. For example, given p and a, the genera- tive model might learn to generate trivial questions such as copying the answers, which does not con- tributed to learning a better D.</p><p>Instead, we leverage the discriminative model to better match the model-generated data distribution with the human-generated data distribution. We propose to define an adversarial training objective J(U G , d true, D). We append the tag "d true" in- stead of "d gen" for the model-generated data to "fool" the discriminative model. Intuitively, the goal of G is to generate "useful" questions where the usefulness is measured by the probability that the generated questions can be answered correctly by D.  The overall objective function now can be writ- ten as</p><formula xml:id="formula_11">max D J(L, d true, D) + J(U G , d gen, D) max G J(U G , d true, D)</formula><p>With the above objective function in mind, we present a training algorithm in Algorithm 1 to train a GDAN. We first pretrain the generative model on the labeled data L with maximum likelihood estimation (MLE):</p><formula xml:id="formula_12">max G N i=1 T t=1 log P G (q (i) t |q (i) &lt;t , p (i) , a (i) )</formula><p>where P G is the probability defined by Eq. 1.</p><p>We then alternatively update D and G based on their objectives. To update D, we sam- ple one batch from the labeled data L and one batch from the unlabeled data U G , and com- bine the two batches to perform a gradient up- date step. Since the output of G is discrete and non-differentiable, we use the Reinforce algorithm <ref type="bibr" target="#b36">(Williams, 1992)</ref> to update G. The action space is all possible questions with length T (possibly with padding) and the reward is the objective func- tion J(U <ref type="figure">G , d true, D)</ref>. Let θ G be the parameters of G. The gradient can be written as</p><formula xml:id="formula_13">∂J(UG, d true, D) ∂θG = E P G (q|p,a) (log P D,d true (a|p, q) − b) ∂ log PG(q|p, a) ∂θG</formula><p>where we use an average reward from samples as the baseline b. We approximate the expectation E P G (q|p,a) by sampling one instance at a time from P G (q|p, a) and then do an update step. This train- ing algorithm is referred to as reinforcement learn- ing (RL) training in the following sections. The overall architecture and training algorithm are il- lustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. MLE vs RL. The generator G has two training phases-MLE training and RL training, which are different in that: 1) RL training does not require labels, so G can explore a broader data domain of p using unlabeled data, while MLE training requires labels; 2) MLE maximizes log P (q|p, a), while RL maximizes log P D (a|q, p). Since log P (q|a, p) is the sum of log P (q|p) and log P (a|q, p) (plus a constant), maximizing log P (a|q, p) does not require mod- eling log P (q|p) that is irrelevant to QA, which makes optimization easier. Moreover, maximizing log P (a|q, p) is consistent with the goal of QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Answer Extraction</head><p>As discussed in Section 2, our model assumes that answers are available for unlabeled data. In this section, we introduce how we use linguistic tags and rules to extract answer chunks from unlabeled text.</p><p>To extract answers from massive unlabelled Wikipedia articles, we first sample 205,511 Wikipedia articles that are not used in the training, development and test sets in the SQuAD dataset. We extract the paragraphs from each article, and limit the length of each paragraph at the word level to be less than 850. In total, we obtain 950,612 <ref type="table">Table 1</ref>: Sampled generated questions given the paragraphs and the answers. P means paragraphs, A means answers, GQ means groundtruth questions, and Q means questions generated by our models. MLE refers to maximum likelihood training, and RL refers to reinforcement learning so as to maximize J(UG, d true, D). We truncate the paragraphs to only show tokens around the answer spans with a window size of 20. P1: is mediated by ige , which triggers degranulation of mast cells and basophils when cross -linked by antigen . type ii hypersensitivity occurs when antibodies bind to antigens on the patient ' s own cells , marking them for destruction . this A: type ii hypersensitivity GQ: antibody -dependent hypersensitivity belongs to what class of hypersensitivity ? Q (MLE): what was the UNK of the patient ' s own cells ? Q (RL): what occurs when antibodies bind to antigens on the patient ' s own cells by antigen when cross P2: an additional warming of the earth ' s surface . they calculate with confidence that co0 has been responsible for over half the enhanced greenhouse effect . they predict that under a " business as usual " ( bau ) scenario , A: over half GQ: how much of the greenhouse effect is due to carbon dioxide ? Q (MLE): what is the enhanced greenhouse effect ? Q (RL): what the enhanced greenhouse effect that co0 been responsible for P3: ) narrow gauge lines , which are the remnants of five formerly government -owned lines which were built in mountainous areas . A: mountainous areas GQ: where were the narrow gauge rail lines built in victoria ? Q (MLE): what is the government government government -owned lines built ? Q (RL): what were the remnants of government -owned lines built in P4: but not both ). in 0000 , bankamericard was renamed and spun off into a separate company known today as visa inc . A: visa inc . GQ: what present -day company did bankamericard turn into ? Q (MLE): what was the separate company bankamericard ? Q (RL): what today as bankamericard off into a separate company known today as spun off into a separate company known today P5: legrande writes that " the formulation of a single all -encompassing definition of the term is extremely difficult , if A: legrande GQ: who wrote that it is difficult to produce an all inclusive definition of civil disobedience ? Q (MLE): what is the term of a single all all all all encompassing definition of a single all Q (RL): what writes " the formulation of a single all -encompassing definition of the term all encompassing encom- passing encompassing encompassing paragraphs from unlabelled articles.</p><p>Answers in the SQuAD dataset can be catego- rized into ten types, i.e., "Date", "Other Numeric", "Person", "Location", "Other Entity", "Common Noun Phrase", "Adjective Phrase", "Verb Phrase", "Clause" and "Other" ( <ref type="bibr" target="#b22">Rajpurkar et al., 2016)</ref>. For each paragraph from the unlabeled articles, we utilize Stanford Part-Of-Speech (POS) tag- ger ( <ref type="bibr" target="#b28">Toutanova et al., 2003</ref>) to label each word with the corresponding POS tag, and imple- ment a simple constituency parser to extract the noun phrase, verb phrase, adjective and clause based on a small set of constituency grammars. Next, we use Stanford Named Entity Recog- nizer (NER) ( <ref type="bibr" target="#b7">Finkel et al., 2005</ref>) to assign each word with one of the seven labels, i.e., "Date", "Money", "Percent", "location", "Organization" and "Time". We then categorize a span of con- secutive words with the same NER tags of either "Money" or "Percent" as the answer of the type "Other Numeric". Similarly, we categorize a span of consecutive words with the same NER tags of "Organization" as the answer of the type "Other Entity". Finally, we subsample five answers from all the extracted answers for each paragraph ac- cording to the percentage of answer types in the SQuAD dataset. We obtain 4,753,060 answers in total, which is about 50 times larger than the num- ber of answers in the SQuAD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings and Comparison Methods</head><p>The original SQuAD dataset consists of 87,636 training instances and 10,600 development in- stances. Since the test set is not published, we split 10% of the training set as the test set, and the remaining 90% serves as the actual training set. Instances are split based on articles; i.e., para- graphs in one article always appear in only one set. We tune the hyper-parameters and perform early stopping on the development set using the F1 scores, and the performance is evaluated on the test set using both F1 scores and exact matching (EM) scores <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref>.</p><p>We compare the following methods. SL is the supervised learning setting where we train the model D solely on the labeled data L. Context is the simple context-based method described in Section 2.1. Context + domain is the "Context" method with domain tags as described in Section 3.1.1. Gen is to train a generative model and use the generated questions as additional training data. Gen + GAN refers to the domain adapta- tion method using GANs ( <ref type="bibr" target="#b8">Ganin and Lempitsky, 2014)</ref>; in contrast to the original work, the gen- erative model is updated using Reinforce. Gen + dual refers to the dual learning method ( . Gen + domain is "Gen" with domain tags, while the generative model is trained with MLE and fixed. Gen + domain + adv is the approach we propose (Cf. <ref type="figure" target="#fig_1">Figure 1</ref> and Algorithm 1), with "adv" meaning adversarial training based on Re- inforce. We use our own implementation of "Gen + GAN" and "Gen + dual", since the GAN model ( <ref type="bibr" target="#b8">Ganin and Lempitsky, 2014</ref>) does not handle dis- crete features and the dual learning model (  cannot be directly applied to question answering. When implementing these two base- lines, we adopt the learning schedule introduced by <ref type="bibr" target="#b8">Ganin and Lempitsky (2014)</ref>, i.e., gradually in- creasing the weights of the gradients for the gen- erative model G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>We study the performance of different models with varying labeling rates and unlabeled dataset sizes. Labeling rates are the percentage of training in- stances that are used to train D. The results are reported in <ref type="table">Table 2</ref>. Though the unlabeled dataset we collect consists of around 5 million instances, we also sample a subset of around 50,000 in- stances to evaluate the effects of the size of un- labeled data. The highest labeling rate in <ref type="table">Table 2</ref> is 0.9 because 10% of the training instances are used for testing. Since we do early stopping on the development set using the F1 scores, we also report the development F1. We report two metrics, the F1 scores and the exact matching (EM) scores <ref type="bibr" target="#b22">(Rajpurkar et al., 2016</ref>), on the test set. All metrics are computed using the official evaluation scripts. SL v.s. SSL. We observe that semi-supervised learning leads to consistent improvements over supervised learning in all cases. Such improve- ments are substantial when labeled data is limited. For example, the GDANs improve over supervised learning by 9.87 points in F1 and 7.26 points in EM when the labeling rate is 0.1. With our semi- supervised learning approach, we can use only 0.1 training instances to obtain even better perfor- mance than a supervised learning approach with 0.2 training instances, saving more than half of the labeling costs.</p><p>Comparison with Baselines. By comparing "Gen + domain + adv" with "Gen + GAN" and "Gen + Dual", it is clear that the GDANs perform substantially better than GANs and dual learning. With labeling rate 0.1, GDANs outperform dual learning and GANs by 2.47 and 4.29 points re- spectively in terms of F1.</p><p>Ablation Study. We also perform an ablation study by examining the effects of "domain" and "adv" when added to "gen". It can be seen that both the domain tags and the adversarial training contribute to the performance of the GDANs when the labeling rate is equal to or less than 0.5. With labeling rate 0.9, adding domain tags still leads to better performance but adversarial training does not seem to improve the performance by much.</p><p>Unlabeled Data Size. Moreover, we observe that the performance can be further improved when a larger unlabeled dataset is used, though the gain is relatively less significant compared to changing the model architectures. For example, increasing the unlabeled dataset size from 50K to 5M, the performance of GDANs increases by 0.38 points in F1 and 0.52 points in EM.</p><p>Context-Based Method. Surprisingly, the simple context-based method, though performing worse than GDANs, still leads to substantial gains; e.g., 7.00 points in F1 with labeling rate 0.1. Adding domain tags can improve the performance of the context-based method as well.</p><p>MLE vs RL. We plot the loss curve of −J(U G , d gen, D) for both the MLE-trained gen- erator ("Gen + domain") and the RL-trained gen- erator ("Gen + domain + adv") in <ref type="figure" target="#fig_2">Figure 2</ref>. We observe that the training loss for D on RL- generated questions is lower than MLE-generated questions, which confirms that RL training maxi- mizes log P (a|p, q).</p><p>Samples of Generated Questions. We present some questions generated by our model in <ref type="table">Table  1</ref>. The generated questions are post-processed by removing repeated subs-sequences. Compared to MLE-generated questions, RL-generated ques- tions are more informative (Cf., P1, P2, and P4), and contain less "UNK" (unknown) tokens (Cf., P1). Moreover, both semantically and syntacti- cally, RL-generated questions are more accurate (Cf., P3 and P5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Semi-Supervised Learning. Semi-supervised learning has been extensively studied in litera- ture ( <ref type="bibr" target="#b43">Zhu, 2005)</ref>. A batch of novel models have been recently proposed for semi-supervised learn- ing based on representation learning techniques, such as generative models ( <ref type="bibr" target="#b18">Kingma et al., 2014</ref>), ladder networks ( <ref type="bibr" target="#b23">Rasmus et al., 2015</ref>) and graph embeddings ( <ref type="bibr" target="#b40">Yang et al., 2016a</ref>). However, most of the semi-supervised learning methods are based on combinations of the supervised loss p(y|x) and an unsupervised loss p(x). In the con- text of reading comprehension, directly model- ing the likelihood of a paragraph would not pos- sibly improve the supervised task of question an- swering. Moreover, traditional graph-based semi- supervised learning ( <ref type="bibr" target="#b44">Zhu and Ghahramani, 2002</ref>) cannot be easily extended to modeling the unla- beled answer chunks.</p><p>Domain Adaptation. Domain adaptation has been successfully applied to various tasks, such as classification ( <ref type="bibr" target="#b8">Ganin and Lempitsky, 2014</ref>) and machine translation <ref type="bibr" target="#b16">(Johnson et al., 2016;</ref><ref type="bibr" target="#b3">Chu et al., 2017)</ref>. Several techniques on domain adap- tation <ref type="bibr" target="#b9">(Glorot et al., 2011</ref>) focus on learning distri- bution invariant features by sharing the intermedi- ate representations for downstream tasks. Another line of research on domain adaptation attempt to match the distance between different domain dis- tributions in a low dimensional space ( <ref type="bibr" target="#b19">Long et al., 2015;</ref><ref type="bibr" target="#b0">Baktashmotlagh et al., 2013)</ref>. There are also methods seeking a domain transition from the source domain to the target domain ( <ref type="bibr" target="#b10">Gong et al., 2012;</ref><ref type="bibr" target="#b12">Gopalan et al., 2011;</ref><ref type="bibr" target="#b21">Pan et al., 2011</ref>). Our work gets inspiration from a practice in <ref type="bibr" target="#b16">Johnson et al. (2016)</ref> and <ref type="bibr" target="#b3">Chu et al. (2017)</ref> based on append- ing domain tags. However, our method is different from the above methods in that we apply domain adaptation techniques to the outputs of a genera- tive model rather than a natural data domain.</p><p>Question Answering. Various neural models based on attention mechanisms ( <ref type="bibr" target="#b34">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b25">Seo et al., 2016;</ref><ref type="bibr" target="#b38">Xiong et al., 2016;</ref><ref type="bibr" target="#b6">Dhingra et al., 2016;</ref><ref type="bibr" target="#b17">Kadlec et al., 2016;</ref><ref type="bibr" target="#b30">Trischler et al., 2016b;</ref><ref type="bibr" target="#b26">Sordoni et al., 2016;</ref><ref type="bibr" target="#b5">Cui et al., 2016;</ref>) have been pro- posed to tackle the tasks of question answering and reading comprehension. However, the perfor- mance of these neural models largely relies on a large amount of labeled data available for training.</p><p>Learning with Multiple Models. GANs ( <ref type="bibr" target="#b11">Goodfellow et al., 2014</ref>) formulated a adversarial game between a discriminative model and a gener- ative model for generating realistic images. <ref type="bibr" target="#b8">Ganin and Lempitsky (Ganin and Lempitsky, 2014</ref>) em- ployed a similar idea to use two models for do- main adaptation. Review networks ( <ref type="bibr" target="#b42">Yang et al., 2016b</ref>) employ a discriminative model as a regu- larizer for training a generative model. In the con- text of machine translation, given a language pair, various recent work studied jointly training mod- els to learn the mappings in both directions ( <ref type="bibr" target="#b31">Tu et al., 2016;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We study a critical and challenging problem, semi-supervised question answering. We pro- pose a novel neural framework called Genera- tive Domain-Adaptive Nets, which incorporate domain adaptation techniques in combination with generative models for semi-supervised learning. Empirically, we show that our approach leads to substantial improvements over supervised learn- ing models and outperforms several strong base- lines including GANs and dual learning. In the future, we plan to apply our approach to more question answering datasets in different domains. It will also be intriguing to generalize GDANs to other applications. <ref type="table">Table 2</ref>: Performance with various labeling rates, unlabeled data sizes |U |, and methods. "Dev" denotes the development set, and "test" denotes the test set. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture and training. Red boxes denote the modules being updated. "d true" and "d gen" are two domain tags. D is the discriminative model and G is the generative model. The objectives for the three cases are all to minimize the cross entropy loss of the answer chunks.</figDesc><graphic url="image-2.png" coords="5,230.74,62.81,136.07,143.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of discriminator training loss −J(UG, d gen, D) on generated QA pairs. The lower the better. MLE refers to questions generated by maximum likelihood training, and RL refers to questions generated by reinforcement learning.</figDesc><graphic url="image-4.png" coords="8,90.43,62.81,181.41,141.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>F1 and EM are two metrics. Labeling rate |U | Method Dev F1 Test F1 Test EM5M Gen + domain + adv 0.6670 0.6102 0.4531</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02858</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical comparison of simple domain adaptation methods for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03214</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08148</idno>
		<title level="m">Pointing the unknown words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural machine translation</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<title level="m">Text understanding with the attention sum reader network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02245</idno>
		<title level="m">Iterative alternating neural attention for machine reading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">Newsqa: A machine comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02270</idno>
		<title level="m">Natural language comprehension with the epireader</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00179</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2361" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
