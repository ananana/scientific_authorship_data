<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Fusion LSTMs for Text Semantic Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Fusion LSTMs for Text Semantic Matching</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1034" to="1043"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, there is rising interest in modelling the interactions of text pair with deep neural networks. In this paper, we propose a model of deep fusion LSTMs (DF-LSTMs) to model the strong interaction of text pair in a recursive matching way. Specifically, DF-LSTMs consist of two interdependent LSTMs, each of which models a sequence under the influence of another. We also use external memory to increase the capacity of LSTMs, thereby possibly capturing more complicated matching patterns. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture. Furthermore, we present an elaborate qualitative analysis of our models, giving an intuitive understanding how our model worked.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Among many natural language processing (NLP) tasks, such as text classification, question answer- ing and machine translation, a common problem is modelling the relevance/similarity of a pair of texts, which is also called text semantic matching. Due to the semantic gap problem, text semantic matching is still a challenging problem.</p><p>Recently, deep learning is rising a substan- tial interest in text semantic matching and has achieved some great progresses ( <ref type="bibr" target="#b13">Hu et al., 2014;</ref>. Accord- ing to their interaction ways, previous models can be classified into three categories:</p><p>Weak interaction Models Some early works focus on sentence level interactions, such as ARC- I( <ref type="bibr" target="#b13">Hu et al., 2014</ref>), CNTN(  Semi-interaction Models Another kind of mod- els use soft attention mechanism to obtain the rep- resentation of one sentence by depending on rep- resentation of another sentence, such as ABCNN ( , Attention LSTM ( <ref type="bibr" target="#b20">Rocktäschel et al., 2015;</ref>). These models can alleviate the weak interaction problem to some extent.</p><p>Strong Interaction Models Some models build the interaction at different granularity (word, phrase and sentence level), such as ARC-II ( <ref type="bibr" target="#b13">Hu et al., 2014</ref>), <ref type="bibr">MultiGranCNN (Yin and Schütze, 2015)</ref>, Multi-Perspective CNN ( <ref type="bibr" target="#b10">He et al., 2015)</ref>, MV-LSTM ( ), MatchPyramid ( ). The final matching score de- pends on these different levels of interactions.</p><p>In this paper, we adopt a deep fusion strat- egy to model the strong interactions of two sen- tences. Given two texts x 1:m and y 1:n , we define a matching vector h i,j to represent the interaction of the subsequences x 1:i and y 1:j . h i,j depends on the matching vectors h s,t on previous interactions 1 ≤ s &lt; i and 1 ≤ t &lt; j. Thus, text match- ing can be regarded as modelling the interaction of two texts in a recursive matching way.</p><p>Following this idea, we propose deep fusion long short-term memory neural networks (DF- LSTMs) to model the interactions recursively. More concretely, DF-LSTMs consist of two in- terconnected conditional LSTMs, each of which models a piece of text under the influence of an- other. The output vector of DF-LSTMs is fed into a task-specific output layer to compute the match- ing score. The contributions of this paper can be summa- rized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Different with previous models, DF-LSTMs</head><p>model the strong interactions of two texts in a recursive matching way, which consist of two inter-and intra-dependent LSTMs.</p><p>2. Compared to the previous works on text matching, we perform extensive empirical studies on two very large datasets. Exper- iment results demonstrate that our proposed architecture is more effective.</p><p>3. We present an elaborate qualitative analysis of our model, giving an intuitive understand- ing how our model worked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recursively Text Semantic Matching</head><p>To facilitate our model, we firstly give some defi- nitions. Given two sequences X = x 1 , x 2 , · · · , x m and Y = y 1 , y 2 , · · · , y n , most deep neural models try to represent their semantic relevance by a match- ing vector h(X, Y ), which is followed by a score function to calculate the matching score.</p><p>The weak interaction methods decompose matching vector by h(X,</p><formula xml:id="formula_0">Y ) = f (h(X), h(Y )),</formula><p>where function f (·) may be one of some basic op- erations or the combination of them: concatena- tion, affine transformation, bilinear, and so on.</p><p>In this paper, we propose a strong interaction of two sequences to decompose matching vec- tor h(X, Y ) in a recursive way. We refer to the interaction of the subsequences x 1:i and y 1:j as h i,j (X, Y ), which depends on previous interac- tions h s,t (X, Y ) for 1 ≤ s &lt; i and 1 ≤ t &lt; j. <ref type="figure">Figure 1</ref> gives an example to illustrate this. For sentence pair X ="Female gymnast warm up before a competition", Y ="Gym- nast get ready for a competition", considering the interaction (h <ref type="bibr">4,</ref><ref type="bibr">4</ref> ) between x 1:4 = "Female gymnast warm up" and y 1:4 = "Gymnast get ready for", which is composed by the interactions between their subsequences (h 1,4 , · · · , h 3,4 , h 4,1 , · · · , h 4,3 ). We can see that a strong interaction between two sequences can be decomposed in recursive topology structure.</p><p>The matching vector h i,j (X, Y ) can be written as</p><formula xml:id="formula_1">h i,j (X, Y ) = h i,j (X|Y ) ⊕ h i,j (Y |X),<label>(1)</label></formula><p>where h i,j (X|Y ) refers to conditional encoding of subsequence x 1:i influenced by y 1:j . Meanwhile, h i,j (Y |X) is conditional encoding of subsequence y 1:j influenced by subsequence x 1:i ; ⊕ is concate- nation operation. These two conditional encodings depend on their history encodings. Based on this, we propose deep fusion LSTMs to model the matching of texts by recursive composition mechanism, which can better capture the complicated interaction of two sentences due to fully considering the interactions between subsequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Long Short-Term Memory Network</head><p>Long short-term memory neural network (LSTM) <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref>) is a type of recurrent neural network (RNN) <ref type="bibr" target="#b6">(Elman, 1990)</ref>, and specifically addresses the issue of learning long-term dependencies. LSTM maintains a memory cell that updates and exposes its content only when deemed necessary.</p><p>While there are numerous LSTM variants, here we use the LSTM architecture used by <ref type="bibr" target="#b14">(Jozefowicz et al., 2015)</ref>, which is similar to the architec- ture of (Graves, 2013) but without peep-hole con- nections.</p><p>We define the LSTM units at each time step t to be a collection of vectors in R d : an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a hidden state h t . d is the number of the LSTM units. The elements of the gating vectors i t , f t and o t are in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>The LSTM is precisely specified as follows. </p><formula xml:id="formula_2">    ˜ c t o t i t f t     =     tanh σ σ σ     T A,b x t h t−1 ,<label>(2)</label></formula><formula xml:id="formula_3">tanh tanh    i x ( ) 1 , x i j c  f i o j y  tanh   tanh ... ... ( ) , x i j r   ( ) 2 , x i j h  ( ) 1 , x i j h  ( ) , x i K j h  ( ) , y i j K h  ( ) , 2 y i j h  ( ) , 1 y i j h  ( ) , x i j c ( ) , x i j h ( ) , y i j h ( ) ,</formula><formula xml:id="formula_4">c t = ˜ c t i t + c t−1 f t ,<label>(3)</label></formula><formula xml:id="formula_5">h t = o t tanh (c t ) ,<label>(4)</label></formula><p>where x t is the input at the current time step; T A,b is an affine transformation which depends on pa- rameters of the network A and b. σ denotes the logistic sigmoid function and denotes elemen- twise multiplication. Intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. The update of each LSTM unit can be written precisely as</p><formula xml:id="formula_6">(h t , c t ) = LSTM(h t−1 , c t−1 , x t ).<label>(5)</label></formula><p>Here, the function LSTM(·, ·, ·) is a shorthand for Eq. <ref type="bibr">(2)</ref><ref type="bibr">(3)</ref><ref type="bibr">(4)</ref>.</p><p>LSTM can map the input sequence of arbi- trary length to a fixed-sized vector, and has been successfully applied to a wide range of NLP tasks, such as machine translation <ref type="bibr" target="#b26">(Sutskever et al., 2014</ref>), language modelling <ref type="bibr" target="#b25">(Sutskever et al., 2011</ref>), text matching ( <ref type="bibr" target="#b20">Rocktäschel et al., 2015)</ref> and text classification ( <ref type="bibr" target="#b16">Liu et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep Fusion LSTMs for Recursively Semantic Matching</head><p>To deal with two sentences, one straightforward method is to model them with two separate LSTMs. However, this method is difficult to model local interactions of two sentences. Following the recursive matching strategy, we propose a neural model of deep fusion LSTMs (DF-LSTMs), which consists of two interdepen- dent LSTMs to capture the inter-and intra- interactions between two sequences. <ref type="figure" target="#fig_1">Figure 2</ref> gives an illustration of DF-LSTMs unit.</p><p>To facilitate our model, we firstly give some definitions.</p><p>Given two sequences X = x 1 , x 2 , · · · , x n and Y = y 1 , y 2 , · · · , y m , we let x i ∈ R d denotes the embedded representation of the word x i . The standard LSTM has one temporal dimension. When dealing with a sentence, LSTM regards the position as time step. At position i of sentence x 1:n , the output h i reflects the meaning of subsequence</p><formula xml:id="formula_7">x 1:i = x 1 , · · · , x i .</formula><p>To model the interaction of two sentences in a recursive way, we define h i,j to represent the in- teraction of the subsequences x 1:i and y 1:j , which is computed by</p><formula xml:id="formula_8">h i,j = h (x) i,j ⊕ h (y) i,j ,<label>(6)</label></formula><p>where h</p><formula xml:id="formula_9">(x)</formula><p>i,j denotes the encoding of subsequence x 1:i in the first LSTM influenced by the output of the second LSTM on subsequence y 1:j ; h (y) i,j is the encoding of subsequence y 1:j in the second LSTM influenced by the output of the first LSTM on sub- sequence x 1:i .</p><p>More concretely,</p><formula xml:id="formula_10">(h (x) i,j , c (x) i,j ) = LSTM(H i,j , c (x) i−1,j , x i ), (7) (h (y) i,j , c (y) i,j ) = LSTM(H i,j , c (y) i,j−1 , x j ), (8)</formula><p>where H i,j is information consisting of history states before position (i, j).</p><p>The simplest setting is</p><formula xml:id="formula_11">H i,j = h (x) i−1,j ⊕ h (y)</formula><p>i,j−1 . In this case, our model can be regarded as grid <ref type="bibr">LSTMs (Kalchbrenner et al., 2015)</ref>.</p><p>However, there are total m×n interactions in re- cursive matching process, LSTM could be stressed to keep these interactions in internal memory. Therefore, inspired by recent neural memory net- work, such as neural Turing machine( <ref type="bibr" target="#b9">Graves et al., 2014</ref>) and memory network <ref type="bibr" target="#b24">(Sukhbaatar et al., 2015)</ref>, we introduce two external memories to keep the history information, which can relieve the pressure on low-capacity internal memory.</p><p>Following <ref type="bibr" target="#b27">(Tran et al., 2016)</ref>, we use exter- nal memory constructed by history hidden states, which is defined as At position i, j, two memory blocks M (x) , M (y) are used to store contextual in- formation of x and y respectively.</p><formula xml:id="formula_12">M (x) i,j = {h (x) i−K,j , . . . , h (x) i−1,j },<label>(10)</label></formula><formula xml:id="formula_13">M (y) i,j = {h (y) i,j−K , . . . , h (y) i,j−1 },<label>(11)</label></formula><p>where h (x) and h (x) are outputs of two conditional LSTMs at different positions. The history information can be read from these two memory blocks. We denote a read vector from external memories as r i,j ∈ R d , which can be computed by soft attention mechanisms.</p><formula xml:id="formula_14">r (x) i,j = a (x) i,j M (x) i,j ,<label>(12)</label></formula><formula xml:id="formula_15">r (y) i,j = a (y) i,j M (y) i,j ,<label>(13)</label></formula><p>where a i,j ∈ R K represents attention distribution over the corresponding memory M i,j ∈ R K×d . More concretely, each scalar a i,j,k in attention distribution a i,j can be obtained:</p><formula xml:id="formula_16">a (x) i,j,k = softmax(g(M (x) i,j,k , r (x) i−1,j , x i )), (14) a (y) i,j,k = softmax(g(M (y) i,j,k , r (y) i,j−1 , y j )), (15)</formula><p>where M i,j,k ∈ R d represents the k-th row mem- ory vector at position (i, j), and g(·) is an align function defined by</p><formula xml:id="formula_17">g(x, y, z) = v T tanh(W a [x; y, z]),<label>(16)</label></formula><p>where v ∈ R d is a parameter vector and W a ∈ R d×3d is a parameter matrix. The history information H i,j in Eq <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref> is computed by</p><formula xml:id="formula_18">H i,j = r (x) i,j ⊕ r (y) i,j .<label>(17)</label></formula><p>By incorporating external memory blocks, DF- LSTMs allow network to re-read history interac- tion information, therefore it can more easily cap- ture complicated and long-distance matching pat- terns. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the forward pass of DF-LSTMs can be unfolded along two dimen- sional ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Related Models</head><p>Our model is inspired by some recently proposed models based on recurrent neural network (RNN).</p><p>One kind of models is multi-dimensional re- current neural network (MD-RNN) (Graves et al., 2007; <ref type="bibr" target="#b7">Graves and Schmidhuber, 2009;</ref><ref type="bibr" target="#b3">Byeon et al., 2015</ref>) in machine learning and computer vi- sion communities. As mentioned above, if we just use the neighbor states, our model can be regarded as grid <ref type="bibr">LSTMs (Kalchbrenner et al., 2015)</ref>. What is different is the dependency relations between the current state and history states. Our model uses external memory to increase its mem- ory capacity and therefore can store large useful interactions of subsequences. Thus, we can dis- cover some matching patterns with long depen- dence.</p><p>Another kind of models is memory augmented RNN, such as long short-term memory-network ( ) and recurrent memory net- work ( <ref type="bibr" target="#b27">Tran et al., 2016)</ref>, which extend memory network ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) and equip the RNN with ability of re-reading the history infor- mation. While they focus on sequence modelling, our model concentrates more on modelling the in- teractions of sequence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Specific Output</head><p>There are two popular types of text matching tasks in NLP. One is ranking task, such as community question answering. Another is classification task, such as textual entailment.</p><p>We use different ways to calculate matching score for these two types of tasks.</p><p>1. For ranking task, the output is a scalar match- ing score, which is obtained by a linear trans- formation of the matching vector obtained by FD-LSTMs.</p><p>2. For classification task, the outputs are the probabilities of the different classes, which is computed by a softmax function on the matching vector obtained by FD-LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss Function</head><p>Accordingly, we use two loss functions to deal with different sentence matching tasks.</p><p>Max-Margin Loss for Ranking Task Given a positive sentence pair (X, Y ) and its correspond- ing negative pair (X, ˆ Y ). The matching score s(X, Y ) should be larger than s(X, ˆ Y ). For this task, we use the contrastive max-margin criterion ( <ref type="bibr" target="#b1">Bordes et al., 2013;</ref><ref type="bibr" target="#b23">Socher et al., 2013</ref>) to train our model on matching task.</p><p>The ranking-based loss is defined as</p><formula xml:id="formula_19">L(X, Y, ˆ Y ) = max(0, 1 − s(X, Y ) + s(X, ˆ Y )).<label>(18)</label></formula><p>where s(X, Y ) is predicted matching score for (X, Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-entropy Loss for Classification Task</head><p>Given a sentence pair (X, Y ) and its label l. The outputîoutputî of neural network is the probabilities of the different classes. The parameters of the net- work are trained to minimise the cross-entropy of the predicted and true label distributions.</p><formula xml:id="formula_20">L(X, Y ; l, ˆ l) = − C j=1 l j log( ˆ l j ),<label>(19)</label></formula><p>where l is one-hot representation of the ground- truth label l; ˆ l is predicted probabilities of labels; C is the class number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimizer</head><p>To minimize the objective, we use stochastic gra- dient descent with the diagonal variant of Ada- Grad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>).</p><p>To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold <ref type="bibr" target="#b9">(Graves, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Initialization and Hyperparameters</head><p>Orthogonal Initialization We use orthogonal initialization of our LSTMs, which allows neurons to react to the diverse patterns and is helpful to train a multi-layer network ( <ref type="bibr" target="#b21">Saxe et al., 2013</ref>). Hyper-parameters MQA RTE K 9 9 Embedding size 100 100 Hidden layer size 50 100 Initial learning rate 0.05 0.005 Regularization 5E−5 1E−5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>In this section, we investigate the empirical per- formances of our proposed model on two different text matching tasks: classification task (recogniz- ing textual entailment) and ranking task (matching of question and answer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Competitor Methods</head><p>• Neural bag-of-words (NBOW): Each se- quence is represented as the sum of the em- beddings of the words it contains, then they are concatenated and fed to a MLP.</p><p>• Single LSTM: Two sequences are encoded by a single LSTM, proposed by <ref type="bibr" target="#b20">(Rocktäschel et al., 2015</ref>).</p><p>• Parallel LSTMs: Two sequences are first en- coded by two LSTMs separately, then they are concatenated and fed to a MLP.</p><p>• Attention LSTMs: Two sequences are en- coded by LSTMs with attention mechanism, proposed by <ref type="bibr" target="#b20">(Rocktäschel et al., 2015</ref>).</p><p>• Word-by-word Attention LSTMs: An im- proved strategy of attention LSTMs, which introduces word-by-word attention mecha- nism and is proposed by <ref type="bibr" target="#b20">(Rocktäschel et al., 2015)</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiment-I: Recognizing Textual Entailment</head><p>Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>. This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human an- notators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. The results of DF-LSTMs outperform all the competitor models with the same number of hid- den states while achieving comparable results to the state-of-the-art and using much fewer param- eters, which indicate that it is effective to model the strong interactions of two texts in a recursive matching way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Results</head><p>All models outperform NBOW by a large mar- gin, which indicate the importance of words order in semantic matching.</p><p>The strong interaction models surpass the weak interaction models, for example, compared with parallel LSTMs, DF-LSTMs obtain improvement by 7.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Understanding Behaviors of Neurons in DF-LSTMs</head><p>To get an intuitive understanding of how the DF- LSTMs work on this problem, we examined the  neuron activations in the last aggregation layer while evaluating the test set. We find that some cells are bound to certain roles. We refer to h i,j,k as the activation of the k- th neuron at the position of (i, j), where i ∈ {1, . . . , n} and j ∈ {1, . . . , m}. By visualizing the hidden state h i,j,k and analyzing the maximum activation, we can find that there exist multiple interpretable neurons. For example, when some contextualized local perspectives are semantically related at point (i, j) of the sentence pair, the ac- tivation value of hidden neuron h i,j,k tends to be maximum, meaning that the model could capture some reasoning patterns. <ref type="figure" target="#fig_5">Figure 4</ref> illustrates this phenomenon. In <ref type="figure" target="#fig_5">Fig- ure 4(a)</ref>, a neuron shows its ability to monitor the word pairs with the property of describing differ- ent things of the same type.</p><p>The activation in the patch, containing the word pair "(cat, dog)", is much higher than others. This is an informative pattern for the relation pre- diction of these two sentences, whose ground truth is contradiction. An interesting thing is there are two "dog" in sentence " Dog running with pet toy being by another dog". Our model ignores the useless word, which indicates this neuron selectively captures pattern by contex- tual understanding, not just word level interaction.</p><p>In <ref type="figure" target="#fig_5">Figure 4</ref>(b), another neuron shows that it can capture the local contextual interactions, such as "(ocean waves, beach)". These patterns can be easily captured by final layer and provide a strong support for the final prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index of Cell Word or Phrase Pairs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-th</head><p>(jeans, shirt), (dog, cat) (retriever, cat), (stand, sitting) different entities or events of the same type 11-th (pool, swimming), (street, outside) (animal, dog), (grass,outside) word pair related to lexical entailment 20-th (skateboard, skateboarding), (running, runs) (advertisement, ad), (grassy, grass) words with different morphology 49-th (blue, blue), (wearing black, wearing white), (green uniform, red uniform) words related to color 55-th (a man, two other men), (a man, two girls) (Two women, No one) subjects with singular or plural forms <ref type="table">Table 3</ref>: Multiple interpretable neurons and the word-pairs/phrase-pairs captured by these neurons. The third column gives the explanations of corresponding neuron's behaviours. <ref type="table">Table 3</ref> illustrates multiple interpretable neu- rons and some representative word or phrase pairs which can activate these neurons. These cases show that our model can capture contextual inter- actions beyond word level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Case Study for Attention Addressing</head><p>Mechanism External memory with attention addressing mech- anism enables the network explicitly to utilize the history information of two sentences simultane- ously. As a by-product, the obtained attention dis- tribution over history hidden states also help us interpret the network and discover underlying de- pendencies present in the data.</p><p>To this end, we randomly sample two good cases with entailment relation from test data and visualize attention distributions over exter- nal memory constructed by last 9 hidden states. As shown in <ref type="figure" target="#fig_9">Figure 5</ref>(a), For the first sentence pair, when the word pair "(competition, competition)" are processed, the model si- multaneously selects "warm, before" from one sentence and "gymnast,ready,for" from the other, which are informative patterns and indicate our model has the capacity of capturing phrase-phrase pair.</p><p>Another case in <ref type="figure" target="#fig_9">Figure 5</ref>(b) also shows by at- tention mechanism, the network can sufficiently utilize the history information and the fusion ap- proach allows two LSTMs to share the history in- formation of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Error Analysis</head><p>Although our model DF-LSTMs are more sensi- tive to the discrepancy of the semantic capacity between two sentences, some cases still can not be solved by our model. For example, our model gives a wrong prediction of the sen- tence pair "A golden retriever nurses puppies/Puppies next to their mother", whose ground truth is entailment. The model fails to realize "nurses" means "next to".</p><p>Besides, despite the large size of the training corpus, it's still very difficult to solve some cases, which depend on the combination of the world knowledge and context-sensitive inferences. For example, given an entailment pair "Several women are playing volleyball/The women are hitting a ball with their arms", all models predict "neutral".</p><p>These analysis suggests that some architectural improvements or external world knowledge are necessary to eliminate all errors instead of simply scaling up the basic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiment-II: Matching Question and Answer</head><p>Matching question answering (MQA) is a typical task for semantic matching ( <ref type="bibr" target="#b32">Zhou et al., 2013)</ref>. Given a question, we need select a correct answer from some candidate answers.</p><p>In this paper, we use the dataset collected from Yahoo! Answers with the getByCategory func- tion provided in Yahoo! Answers API, which pro- duces 963, 072 questions and corresponding best answers. We then select the pairs in which the length of questions and answers are both in the interval <ref type="bibr">[4,</ref><ref type="bibr">30]</ref>, thus obtaining 220, 000 question answer pairs to form the positive pairs.</p><p>For negative pairs, we first use each question's best answer as a query to retrieval top 1, 000 re-  A female gymnast in black and red being coached on bar skills</p><p>The female gymnast is training   sults from the whole answer set with Lucene, where 4 or 9 answers will be selected randomly to construct the negative pairs. The whole dataset 1 is divided into training, val- idation and testing data with proportion 20 : 1 : 1. Moreover, we give two test settings: selecting the best answer from 5 and 10 candidates respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Results</head><p>Results of MQA are shown in the <ref type="table" target="#tab_6">Table 4</ref>. we can see that the proposed model also shows its supe- riority on this task, which outperforms the state- of-the-arts methods on both metrics (P@1(5) and P@1(10)) with a large margin.</p><p>By analyzing the evaluation results of question- answer matching in <ref type="table" target="#tab_6">Table 4</ref>, we can see strong interaction models (attention LSTMs, our DF- LSTMs) consistently outperform the weak interac- tion models (NBOW, parallel LSTMs) with a large margin, which suggests the importance of mod- elling strong interaction of two sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Our model can be regarded as a strong interaction model, which has been explored in previous meth- ods.</p><p>One kind of methods is to compute similari- ties between all the words or phrases of the two sentences to model multiple-granularity interac- tions of two sentences, such as RAE (Socher et  <ref type="bibr" target="#b13">Hu et al. (2014)</ref> proposed to an end-to-end architecture with convolutional neural network (Arc-II) to model multiple-granularity interactions of two sentences.  used LSTM to enhance the positional contextual interactions of the words or phrases between two sentences. The input of LSTM for one sentence does not involve another sentence.</p><p>Another kind of methods is to model the con- ditional encoding, in which the encoding of one sentence can be affected by another sentence. <ref type="bibr" target="#b20">Rocktäschel et al. (2015)</ref> and <ref type="bibr" target="#b29">Wang and Jiang (2015)</ref> used LSTM to read pairs of sequences to produce a final representation, which can be re- garded as interaction of two sequences. By incor- porating an attention mechanism, they got further improvements to the predictive abilities.</p><p>Different with these two kinds of methods, we model the interactions of two texts in a recursively matching way. Based on this idea, we propose a model of deep fusion LSTMs to accomplish recur- sive conditional encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper, we propose a model of deep fu- sion LSTMs to capture the strong interaction for text semantic matching. Experiments on two large scale text matching tasks demonstrate the efficacy of our proposed model and its superiority to com- petitor models. Besides, our visualization analysis revealed that multiple interpretable neurons in our model can capture the contextual interactions of the words or phrases.</p><p>In future work, we would like to investigate our model on more text matching tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: A motivated example to illustrate our recursive composition mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of DF-LSTMs unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of unfolded DF-LSTMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Unsupervised Initialization The word embed- dings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pen- nington et al., 2014)). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of two interpretable neurons and some word-pairs captured by these neurons. The darker patches denote the corresponding activations are higher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Explanation</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Female gymnast warm up before a competition Gymnast get ready for a competition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of external memory positions attended when encoding the next word pair (bold and marked by a box)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1</head><label></label><figDesc>http://nlp.fudan.edu.cn/data/. al., 2011), Arc-II (Hu et al., 2014),ABCNN (Yin et al., 2015),MultiGranCNN (Yin and Schütze, 2015), Multi-Perspective CNN (He et al., 2015), MV-LSTM (Wan et al., 2016). Socher et al. (2011) firstly used this paradigm for paraphrase detection. The representations of words or phrases are learned based on recursive autoencoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Hyper-parameters for our model on two 
tasks. 

Hyperparameters For each task, we used 
a stacked DF-LSTM and take the hyperpa-
rameters which achieve the best performance 
on the development set via an small grid 
search over combinations of the initial learn-
ing rate [0.05, 0.0005, 0.0001], l 2 regularization 
[0.0, 5E−5, 1E−5, 1E−6] and the values of K 
[1, 3, 6, 9, 12]. The final hyper-parameters are set 
as Table 1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracies of our proposed model against 
other neural models on SNLI corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 shows the evaluation results on SNLI.</head><label>2</label><figDesc></figDesc><table>The 
2nd column of the table gives the number of hid-
den states. 
From experimental results, we have several ex-
perimental findings. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of our proposed model against 
other neural models on Yahoo! question-answer 
pairs dataset. 

</table></figure>

			<note place="foot">M t = {h t−K ,. .. , h t−1 } ∈ R K×d , (9) where K is the number of memory segments, which is generally instance-independent and predefined as hyper-parameter; d is the size of each segment; and h t is the hidden state at time t emitted by LSTM.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments. This work was par-tially funded by National Natural Science Foun-dation of <ref type="bibr">China (No. 61532011, 61473092, and 61472088)</ref>, the National High Technology Re-search and Development Program of China (No. 2015AA015408).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3547" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks-ICANN 2007</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alex Graves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<idno>arXiv:1308.0850</idno>
	</analytic>
	<monogr>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Neural turing machines</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01526</idno>
		<title level="m">Grid long short-term memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-timescale long short-term memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on EMNLP</title>
		<meeting>the Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for community-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<title level="m">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Recurrent memory network for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monz</surname></persName>
		</author>
		<idno>abs/1601.01272</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08849</idno>
		<title level="m">Learning natural language inference with lstm</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural network for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving question retrieval in community question answering using world knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
