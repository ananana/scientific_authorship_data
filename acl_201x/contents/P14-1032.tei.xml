<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Product Feature Mining: Semantic Clues versus Syntactic Constituents</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Product Feature Mining: Semantic Clues versus Syntactic Constituents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="336" to="346"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Product feature mining is a key subtask in fine-grained opinion mining. Previous works often use syntax constituents in this task. However, syntax-based methods can only use discrete contextual information , which may suffer from data sparsity. This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues. Lexical semantic clue verifies whether a candidate term is related to the target product, and contextual semantic clue serves as a soft pattern miner to find candidates, which exploits semantics of each word in context so as to alleviate the data sparsity problem. We build a semantic similarity graph to encode lexical semantic clue, and employ a convolutional neural model to capture contextual semantic clue. Then Label Propagation is applied to combine both semantic clues. Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches, which not only mines product features more accurately, but also extracts more infrequent product features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, opinion mining has helped cus- tomers a lot to make informed purchase decisions. However, with the rapid growth of e-commerce, customers are no longer satisfied with the over- all opinion ratings provided by traditional senti- ment analysis systems. The detailed functions or attributes of products, which are called product features, receive more attention. Nevertheless, a product may have thousands of features, which makes it impractical for a customer to investigate them all. Therefore, mining product features au- tomatically from online reviews is shown to be a key step for opinion summarization ( <ref type="bibr" target="#b7">Hu and Liu, 2004;</ref><ref type="bibr" target="#b18">Qiu et al., 2009</ref>) and fine-grained sentiment analysis <ref type="bibr" target="#b9">(Jiang et al., 2011;</ref><ref type="bibr" target="#b12">Li et al., 2012)</ref>.</p><p>Previous works often mine product features via syntactic constituent matching ( <ref type="bibr" target="#b17">Popescu and Etzioni, 2005;</ref><ref type="bibr" target="#b18">Qiu et al., 2009;</ref><ref type="bibr" target="#b26">Zhang et al., 2010)</ref>. The basic idea is that reviewers tend to comment on product features in similar syntactic structures. Therefore, it is natural to mine product features by using syntactic patterns. For example, in <ref type="figure">Figure 1</ref>, the upper box shows a dependency tree produced by Stanford Parser (de <ref type="bibr" target="#b3">Marneffe et al., 2006</ref>), and the lower box shows a common syntactic pattern from ( <ref type="bibr" target="#b26">Zhang et al., 2010)</ref>, where &lt;feature/NN&gt; is a wildcard to be fit in reviews and NN denotes the required POS tag of the wildcard. Usually, the product name mp3 is specified, and when screen matches the wildcard, it is likely to be a product feature of mp3.</p><p>Figure 1: An example of syntax-based prod- uct feature mining procedure. The word screen matches the wildcard &lt;feature/NN&gt;. Therefore, screen is likely to be a product feature of mp3.</p><p>Generally, such syntactic patterns extract prod- uct features well but they still have some limita- tions. For example, the product-have-feature pat- tern may fail to find the fm tuner in a very similar case in Example 1(a), where the product is men- tioned by using player instead of mp3. Similarly, it may also fail on Example 1(b), just with have re- placed by support. In essence, syntactic pattern is a kind of one-hot representation for encoding the contexts, which can only use partial and discrete features, such as some key words (e.g., have) or shallow information (e.g., POS tags). Therefore, such a representation often suffers from the data sparsity problem ( <ref type="bibr" target="#b21">Turian et al., 2010)</ref>.</p><p>One possible solution for this problem is us- ing a more general pattern such as NP-VB-feature, where NP represents a noun or noun phrase and VB stands for any verb. However, this pattern be- comes too general that it may find many irrelevant cases such as the one in Example 1(c), which is not talking about the product. Consequently, it is very difficult for a pattern designer to balance between precision and generalization. To solve the problems stated above, it is ar- gued that deeper semantics of contexts shall be ex- ploited. For example, we can try to automatically discover that the verb have indicates a part-whole relation ( <ref type="bibr" target="#b26">Zhang et al., 2010)</ref> and support indicates a product-function relation, so that both sth. have and sth. support suggest that terms following them are product features, where sth. can be replaced by any terms that refer to the target product (e.g., mp3, player, etc.). This is called contextual se- mantic clue. Nevertheless, only using contexts is not sufficient enough. As in Example 1(d), we can see that the word flaws follows mp3 have, but it is not a product feature. Thus, a noise term may be extracted even with high contextual support. Therefore, we shall also verify whether a candi- date is really related to the target product. We call it lexical semantic clue.</p><p>This paper proposes a novel bootstrapping ap- proach for product feature mining, which lever- ages both semantic clues discussed above. Firstly, some reliable product feature seeds are automat- ically extracted. Then, based on the assumption that terms that are more semantically similar to the seeds are more likely to be product features, a graph which measures semantic similarities be- tween terms is built to capture lexical semantic clue. At the same time, a semi-supervised con- volutional neural model <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>) is employed to encode contextual semantic clue. Fi- nally, the two kinds of semantic clues are com- bined by a Label Propagation algorithm.</p><p>In the proposed method, words are represented by continuous vectors, which capture latent se- mantic factors of the words ( <ref type="bibr" target="#b21">Turian et al., 2010</ref>). The vectors can be unsupervisedly trained on large scale corpora, and words with similar semantics will have similar vectors. This enables our method to be less sensitive to lexicon change, so that the data sparsity problem can be alleviated . The con- tributions of this paper include:</p><p>• It uses semantics of words to encode contextual clues, which exploits deeper level information than syntactic constituents. As a result, it mines product features more accurately than syntax- based methods.</p><p>• It exploits semantic similarity between words to capture lexical clues, which is shown to be more effective than co-occurrence relation be- tween words and syntactic patterns. In addition, experiments show that the semantic similarity has the advantage of mining infrequent product features, which is crucial for this task. For ex- ample, one may say "This hotel has low water pressure", where low water pressure is seldom mentioned, but fatal to someone's taste.</p><p>• We compare the proposed semantics-based ap- proach with three state-of-the-art syntax-based methods. Experiments show that our method achieves significantly better results. The rest of this paper is organized as follows. Sec- tion 2 introduces related work. Section 3 describes the proposed method in details. Section 4 gives the experimental results. Lastly, we conclude this pa- per in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In product feature mining task, <ref type="bibr" target="#b7">Hu and Liu (2004)</ref> proposed a pioneer research. However, the asso- ciation rules they used may potentially introduce many noise terms. Based on the observation that product features are often commented on by simi- lar syntactic structures, it is natural to use patterns to capture common syntactic constituents around product features.</p><p>Popescu and Etzioni (2005) designed some syn- tactic patterns to search for product feature candi- dates and then used Pointwise Mutual Information (PMI) to remove noise terms. <ref type="bibr" target="#b18">Qiu et al. (2009)</ref> proposed eight heuristic syntactic rules to jointly extract product features and sentiment lexicons, where a bootstrapping algorithm named Double Propagation was applied to expand a given seed set. <ref type="bibr" target="#b26">Zhang et al. (2010)</ref> improved Qiu's work by adding more feasible syntactic patterns, and the HITS algorithm <ref type="bibr" target="#b11">(Kleinberg, 1999</ref>) was employed to rank candidates. <ref type="bibr" target="#b15">Moghaddam and Ester (2010)</ref> extracted product features by automatical opinion pattern mining. <ref type="bibr" target="#b28">Zhuang et al. (2006)</ref> used various syntactic templates from an annotated movie cor- pus and applied them to supervised movie feature extraction. <ref type="bibr" target="#b24">Wu et al. (2009)</ref> proposed a phrase level dependency parsing for mining aspects and features of products.</p><p>As discussed in the first section, syntactic pat- terns often suffer from data sparsity. Further- more, most pattern-based methods rely on term frequency, which have the limitation of finding infrequent but important product features. A re- cent research ( <ref type="bibr" target="#b25">Xu et al., 2013</ref>) extracted infrequent product features by a semi-supervised classifier, which used word-syntactic pattern co-occurrence statistics as features for the classifier. However, this kind of feature is still sparse for infrequent candidates. Our method adopts a semantic word representation model, which can train dense fea- tures unsupervisedly on a very large corpus. Thus, the data sparsity problem can be alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>We propose a semantics-based bootstrapping method for product feature mining. Firstly, some product feature seeds are automatically extracted. Then, a semantic similarity graph is created to capture lexical semantic clue, and a Convolutional Neural Network (CNN) <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>) is trained in each bootstrapping iteration to encode contextual semantic clue. Finally we use Label Propagation to find some reliable new seeds for the training of the next bootstrapping iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Seed Generation</head><p>The seed set consists of positive labeled examples (i.e. product features) and negative labeled exam- ples (i.e. noise terms). Intuitively, popular product features are frequently mentioned in reviews, so they can be extracted by simply mining frequently occurring nouns ( <ref type="bibr" target="#b7">Hu and Liu, 2004</ref>). However, this strategy will also find many noise terms (e.g., commonly used nouns like thing, one, etc.). To produce high quality seeds, we employ a Domain Relevance Measure (DRM) <ref type="bibr" target="#b8">(Jiang and Tan, 2010)</ref>, which combines term frequency with a domain- specific measuring metric called Likelihood Ratio Test (LRT) <ref type="bibr" target="#b4">(Dunning, 1993)</ref>. Let λ(t) denotes the LRT score of a product feature candidate t,</p><formula xml:id="formula_0">λ(t) = p k 1 (1 − p) n 1 −k 1 p k 2 (1 − p) n 2 −k 2 p k 1 1 (1 − p 1 ) n 1 −k 1 p k 2 2 (1 − p 2 ) n 2 −k 2 (1)</formula><p>where k 1 and k 2 are the frequencies of t in the review corpus R and a background corpus 1 B, n 1 and n 2 are the total number of terms in R and B, p = (k 1 + k 2 )/(n 1 + n 2 ), p 1 = k 1 /n 1 and p 2 = k 2 /n 2 . Then a modified DRM 2 is proposed,</p><formula xml:id="formula_1">DRM (t) = tf (t) max[tf (·)] × 1 log df (t) × | log λ(t)| − min| log λ(·)| max| log λ(·)| − min| log λ(·)| (2)</formula><p>where tf (t) is the frequency of t in R and df (t) is the frequency of t in B.</p><p>All nouns in R are ranked by DRM (t) in de- scent order, where top N nouns are taken as the positive example set V + s . On the other hand, <ref type="bibr" target="#b25">Xu et al. (2013)</ref> show that a set of general nouns sel- dom appear to be product features. Therefore, we employ their General Noun Corpus to create the negative example set V − s , where N most frequent terms are selected. Besides, it is guaranteed that V + s ∩ V − s = ∅, i.e., conflicting terms are taken as negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Capturing Lexical Semantic Clue in a Semantic Similarity Graph</head><p>To capture lexical semantic clue, each word is first converted into word embedding, which is a con- tinuous vector with each dimension's value corre- sponds to a semantic or grammatical interpretation ( <ref type="bibr" target="#b21">Turian et al., 2010</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Learning Word Embedding for Semantic Representation</head><p>Given a sequence of training words W = {w 1 , w 2 , ..., w m }, the goal of the Skip-gram model is to learn a continuous vector space EB = {e 1 , e 2 , ..., e m }, where e i is the word embedding of w i . The training objective is to maximize the average log probability of using word w t to pre- dict a surrounding word w t+j ,</p><formula xml:id="formula_2">ˆ EB = argmax et∈EB 1 m m t=1 −c≤j≤c,j =0 log p(w t+j |w t ; e t ) (3)</formula><p>where c is the size of the training window. Basi- cally, p(w t+j |w t ; e t ) is defined as,</p><formula xml:id="formula_3">p(w t+j |w t ; e t ) = exp(e T t+j e t ) m w=1 exp(e T w e t )<label>(4)</label></formula><p>where e i is an additional training vector associ- ated with e i . This basic formulation is impracti- cal because it is proportional to m. A hierarchical softmax approximation can be applied to reduce the computational cost to log 2 (m), see (Morin and <ref type="bibr" target="#b16">Bengio, 2005</ref>) for details.</p><p>To alleviate the data sparsity problem, EB is first trained on a very large corpus 3 (denoted by C), and then fine-tuned on the target review cor- pus R. Particularly, for phrasal product features, a statistic-based method in ( <ref type="bibr" target="#b27">Zhu et al., 2009</ref>) is used to detect noun phrases in R. Then, an Unfold- ing Recursive Autoencoder <ref type="bibr" target="#b19">(Socher et al., 2011</ref>) is trained on C to obtain embedding vectors for noun phrases. In this way, semantics of infrequent terms in R can be well captured. Finally, the phrase- based Skip-gram model in ( <ref type="bibr" target="#b14">Mikolov et al., 2013</ref>) is applied on R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Building the Semantic Similarity Graph</head><p>Lexical semantic clue is captured by measuring se- mantic similarity between terms. The underlying motivation is that if we have known some product feature seeds, then terms that are more semanti- cally similar to these seeds are more likely to be product features. For example, if screen is known to be a product feature of mp3, and lcd is of high semantic similarity with screen, we can infer that lcd is also a product feature. Analogously, terms that are semantically similar to negative labeled seeds are not product features.</p><p>Word embedding naturally meets the demand above: words that are more semantically similar to each other are located closer in the embedding space <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>). Therefore, we can use cosine distance between two embedding vec- tors as the semantic distance measuring metric. Thus, our method does not rely on term frequency to rank candidates. This could potentially improve the ability of mining infrequent product features.</p><p>Formally, we create a semantic similarity graph G = (V, E, W ), where V = {V s ∪ V c } is the vertex set, which contains the labeled seed set V s and the unlabeled candidate set V c ; E is the edge set which connects every vertex pair (u, v), where u, v ∈ V ; W = {w uv : cos(EB u , EB v )} is a function which associates a weight to each edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Contextual Semantic Clue Using Convolutional Neural Network</head><p>The CNN is trained on each occurrence of seeds that is found in review texts. Then for a candidate term t, the CNN classifies all of its occurrences.</p><p>Since seed terms tend to have high frequency in review texts, only a few seeds will be enough to provide plenty of occurrences for the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">The architecture of the Convolutional Neural Network</head><p>The architecture of the Convolutional Neural Net- work is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For a product feature candidate t in sentence s, every consecutive sub- sequence q i of s that containing t with a window of length l is fed to the CNN. For example, as in <ref type="figure" target="#fig_1">Figure 2</ref>, if t = {screen}, and l = 3, there are three inputs:</p><formula xml:id="formula_4">q 1 = [the, ipod, screen], q 2 = [ipod, screen, is], q 3 = [screen, is, impressive].</formula><p>Partially, t is replaced by a token "*PF*" to re- move its lexicon influence 4 . To get the output score, q i is first converted into a concatenated vector x i = [e 1 ; e 2 ; ...; e l ], where e j is the word embedding of the j-th word. In this way, the CNN serves as a soft pattern miner: since words that have similar semantics have sim- ilar low-dimension embedding vectors, the CNN is less sensitive to lexicon change. The network is computed by,</p><formula xml:id="formula_5">y (1) i = tanh(W (1) x i + b (1) )<label>(5)</label></formula><p>y (2) = max(y</p><formula xml:id="formula_6">i )<label>(1)</label></formula><formula xml:id="formula_7">y (3) = W (3) y (2) + b (3)<label>(6)</label></formula><p>where y (i) is the output score of the i-th layer, and b (i) is the bias of the i-th layer; W (1) ∈ R h×(nl) and W (3) ∈ R 2×h are parameter matrixes, where n is the dimension of word embedding, and h is the size of nodes in the hidden layer.</p><p>In conventional neural models, the candidate term t is placed in the center of the window. How- ever, from Example 2, when l = 5, we can see that the best windows should be the bracketed texts (Because, intuitively, the windows should contain mp3, which is a strong evidence for finding the product feature), where t = {screen} is at the boundary. Therefore, we use Equ. 6 to formulate a max-convolutional layer, which is aimed to en- able the CNN to find more evidences in contexts than conventional neural models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Training</head><p>Let θ = {EB, W (·) , b (·) } denotes all the trainable parameters. The softmax function is used to con- vert the output score of the CNN to a probability,</p><formula xml:id="formula_9">p(t|X; θ) = exp(y (3) ) |C| j=1 exp(y (3) j )<label>(8)</label></formula><p>where X is the input set for term t, and C = {0, 1} is the label set representing product feature and non-product feature, respectively.</p><p>To train the CNN, we first use V s to collect each occurrence of the seeds in R to form a training set T s . Then, the training criterion is to minimize cross-entropy over T s ,</p><formula xml:id="formula_10">ˆ θ = argmin θ |Ts| i=1 − log δ i p(t i |X i ; θ)<label>(9)</label></formula><p>where δ i is the binomial target label distribution for one entry. Backpropagation algorithm with mini-batch stochastic gradient descent is used to solve this optimization problem. In addition, some useful tricks can be applied during the training. The weight matrixes W (·) are initialized by nor- malized initialization <ref type="bibr" target="#b5">(Glorot and Bengio, 2010)</ref>. W (1) is pre-trained by an autoencoder <ref type="bibr" target="#b6">(Hinton, 1989)</ref> to capture semantic compositionality. To speed up the learning, a momentum method is ap- plied (Sutskever et al., 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combining Lexical and Contextual Semantic Clues by Label Propagation</head><p>We propose a Label Propagation algorithm to combine both semantic clues in a unified process. Each term t ∈ V is assumed to have a label dis- tribution L t = (p + t , p − t ), where p + t denotes the probability of the candidate being a product fea- ture, and on the contrary, p − t = 1 − p + t . The clas- sified results of the CNN which encode contextual semantic clue serve as the prior knowledge,</p><formula xml:id="formula_11">I t =    (1, 0), if t ∈ V + s (0, 1), if t ∈ V − s (r + t , r − t ), if t ∈ V c<label>(10)</label></formula><p>where (r + t , r − t ) is estimated by,</p><formula xml:id="formula_12">r + t = count + (t) count + (t) + count − (t)<label>(11)</label></formula><p>where count + (t) is the number of occurrences of term t that are classified as positive by the CNN, and count − (t) represents the negative count.</p><p>Label Propagation is applied to propagate the prior knowledge distribution I to the product fea- ture distribution L via semantic similarity graph G, so that a product feature candidate is deter- mined by exploring its semantic relations to all of the seeds and other candidates globally. We pro- pose an adapted version on the random walking view of the Adsorption algorithm ( <ref type="bibr" target="#b0">Baluja et al., 2008</ref>) by updating the following formula until L converges,</p><formula xml:id="formula_13">L i+1 = (1 − α)M T L i + αDI (12)</formula><p>where M is the semantic transition matrix built from G; D = Diag[log tf (t)] is a diagonal ma- trix of log frequencies, which is designed to as- sign higher "confidence" scores to more frequent seeds; and α is a balancing parameter. Particu- larly, when α = 0, we can set the prior knowledge I without V c to L 0 so that only lexical semantic clue is used; otherwise if α = 1, only contextual semantic clue is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Bootstrapping Framework</head><p>We summarize the bootstrapping framework of the proposed method in Algorithm 1. During boot- strapping, the CNN is enhanced by Label Propaga- tion which finds more labeled examples for train- ing, and then the performance of Label Propaga- tion is also improved because the CNN outputs a more accurate prior distribution. After running for several iterations, the algorithm gets enough seeds, and a final Label Propagation is conducted to pro- duce the results.</p><p>Algorithm 1: Bootstrapping using semantic clues Input: The review corpus R, a large corpus C Output: The mined product feature list P Initialization: Train word embedding set EB first on C, and then on R</p><p>Step 1: Generate product feature seeds Vs (Section 3.1)</p><p>Step 2: Build semantic similarity graph G (Section 3.2)</p><p>while iter &lt; MAX ITER do</p><p>Step 3: Use Vs to collect occurrence set Ts from R for training</p><p>Step 4: Train a CNN N on Ts (Section 3.3) Apply mini-batch SGD on Equ. 9;</p><p>Step 5: Run Label Propagation (Section 3.4) Classify candidates using N to setup I;</p><formula xml:id="formula_14">L 0 ← I; repeat L i+1 ← (1 − α)M T L i + αDI; until ||L i+1 − L i || 2 &lt; ε;</formula><p>Step 6: Expand product feature seeds Move top T terms from Vc to Vs; iter++ end</p><p>Step  <ref type="bibr">6</ref> , where two review sets (Camera and Car) are selected. <ref type="bibr" target="#b25">Xu et al. (2013)</ref> had manually annotated product features on these four domains, so we directly employ their annota- tion as the gold standard. The detailed information can be found in their original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>For English corpora, the pre-processing are the same as that in ( <ref type="bibr" target="#b18">Qiu et al., 2009)</ref>, and for Chinese corpora, the Stanford Word Segmenter ( <ref type="bibr" target="#b1">Chang et al., 2008</ref>) is used to perform word segmenta- tion. We select three state-of-the-art syntax-based methods to be compared with our method: DP uses a bootstrapping algorithm named as Double Propagation ( <ref type="bibr" target="#b18">Qiu et al., 2009)</ref>, which is a conventional syntax-based method.</p><p>DP-HITS is an enhanced version of DP pro- posed by <ref type="bibr" target="#b26">Zhang et al. (2010)</ref>, which ranks product feature candidates by</p><formula xml:id="formula_15">s(t) = log tf (t) * importance(t)<label>(13)</label></formula><p>where importance(t) is estimated by the HITS al- gorithm <ref type="bibr" target="#b11">(Kleinberg, 1999)</ref>. SGW is the Sentiment Graph Walking algo- rithm proposed in ( <ref type="bibr" target="#b25">Xu et al., 2013)</ref>, which first extracts syntactic patterns and then uses random walking to rank candidates. Afterwards, word- syntactic pattern co-occurrence statistic is used as feature for a semi-supervised classifier TSVM <ref type="bibr" target="#b10">(Joachims, 1999</ref>) to further refine the results. This two-stage method is denoted as SGW-TSVM.</p><p>LEX only uses lexical semantic clue. Label Propagation is applied alone in a self-training manner. The dimension of word embedding n = 100, the convergence threshold ε = 10 −7 , and the number of expanded seeds T = 40. The size of the seed set N is 40. To output product features, it ranks candidates in descent order by using the positive score L + f (t). CONT only uses contextual semantic clue, which only contains the CNN. The window size l is 5. The CNN is trained with a mini-batch size of 50. The hidden layer size h = 250. Finally, importance(t) in Equ. 13 is replaced with r + t in Equ. 11 to rank candidates.</p><p>LEX&amp;CONT leverages both semantic clues.</p><formula xml:id="formula_16">Method MP3 Hotel Camera Car Avg. P R F P R F P R F P R F F DP</formula><p>0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66 DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68 SGW 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69 LEX 0.64 0.74 0.69 0.65 0.75 0.70 0.69 0.84 0.76 0.68 0.78 0.73 0.72 CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72 0.71 SGW-TSVM 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75 LEX&amp;CONT 0.74 0.75 0.74 0.75 0.77 0.76 0.80 0.84 0.82 0.79 0.79 0.79 0.78 <ref type="table">Table 1</ref>: Experimental results of product feature mining. The precision or recall of CONT is the average performance over five runs with different random initialization of parameters of the CNN. Avg. stands for the average score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Semantics-based Methods vs.</head><p>State-of-the-art Syntax-based Methods</p><p>The experimental results are shown in <ref type="table">Table 1</ref>, from which we have the following observations:</p><p>(i) Our method achieves the best performance among all of the compared methods. We also equally split the dataset into five sub- sets, and perform one-tailed t-test (p ≤ 0.05), which shows that the proposed semantics- based method (LEX&amp;CONT) significantly out- performs the three syntax-based strong com- petitors (DP, DP-HITS and SGW-TSVM).</p><p>(ii) LEX&amp;CONT which leverages both lexical and contextual semantic clues outperforms ap- proaches that only use one kind of semantic clue (LEX and CONT), showing that the com- bination of the semantic clues is helpful.</p><p>(iii) Our methods which use only one kind of semantic clue (LEX and CONT) outperform syntax-based methods (DP, DP-HITS and SGW). Comparing DP-HITS with LEX and CONT, the difference between them is that DP-HITS uses a syntax-pattern-based algo- rithm to estimate importance(t) in Equ. 13, while our methods use lexical or contextual se- mantic clue instead. We believe the reason that LEX or CONT is better is that syntactic pat- terns only use discrete and local information. In contrast, CONT exploits latent semantics of each word in context, and LEX takes advantage of word embedding, which is induced from global word co-occurrence statistic. Further- more, comparing SGW and LEX, both methods are base on random surfer model, but LEX gets better results than SGW. Therefore, the word- word semantic similarity relation used in LEX is more reliable than the word-syntactic pattern relation used in SGW.</p><p>(iv) LEX&amp;CONT achieves the highest recall among all of the evaluated methods. Since DP and DP-HITS rely on frequency for rank- ing product features, infrequent candidates are ranked low in their extracted list. As for SGW- TSVM, the features they used for the TSVM suffer from the data sparsity problem for in- frequent terms. In contrast, LEX&amp;CONT is frequency-independent to the review corpus. Further discussions on this observation are given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Results on Extracting Infrequent Product Features</head><p>We conservatively regard 30% product features with the highest frequencies in R as frequent fea- tures, so the remaining terms in the gold standard are infrequent features. In product feature mining task, frequent features are relatively easy to find. <ref type="table">Table 2</ref> shows the recall of all the four approaches for mining frequent product features. We can see that the performance are very close among differ- ent methods. Therefore, the recall mainly depends on mining the infrequent features.  <ref type="table">Table 2</ref>: The recall of frequent product features. <ref type="figure" target="#fig_4">Figure 3</ref> gives the recall of infrequent prod- uct features, where LEX&amp;CONT achieves the best performance. So our method is less influenced by term frequency. Furthermore, LEX gets better recall than CONT and all syntax-based methods, which indicates that lexical semantic clue does aid to mine more infrequent features as expected. .7</p><p>.8</p><p>.9</p><p>1.0</p><formula xml:id="formula_17">LEX&amp;CONT CONT LEX (d) Car</formula><p>Figure 4: Accuracy (y-axis) of product feature seed expansion at each bootstrapping iteration (x-axis). The error bar shows the standard deviation over five runs.   </p><formula xml:id="formula_18">Method MP3 Hotel Camera Car P R F P R F P R F P R F FW-5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Lexical Semantic Clue vs. Contextual Semantic Clue</head><p>This section studies the effects of lexical seman- tic clue and contextual semantic clue during seed expansion (Step 6 in Algorithm 1), which is con- trolled by α. When α = 1, we get the CONT; and if α is set 0, we get the LEX. To take into account the correctly expanded terms for both positive and negative seeds, we use Accuracy as the evaluation metric,</p><formula xml:id="formula_19">Accuracy = #T P + #T N # Extracted Seeds</formula><p>where T P denotes the true positive seeds, and T N denotes the true negative seeds. <ref type="figure">Figure 4</ref> shows the performance of seed ex- pansion during bootstrapping, in which the accu- racy is computed on 40 seeds (20 being positive and 20 being negative) expanded in each itera- tion. We can see that the accuracies of CONT and LEX&amp;CONT retain at a high level, which shows that they can find reliable new product feature seeds. However, the performance of LEX oscil- lates sharply and it is very low for some points, which indicates that using lexical semantic clue alone is infeasible. On another hand, comparing CONT with LEX in <ref type="table">Table 1</ref>, we can see that LEX performs generally better than CONT. Although LEX is not so accurate as CONT during seed ex- pansion, its final performance surpasses CONT. Consequently, we can draw conclusion that CONT is more suitable for the seed expansion, and LEX is more robust for the final result production.</p><p>To combine advantages of the two kinds of se- mantic clues, we set α = 0.7 in Step 5 of Algo- rithm 1, so that contextual semantic clue plays a key role to find new seeds accurately. For Step 7, we set α = 0.3. Thus, lexical semantic clue is emphasized for producing the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Effect of Convolutional Layer</head><p>Two non-convolutional variations of the proposed method are used to be compared with the convo- lutional method in CONT. FW-5 uses a traditional neural network with a fixed window size of 5 to replace the CNN in CONT, and the candidate term to be classified is placed in the center of the win- dow. Similarly, FW-9 uses a fixed window size of 9. Note that CONT uses a 5-term dynamic window containing the candidate term, so the ex- ploited number of words in the context is equiva- lent to FW-9. <ref type="table" target="#tab_4">Table 3</ref> shows the experimental results. We can see that the performance of FW-5 is much worse than CONT. The reason is that FW-5 only exploits half of the context as that of CONT, which is not sufficient enough. Meanwhile, although FW-9 ex- ploits equivalent range of context as that of CONT, it gets lower precisions. It is because FW-9 has approximately two times parameters in the param- eter matrix W (1) than that in Equ. 5 of CONT, which makes it more difficult to be trained with the same amount of data. Also, lengths of many sentences in the review corpora are shorter than 9. Therefore, the convolutional approach in CONT is the most effective way among these settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Parameter Study</head><p>We investigate two key parameters of the proposed method: the initial number of seeds N , and the size of the window l used by the CNN. <ref type="figure" target="#fig_5">Figure 5</ref> shows the performance under differ- ent N , where the F-Measure saturates when N equates to 40 and beyond. Hence, very few seeds are needed for starting our algorithm. Figure 6 shows F-Measure under different win- dow size l. We can see that the performance is improved little when l is larger than 5. Therefore, l = 5 is a proper window size for these datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper proposes a product feature mining method by leveraging contextual and lexical se- mantic clues. A semantic similarity graph is built to capture lexical semantic clue, and a convo- lutional neural network is used to encode con- textual semantic clue. Then, a Label Propaga- tion algorithm is applied to combine both seman- tic clues. Experimental results prove the effec- tiveness of the proposed method, which not only mines product features more accurately than con- ventional syntax-based method, but also extracts more infrequent product features.</p><p>In future work, we plan to extend the proposed method to jointly mine product features along with customers' opinions on them. The learnt seman- tic representations of words may also be utilized to predict fine-grained sentiment distributions over product features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This player has an :: fm ::::: tuner. (b) This mp3 supports :::: wma ::: file. (c) This review has helped ::::: people a lot. (d) This mp3 has some ::::: flaws.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of the Convolutional Neural Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The [screen of this mp3 is] great. (b) This [mp3 has a great screen].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The recall of infrequent features. The error bar shows the standard deviation over five different runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: F-Measure vs. N for the final results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: F-Measure vs. l for the final results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>7 :</head><label>7</label><figDesc></figDesc><table>Run Label Propagation for a final result L f 
Rank terms by L + 
f to get P , where L + 
f &gt; L − 
f ; 

4 Experiments 

4.1 Datasets and Evaluation Metrics 

Datasets: We select two real world datasets to 
evaluate the proposed method. The first one 
is a benchmark dataset in Wang et al. (2011), 
which contains English review sets on two do-
mains (MP3 and Hotel) 5 . The second dataset is 
proposed by Chinese Opinion Analysis Evalua-
tion 2008 (COAE 2008) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The results of convolutional method vs. the results of non-convolutional methods. 

MP3 
Hotel 
Camera 
Car 

Recall 

.4 

.5 

.6 

.7 

.8 

.9 

DP 
DP-HITS 
SGW-TSVM 
CONT 
LEX 
LEX&amp;CONT 

</table></figure>

			<note place="foot" n="1"> Google-n-Gram (http://books.google.com/ngrams) is used as the background corpus. 2 The df (t) part of the original DRM is slightly modified because we want a tf × idf-like scheme (Liu et al., 2012).</note>

			<note place="foot" n="3"> Wikipedia(http://www.wikipedia.org) is used in practice.</note>

			<note place="foot" n="4"> Otherwise, the CNN will quickly get overfitting on t, because very few seed lexicons are used for the training.</note>

			<note place="foot" n="5"> http://timan.cs.uiuc.edu/downloads.html 6 http://ir-china.org.cn/coae2008.html Evaluation Metrics: We evaluate the proposed method in terms of precision(P), recall(R) and Fmeasure(F). The English results are evaluated by exact string match. And for Chinese results, we use an overlap matching metric, because determining the exact boundaries is hard even for human (Wiebe et al., 2005).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video suggestion and discovery for youtube: Taking random walks through the view graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on World Wide Web, WWW &apos;08</title>
		<meeting>the 17th International Conference on World Wide Web, WWW &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing chinese word segmentation for machine translation performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation, StatMT &apos;08</title>
		<meeting>the Third Workshop on Statistical Machine Translation, StatMT &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating typed 344 dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE / ACL&apos;06 Workshop on Spoken Language Technology</title>
		<meeting>the IEEE / ACL&apos;06 Workshop on Spoken Language Technology</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist learning procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1C3</biblScope>
			<biblScope unit="page" from="185" to="234" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;04</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crctol: A semantic-based domain ontology learning system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="168" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Machine Learning</title>
		<meeting>the 16th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-domain co-extraction of sentiment and topic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Opinion target extraction using word-based translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1346" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opinion digger: An unsupervised opinion miner from unstructured product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM &apos;10</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management, CIKM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1825" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on artificial intelligence and statistics, AISTATS05</title>
		<meeting>the international workshop on artificial intelligence and statistics, AISTATS05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expanding domain sentiment lexicon through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI&apos;09</title>
		<meeting>the 21st international jont conference on Artifical intelligence, IJCAI&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1199" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30 th International Conference on Machine Learning</title>
		<meeting>the 30 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis without aspect keyword supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phrase dependency parsing for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lide</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1533" to="1541" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining opinion words and opinion targets in a two-stage framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1764" to="1773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Extracting and ranking product features in opinion documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk</forename><forename type="middle">Hwan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eamonn O&amp;apos;brien-Strain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1462" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-aspect opinion polling from textual reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1799" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management, CIKM &apos;06</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management, CIKM &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
