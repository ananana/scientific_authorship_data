<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifelong Learning for Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianzu</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lifelong Learning for Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="750" to="756"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochas-tic gradient descent. Our experimental results show that the proposed method out-performs baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification is the task of classifying an opinion document as expressing a positive or negative sentiment. <ref type="bibr" target="#b17">Liu (2012)</ref> and <ref type="bibr" target="#b19">Pang and Lee (2008)</ref> provided good surveys of the existing re- search. In this paper, we tackle sentiment clas- sification from a novel angle, lifelong learning (LL), or lifelong machine learning. This learn- ing paradigm aims to learn as humans do: re- taining the learned knowledge from the past and use the knowledge to help future learning <ref type="bibr" target="#b27">(Thrun, 1998</ref><ref type="bibr" target="#b8">, Chen and Liu, 2014b</ref><ref type="bibr" target="#b25">, Silver et al., 2013</ref>.</p><p>Although many machine learning topics and techniques are related to LL, e.g., lifelong learn- ing <ref type="bibr" target="#b27">(Thrun, 1998</ref><ref type="bibr" target="#b8">, Chen and Liu, 2014b</ref><ref type="bibr" target="#b25">, Silver et al., 2013</ref>, transfer learning <ref type="bibr">(Jiang, 2008, Pan and</ref><ref type="bibr" target="#b18">Yang, 2010)</ref>, multi-task learning <ref type="bibr" target="#b6">(Caruana, 1997)</ref>, never-ending learning <ref type="bibr" target="#b5">(Carlson et al., 2010)</ref>, self- taught learning ( <ref type="bibr" target="#b21">Raina et al., 2007)</ref>, and online learning <ref type="bibr" target="#b4">(Bottou, 1998)</ref>, there is still no unified definition for LL.</p><p>Based on the prior work and our research, to build an LL system, we believe that we need to answer the following key questions:</p><p>1. What information should be retained from the past learning tasks? 2. What forms of knowledge will be used to help future learning? 3. How does the system obtain the knowledge? 4. How does the system use the knowledge to help future learning? Motivated by these questions, we present the following definition of lifelong learning (LL).</p><p>Definition (Lifelong Learning): A learner has performed learning on a sequence of tasks, from 1 to N − 1. When faced with the N th task, it uses the knowledge gained in the past N − 1 tasks to help learning for the N th task. An LL system thus needs the following four general components: 1. Past Information Store (PIS): It stores the in- formation resulted from the past learning. This may involve sub-stores for information such as (1) the original data used in each past task, (2) intermediate results from the learning of each past task, and (3) the final model or patterns learned from the past task, respectively. 2. Knowledge Base (KB): It stores the knowledge mined or consolidated from PIS (Past Informa- tion Store). This requires a knowledge repre- sentation scheme suitable for the application. 3. Knowledge Miner (KM). It mines knowledge from PIS (Past Information Store). This min- ing can be regarded as a meta-learning process because it learns knowledge from information resulted from learning of the past tasks. The knowledge is stored to KB (Knowledge Base). 4. Knowledge-Based Learner (KBL): Given the knowledge in KB, this learner is able to lever- age the knowledge and/or some information in PIS for the new task.</p><p>Based on this, we can define lifelong sentiment classification (LSC):</p><formula xml:id="formula_0">Definition (Lifelong Sentiment Classification):</formula><p>A learner has performed a sequence of supervised sentiment classification tasks, from 1 to N − 1, where each task consists of a set of training doc- uments with positive and negative polarity labels. Given the N th task, it uses the knowledge gained in the past N − 1 tasks to learn a better classifier for the N th task.</p><p>It is useful to note that although many re- searchers have used transfer learning for super- vised sentiment classification, LL is different from the classic transfer learning or domain adapta- tion <ref type="bibr" target="#b18">(Pan and Yang, 2010)</ref>. Transfer learning typi- cally uses labeled training data from one (or more) source domain(s) to help learning in the target do- main that has little or no labeled data <ref type="bibr" target="#b1">(Aue and</ref><ref type="bibr">Gamon, 2005, Bollegala et al., 2011</ref>). It does not use the results of the past learning or knowledge mined from the results of the past learning. Fur- ther, transfer learning is usually inferior to tradi- tional supervised learning when the target domain already has good training data. In contrast, our target (or future) domain/task has good training data and we aim to further improve the learning using both the target domain training data and the knowledge gained in past learning. To be consis- tent with prior research, we treat the classification of one domain as one learning task.</p><p>One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled training data. The key reason is that the training data may not be fully representative of the test data due to the sample selection bias <ref type="bibr" target="#b11">(Heckman, 1979</ref><ref type="bibr" target="#b24">, Shimodaira, 2000</ref><ref type="bibr" target="#b31">, Zadrozny, 2004</ref>). In few real-life applications, the training data are fully represen- tative of the test data. For example, in a senti- ment classification application, the test data may contain some sentiment words that are absent in the training data of the target domain, while these sentiment words have appeared in some past do- mains. So the past domain knowledge can provide the prior polarity information in this situation.</p><p>Like most existing sentiment classification pa- pers ( <ref type="bibr" target="#b17">Liu, 2012)</ref>, this paper focuses on binary clas- sification, i.e., positive (+) and negative (−) polar- ities. But the proposed method is also applicable to multi-class classification. To embed and use the knowledge in building the target domain classifier, we propose a novel optimization method based on the Na¨ıveNa¨ıve Bayesian (NB) framework and stochas- tic gradient descent. The knowledge is incorpo- rated using penalty terms in the optimization for- mulation. This paper makes three contributions: 1. It proposes a novel lifelong learning approach to sentiment classification, called lifelong sen- timent classification (LSC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">It proposes an optimization method that uses</head><p>penalty terms to embed the knowledge gained in the past and to deal with domain dependent sentiment words to build a better classifier. 3. It creates a large corpus containing reviews from 20 diverse product domains for extensive evaluation. The experimental results demon- strate the superiority of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is mainly related to lifelong learning and multi-task learning <ref type="bibr" target="#b27">(Thrun, 1998</ref><ref type="bibr" target="#b6">, Caruana, 1997</ref><ref type="bibr" target="#b8">, Chen and Liu, 2014b</ref><ref type="bibr" target="#b25">, Silver et al., 2013</ref>. Existing lifelong learning approaches focused on exploiting invariances <ref type="bibr" target="#b27">(Thrun, 1998</ref>) and other types of knowledge (Chen and Liu, 2014b, Chen and Liu, 2014a, Ruvolo and Eaton, 2013) across multiple tasks. Multi-task learning optimizes the learning of multiple related tasks at the same time <ref type="bibr" target="#b6">(Caruana, 1997</ref><ref type="bibr" target="#b9">, Chen et al., 2011</ref><ref type="bibr" target="#b23">, Saha et al., 2011</ref><ref type="bibr" target="#b32">, Zhang et al., 2008</ref>. However, these meth- ods are not for sentiment analysis. Also, our na¨ıvena¨ıve Bayesian optimization based LL method is quite different from all these existing techniques.</p><p>Our work is also related to transfer learning or domain adaptation <ref type="bibr" target="#b18">(Pan and Yang, 2010)</ref>. In the sentiment classification context, Aue and Gamon (2005) trained sentiment classifiers for the target domain using various mixes of labeled and un- labeled reviews. <ref type="bibr" target="#b2">Blitzer et al. (2007)</ref> proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. <ref type="bibr" target="#b14">Li and Zong (2008)</ref>   <ref type="bibr" target="#b17">Liu, 2012)</ref>. However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new learning in a lifelong manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed LSC Technique</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Na¨ıveNa¨ıve Bayesian Text Classification</head><p>Before presenting the proposed method, we briefly review the Na¨ıveNa¨ıve Bayesian (NB) text classification as our method uses it as the foundation.</p><p>NB text classification <ref type="bibr">(McCallum and Nigam, 1998</ref>) basically computes the conditional proba- bility of each word w given each class c j (i.e., P (w|c j )) and the prior probability of each class c j (i.e., P (c j )), which are used to calculate the posterior probability of each class c j given a test document d (i.e., P (c j |d)). c j is either positive (+) or negative (−) in our case.</p><p>The key parameter P (w|c j ) is computed as:</p><formula xml:id="formula_1">P (w|c j ) = λ + N c j ,w λ |V | + |V | v=1 N c j ,v<label>(1)</label></formula><p>where N c j ,w is the frequency of word w in docu- ments of class c j . |V | is the size of vocabulary V and λ (0 ≤ λ ≤ 1) is used for smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Components in LSC</head><p>This subsection describes our proposed method corresponding to the proposed LL components.</p><p>1. Past Information Store (PIS): In this work, we do not store the original data used in the past learning tasks, but only their results. For each past learning taskˆttaskˆ taskˆt, we store a) P ˆ t (w|+) and P ˆ t (w|−) for each word w which are from taskˆt taskˆ taskˆt's NB classifier (see Eq 1); and b) the number of times that w appears in a positive (+) doc- ument N ˆ incorporates knowledge using regularization as penalty terms in our optimization. See the de- tails in 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Objective Function</head><p>In this subsection, we introduce the objective func- tion used in our method. The key parameters that affect NB classification results are P (w|c j ) which are computed using empirical counts of word w with class c j , i.e., N c j ,w (Eq. 1). In binary classifi- cation, they are N +,w and N −,w . This suggests that we can revise these counts appropriately to improve classification. In our optimization, we denote the optimized variables X +,w and X −,w as the number of times that a word w appears in the positive and negative class. We called them virtual counts to distinguish them from empirical counts N +,w and N −,w . For correct classification, ideally, we should have the posterior probability P (c j |d i ) = 1 for labeled class c j , and for the other class c f , we should have P (c f |d i ) = 0. Formally, given a new domain training data D t , our objective function is:</p><formula xml:id="formula_2">|D t | i=1 (P (c j |d i ) − P (c f |d i ))<label>(2)</label></formula><p>Here c j is the actual labeled class of d i ∈ D t . In this paper, we use stochastic gradient descent (SGD) to optimize on the classification of each document d i ∈ D t . Due to the space limit, we only show the optimization process for a positive document (the process for a negative document is similar). The objective function under SGD for a positive document is:</p><formula xml:id="formula_3">F +,i = P (+|d i ) − P (−|d i )<label>(3)</label></formula><p>To further save space, we omit the derivation steps and give the final derivatives below (See the detailed derivation steps in the separate supple- mentary note):  are served as a reasonable starting point for SGD, where N t +,w and N t −,w are the empirical counts of word w and classes + and − from domain D t , and N KB +,w and N KB −,w are from knowledge KB (Sec- tion 3.2). The SGD runs iteratively using the fol- lowing rules for the positive document d i until convergence, i.e., when the difference of Eq. 2 for two consecutive iterations is less than 1e−3 (same for the negative document), where γ is the learning rate:</p><formula xml:id="formula_4">g (X) = λ |V | + |V | v=1 X+,v λ |V | + |V | v=1 X−,v |d i |<label>(4</label></formula><formula xml:id="formula_5">n w,d i × g(X) − n u,d i λ + X+,u<label>(5)</label></formula><formula xml:id="formula_6">∂F+,i ∂X−,u = n u,d i λ+X −,u × g(X) + ∂g ∂X −,u P (+) P (−) w∈d i λ+X +,w λ+X −,w n w,d i + g(X)<label>(6)</label></formula><formula xml:id="formula_7">X l +,u = X l−1 +,u −γ ∂F +,i ∂X +,u , X l −,u = X l−1 −,u −γ ∂F +,i ∂X −,u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Exploiting Knowledge via Penalty Terms</head><p>The above optimization is able to update the vir- tual counts for a better classification in the target domain. However, it does not deal with the issue of domain dependent sentiment words, i.e., some words may change the polarity across different do- mains. Nor does it utilize the domain-level knowl- edge in the knowledge base KB (Section 3.2). We thus propose to add penalty terms into the opti- mization to accomplish these. The intuition here is that if a word w can dis- tinguish classes very well from the target domain training data, we should rely more on the target domain training data in computing counts related to w. So we define a set of words V T that consists of distinguishable target domain dependent words. A word w belongs to V T if P (w|+) is much larger or much smaller than P (w|−) in the target do- main, i.e., P (w|+) P (w|−) ≥ σ or P (w|−) P (w|+) ≥ σ, where σ is a parameter. Such words are already effective in classification for the target domain, so the vir- tual counts in optimization should follow the em- pirical counts (N t +,w and N t −,w ) in the target do- main, which are reflected in the L2 regularization penalty term below (α is the regularization coeffi- cient):</p><formula xml:id="formula_8">1 2 α w∈V T X+,w − N t +,w 2 + X−,w − N t −,w 2<label>(7)</label></formula><p>To leverage domain-level knowledge (the sec- ond type of knowledge in KB in Section 3.2), we want to utilize only those reliable parts of knowl- edge. The rationale here is that if a word only appears in one or two past domains, the knowl- edge associated with it is probably not reliable or it is highly specific to those domains. Based on it, we use domain frequency to define the relia- bility of the domain-level knowledge. For w, if M KB +,w ≥ τ or M KB −,w ≥ τ (τ is a parameter), we regard it as appearing in a reasonable number of domains, making its knowledge reliable. We de- note the set of such words as V S . Then we add the second penalty term as follows:</p><formula xml:id="formula_9">1 2 α w∈V S X +,w − R w × X 0 +,w 2 + 1 2 α w∈V S X −,w − (1 − R w ) × X 0 −,w 2 (8)</formula><p>where the ratio R w is defined as M KB +,w /(M KB +,w + M KB −,w ). X 0 +,w and X 0 −,w are the starting points for SGD (Section 3.3). Finally, we revise the partial derivatives in Eqs. 4-6 by adding the correspond- ing partial derivatives of Eqs. 7 and 8 to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We created a large corpus contain- ing reviews from 20 types of diverse products or domains crawled from Amazon.com (i.e., 20 datasets). The names of product domains are listed in <ref type="table" target="#tab_2">Table 1</ref>. Each domain contains 1,000 re- views. Following the existing work of other re- searchers ( <ref type="bibr" target="#b2">Blitzer et al., 2007</ref><ref type="bibr" target="#b20">, Pang et al., 2002</ref>), we treat reviews with rating &gt; 3 as positive and reviews with rating &lt; 3 as negative. The datasets are publically available at the authors websites.</p><p>Natural class distribution: We keep the natural (or skewed) distribution of the positive and nega- tive reviews to experiment with the real-life situa- tion. F1-score is used due to the imbalance.   Balanced class distribution: We also created a balance dataset with 200 reviews (100 positive and 100 negative) in each domain dataset. This set is smaller because of the small number of negative reviews in each domain. Accuracy is used for eval- uation in this balanced setting.</p><formula xml:id="formula_10">NB-T NB-S NB-ST SVM-T SVM-S SVM-ST CLF</formula><p>We used unigram features with no feature se- lection in classification. We followed ( <ref type="bibr" target="#b20">Pang et al., 2002</ref>) to deal with negation words. For evalua- tion, each domain is treated as the target domain with the rest 19 domains as the past domains. All the models are evaluated using 5-fold cross vali- dation.</p><p>Baselines. We compare our proposed LSC model with Na¨ıveNa¨ıve Bayes (NB), SVM 1 , and CLF ( <ref type="bibr" target="#b14">Li and Zong, 2008)</ref>. Note that NB and SVM can only work on a single domain data. To have a comprehensive comparison, they are fed with three types of training data: a) labeled training data from the target domain only, denoted by NB-T and SVM-T; b) labeled training data from all past source do- mains only, denoted by NB-S and SVM-S; c) merged (labeled) training data from all past do- mains and the target domain, referred to as NB- ST and SVM-ST. For LSC, we empirically set σ = 6 and τ = 6. The learning rate λ and regularization coefficient α are set to 0.1 empirically. λ is set to 1 for (Laplace) smoothing. <ref type="table" target="#tab_4">Table 2</ref> shows the average F1-scores for the negative class in the natural class distribution, and <ref type="table" target="#tab_5">Table 3</ref> shows the average accuracies in the bal- anced class distribution. We can clearly see that our proposed model LSC achieves the best perfor- mance in both cases. In general, NB-S (and SVM- S) are worse than NB-T (and SVM-T), both of which are worse than NB-ST (and SVM-ST). This shows that simply merging both past domains and the target domain data is slightly beneficial. Note that the average F1-score for the positive class is not shown as all classifiers perform very well be- cause the positive class is the majority class (while our model performs slightly better than the base- lines). The improvements of the proposed LSC model over all baselines in both cases are statisti- cally significant using paired t-test (p &lt; 0.01 com- pared to NB-ST and CLF, p &lt; 0.0001 compared to the others). In the balanced class setting (Ta- ble 3), CLF performs better than NB-T and SVM- T, which is consistent with the results in ( <ref type="bibr" target="#b14">Li and Zong, 2008)</ref>. However, it is still worse than our LSC model. Effects of #Past Domains. <ref type="figure" target="#fig_1">Figure 1</ref> shows the effects of our model using different number of past domains. We clearly see that LSC performs bet- ter with more past domains, showing it indeed has the ability to accumulate knowledge and use the knowledge to build better classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a lifelong learning ap- proach to sentiment classification using optimiza- tion, which is based on stochastic gradient de- scent in the framework of Bayesian probabilities. Penalty terms are introduced to effectively exploit the knowledge gained from past learning. Our experimental results using 20 diverse product re- view domains demonstrate the effectiveness of the method. We believe that lifelong learning is a promising direction for building better classifiers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>t</head><label></label><figDesc>+,w and the number of times that w appears in a negative documents N ˆ t −,w . 2. Knowledge Base (KB): Our knowledge base contains two types of knowledge: (a) Document-level knowledge N KB +,w (and N KB −,w ): number of occurrences of w in the documents of the positive (and nega- tive) class in the past tasks, i.e., N KB +,w = ˆ t N ˆ t +,w and N KB −,w = ˆ t N ˆ t −,w . (b) Domain-level knowledge M KB +,w (and M KB −,w ): number of past tasks in which P (w|+) &gt; P (w|−) (and P (w|+) &lt; P (w|−)). 3. Knowledge Miner (KM). Knowledge miner is straightforward as it just performs counting and aggregation of information in PIS to generate knowledge (see 2(a) and 2(b) above). 4. Knowledge-Based Learner (KBL): This learner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1Figure 1 :</head><label>1</label><figDesc>Figure 1: (Left): Negative class F1-score of LSC with #past domains in natural class distribution. (Right): Accuracy of LSC with #past domains in balanced class distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Names of the 20 product domains and the proportion of negative reviews in each domain.</head><label>1</label><figDesc></figDesc><table>Alarm Clock 

30.51 
Flashlight 
11.69 
Home Theater System 
28.84 
Projector 
20.24 
Baby 
16.45 
GPS 
19.50 
Jewelry 
12.21 
Rice Cooker 
18.64 
Bag 
11.97 
Gloves 
13.76 
Keyboard 
22.66 
Sandal 
12.11 
Cable Modem 
12.53 
Graphics Card 
14.58 
Magazine Subscriptions 
26.88 
Vacuum 
22.07 
Dumbbell 
16.04 
Headphone 
20.99 
Movies TV 
10.86 
Video Games 
20.93 

where n u,d i is the term frequency of word u in 
document d i . X denotes all the variables consist-
ing of X +,w and X −,w for each word w. The par-
tial derivatives for a word u, i.e., ∂g 

∂X +,u 

and ∂g 

∂X −,u 

, 
are quite straightforward and thus not shown here. 
X 0 
+,w = N t 
+,w +N KB 
+,w and X 0 
−,w = N t 
−,w +N KB 

−,w 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : Natural class distribution: Average F1-score of the negative class over 20 domains. Negative class is the minority class and thus harder to classify.</head><label>2</label><figDesc></figDesc><table>NB-T 
NB-S 
NB-ST 
SVM-T 
SVM-S 
SVM-ST 
CLF 
LSC 
80.15 
77.35 
80.85 
78.45 
78.20 
79.40 
80.49 
83.34 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Balanced class distribution: Average accuracy over 20 domains for each system. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Andreevskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="290" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Customizing Sentiment Classifiers to New Domains: A Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<editor>RANLP</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL HLT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Online algorithms and stochastic approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online Learning and Neural Networks</title>
		<editor>David Saad</editor>
		<meeting><address><addrLine>Cambridge, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an Architecture for Never-Ending Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1306" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multitask Learning. Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining Topics in Documents : Standing on the Shoulders of Big Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1116" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrating low-rank and group-sparse structures for robust multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="42" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the econometric society</title>
		<imprint>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A literature survey on domain adaptation of statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using morphological and syntactic structures for Chinese opinion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1260" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multidomain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL HLT</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="257" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-domain Co-extraction of Sentiment and Topic Lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active learning for crossdomain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxia</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2127" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Survey on Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thumbs up? Sentiment Classification using Machine Learning Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-taught Learning : Transfer Learning from Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ELLA: An efficient lifelong learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Eaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online learning of multiple tasks and their relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="643" to="651" />
		</imprint>
	</monogr>
	<note>Suresh Venkatasubramanian, and Hal Daume</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the loglikelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lifelong Machine Learning Systems: Beyond Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Lifelong Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel scheme for domain-transfer problem in the context of sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaowei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="979" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lifelong Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning To Learn</title>
		<editor>S Thrun and L Pratt</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Ranking for Sentiment Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACLIJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="317" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A POS-based Ensemble Model for Cross-domain Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transfer Learning for Multiple-Domain Sentiment Analysis-Identifying Domain Dependent/Independent Word Polarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1286" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flexible latent variable models for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="221" to="242" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
