<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Don&apos;t understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Haponchyk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
						</author>
						<title level="a" type="main">Don&apos;t understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1018" to="1028"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1094</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An assential aspect of structured prediction is the evaluation of an output structure against the gold standard. Especially in the loss-augmented setting, the need of finding the max-violating constraint has severely limited the expressivity of effective loss functions. In this paper, we trade off exact computation for enabling the use of more complex loss functions for coreference resolution (CR). Most noteworthily , we show that such functions can be (i) automatically learned also from controversial but commonly accepted CR measures, e.g., MELA, and (ii) successfully used in learning algorithms. The accurate model comparison on the standard CoNLL-2012 setting shows the benefit of more expressive loss for Arabic and En-glish data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, interesting structured predic- tion methods have been developed for coref- erence resolution (CR), e.g., <ref type="bibr" target="#b8">(Fernandes et al., 2014;</ref><ref type="bibr" target="#b1">Björkelund and Kuhn, 2014;</ref><ref type="bibr" target="#b13">Martschat and Strube, 2015</ref>). These models are supposed to out- put clusters but, to better control the exponential nature of the problem, the clusters are converted into tree structures. Although this simplifies the problem, optimal solutions are associated with an exponential set of trees, requiring to maximize over such a set. This originated latent models ( <ref type="bibr" target="#b22">Yu and Joachims, 2009</ref>) optimizing the so-called loss- augmented objective functions.</p><p>In this setting, loss functions need to be factor- izable together with the feature representations for finding the max-violating constraints. The conse- quence is that only simple loss functions, basically just counting incorrect edges, were applied in pre- vious work, giving up expressivity for simplicity. This is a critical limitation as domain experts con- sider more information than just counting edges.</p><p>In this paper, we study the use of more ex- pressive loss functions in the structured predic- tion framework for CR, although some findings are clearly applicable to more general settings. We attempted to optimize the complicated offi- cial MELA measure 1 ( <ref type="bibr" target="#b15">Pradhan et al., 2012)</ref> of CR within the learning algorithm. Unfortunately, MELA is the average of measures, among which CEAF e has an excessive computational complex- ity preventing its direct use. To solve this prob- lem, we defined a model for learning MELA from data using a fast linear regressor, which can be then effectively used in structured prediction al- gorithms. We defined features to learn such a loss function, e.g., different link counts or aggregations such as Precision and Recall. Moreover, we de- signed methods for generating training data from which our regression loss algorithm (RL) can gen- eralize well and accurately predict MELA values on unseen data.</p><p>Since RL is not factorizable 2 over a mention graph, we designed a latent structured percep- tron (LSP) that can optimize non-factorizable loss functions on CR graphs. We tested LSP using RL and other traditional loss functions using the same setting of the CoNLL-2012 Shared Task, thus en- abling an exact comparison with previous work. The results confirmed that RL can be effectively learned and used in LSP, although the improve- ment was smaller than expected, considering that our RL provides the algorithm with a more accu- rate feedback.</p><p>Thus, we analyzed the theory behind this pro-cess by also contributing to the definition of the properties of loss optimality. These show that the available loss functions, e.g., by Fernandes et al.; Yu and Joachims, are enough for optimizing MELA on the training set, at least when the data is separable. Thus, in such conditions, we cannot expect a very large improvement from RL.</p><p>To confirm such a conjecture, we tested the models in a more difficult setting, in terms of sepa- rability. We used different feature sets of a smaller size and found out that in such conditions, RL re- quires less epochs for converging and produces better results than the other simpler loss functions. The accuracy of RL-based model, using 16 times less features, decreases by just 0.3 points, still im- proving the state of the art in structured predic- tion. Accordingly, in the Arabic setting, where the available features are less discriminative, our ap- proach highly improves the standard LSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a number of works attempting to di- rectly optimize coreference metrics. The solu- tion proposed by <ref type="bibr" target="#b23">Zhao and Ng (2010)</ref> consists in finding an optimal weighting (by beam search) of training instances, which would maximize the tar- get coreference metric. Their models, optimiz- ing MUC and B 3 , deliver a significant improve- ment on the MUC and ACE corpora. <ref type="bibr" target="#b19">Uryupina et al. (2011)</ref> benefited from applying genetic algo- rithms for the selection of features and architecture configuration by multi-objective optimization of MUC and the two CEAF variants. Our approach is different in that the evaluation measure (its ap- proximation) is injected directly into the learning algorithm. <ref type="bibr" target="#b4">Clark and Manning (2016)</ref> optimize B 3 directly as well within a mention-ranking model. For the efficiency reasons, they omit optimization of CEAF, which we enable in this work.</p><p>SVM cluster -a structured output approach by <ref type="bibr" target="#b9">Finley and Joachims (2005)</ref> -enables optimiza- tion to any clustering loss function (including non- decomposable ones). The authors experimentally show that optimizing particular loss functions re- sults into a better classification accuracy in terms of the same functions. However, these are in gen- eral fast to compute, which is not the MELA case.</p><p>While Finley and Joachims are compelled to perform approximate inference to overcome the intractability of finding an optimal clustering, the latent variable structural approaches -SVM of <ref type="bibr" target="#b22">Yu and Joachims (2009)</ref> and perceptron of Fernan-  <ref type="bibr" target="#b1">Björkelund and Kuhn (2014)</ref>, <ref type="bibr" target="#b13">Martschat and Strube (2015)</ref>, and <ref type="bibr" target="#b11">Lassalle and Denis (2015)</ref>. Like us, the first couples such approach with ap- proximate inference but for enabling the use of non-local features. The current state-of-the-art model of <ref type="bibr" target="#b21">Wiseman et al. (2016)</ref> also employs a greedy inference procedure as it has global fea- tures from an RNN as a non-decomposable term in the inference objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structure Output Learning for CR</head><p>We consider online learning algorithms for link- ing structured input and output patterns. More formally, such algorithms find a linear mapping f (x, y) = w, Φ(x, y), where f : X × Y → R, w is a linear model, Φ(x, y) is a combined fea- ture vector of input variables X and output vari- ables Y . The predicted structure is derived with the argmax y∈Y f (x, y). In the next sections, we show how to learn w for CR using structured percep- tron. Additionally, we provide a characterization of effective loss functions for separable cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling CR</head><p>In this framework, CR is essentially modeled as a clustering problem, where an input-output exam- ple is described by a tuple (x, y, h), x is a set of entity mentions contained in a text document, y is set of the corresponding mention clusters, and h is a latent variable, i.e., an auxiliary structure that can represent the clusters of y. For example, given the following text:</p><p>Although (she) m 1 was supported by (President Obama) m 2 , (Mrs. Clinton) m 3 missed (her) m 4 (chance) m 5 , (which) m 6 looked very good before counting votes. the clusters of the entity mentions are represented by the latent tree in <ref type="figure" target="#fig_0">Figure 1</ref>, where its nodes are</p><formula xml:id="formula_0">Algorithm 1 Latent Structured Perceptron 1: Input: X = {(xi, yi)} n i=1</formula><p>, w0, C, T 2: w ← w0; t ← 0 3: repeat 4:</p><p>for i = 1, ..., n do 5:</p><formula xml:id="formula_1">h * i ← argmax h∈H(x i ,y i ) wt, Φ(xi, h) 6: ˆ hi ← argmax h∈H(x i ) wt, Φ(xi, h)+C ×∆(yi, h * i , h) 7: if ∆(yi, h * i, ˆ hi) &gt; 0 then 8: wt+1 ← wt + Φ(xi, h * i ) − Φ(xi, ˆ hi) 9:</formula><p>end if 10:</p><p>end for 11:</p><p>t ← t + 1 12: until t &lt; nT</p><formula xml:id="formula_2">13: w ← 1 t t i=1</formula><p>wi return w mentions and the subtrees connected to the addi- tional root node form distinct clusters. The tree h is called a latent variable as it is consistent with y, i.e., it contains only links between mention nodes that corefer or fall into the same cluster according to y. Clearly, an exponential set of trees, H, can be associated with one and the same clustering y. Using only one tree to represent a clustering makes the search for optimal mention clusters tractable. In particular, structured prediction algorithms se- lect h that maximizes the model learned at time t as shown in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Latent Structured Perceptron (LSP)</head><p>The LSP model proposed by <ref type="bibr" target="#b16">Sun et al. (2009)</ref> and specialized for solving CR tasks by <ref type="bibr" target="#b7">Fernandes et al. (2012)</ref> is described by Alg. 1.</p><p>Given a training set {(x i , y i )} n i=1 , initial w 0 3 , a trade off parameter C, and the maximum num- ber of epochs T , LSP iterates the following opera- tions: Line 5 finds a latent tree h * i that maximizes w t , Φ(x i , h) for the current example (x i , y i ). It basically finds the max ground truth tree with re- spect to the current w t . Finding such max re- quires an exploration over the tree set H(x i , y i ), which only contains arcs between mentions that corefer according to the gold standard clustering y i . Line 6 seeks for the max-violating treê h i in H(x i ), which is the set of all candidate trees using any possible combination of arcs. Line 7 tests if the produced treê h i has some mistakes with re- spect to the gold clustering y i , using loss function</p><formula xml:id="formula_3">∆(y i , h * i , ˆ h i ).</formula><p>Note that some models define a loss exploiting also the current best latent tree h * i . If the test is verified, the model is updated with the vector Φ(  <ref type="bibr">et al. (2012)</ref> used exactly the di- rected trees we showed as latent structures and applied Edmonds' spanning tree algorithm <ref type="bibr" target="#b6">(Edmonds, 1967</ref>) for finding the max. Their model achieved the best results in the CoNLL-2012 Shared Task, a challenge for CR systems <ref type="bibr" target="#b15">(Pradhan et al., 2012)</ref>. Their selected loss function also plays an important role as shown in the following.</p><formula xml:id="formula_4">x i , h * i ) − Φ(x i , ˆ h i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss functions</head><p>When defining a loss, it is very important to pre- serve the factorization of the model components along the latent tree edges since this leads to effi- cient maximization algorithms (see Section 5).</p><p>Fernandes et al. uses a loss function that (i) compares a predicted treê h against the gold tree h * and (ii) factorizes over the edges in the way the model does. Its equation is:</p><formula xml:id="formula_5">∆ F (h * , ˆ h) = M i=1 1 ˆ h(i) =h * (i) (1 +r ·1 h * (i)=0 ), (1)</formula><p>where h * (i) andˆhandˆ andˆh(i) output the parent of the men- tion node i in the gold and predicted tree, respec- tively, whereas 1 h * (i) = ˆ h(i) just checks if the par- ents are different, and if yes, penalty of 1 (or 1 + r if the gold parent is the root) is added.</p><p>Yu and Joachims's loss is based on undirected tree without a root and on the gold clustering y. It is computed as:</p><formula xml:id="formula_6">∆ Y J (y, ˆ h) = n(y) − k(y) + e∈ˆhe∈ˆ e∈ˆh l(y, e), (2)</formula><p>where n(y) is the number of graph nodes, k(y) is the number of clusters in y, and l(y, e) assigns −1 to any edge e that connects nodes from the same cluster in y, and r otherwise.</p><p>In our experiments, we adopt both loss func- tions, however, in contrast to Fernandes et al., we always measure ∆ F against the gold label y and not against the current h * , i.e., in the way it is done by <ref type="bibr" target="#b13">Martschat and Strube (2015)</ref>, who employ an equivalent LSP model in their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">On optimality of simple loss functions</head><p>The above loss functions are rather simple and mainly based on counting the number of mistaken edges. Below, we show that such simple loss func- tions achieve training data separation (if it exists) of a general task measure reaching its max on their 0 mistakes. The latter is a desirable characteristic of many measures used in CR and NLP research. Proposition 1 (Sufficient condition for optimal- ity of loss functions for learning graphs). Let ∆(y, h * , ˆ h) ≥ 0 be a simple, edge-factorizable loss function, which is also monotone in the num- ber of edge errors, and let µ(y, ˆ h) be any graph- based measure maximized by no edge errors. Then, if the training set is linearly separable LSP optimizing ∆ converges to the µ optimum.</p><p>Proof. If the data is linearly separable the percep-</p><formula xml:id="formula_7">tron converges ⇒ ∆(y i , h * i , ˆ h i ) = 0, ∀x i . The loss is factorizable, i.e., ∆(y i , h * i , ˆ h i ) = e∈ˆhe∈ˆ e∈ˆh i l(y i , h * i , e),<label>(3)</label></formula><p>where l(·) is an edge loss function. Thus,</p><formula xml:id="formula_8">e∈ˆhe∈ˆ e∈ˆh i l(y i , h * i , e) = 0.</formula><p>The latter equation and</p><formula xml:id="formula_9">monotonicity imply l(y i , h * i , e) = 0, ∀e ∈ ˆ h i , i</formula><p>.e., there are no edge mistakes, otherwise by fix- ing such edges, we would have a smaller ∆, i.e., negative, contradicting the initial positiveness hy- pothesis. Thus, no edge mistake in any x i implies that µ(y, ˆ h) is maximized on the training set.</p><formula xml:id="formula_10">Corollary 1. ∆ F (h * , ˆ h) and ∆ Y J (y, ˆ h</formula><p>) are both optimal loss functions for graphs.</p><p>Proof. Equations 1 and 2 show that both are 0 when applied to a clustering with no mistake on the edges. Additionally, for each edge mis- take more, both loss functions increase, implying monotonicity. Thus, they satisfy all the assump- tions of Proposition 1.</p><p>The above characteristic suggests that ∆ F and ∆ Y J can optimize any measure that reasonably targets no mistakes as its best outcome. Clearly, this property does not guarantee loss functions to be suitable for a given task measure, e.g., the latter may have different max points and behave rather discontinuously. However, a common practice in NLP is to optimize the maximum of a measure, e.g., in case of Precision and Recall, or Accuracy, therefore, loss functions able to at least achieve such an optimum are preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatically learning a loss function</head><p>How to measure a complex task such as CR has generated a long and controversial discussion in the research community. While such a debate is progressing, the most accepted and used measure is the so-called Mention, Entity, and Link Average (MELA) score. As it will be clear from the de- scription below, MELA is not easily interpretable and not robust to the mention identification ef- fect ( <ref type="bibr" target="#b14">Moosavi and Strube, 2016)</ref>. Thus, loss func- tions showing the optimality property may not be enough to optimize it. Our proposal is to use a version of MELA transformed in a loss function optimized by an LSP algorithm with inexact in- ference. However, the computational complexity of the measure prevents to carry out an effective learning. Our solution is thus to learn MELA with a fast linear regressor, which also produces a con- tinuos version of the measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Measures for CR</head><p>MELA is the unweighted average of MUC (Vi- lain et al., 1995), B 3 (Bagga and <ref type="bibr" target="#b0">Baldwin, 1998)</ref> and CEAF e (CEAF variant with entity-based sim- ilarity) <ref type="bibr" target="#b12">(Luo, 2005;</ref><ref type="bibr" target="#b2">Cai and Strube, 2010</ref>) scores, having heterogeneous nature.</p><p>MUC is based on the number of correctly pre- dicted links between mentions. The number of links required for obtaining the key entity set K is k i ∈K (|k i | − 1), where k i are key entities in K (cardinality of each entity minus one). MUC recall computes what fraction of these were predicted, and the predicted were as many as</p><formula xml:id="formula_11">k i ∈K (|k i | − |p(k i )|) = k i ∈K (|k i | − 1 − (|p(k i )| − 1)),</formula><p>where p(k i ) is a partition of the key entity k i formed by intersecting it with the corresponding response en- tities r j ∈ R, s.t., k i ∩ r j = ∅. This number equals to the number of the key links minus the number of missing links, required to unite the parts of the partition p(k i ) to obtain k i . B 3 computes Precision and Recall individually for each mention. For mention m: Recall m = |k m i ∩r m j | |k m i | , where k m i and r m j , subscripted with m, denote, correspondingly, the key and response en- tities into which m falls. The over-document Re- call is then an average of these taken with respect to the number of the key mentions. The MUC and B 3 Precision is computed by interchanging the roles of the key and response entities.</p><p>CEAF e computes similarity between key and system entities after finding an optimal alignment between them. Using ψ(k i , r j ) = 2|k i ∩r j | |k i |+|r j | as the entity similarity measure, it finds an optimal one- to-one map g * : K → R, which maps every key entity to a response entity, maximazing an overall similarity Ψ(g) = k i ∈K ψ(k i , g(k i )) of the ex- ample. This is solved as a bipartite matching prob- lem by the Kuhn-Munkres algorithm. Then Preci-Algorithm 2 Finding a Max-violating Spanning Tree 1: Input: training example (x, y); graph G(x) with ver- tices V denoting mentions; set of the incoming candidate edges, E(v), v ∈ V ; weight vector w 2: h * ← ∅ 3: for v ∈ V do 4:</p><formula xml:id="formula_12">e * = argmax e∈E(v)</formula><p>w, e + C × l(y, e)</p><p>5:</p><formula xml:id="formula_13">h * = h * ∪ e * 6</formula><note type="other">: end for 7: return max-violating tree h * 8: (clustering y * is induced by the tree h * ) sion and Recall are Ψ(g * ) r j ∈R ψ(r j ,r j ) and</note><formula xml:id="formula_14">Ψ(g * ) k i ∈K ψ(k i ,k i ) , respectively.</formula><p>MELA computation is rather expensive mostly because of CEAF e . Its complexity is bounded by O(M l 2 log l) <ref type="bibr" target="#b12">(Luo, 2005)</ref>, where M and l are, correspondingly, the maximum and minimum number of entities in y andˆyandˆ andˆy. Computing CEAF e is especially slow for the candidate outputsˆyoutputsˆ outputsˆy with a low quality of prediction, i.e, when l is big, and the coherence with the gold y is scarse.</p><p>Finally, B <ref type="bibr">3</ref> and CEAF e are strongly influenced by the mention identification effect <ref type="bibr" target="#b14">(Moosavi and Strube, 2016</ref>). Thus, ∆ F and ∆ Y J may output identical values for different clusterings that can have a big gap in terms of MELA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Features for learning measures</head><p>As computational reasons prevent to use MELA in LSP (see our inexact search algorithm in Sec- tion 5), we study methods for approximating it with a linear regressor. For this purpose, we define nine features, which count either exact or simpli- fied versions of Precision, Recall and F1 of each of the three metric-components of MELA. Clearly, neither ∆ F nor ∆ Y J provide the same values.</p><p>Apart from the computational complexity, the difficulty of evaluating the quality of the predicted clusteringˆyclusteringˆ clusteringˆy during training is also due to the fact that CR is carried out on automatically detected mentions, while it needs to be compared against a gold standard clustering of a gold mention set. However, we can use simple information about au- tomatic mentions and how they relate to gold men- tions and gold clusters. In particular, we use four numbers: (i) correctly detected automatic men- tions, (ii) links they have in the gold standard, (iii) gold mentions, and (iv) gold links. The last one enables the precise computation of Precision, Re- call and F1-measure values of MUC; the required partitions p(k i ) of key entities are also available at training time as they contain only automatic men- tions. These are the first three features that we de- sign. Likewise for B 3 , the feature values can be derived using (ii) and (iii).</p><p>For computing CEAF e heuristics, we do not perform cluster alignment to find an optimal Ψ(g *</p><note type="other">). Instead of Ψ(g * ), which can be rewrit- ten as m∈K∩R 2 |k m i |+|g * (k m i )| if summing up over the mentions not the entities, we simply use˜Ψuse˜ use˜Ψ = m∈K∩R 2 |k m</note><p>i |+|r m j | , pretending that for each m its key k m i and response r m j entities are aligned. r j ∈R ψ(r j , r j ) and k i ∈K ψ(k i , k i ) in the de- nominators of the Precision and Recall are the number of predicted and gold clusters, corre- spondingly. The imprecision of the CEAF e related features is expected to be leveraged when put to- gether with the exact B <ref type="bibr">3</ref> and MUC values into the regression learning using the exact MELA values (implicitly exact CEAF e values as well).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generating training and test data</head><p>The features described above can be used to characterize the clustering variablesˆyvariablesˆ variablesˆy. For gen- erating training data, we collected all the max- violatingˆyviolatingˆ violatingˆy produced during LSP F (using ∆ F ) learning and associate them with their correct MELA scores from the scorer. This way, we can have both training and test data for our regressor. In our experiments, for the generation purpose, we decided to run LSP F on each document separately to obtain more variability inˆyinˆ inˆy's. We use a simple linear SVM to learn a model w ρ . Considering that MELA(y, ˆ y) score lies in the interval <ref type="bibr">[100,</ref><ref type="bibr">0]</ref>, a simple approximation of the loss could be:</p><formula xml:id="formula_15">∆ ρ (y, ˆ y) = 100 − w ρ · φ(y, ˆ y).<label>(4)</label></formula><p>Below, we show its improved version and an LSP for learning with it based on inexact search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning with learned loss functions</head><p>Our experiments will demonstrate that ∆ ρ can be accurately learned from data. However, the fea- tures we used for this are not factorizable over the edges of the latent trees. Thus, we design a new LSP algorithm that can use our learned loss in an approximated max search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A general inexact algorithm for CR</head><p>If the loss function can be factorized over tree edges (see Equation 3) the max-violating con- straint in Line 6 of Alg. 1 can be efficiently found by exact decoding, e.g., using Edmonds' algo- rithm as in <ref type="bibr" target="#b8">Fernandes et al. (2014)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>or Kruskal's as</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Inexact Inference of a Max-violating</head><p>Spanning Tree with a Global Loss 1: Input: training example (x, y); graph G(x) with ver- tices V denoting mentions; set of the incoming candidate edges, E(v), v ∈ V ; w, ground truth tree h * 2: ˆ h ← ∅ 3: score ← 0 4: repeat 5:</p><p>prev score = score 6: score = 0 7:</p><p>for v ∈ V do 8:</p><formula xml:id="formula_16">h = ˆ h \ e(v) 9: ˆ e = argmax e∈E(v) w, e + C × ∆(y, h * , h ∪ e)</formula><p>10:</p><formula xml:id="formula_17">ˆ h = h ∪ ˆ e 11: score = score + w, ˆ e 12:</formula><p>end for 13: score = score + ∆(y, h * , ˆ h) 14: until score = prev score 15: return max-violating treê h in Yu and Joachims (2009). The candidate graph, by construction, does not contain cycles, and the inference by Edmonds' algorithm does technically the same as the "best-left-link" inference algo- rithm by <ref type="bibr" target="#b3">Chang et al. (2012)</ref>. This can be schemat- ically represented in Alg. 2. When we deal with ∆ ρ , Alg. 2 cannot be longer applied as our new loss function is non- factorizable. Thus, we designed a greedy solution, Alg. 3, which still uses the spanning tree algo- rithm, though, it is not guaranteed to deliver the max-violating constraint. However, finding even a suboptimal solution optimizing a more accurate loss function may achieve better performance both in terms of speed and accuracy.</p><p>We reformulate Step 4 of Alg. 2, where a max- violating incoming edgê e is identified for a ver- tex v. The new max-violating inference objective contains now a global loss measured on the par- tial structurê h built up to now plus a candidate edge e for a vertex v in consideration (Line 10 of Alg. 3). On a high level, this resembles the infer- ence procedure of <ref type="bibr" target="#b21">Wiseman et al. (2016)</ref>, who use it for optimizing global features coming from an RNN. Differently though, after processing all the vertices, we repeat the procedure until the score ofˆh ofˆ ofˆh no longer improves.</p><p>Note that Björkelund and Kuhn (2014) perform inexact search on the same latent tree structures to extend the model to non-local features. In contrast to our approach, they use beam search and accu- mulate the early updates.</p><p>In addition to the design of an algorithm en- abling the use of our ∆ ρ , there are other intricacies  caused by the lack of factorization that need to be taken into account (see the next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approaching factorization properties</head><p>The ∆ ρ defined by Equation 4 approximately falls into the interval <ref type="bibr">[0,</ref><ref type="bibr">100]</ref>. However, the sim- ple optimal loss functions, ∆ F and ∆ Y J , output a value dependent on the size of the input train- ing document in terms of edges (as they factorize in terms of edges). Since this property cannot be learned from MELA by our regression algorithm, we calibrate our loss with respect to the number of correctly predicted mentions, c, in that document, obtaining ∆ ρ = c 100 ∆ ρ . Finally, another important issue is connected to the fact that on the way as we incrementally con- struct a max-violating tree according to Alg. 3, ∆ ρ decreases (and MELA grows), as we add more mentions to the output, traversing the tree nodes v. Thus, to equalize the contribution of the loss among the candidate edges of different nodes, we also scale the loss of the candidate edges of the node v having order i in the document, accord- ing to the formula ∆ ρ = i |V | ∆ ρ . This can be interpreted as giving more weight to the hard-to- classify instances -an important issue alleviated by <ref type="bibr" target="#b23">Zhao and Ng (2010)</ref>. Towards the end of the document, the probability of correctly predicting an incoming edge for a node generally decreases, as increases the number of hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In our experiments, we first show that our re- gressor for learning MELA approximates it rather accurately. Then, we examine the impact of our ∆ ρ on state-of-the-art systems in comparison with other loss functions. Finally, we show that the im- pact of our model is amplified when learning in smaller feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>Data We conducted our experiments on En- glish and Arabic parts of the corpus from CoNLL 2012-Shared Task <ref type="bibr">4</ref>   <ref type="formula">(2015)</ref> both to preprocess the English data and to extract candidate mentions and features (the basic set). For Arabic, we used mentions and features from BART 6 ( ). We extended the initial feature set for Arabic with the feature com- binations proposed by <ref type="bibr" target="#b5">Durrett and Klein (2013)</ref>, those permitted by the available initial features. Parametrization All the perceptron models re- quire tuning of a regularization parameter C. LSP F and LSP Y J -also tuning of a specific loss parameter r. We select the parameters on the entire dev. set by training on 100 random documents from the training set. We pick up C ∈ {1.0, 100.0, 1000.0, 2000.0}, the r val- ues for LSP F from the interval [0.5, 2.5] with step 0.5, and the r values for LSP Y J -from {0.05, 0.1, 0.5}. Ultimately, for English, we used C = 1000.0 in all the models; r = 1.0 in LSP F and r = 0.1 in LSP Y J . And wider ranges of pa- rameter values were considered for Arabic, due to the lower mention detection rate: C = 1000.0, r = 6.0 for LSP F , C = 1000.0, r = 0.01 for LSP Y J , and C = 5000.0 -for LSP ρ . A standard previous work setting for the number of epochs T of LSP is 5 (Martschat and Strube, 2015). Fernan- des et al. (2014) noted that T = 50 was sufficient for convergence. We selected the best T from 1 to 50 on the dev. set. Evaluation measure We used MUC, B <ref type="bibr">3</ref> , CEAF e and their average MELA for evaluation, computed by the version 8 of the official CoNLL scorer. <ref type="bibr">5</ref> http://smartschat.de/software 6 http://www.bart-coref.org/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Selected  <ref type="table" target="#tab_6">Table 2</ref>: Results of our and previous work models evaluated on the dev. and test sets following the exact CoNLL-2012 En- glish setting, using all training documents with All and 1M features. T best is evaluated on the dev. set.</p><formula xml:id="formula_18">(N = 1M ) All (N ∼ 16.8M ) Dev. Test T best Dev.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Learning loss functions</head><p>For learning MELA, we generated training and test examples from LSP F according to the proce- dure described in Section 4.3. In the first experi- ment, we trained the w ρ model on a set of exam- ples S 1 , generated from a sample of 100 English documents and tested on a set of examples S 2 , gen- erated from another sample of the same size, and vice versa. The results in <ref type="table" target="#tab_1">Table 1</ref> show that with just 5, 000/6, 000, the Mean Squared Error (MSE) is roughly between ∼ 2.4 − 2.7: these are rather small numbers considering that the regression out- put values in the interval <ref type="bibr">[0,</ref><ref type="bibr">100]</ref>. Squared Cor- relation Coefficient (SCC) reaches a correlation of about 99.7%, demonstrating that our regression approach is effective in estimating MELA.</p><p>Additionally, <ref type="figure">Figure 2</ref> shows the regression learning curves evaluated with MSE and SCC. The former rapidly decreases and, with about 1, 000 examples, reaches a plateau of around 2.3. The lat- ter shows a similar behaviour, approaching a cor- relation of about 99.8% with real MELA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">State of the art and model comparison</head><p>We first experimented with the standard CoNLL setting to compare the LSP accuracy in terms of MELA using the three different loss functions, i.e., LSP F , LSP Y J and LSP ρ . In particular, we used all the documents of the training set and all N ∼ 16.8M features from cort, and tested on the both dev. and test sets. The results are reported in Columns All of <ref type="table" target="#tab_6">Table 2</ref>.</p><p>We note first that our ∆ ρ is effective as it stays on a par with ∆ F and ∆ Y J on the dev. set. This is interesting as Corollary 1 shows that such func- tions can optimize MELA, the reported values re- fer to the optimal epoch numbers. Also, LSP ρ im- proves the other models on the test set by 0.3 per- cent points (statistical significant at the 93% level of confidence).    Secondly, all the three models improve the state of the art on CR using LSP, i.e., by Martschat and Strube (2015) using antecedent trees (M&amp;S AT) or mention ranking (M&amp;S MR), <ref type="bibr" target="#b1">Björkelund and Kuhn (2014)</ref> using a global feature model (B&amp;K) and <ref type="bibr">Fernandes et al. (2014) (Fer)</ref>. Noted that all the LSP models were trained on the train- ing set only, without retraining on the training and dev. sets together, thus our scores can be improved.</p><p>Thirdly, <ref type="table" target="#tab_5">Table 3</ref> shows the breakdown of the MELA results in terms of its components on the test set. Interestingly, LSP ρ is noticeably better in terms of B 3 and CEAF e , while LSP with simple losses, as expected, deliver higher MUC score.</p><p>Finally, the overall improvement of ∆ ρ is not impressive. This mainly depends on the optimal- ity of the competing loss functions, which in a set- ting of ∼ 16.8M features, satisfy the separability condition of Proposition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Learning in more challenging conditions</head><p>In these experiments, we verify the hypothesis that when the optimality property is partially or totally missing ∆ ρ is more visibly superior to ∆ F and ∆ Y J . As we do not want to degrade their ef- fectiveness, the only condition dependent on the setting is the data inseparability or at least harder to be separated. These conditions can be obtained by reducing the size of the feature space. How- ever, since we aim at testing conditions, where ∆ ρ is practically useful, we filter out less important features, preserving the model accuracy (at least when the selection is not extremely harsh). For this purpose, we use a feature selection approach using a basic binary classifier trained to discrimi- nate between correct and incorrect mention pairs. It is typically used in non structured CR methods and has a nice property of using the same fea- tures of LSP (we do not use global features in our study). We carried out a selection using the abso- lute values of the model weights of the classifier for ranking features and then selecting those hav- ing higher rank <ref type="bibr" target="#b10">(Haponchyk and Moschitti, 2017</ref>).</p><p>The MELA produced by our models using all the training data is presented in <ref type="figure" target="#fig_2">Figure 3</ref>. The first 7 plots show learning curves in terms of LSP epochs for different feature sets with increasing size N , evaluated on the dev. set. We note that: firstly, the fewer features are available, the better LSP ρ curves are than those of LSP F and LSP Y J in terms of accuracy and convergence speed. The intuition is that finding a separation of the training set (generalizing well) becomes more challenging (e.g., with 10k features, the data is not linearly sep-arable) thus a loss function which is closer to the real measure provides some advantages.</p><p>Secondly, when using all features, LSP ρ is still overall better than the other models but clearly the latter can achieve the same MELA on the dev. set.</p><p>Thirdly, the last plot shows the MELA produced by LSP models on the test set, when trained with the best epoch derived from the dev. set (previous plots). We observe that LSP ρ is constantly better than the other models, though decreasing its effect as the feature number increases.</p><p>Next, in Column 1 (Selected) of <ref type="table" target="#tab_6">Table 2</ref>, we report the model MELA using 1 million features. We note that LSP ρ improves the other models by at least 0.6 percent points, achieving the same ac- curacy as the best of its competitors, i.e., LSP F , using all the features.</p><p>Finally, ∆ ρ does not satisfy Proposition 1, therefore, generally, we do not know if it can op- timize any µ-type measure over graphs. How- ever, being learned to optimize MELA, it clearly separates data maximizing such a measure. We empirically verified this by checking the MELA score obtained on the training set: we found that LSP ρ always optimizes MELA, iterating for fewer epochs than the other loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Generalization to other languages</head><p>Here, we test the effectiveness of the proposed method on Arabic using all available data and fea- tures. The results in <ref type="table" target="#tab_8">Table 4</ref> reveal an indisputable superiority of LSP ρ over the counterparts optimiz- ing simple loss functions. They support the results of the previous section as we had to deal with the insufficiency of the expert-based features for Ara- bic. In such an uneasy case, LSP ρ was able to im- prove over LSP F by more than 4.7 points.</p><p>We also tested the loss model w ρ trained for the experiments on the English data (resp. setting All of Section 6.3) in LSP ρ on Arabic. This cor- responds to LSP EN ρ model. Notably, it performs even better, 1.5 points more, than LSP ρ using a loss learned from Arabic examples. This suggests a nice property of data invariance of ∆ ρ . The im- provement delivered by the "English" w ρ is due to the fact that it was trained on the data which is richer: (i) quantitatively, since coming from al- most 8 times more training documents in compar- ison to Arabic and (ii) qualitatively, in a sense of diversity with respect to the RL target value. In- deed, the Arabic data is much less separable than  the English data and this prevents to have exam- ples where MELA values are higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we studied the use of complex loss functions in structured prediction for CR. Given the scale of our investigation, we limited our study to LSP, which is anyway considered state of the art. We derived several findings: (i) for the first time, up to our knowledge, we showed that a com- plex measure, such as MELA, can be learned by a linear regressor (RL) with high accuracy and ef- fective generalization. (ii) The latter was essential for designing a new LSP based on inexact search and RL. (iii) We showed that an automatically learned loss can be optimized and provides state- of-the-art performance in a real setting, including thousands of documents and millions of features, such as CoNLL-2012 Shared Task. (iv) We de- fined a property of optimal loss functions for CR, which shows that in separable cases, such losses are enough to get the state of the art. However, as soon as separability becomes more complex sim- ple loss functions lose optimality and RL becomes more accurate and faster. (v) Our MELA approxi- mation provides a loss that is data invariant which, once learned, can be optimized in LSP on different datasets and in different languages.</p><p>Our study opens several future directions, rang- ing from defining algorithms based on automati- cally learned loss functions to learning more ef- fective measures from expert examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Latent tree used for structural learning des et al. (2014)-render exact inference possible by introducing auxiliary graph structures. The modeling of Fernandes et al. (also referred to as the antecedent tree approach) is exploited in the works of Björkelund and Kuhn (2014), Martschat and Strube (2015), and Lassalle and Denis (2015). Like us, the first couples such approach with approximate inference but for enabling the use of non-local features. The current state-of-the-art model of Wiseman et al. (2016) also employs a greedy inference procedure as it has global features from an RNN as a non-decomposable term in the inference objective.</figDesc><graphic url="image-1.png" coords="2,347.02,51.90,136.07,108.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fernandes</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of LSP models on the dev. set using different number of features, N. The last plot reports MELA score on the test set of the models using the optimal number of epochs tuned on the dev. set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Accuracy of the loss regressor on two different sets of examples generated from different documents samples.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>The English data contains 
2,802, 343, and 348 documents in the training, 10 1 

10 2 
10 3 

2 

4 

6 

8 

10 number of training examples 

MSE 

10 1 
10 2 
10 3 

98.8 

99.0 

99.2 

99.4 

99.6 

99.8 

number of training examples 

SCC 

Figure 2: Regressor Learning curves. 

dev. and test parts, respectively. The Arabic data 
includes 359, 44, and 44 documents for training, 
dev. and test sets, respectively. 
Models 
We implement our version of LSP, 
where LSP F , LSP Y J , and LSP ρ use the loss func-
tions, ∆ F , ∆ Y J , and ∆ ρ , defined in Section 3.3 
and 5.2, respectively. We used cort 5 -coref-
erence toolkit by Martschat and Strube </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Results on the test set using the same setting of</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 and the measures composing MELA.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of our and baseline models evaluated on 

the dev. and test sets following the exact CoNLL-2012 Arabic 
setting, using all training documents. T best is evaluated on 
the dev. set. 

</table></figure>

			<note place="foot" n="1"> Received most consensus in the NLP community. 2 We have not found yet a possible factorization.</note>

			<note place="foot" n="3"> Either 0 or a random vector.</note>

			<note place="foot" n="4"> conll.cemantix.org/2012/data.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Olga Uryupina for pro-viding us with the preprocessed data from BART for Arabic. This work has been supported by the EC project CogNet, 671625 (H2020-ICT-2014-2, Research and Innovation action). Many thanks to the anonymous reviewers for their valuable sug-gestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linguistic Coreference Workshop at the First International Conference on Language Resources and Evaluation</title>
		<meeting>the Linguistic Coreference Workshop at the First International Conference on Language Resources and Evaluation<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation metrics for end-to-end coreference resolution systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Stroudsburg, PA, USA, SIGDIAL &apos;10</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Illinoiscoref: The ui system in the conll-2012 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-4513" />
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="113" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruy Luiz</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milidiú</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-4502" />
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLLShared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cícero Nogueira dos Santos, and Ruy Luiz Milidiú</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="801" to="835" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Latent trees for coreference resolution</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised clustering with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="doi">10.1145/1102351.1102379</idno>
		<ptr target="https://doi.org/10.1145/1102351.1102379" />
	</analytic>
	<monogr>
		<title level="m">ICML &apos;05: Proceedings of the 22nd international conference on Machine learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A practical perspective on latent structured prediction for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Haponchyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-2023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="143" to="149" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint anaphoricity detection and coreference resolution with constrained latent structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Lassalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. AAAI Press, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence. AAAI Press, AAAI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2274" to="2280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<idno type="doi">10.3115/1220575.1220579</idno>
		<ptr target="https://doi.org/10.3115/1220575.1220579" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA, HLT &apos;05</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Which coreference evaluation metric do you trust? a proposal for a link-based entity aware metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-4501" />
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent variable perceptron algorithm for structured classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Okanohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Jont Conference on Artifical Intelligence</title>
		<meeting>the 21st International Jont Conference on Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">IJCAI&apos;09</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bart goes multilingual: The unitn/essex submission to the conll2012 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA, CoNLL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="122" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-metric optimization for coreference: The unitn/iitp/essex submission to the 2011 conll shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
	<note>CONLL Shared Task &apos;11</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Message Understanding Conference</title>
		<meeting>the 6th Message Understanding Conference</meeting>
		<imprint>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="doi">10.1145/1553374.1553523</idno>
		<ptr target="https://doi.org/10.1145/1553374.1553523" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>New York, NY, USA, ICML &apos;09</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum metric score training for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C10-1147" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1308" to="1316" />
		</imprint>
	</monogr>
	<note>Coling 2010 Organizing Committee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
