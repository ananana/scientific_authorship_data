<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualizing and Understanding Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhuo</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visualizing and Understanding Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1150" to="1159"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1106</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoder-decoder framework. We show that visu-alization with LRP helps to interpret the internal workings of NMT and analyze translation errors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end neural machine translation (NMT), which leverages neural networks to directly map between natural languages, has gained increasing popularity recently <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. NMT proves to outperform conventional statistical machine translation (SMT) significantly across a variety of language pairs <ref type="bibr" target="#b6">(Junczys-Dowmunt et al., 2016)</ref> and becomes the new de facto method in practical MT systems ( ).</p><p>However, there still remains a severe challenge: it is hard to interpret the internal workings of NMT. In SMT ( <ref type="bibr" target="#b8">Koehn et al., 2003;</ref><ref type="bibr" target="#b2">Chiang, 2005)</ref>, the translation process can be denoted as a deriva- tion that comprises a sequence of translation rules (e.g., phrase pairs and synchronous CFG rules). Defined on language structures with varying gran- ularities, these translation rules are interpretable from a linguistic perspective. In contrast, NMT takes an end-to-end approach: all internal infor- mation is represented as real-valued vectors or * Corresponding author. matrices. It is challenging to associate hidden states in neural networks with interpretable lan- guage structures. As a result, the lack of inter- pretability makes it very difficult to understand translation process and debug NMT systems.</p><p>Therefore, it is important to develop new meth- ods for visualizing and understanding NMT. Ex- isting work on visualizing and interpreting neu- ral models has been extensively investigated in computer vision ( <ref type="bibr" target="#b9">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b11">Mahendran and Vedaldi, 2015;</ref><ref type="bibr" target="#b16">Szegedy et al., 2014;</ref><ref type="bibr" target="#b14">Simonyan et al., 2014;</ref><ref type="bibr" target="#b12">Nguyen et al., 2015;</ref><ref type="bibr" target="#b5">Girshick et al., 2014;</ref><ref type="bibr" target="#b0">Bach et al., 2015)</ref>. Although visu- alizing and interpreting neural models for natural language processing has started to attract attention recently ( <ref type="bibr" target="#b7">Karpathy et al., 2016;</ref>, to the best of our knowledge, there is no exist- ing work on visualizing NMT models. Note that the attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) is restricted to demonstrate the connection between words in source and target languages and unable to offer more insights in interpreting how target words are generated (see Section 4.5).</p><p>In this work, we propose to use layer-wise rel- evance propagation (LRP) ( <ref type="bibr" target="#b0">Bach et al., 2015</ref>) to visualize and interpret neural machine translation. Originally designed to compute the contributions of single pixels to predictions for image classi- fiers, LRP back-propagates relevance recursively from the output layer to the input layer. In con- trast to visualization methods relying on deriva- tives, a major advantage of LRP is that it does not require neural activations to be differentiable or smooth ( <ref type="bibr" target="#b0">Bach et al., 2015)</ref>. We adapt LRP to the attention-based encoder-decoder framework ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) to calculate relevance that measures the association degree between two ar- bitrary neurons in neural networks. Case studies on Chinese-English translation show that visual- ization helps to interpret the internal workings of  NMT and analyze translation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Given a source sentence x = x 1 , . . . , x i , . . . , x I with I source words and a target sentence y = y 1 , . . . , y j , . . . , y J with J target words, neu- ral machine translation (NMT) decomposes the sentence-level translation probability as a product of word-level translation probabilities:</p><formula xml:id="formula_0">P (y|x; θ) = J j=1 P (y j |x, y &lt;j ; θ),<label>(1)</label></formula><p>where y &lt;j = y 1 , . . . , y j−1 is a partial translation. In this work, we focus on the attention-based encoder-decoder framework ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. As shown in <ref type="figure">Figure 1</ref>, given a source sen- tence x, the encoder first uses source word embed- dings to map each source word x i to a real-valued vector x i . <ref type="bibr">1</ref> Then, a forward recurrent neural network (RNN) with GRU units ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>) runs to calculate source forward hidden states:</p><formula xml:id="formula_1">− → h i = f ( − → h i−1 , x i ),<label>(2)</label></formula><p>where f (·) is a non-linear function. Similarly, the source backward hidden states can be obtained using a backward RNN:</p><formula xml:id="formula_2">← − h i = f ( ← − h i+1 , x i ).<label>(3)</label></formula><p>To capture global contexts, the forward and backward hidden states are concatenated as the hidden state for each source word: <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> propose an attention mechanism to dynamically determine the relevant source context c j for each target word:</p><formula xml:id="formula_3">h i = [ − → h i ; ← − h i ].<label>(4)</label></formula><formula xml:id="formula_4">c j = I+1 i=1 α j,i h i ,<label>(5)</label></formula><p>where α j,i is an attention weight that indicates how well the source word x i and the target word y j match. Note that an end-of-sentence token is appended to the source sentence.</p><p>In the decoder, a target hidden state for the j-th target word is calculated as</p><formula xml:id="formula_5">s j = g(s j−1 , y j , c j ),<label>(6)</label></formula><p>where g(·) is a non-linear function, y j−1 denotes the vector representation of the (j − 1)-th target word. Finally, the word-level translation probability is given by</p><formula xml:id="formula_6">P (y j |x, y &lt;j ; θ) = ρ(y j−1 , s j , c j ),<label>(7)</label></formula><p>where ρ(·) is a non-linear function.</p><p>Although NMT proves to deliver state-of-the- art translation performance with the capability to handle long-distance dependencies due to GRU and attention, it is hard to interpret the internal information such as</p><formula xml:id="formula_7">− → h i , ← − h i , h i , c j</formula><p>, and s j in the encoder-decoder framework. Though project- ing word embedding space into two dimensions <ref type="bibr" target="#b4">(Faruqui and Dyer, 2014</ref>) and the attention matrix ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> shed partial light on how NMT works, how to interpret the entire network still remains a challenge.</p><p>Therefore, it is important to develop new meth- ods for understanding the translation process and analyzing translation errors for NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Recent efforts on interpreting and visualizing neu- ral models has focused on calculating the contribu- tion of a unit at the input layer to the final decision at the output layer ( <ref type="bibr" target="#b14">Simonyan et al., 2014;</ref><ref type="bibr" target="#b11">Mahendran and Vedaldi, 2015;</ref><ref type="bibr" target="#b12">Nguyen et al., 2015;</ref><ref type="bibr"></ref> in New &lt;/s&gt; York 在 纽约 &lt;/s&gt; in New zai niuyue <ref type="figure">Figure 2</ref>: Visualizing the relevance between the vector representation of a target word "New York" and those of all source words and preceding target words. <ref type="bibr" target="#b5">Girshick et al., 2014;</ref><ref type="bibr" target="#b0">Bach et al., 2015;</ref>. For example, in image classification, it is important to understand the contribution of a sin- gle pixel to the prediction of classifier ( <ref type="bibr" target="#b0">Bach et al., 2015)</ref>.</p><p>In this work, we are interested in calculating the contribution of source and target words to the fol- lowing internal information in the attention-based encoder-decoder framework: For example, as shown in <ref type="figure">Figure 2</ref>, the gener- ation of the third target word "York" depends on both the source context (i.e., the source sentence "zai niuyue &lt;/s&gt;") and the target context (i.e., the partial translation "in New"). Intuitively, the source word "niuyue" and the target word "New" are more relevant to "York" and should receive higher relevance than other words. The problem is how to quantify and visualize the relevance be- tween hidden states and contextual word vectors.</p><p>More formally, we introduce a number of defi- nitions to facilitate the presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1</head><p>The contextual word set of a hidden state v ∈ R M ×1 is denoted as C(v), which is a set of source and target contextual word vectors u ∈ R N ×1 that influences the generation of v. For example, the context word set for</p><formula xml:id="formula_8">− → h i is {x 1 , . . . , x i }, for ← − h i is {x i , . . . , x I+1 }, and for h i is {x 1 , . . . , x I+1 }. The contextual word set for c j is {x 1 , . . . , x I+1 }, for s j and y j is {x 1 , . . . , x I+1 , y 1 , . . . , y j−1 }.</formula><p>As both hidden states and contextual words are represented as real-valued vectors, we need to fac- torize vector-level relevance at the neuron level. Definition 2 The neuron-level relevance be- tween the m-th neuron in a hidden state v m ∈ R and the n-th neuron in a contextual word vector u n ∈ R is denoted as r un←vm ∈ R, which satis- fies the following constraint:</p><formula xml:id="formula_9">v m = u∈C(v) N n=1 r un←vm (8) Definition 3</formula><p>The vector-level relevance between a hidden state v and one contextual word vector u ∈ C(v) is denoted as R u←v ∈ R, which quanti- fies the contribution of u to the generation of v. It is calculated as</p><formula xml:id="formula_10">R u←v = M m=1 N n=1 r un←vm (9) Definition 4</formula><p>The relevance vector of a hidden state v is a sequence of vector-level relevance of its contextual words:</p><formula xml:id="formula_11">R v = {R u 1 ←v , . . . , R u |C(v)| ←v }<label>(10)</label></formula><p>Therefore, our goal is to compute relevance vec- tors for hidden states in a neural network, as shown in <ref type="figure">Figure 2</ref>. The key problem is how to compute neuron-level relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layer-wise Relevance Propagation</head><p>We follow ( <ref type="bibr" target="#b0">Bach et al., 2015</ref>) to use layer-wise relevance propagation (LRP) to compute neuron- level relevance. We use a simple feed-forward net- work shown in <ref type="figure" target="#fig_3">Figure 3</ref> to illustrate the central idea of LRP.</p><p>Input: A neural network G for a sentence pair and a set of hidden states to be visualized V. Output: Vector-level relevance set R. LRP first propagates the relevance from the out- put layer to the intermediate layer:</p><formula xml:id="formula_12">1 for u ∈ G in a</formula><formula xml:id="formula_13">r z 1 ←v 1 = W (2) 1,1 z 1 W (2) 1,1 z 1 + W (2) 2,1 z 2 v 1 (11) r z 2 ←v 1 = W (2) 2,1 z 2 W (2) 1,1 z 1 + W (2) 2,1 z 2 v 1<label>(12)</label></formula><p>Note that we ignore the non-linear activation func- tion because <ref type="bibr" target="#b0">Bach et al. (2015)</ref> indicate that LRP is invariant against the choice of non-linear func- tion. Then, the relevance is further propagated to the input layer:</p><formula xml:id="formula_14">r u 1 ←v 1 = W (1) 1,1 u 1 W (1) 1,1 u 1 + W (1) 2,1 u 2 r z 1 ←v 1 + W (1) 1,2 u 1 W (1) 1,2 u 1 + W (1) 2,2 u 2 r z 2 ←v 1 (13) r u 2 ←v 1 = W (1) 2,1 u 2 W (1) 1,1 u 1 + W (1) 2,1 u 2 r z 1 ←v 1 + W (1) 2,2 u 2 W (1) 1,2 u 1 + W (1) 2,2 u 2 r z 2 ←v 1 (14)</formula><p>Note that r u 1 ←v 1 + r u 2 ←v 1 = v 1 .</p><p>More formally, we introduce the following def- initions to ease exposition.</p><p>Definition 5 Given a neuron u, its incoming neu- ron set IN(u) comprises all its direct connected preceding neurons in the network.</p><p>For example, in <ref type="figure" target="#fig_3">Figure 3</ref>, the incoming neuron set of z 1 is IN(z 1 ) = {u 1 , u 2 }.</p><p>Definition 6 Given a neuron u, its outcoming neuron set OUT(u) comprises all its direct con- nected descendant neurons in the network.</p><p>For example, in <ref type="figure" target="#fig_3">Figure 3</ref>, the incoming neuron set of z 1 is OUT(z 1 ) = {v 1 , v 2 }. Definition 7 Given a neuron v and its incoming neurons u ∈ IN(v), the weight ratio that mea- sures the contribution of u to v is calculated as</p><formula xml:id="formula_15">w u→v = W u,v u u ∈IN(v) W u ,v u<label>(15)</label></formula><p>Although the NMT model usually involves multiple operators such as matrix multiplication, element-wise multiplication, and maximization, they only influence the way to calculate weight ra- tios in Eq. <ref type="bibr">(15)</ref>.</p><p>For matrix multiplication such as v = Wu, its basic form that is calculated at the neuron level is given by v = u∈IN(v) W u,v u . We follow <ref type="bibr" target="#b0">Bach et al. (2015)</ref> to calculate the weight ratio using Eq. </p><note type="other">1 2 3 4 5 6 1 2 3 4 5 6</note><p>Figure 4: Visualizing source hidden states for a source content word "nian" (years).</p><p>For element-wise multiplication such as v = u 1 •u 2 , its basic form is given by v = u∈IN(v) u. We use the following method to calculate its weight ratio:</p><formula xml:id="formula_17">w u→v = u u ∈IN(v) u<label>(16)</label></formula><p>For maximization such as v = max{u 1 , u 2 }, we calculate its weight ratio as follows:</p><formula xml:id="formula_18">w u→v = 1 if u = max u ∈IN(v) {u } 0 otherwise<label>(17)</label></formula><p>Therefore, the general local redistribution rule for LRP is given by</p><formula xml:id="formula_19">r u←v = z∈OUT(u) w u→z r z←v<label>(18)</label></formula><p>Algorithm 1 gives the layer-wise relevance propagation algorithm for neural machine trans- lation. The input is an attention-based encoder- decoder neural network for a sentence pair after decoding G and a set of hidden states to be visu- alized V. The output is a set of vector-level rel- evance between intended hidden states and their contextual words R. The algorithm first com- putes weight ratios for each neuron in a forward pass (lines 1-4). Then, for each hidden state to be visualized (line 6), the algorithm initializes the neuron-level relevance for itself (lines 7-9). After initialization, the neuron-level relevance is back- propagated through the network (lines 10-12). Fi- nally, vector-level relevance is calculated based on neuron-level relevance (lines <ref type="bibr">[13]</ref><ref type="bibr">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr">[16]</ref> Figure 5: Visualizing target hidden states for a tar- get content word "visit".</p><p>where |G| is the number of neuron units in the neu- ral network G, |V| is the number of hidden states to be visualized and O max is the maximum of out- degree for neurons in the network. Calculating relevance is more computationally expensive than computing attention as it involves all neurons in the network. Fortunately, it is possible to take ad- vantage of parallel architectures of GPUs and rel- evance caching for speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>We evaluate our approach on Chinese-English translation. We use the open-source toolkit GROUNDHOG ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which implements the attention-based encoder-decoder framework. Af- ter model training and selection on the training and development sets, we use the resulting NMT model to translate the test set. Therefore, the vi- sualization examples in the following subsections are taken from the test set. <ref type="figure">Figure 4</ref> visualizes the source hidden states for a source content word "nian" (years). For each word in the source string "jin liang nian lai , meiguo" (in recent two years, USA), we attach a number to denote the position of the word in the sentence. For example, "nian" (years) is the third word. We are interested in visualizing the relevance between the third source forward hidden state − → h 3 and all its contextual words "jin" (recent) and "liang" (two). We observe that the direct preced- ing word "liang" (two) contributes more to form- ing the forward hidden state of "nian" (years). For the third source backward hidden state ← − h 3 , the relevance of contextual words generally decreases with the increase of the distance to "nian" (years). Clearly, the concatenation of forward and back- ward hidden states h 3 capture contexts in both di- rections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualization of Hidden States</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Source Side</head><p>The situations for function words and punctua- tion marks are similar but the relevance is usually more concentrated on the word itself. We omit the visualization due to space limit. <ref type="figure">Figure 5</ref> visualizes the target-side hidden states for the second target word "visit". For comparison, we also give the attention weights α 2 , which cor- rectly identifies the second source word "canbai" ("visit") is most relevant to "visit".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Target Side</head><p>The relevance vector of the source context c 2 is generally consistent with the attention but reveals that the third word "shi" (is) also contributes to the generation of "visit".</p><p>For the target hidden state s 2 , the contextual word set includes the first target word "my". We find that most contextual words receive high val- ues of relevance. This phenomenon has been fre- quently observed for most target words in other sentences. Note that relevance vector is not nor- malized. This is an essential difference between attention and relevance. While attention is defined to be normalized, the only constraint on relevance is that the sum of relevance of contextual words is identical to the value of intended hidden state neuron.</p><p>For the target word embedding y 2 , the relevance is generally consistent with the attention by iden- tifying that the second source word contributes more to the generation of "visit". But R y 2 further indicates that the target word "my" is also very im- portant for generating "visit". <ref type="figure" target="#fig_4">Figure 6</ref> shows the hidden states of a target UNK word, which is very common to see in NMT because of limited vocabulary. It is interesting to investigate whether the attention mechanism could put a UNK in the right place in the translation. In this example, the 6-th source word "zhaiwuguo" is a UNK. We find that the model successfully pre- dicts the correct position of UNK by exploiting surrounding source and target contexts. But the ordering of UNK usually becomes worse if multi- ple UNK words exist on the source side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Translation Error Analysis</head><p>Given the visualization of hidden states, it is possi- ble to offer useful information for analyzing trans- lation errors commonly observed in NMT such as word omission, word repetition, unrelated words and negation reversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Word Omission</head><p>Given a source sentence "bajisitan zongtong muxi- alafu yingde can zhong liang yuan xinren toupiao" (pakistani president musharraf wins votes of con- fidence in senate and house), the NMT model pro- duces a wrong translation "pakistani president win over democratic vote of confidence in the senate". One translation error is that the 6-th source word "zhong" (house) is incorrectly omitted for transla- tion.</p><p>As the end-of-sentence token "&lt;/s&gt;" occurs early than expected, we choose to visualize its cor- responding target hidden states. Although the at- tention correctly identifies the 6-th source word "zhong" (house) to be important for generating the next target word, the relevance of source con- text R c 12 attaches more importance to the end-of- sentence token.</p><p>Finally, the relevance of target word R y 12 re- veals that the end-of-sentence token and the 11-th target word "senate" become dominant in the soft- max layer for generating the target word.</p><p>This example demonstrates that only using at- tention matrices does not suffice to analyze the internal workings of NMT. The values of rele- vance of contextual words might vary significantly across different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Word Repetition</head><p>Given a source sentence "meiguoren lishi shang you jiang chengxi de chuantong , you fancuo ren- cuo de chuantong" (in history , the people of amer- ica have the tradition of honesty and would not hesitate to admit their mistakes), the NMT model produces a wrong translation "in the history of the history of the history of the americans , there is a tradition of faith in the history of mistakes". The  translation error is that "history" repeats four times in the translation. <ref type="figure" target="#fig_6">Figure 8</ref> visualizes the target hidden states of the 6-th target word "history". According to the relevance of the target word embedding R y 6 , the first source word "meiguoren" (american), the second source word "lishi" (history) and the 5-th target word "the" are most relevant to the gen- eration of "history". Therefore, word repetition not only results from wrong attention but also is significantly influenced by target side context. This finding confirms the importance of control- ling source and target contexts to improve fluency and adequacy ( <ref type="bibr" target="#b17">Tu et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Unrelated Words</head><p>Given a source sentence "ci ci huiyi de yi ge zhongyao yiti shi kuadaxiyang guanxi" (one the the top agendas of the meeting is to discuss the cross-atlantic relations), the model prediction is "a key topic of the meeting is to forge ahead". One translation error is that the 9-th English word "forge" is totally unrelated to the source sentence. <ref type="figure" target="#fig_7">Figure 9</ref> visualizes the hidden states of the 9-th target word "forge". We find that while the attention identifies the 10-th source word "kuadaxiyang" (cross-atlantic) to be most rele- vant, the relevance vector of the target word R y 9 finds that multiple source and target words should contribute to the generation of the next target word.</p><p>We observe that unrelated words are more likely to occur if multiple contextual words have high values in the relevance vector of the target word being generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Negation Reversion</head><p>Given a source sentence "bu jiejue shengcun wenti , jiu tan bu shang fa zhan , geng tan bu shang ke chixu fazhan" (without solution to the issue of sub- sistence , there will be no development to speak of , let alone sustainable development), the model pre- diction is "if we do not solve the problem of liv- ing , we will talk about development and still less can we talk about sustainable development". The translation error is that the 8-th negation source word "bu" (not) is untranslated. The omission of negation is a severe translation error it reverses the meaning of the source sentence.</p><p>As shown in <ref type="figure" target="#fig_8">Figure 10</ref>, while both attention and relevance correctly identify the 8-th negation word "bu" (not) to be most relevant, the model still gen- erates "about" instead of a negation target word. One possible reason is that target context words "will talk" take the lead in determining the next target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Extra Words</head><p>Given a source sentence "bajisitan zongtong mux- ialafu yingde can zhong liang yuan xinren tou- piao"(pakistani president musharraf wins votes of confidence in senate and house), the model predic- tion is "pakistani president win over democratic vote of confidence in the senate" The translation error is that the 5-th target word "democratic" is extra generated.  <ref type="figure">Figure 11</ref>: Analyzing translation error: extra word. The 5-th target word "democratic" is an ex- tra word. <ref type="figure">Figure 11</ref> visualizes the hidden states of the 9-th target word "forge". We find that while the attention identifies the 9-th source word "xin- ren"(confidence) to be most relevant, the relevance vector of the target word R y 9 indicates that the end-of-sentence token and target words contribute more to the generation of "democratic".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Summary of Findings</head><p>We summarize the findings of visualizing and an- alyzing the decoding process of NMT as follows:</p><p>1. Although attention is very useful for under- standing the connection between source and target words, only using attention is not suf- ficient for deep interpretation of target word generation ( <ref type="figure" target="#fig_7">Figure 9</ref>);</p><p>2. The relevance of contextual words might vary significantly across different layers of hidden states ( <ref type="figure" target="#fig_7">Figure 9</ref>);</p><p>3. Target-side context also plays a critical role in determining the next target word being gen- erated. It is important to control both source and target contexts to produce correct trans- lations ( <ref type="figure" target="#fig_8">Figure 10</ref>);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is closely related to previous visualiza- tion approaches that compute the contribution of a unit at the input layer to the final decision at the output layer ( <ref type="bibr" target="#b14">Simonyan et al., 2014;</ref><ref type="bibr" target="#b11">Mahendran and Vedaldi, 2015;</ref><ref type="bibr" target="#b12">Nguyen et al., 2015;</ref><ref type="bibr" target="#b5">Girshick et al., 2014;</ref><ref type="bibr" target="#b0">Bach et al., 2015;</ref>). Among them, our approach bears most re- semblance to ( <ref type="bibr" target="#b0">Bach et al., 2015</ref>) since we adapt layer-wise relevance propagation to neural ma- chine translation. The major difference is that word vectors rather than single pixels are the ba- sic units in NMT. Therefore, we propose vector- level relevance based on neuron-level relevance for NMT. Calculating weight ratios has also been carefully designed for the operators in NMT. The proposed approach also differs from ( ) in that we use relevance rather than partial derivative to quantify the contributions of contextual words. A major advantage of using rel- evance is that it does not require neural activations to be differentiable or smooth ( <ref type="bibr" target="#b0">Bach et al., 2015)</ref>.</p><p>The relevance vector we used is significantly different from the attention matrix ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. While attention only demonstrates the association degree between source and target words, relevance can be used to calculate the as- sociation degree between two arbitrary neurons in neural networks. In addition, relevance is effective in analyzing the effect of source and target con- texts on generating target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose to use layer-wise rele- vance propagation to visualize and interpret neural machine translation. Our approach is capable of calculating the relevance between arbitrary hidden states and contextual words by back-propagating relevance along the network recursively. Analyses of the state-of-art attention-based encoder-decoder framework on Chinese-English translation show that our approach is able to offer more insights than the attention mechanism for interpreting neu- ral machine translation.</p><p>In the future, we plan to apply our approach to more NMT approaches <ref type="bibr" target="#b13">Shen et al., 2016;</ref><ref type="bibr" target="#b18">Tu et al., 2016;</ref> on more language pairs to further verify its effec- tiveness. It is also interesting to develop relevance- based neural translation models to explicitly con- trol relevance to produce better translations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: The attention-based encoder-decoder architecture for neural machine translation (Bahdanau et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>: the i-th source forward hidden state, 2. ← − h i : the i-th source backward hidden state, 3. h i : the i-th source hidden state, 4. c j : the j-th source context vector, 5. s j : the j-th target hidden state, 6. y j : the j-th target word embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A simple feed-forward network for illustrating layer-wise relevance propagation (Bach et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualizing target hidden states for a target UNK word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Analyzing translation error: word omission. The 6-th source word "zhong" is untranslated incorrectly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Analyzing translation error: word repetition. The target word "history" occurs twice in the translation incorrectly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Analyzing translation error: unrelated words. The 9-th target word "forge" is totally unrelated to the source sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Analyzing translation error: negation. The 8-th negation source word "bu" (not) is not translated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>近 

两 

jin 
liang 

年 

nian 

来 

lai 

， 
美国 

, 
meiguo 

近 
两 
年 
来 
， 
美国 

jin 
liang 
nian 
lai 
, 
meiguo 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>The training set consists of 1.25M pairs of sentences with 27.93M Chinese words and 34.51M English words. We use the NIST 2003 dataset as the development set for model selection and the NIST 2004 dataset as test set. The BLEU score on NIST 2003 is 32.73.</figDesc><table></table></figure>

			<note place="foot" n="1"> Note that we use x to denote a source sentence and x to denote the vector representation of a single source word.</note>

			<note place="foot" n="4">. Generating the end-of-sentence token too early might lead to many problems such as word omission, unrelated word generation, and truncated translation (Figures 7 and 9).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Natu-ral Science Foundation of China (No.61522204), the 863 Program (2015AA015407), and the National Natural Science Foundation of China (No.61432013). This research is also supported by the Singapore National Research Foundation un-der its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davie</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Is neural machine translation ready for deployment? a case study on 30 translation directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01108v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visualing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional nerual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecignizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualizing image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144v2</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<publisher>Macduff Hughes, and Jeffrey Dean</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
