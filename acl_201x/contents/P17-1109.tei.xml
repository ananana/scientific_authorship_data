<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Typology: Deep Generative Models of Vowel Inventories</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
						</author>
						<title level="a" type="main">Probabilistic Typology: Deep Generative Models of Vowel Inventories</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1182" to="1192"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1109</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example , all languages have vowels, while most-but not all-languages have an [u] sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human languages exhibit a wide range of phenom- ena, within some limits. However, some structures seem to occur or co-occur more frequently than oth- ers. Linguistic typology attempts to describe the range of natural variation and seeks to organize and quantify linguistic universals, such as patterns of co-occurrence. Perhaps one of the simplest typolog- ical questions comes from phonology: which vow- els tend to occur and co-occur within the phoneme inventories of different languages? Drawing in- spiration from the linguistic literature, we propose models of the probability distribution from which the attested vowel inventories have been drawn.</p><p>It is a typological universal that every language contains both vowels and consonants <ref type="bibr" target="#b37">(Velupillai, 2012)</ref>. But which vowels a language contains is guided by softer constraints, in that certain configurations are more widely attested than oth- ers. For instance, in a typical phoneme inven- tory, there tend to be far fewer vowels than con- sonants. Likewise, all languages contrast vowels based on height, although which contrast is made is language-dependent ( <ref type="bibr" target="#b17">Ladefoged and Maddieson, 1996)</ref>. Moreover, while over 600 unique vowel <ref type="figure">Figure 1</ref>: The transformed vowel space that is constructed within one of our deep generative models (see §7.1). A deep network nonlinearly maps the blue grid ("formant space") to the red grid ("metric space"), with individual vowels mapped from blue to red position as shown. Vowel pairs such as <ref type="bibr">[@]</ref>- <ref type="bibr">[O]</ref> that are brought close together are anti-correlated in the point process. Other pairs such as [y]- <ref type="bibr">[1]</ref> are driven apart. For purposes of the visualization, we have transformed the red coordinate system to place red vowels near their blue positions-while preserving distances up to a constant factor (a "Procrustes transformation").</p><p>phonemes have been attested cross-linguistically <ref type="bibr" target="#b25">(Moran et al., 2014</ref>), certain regions of acoustic space are used much more often than others, e.g., the regions conventionally transcribed as <ref type="bibr">[a]</ref>, <ref type="bibr">[i]</ref>, and <ref type="bibr">[u]</ref>. Human language also seems to prefer in- ventories where phonologically distinct vowels are spread out in acoustic space ("dispersion") so that they can be easily distinguished by a listener. We depict the acoustic space for English in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>In this work, we regard the proper goal of lin- guistic typology as the construction of a universal prior distribution from which linguistic systems are drawn. For vowel system typology, we propose three formal probability models based on stochas- tic point processes. We estimate the parameters of the model on one set of languages and evaluate performance on a held-out set. We explore three questions: (i) How well do the properties of our proposed probability models line up experimen- tally with linguistic theory? (ii) How well can our models predict held-out vowel systems? (iii) Do our models benefit from a "deep" transformation from formant space to metric space? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Vowel Inventories and their Typology</head><p>Vowel inventories are a simple entry point into the study of linguistic typology. Every spoken lan- guage chooses a discrete set of vowels, and the number of vowel phonemes ranges from 3 to 46, with a mean of 8.7 <ref type="bibr" target="#b10">(Gordon, 2016)</ref>. Nevertheless, the empirical distribution over vowel inventories is remarkably peaked. The majority of languages have 5-7 vowels, and there are only a handful of distinct 4-vowel systems attested despite many possibilities. Reigning linguistic theory <ref type="bibr">(BeckerKristal, 2010</ref>) has proposed that vowel inventories are shaped by the principles discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Acoustic Phonetics</head><p>One way to describe the sound of a vowel is through its acoustic energy at different frequencies. A spectrogram <ref type="figure" target="#fig_1">(Figure 3</ref>) is a visualization of the energy at various frequencies over time. Consider the "peak" frequencies F 0 &lt; F 1 &lt; F 2 &lt; . . . that have a greater energy than their neighboring fre- quencies. F 0 is called the fundamental frequency or pitch. The other qualities of the vowel are largely determined by F 1 , F 2 , . . ., which are known as for- mants <ref type="bibr" target="#b16">(Ladefoged and Johnson, 2014</ref>). In many languages, the first two formants F 1 and F 2 contain enough information to identify a vowel: <ref type="figure" target="#fig_1">Figure 3</ref> shows how these differ across three English vowels. We consider each vowel listed in the International Phonetic Alphabet (IPA) to be cross-linguistically characterized by some (F 1 , F 2 ) pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dispersion</head><p>The dispersion criterion <ref type="bibr" target="#b18">(Liljencrants and Lindblom, 1972;</ref><ref type="bibr" target="#b19">Lindblom, 1986)</ref> states that the phonemes of a language must be "spread out" so that they are easily discriminated by a listener. A  . The x-axis is time and y-axis is frequency. The first two formants F1 and F2 are marked in with colored arrows for each vowel. We used the Praat toolkit to generate the spectrogram and find the formants ( <ref type="bibr" target="#b3">Boersma et al., 2002</ref>).</p><p>language seeks phonemes that are sufficiently "dis- tant" from one another to avoid confusion. Dis- tances between phonemes are defined in some la- tent "metric space." We use this term rather than "perceptual space" because the confusability of two vowels may reflect not just their perceptual similar- ity, but also their common distortions by imprecise articulation or background noise. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Focalization</head><p>The dispersion criterion alone does not seem to capture the whole story. Certain vowels are simply more popular cross-linguistically. A commonly ac- cepted explanation is the quantal theory of speech <ref type="bibr" target="#b33">(Stevens, 1972</ref><ref type="bibr" target="#b34">(Stevens, , 1989</ref>. The quantal theory states that certain sounds are easier to articulate and to perceive than others. These vowels may be charac- terized as those where F 1 and F 2 have frequen- cies that are close to one another. On the pro- duction side, these vowels are easier to pronounce since they allow for greater articulatory impreci- sion. On the perception side, they are more salient since the two spectral peaks aggregate and act as one, larger peak to a certain degree. In general, languages will prefer these vowels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dispersion-Focalization Theory</head><p>The dispersion-focalization theory (DFT) combines both of the above notions. A good vowel system now consists of vowels that contrast with each other and are individually desirable ( <ref type="bibr" target="#b31">Schwartz et al., 1997</ref>). This paper provides the first probabilis- tic treatment of DFT, and new evaluation metrics for future probabilistic and non-probabilistic treat- ments of vowel inventory typology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Point Process Models</head><p>Given a base set V, a point process is a distribution over its subsets. <ref type="bibr">2</ref> In this paper, we take V to be the set of all IPA symbols corresponding to vow- els. Thus a draw from a point process is a vowel inventory V ⊆ V, and the point process itself is a distribution over such inventories. We will con- sider three basic point process models for vowel systems: the Bernoulli Point Process, the Markov Point Process and the Determinantal Point Process. In this section, we review the relevant theory of point processes, highlighting aspects related to §2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bernoulli Point Processes</head><p>Taking V = {v 1 , . . . , v N }, a Bernoulli point pro- cess (BPP) makes an independent decision about whether to include each vowel in the subset. The probability of a vowel system V ⊆ V is thus</p><formula xml:id="formula_0">p(V ) ∝ v i ∈V φ(v i ),<label>(1)</label></formula><p>where φ is a unary potential function, i.e., φ(v i ) ≥ 0. Qualitatively, this means that φ(v i ) should be large if the i th vowel is good in the sense of §2.3.</p><p>Marginal inference in a BPP is computationally trivial. The probability that the inventory V con- tains v i is φ(v i )/(1 + φ(v i )), independent of the other vowels in V . Since a BPP predicts each vowel independently, it only models focalization. Thus, the model provides an appropriate baseline that will let us measure the importance of the dis- persion principle-how far can we get with just focalization? A BPP may still tend to generate well-dispersed sets if it defines φ to be large only on certain vowels in V and these are well-dispersed (e.g., <ref type="bibr">[i]</ref>, <ref type="bibr">[u]</ref>, <ref type="bibr">[a]</ref>). More precisely, it can define φ so that φ(v i )φ(v j ) is small whenever v i , v j are sim- ilar. <ref type="bibr">3</ref> But it cannot actively encourage dispersion:</p><p>2 A point process is a specific kind of stochastic process, which is the technical term for a distribution over functions. Under this view, drawing some subset of V from the point process is regarded as drawing some indicator function on V.</p><p>3 We point out that such a scheme would break down if we extended our work to cover fine-grained phonetic modeling of the vowel inventory. In that setting, we ask not just whether the inventory includes /i/ but exactly which pronunciation of /i/ it contains. In the limit, φ becomes a function over a continuous vowel space V = R 2 , turning the BPP into an inhomogeneous spatial Poisson process. A continuous φ function implies that the model places similar probability on similar vowels. Then if most vowel inventories contain some version of /i/, then many of them will contain several closely related variants of /i/ (independently chosen). By contrast, the other methods in this paper do extend nicely to fine-grained phonetic modeling. including v i does not lower the probability of also including v j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Markov Point Processes</head><p>A Markov Point Process (MPP) <ref type="bibr" target="#b36">(Van Lieshout, 2000</ref>)-also known as a Boltzmann machine <ref type="bibr" target="#b0">(Ackley et al., 1985;</ref><ref type="bibr" target="#b11">Hinton and Sejnowski, 1986</ref>)- generalizes the BPP by adding pairwise interac- tions between vowels. The probability of a vowel system V ⊆ V is now</p><formula xml:id="formula_1">p(V ) ∝ v i ∈V φ(v i ) v i ,v j ∈V ψ(v i , v j ),<label>(2)</label></formula><p>where each φ(v i ) ≥ 0 is, again, a unary potential that scores the quality of the i th vowel, and each ψ(v i , v j ) ≥ 0 is a binary potential that scores the combination of the i th and j th vowels. Roughly speaking, the potential ψ(v i , v j ) should be large if the i th and j th vowel often co-occur. Recall that under the principle of dispersion, the vowels that often co-occur are easily distinguishable. Thus, confusable vowel pairs should tend to have poten- tial ψ(v i , v j ) &lt; 1. Unlike the BPP, the MPP can capture both fo- calization and dispersion. In this work, we will consider a fully connected MPP, i.e., there is a po- tential function for each pair of vowels in V. MPPs closely resemble Ising models <ref type="bibr" target="#b12">(Ising, 1925)</ref>, but with the difference that Ising models are typically lattice-structured, rather than fully connected.</p><p>Inference in MPPs. Inference in fully connected MPPs, just as in general Markov Random Fields (MRFs), is intractable <ref type="bibr" target="#b7">(Cooper, 1990)</ref> and we must rely on approximation. In this work, we estimate any needed properties of the MPP distribution by (approximately) drawing vowel inventories from it via Gibbs sampling <ref type="bibr" target="#b9">(Geman and Geman, 1984;</ref><ref type="bibr" target="#b29">Robert and Casella, 2005</ref>). Gibbs sampling simu- lates a discrete-time Markov chain whose station- ary distribution is the desired MPP distribution. At each time step, for some random v i ∈ V, it stochastically decides whether to replace the cur- rent inventory V with ¯ V , where ¯ V is a copy of V with</p><formula xml:id="formula_2">v i added (if v i / ∈ V ) or removed (if v i ∈ V ). The probability of replacement is p( ¯ V ) p(V )+p( ¯ V )</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Determinantal Point Processes</head><p>A determinantal point process (DPP) <ref type="bibr" target="#b23">(Macchi, 1975)</ref> provides an elegant alternative to an MPP, and one that is directly suited to modeling both fo- calization and dispersion. Inference requires only a few matrix computations and runs tractably in O(|V| 3 ) time, even though the model may encode a rich set of multi-way interactions. We focus on the L-ensemble parameterization of the DPP, due to <ref type="bibr" target="#b4">Borodin and Rains (2005)</ref>. <ref type="bibr">4</ref> This type of DPP defines the probability of an inventory V ⊆ V as</p><formula xml:id="formula_3">p(V ) ∝ det L V ,<label>(3)</label></formula><p>where L ∈ R N ×N (for N = |V|) is a symmetric positive semidefinite matrix, and L V refers to the submatrix of L with only those rows and columns corresponding to those elements in the subset V . Although MAP inference remains NP-hard in DPPs (just as in MPPs), marginal inference be- comes tractable. We may compute the normalizing constant in closed form as follows:</p><formula xml:id="formula_4">V ∈2 V det L V = det (L + I) .<label>(4)</label></formula><p>How does a DPP ensure focalization and disper- sion? L is positive semidefinite iff it can be written as E E for some matrix E ∈ R N ×N . It is possi- ble to express p(V ) in terms of the column vectors of E, which we call e 1 , . . . , e N :</p><formula xml:id="formula_5">• For inventories of size 2, p({v i , v j }) ∝ (φ(v i )φ(v j ) sin θ) 2 , where φ(v i ), φ(v j )</formula><p>repre- sent the quality of vowels v i , v j (as in the BPP) while sin θ ∈ [0, 1] represents their dissimi- larity. More precisely, φ(v i ), φ(v j ) are the lengths of vectors e i , e j while θ is the angle between them. Thus, we should choose the columns of E so that focal vowels get long vectors and similar vowels get vectors of simi- lar direction.</p><p>• Generalizing beyond inventories of size 2, p(V ) is proportional to the square of the vol- ume of the parallelepiped whose sides are given by {e i : v i ∈ V }. This volume can be regarded as v i ∈V φ(v i ) times a term that ranges from 1 for an orthogonal set of vowels to 0 for a linearly dependent set of vowels.</p><p>• The events v i ∈ V and v j ∈ V are anti- correlated (when not independent). That is, while both vowels may individually have high probabilities (focalization), having either one in the inventory lowers the probability of the other (dispersion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>At this point it is helpful to introduce the empirical dataset we will model. For each of 223 languages, <ref type="bibr">5</ref> Becker-Kristal (2010) provides the vowel inventory as a set of IPA symbols, listing the first 5 formants for each vowel (or fewer when not available in the original source). Some corpus statistics are shown in Figs. 4 and 5. <ref type="bibr">6</ref> For the present paper, we take V to be the set of all 53 IPA symbols that appear in the corpus. We treat these IPA labels as meaningful, in that we consider two vowels in dif- ferent languages to be the same vowel in V if (for example) they are both annotated as <ref type="bibr">[O]</ref>. We char- acterize that vowel by its average formant vector across all languages in the corpus that contain the vowel: e.g., (F 1 , F 2 , . . .) = (500, 700, . . .) for <ref type="bibr">[O]</ref>.</p><p>In future work, we plan to relax this idealization (see footnote 3), allowing us to investigate natural questions such as whether [u] is pronounced higher (smaller F 1 ) in languages that also contain <ref type="bibr">[o]</ref> (to achieve better dispersion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Parameterization</head><p>The BPP, MPP, and DPP models ( §3) require us to specify parameters for each vowel in V. In §5.1, we will accomplish this by deriving the parameters for each vowel v i from a possibly high-dimensional embedding of that vowel, e(v i ) ∈ R r . In §5.2, e(v i ) ∈ R r will in turn be defined as some learned function of f (v i ) ∈ R k , where f : V → R k is the function that maps a vowel to a k-vector of its measurable acoustic properties. This approach allows us to determine reasonable parameters even for rare vowels, based on their measurable properties. It will even enable us in <ref type="bibr">5</ref> Becker-Kristal lists some languages multiple times with different measurements. When a language had multiple list- ings, we selected one randomly for our experiments.</p><p>6 Caveat: The corpus is a curation of information from various phonetics papers into a common electronic format. No standard procedure was followed across all languages: it was up to individual phoneticists to determine the size of each vowel inventory, the choice of IPA symbols to describe it, and the procedure for measuring the formants. Moreover, it is an idealization to provide a single vector of formants for each vowel type in the language. In real speech, different to- kens of the same vowel are pronounced differently, because of coarticulation with the vowel context, allophony, interspeaker variation, and stochastic intraspeaker variation. Even within a token, the formants change during the duration of the vowel. Thus, one might do better to represent a vowel's pronuncia- tion not by a formant vector, but by a conditional probability distribution over its formant trajectories given its context, or by a parameter vector that characterizes such a conditional distribution. This setting would require richer data than we present here.</p><p>future to generalize to vowels that were unseen in the training set, letting us scale to very large or infinite V (footnote 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Deep Point Processes</head><p>We consider deep versions of all three processes.</p><p>Deep Bernoulli Point Process. We define</p><formula xml:id="formula_6">φ(v i ) = ||e(v i )|| ≥ 0<label>(5)</label></formula><p>Deep Markov Point Process. The MPP em- ploys the same unary potential as the BPP, as well as the binary potential</p><formula xml:id="formula_7">ψ(v i , v j ) = exp − 1 T · ||e(v i )−e(v j )|| 2 &lt; 1 (6)</formula><p>where the learned temperature T &gt; 0 controls the relative strength of the unary and binary potentials. This formula is inspired by Coulomb's law for describing the repulsion of static electrically charged particles. Just as the repulsive force be- tween two particles approaches ∞ as they approach each other, the probability of finding two vowels in the same inventory approaches exp −∞ = 0 as they approach each other. The formula is also reminiscent of <ref type="bibr" target="#b32">Shepard (1987)</ref>'s "universal law of generalization," which says here that the proba- bility of responding to v i as if it were v j should fall off exponentially with their distance in some "psychological space" (here, embedding space).</p><p>Deep Determinantal Point Process. For the DPP, we simply define the vector e i to be e(v i ), and proceed as before.</p><p>Summary. In the deep BPP, the probability of a set of vowels is proportional to the product of the lengths of their embedding vectors. The deep MPP modifies this by multiplying in pairwise re- pulsion terms in (0, 1) that increase as the vectors' endpoints move apart in Euclidean space (or as T → ∞). The deep DPP instead modifies it by multiplying in a single setwise repulsion term in (0, 1) that increases as the embedding vectors be- come more mutually orthogonal. In the limit, then, the MPP and DPP both approach the BPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Embeddings</head><p>Throughout this work, we simply have f extract the first k = 2 formants, since our dataset does not provide higher formants for all languages. 7 For example, we have f ([O]) = (500, 700). We now describe three possible methods for mapping f (v i ) to an embedding e(v i ). Each of these maps has learnable parameters.</p><p>Neural Embedding. We first consider directly embedding each vowel v i into a vector space R r . We achieve this through a feed-forward neural net</p><formula xml:id="formula_8">e(v i ) = W 1 tanh (W 0 f (v i ) + b 0 ) + b 1 , (7)</formula><p>Equation <ref type="formula">(7)</ref> gives an architecture with 1 layer of nonlinearity; in general we consider stacking d ≥ 0 layers. Here W 0 ∈ R r×k , W 1 ∈ R r×r , . . . W d ∈ R r×r are weight matrices, b 0 , . . . b d ∈ R r are bias vectors, and tanh could be replaced by any point- wise nonlinearity. We treat both the depth d and the embedding size r as hyperparameters, and select the optimal values on a development set.</p><p>Interpretable Neural Embedding. We are in- terested in the special case of neural embeddings when r = k since then (for any d) the mapping f (v i ) → e(v i ) is a diffeomorphism: 8 a smooth invertible function of R k . An example of such a diffeomorphism is shown in <ref type="figure">Figure 1</ref>.</p><p>There is a long history in cognitive psychology of mapping stimuli into some psychological space. The distances in this psychological space may be predictive of generalization <ref type="bibr" target="#b32">(Shepard, 1987)</ref> or of perception. Due to the anatomy of the ear, the map- ping of vowels from acoustic space to perceptual space is often presumed to be nonlinear <ref type="bibr" target="#b30">(Rosner and Pickering, 1994;</ref><ref type="bibr" target="#b26">Nearey and Kiefte, 2003)</ref>, and there are many perceptually-oriented phonetic scales, e.g., Bark and Mel, that carry out such non- linear transformations while preserving the dimen- sionality k, as we do here. As discussed in §2.2, vowel system typology is similarly believed to be influenced by distances between the vowels in a latent metric space. We are interested in whether a constrained k-dimensional model of these dis- tances can do well in our experiments.</p><p>Prototype-Based Embedding. Unfortunately, our interpretable neural embedding is unfortunately incompatible with the DPP. The DPP assigns probability 0 to any vowel inventory V whose e vectors are linearly dependent. If the vectors are in R k , then this means that p(V ) = 0 whenever |V | &gt; k. In our setting, this would limit vowel inventories to size 2.</p><p>Our solution to this problem is to still construct our interpretable metric space R k , but then map that nonlinearly to R r for some large r. This latter map is constrained. Specifically, we choose "prototype" points µ 1 , . . . , µ r ∈ R k . These prototype points are parameters of the model: their coordinates are learned and do not necessarily correspond to any actual vowel. We then construct e(v i ) ∈ R r as a "response vector" of similarities of our vowel v i to these prototypes. Crucially, the responses depend on distances measured in the interpretable metric space R k . We use a Gaussian-density response function, where x(v i ) denotes the representation of our vowel v i in the interpretable space:</p><formula xml:id="formula_9">e(v i ) = w p(x(v i ); µ , σ 2 I) (8) = w (2πσ 2 ) −( k 2 ) exp −||x − µ || 2 2σ 2 .</formula><p>for = 1, 2, . . . , r. We additionally impose the constraints that each w ≥ 0 and r =1 w = 1. Notice that the sum r =1 e(v i ) may be viewed as the density at x(v i ) under a Gaussian mixture model. We use this fact to construct a prototype- based MPP as well: we redefine φ(v i ) to equal this positive density, while still defining ψ via equa- tion (6). The idea is that dispersion is measured in the interpretable space R k , and focalization is defined by certain "good" regions in that space that are centered at the r prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Metrics</head><p>Fundamentally, we are interested in whether our model has abstracted the core principles of what makes a good vowel system. Our choice of a proba- bilistic model provides a natural test: how surprised is our model by held-out languages? In other words, how likely does our model think unobserved, but attested vowel systems are? While this is a natural evaluation paradigm in NLP, it has not-to the best of our knowledge-been applied to a quantitative investigation of linguistic typology.</p><p>As a second evaluation, we introduce a vowel system cloze task that could also be used to evalu- ate non-probabilistic models. This task is defined by analogy to the traditional semantic cloze task <ref type="bibr" target="#b35">(Taylor, 1953)</ref>, where the reader is asked to fill in a missing word in the sentence from the con- text. In our vowel system cloze task, we present a learner with a subset of the vowels in a held-out vowel system and ask them to predict the remain- ing vowels. Consider, as a concrete example, the general American English vowel system (exclud- ing long vowels) { <ref type="bibr">[E]</ref>, <ref type="bibr">[ae]</ref>, <ref type="bibr">[O]</ref>, <ref type="bibr">[A]</ref>, <ref type="bibr">[@]</ref>} and the fact that two vowels are missing from the in- ventory. Within the cloze task, we report accuracy, i.e., did we guess the missing vowel right? We consider three versions of the cloze tasks. First, we predict one missing vowel in a setting where exactly one vowel was deleted. Second, we predict up to one missing vowel where a vowel may have been deleted. Third, we predict up to two missing vowels, where one or two vowels may be deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[i], [I], [u], [U], [E], [ae], [O], [A], [@]}. One potential cloze task would be to predict</head><formula xml:id="formula_10">{[i], [u]} given {[I], [U],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We evaluate our models using 10-fold cross- validation over the 223 languages. We report the mean performance over the 10 folds. The per- formance on each fold ("test") was obtained by training many models on 8 of the other 9 folds ("train"), selecting the model that obtained the best task-specific performance on the remaining fold ("development"), and assessing it on the test fold. Minimization of the parameters is performed with the L-BFGS algorithm ( <ref type="bibr" target="#b20">Liu and Nocedal, 1989)</ref>. As a preprocessing step, the first two formants val- ues F 1 and F 2 are centered around zero and scaled down by a factor of 1000 since the formant values themselves may be quite large.</p><p>Specifically, we use the development fold to select among the following combinations of hy- perparameters. For neural embeddings, we tried r ∈ {2, 10, 50, 100, 150, 200}. For prototype em- beddings, we took the number of components r ∈ {20, 30, 40, 50}. We tried network depths d ∈ {0, 1, 2, 3}. We sweep the coefficient for an L 2 regularizer on the neural network parameters. <ref type="figure">Figure 1</ref> visualizes the diffeomorphism from for- mant space to metric space for one of our DPP models (depth d = 3 with r = 20 prototypes). Similar figures can be generated for all of the inter- pretable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Results and Discussion</head><p>We report results for cross-entropy and the cloze evaluation in <ref type="table">Table 1</ref>. <ref type="bibr">9</ref> Under both metrics, we see that the DPP is slightly better than the MPP; both are better than the BPP. This ranking holds for 69.55% 69.55% 72.05% 73.18% 64.13% 67.02% 65.13% 68.18% 68.18% cloze-01 60.00% 60.00% 61.01% 62.27% 61.78% 61.04% 61.02% 63.04% 63.63% cloze-012 53.18% 53.18% 57.92% 58.18% 39.04% 43.02% 40.56% 45.01% 45.46% <ref type="table">Table 1</ref>: Cross-entropy in nats (lower is better) and cloze prediction accuracy (higher is better). "BPP" is a simple BPP with one parameter for each of the 53 vowels in V. This model does artificially well by modeling an "accidental" feature of our data: it is able to learn not only which vowels are popular among languages, but also which IPA symbols are popular or conventional among the descriptive phoneticists who created our dataset (see footnote 6), something that would become irrelevant if we upgraded our task to predict actual formant vectors rather than IPA symbols (see footnote 3). Our point processes, by contrast, are appropriately allowed to consider a vowel only through its formant vector. The "u-" versions of the models use the uninterpretable neural embedding of the formant vector into R r : by taking r to be large, they are still able to learn special treatment for each vowel in V (which is why uBPP performs identically to BPP, before being beaten by uMPP and uDPP). The "i-" versions limit themselves to an interpretable neural embedding into R k , giving a more realistic description that does not perform as well. The "p-"versions lift that R k embedding into R r by measuring similarities to r prototypes; they thereby improve on the corresponding i-versions. For each result shown, the depth d of our neural network was tuned on a development set (typically d = 2). r was also tuned when applicable (typically r &gt; 100 dimensions for the u-models and r ≈ 30 prototypes for the p-models).</p><note type="other">BPP uBPP uMPP uDPP iBPP iMPP iDPP pBPP pMPP pDPP x-</note><p>each of the 3 embedding schemes. The embedding schemes themselves are compared in the caption. Within each embedding scheme, the BPP per- forms several points worse on the cloze tasks, con- firming that dispersion is needed to model vowel inventories well. Still, the BPP's respectable per- formance shows that much of the structure can be capture by focalization. As §3 noted, the BPP may generate well-dispersed sets, as the common vow- els tend to be dispersed already (see <ref type="figure" target="#fig_2">Figure 4)</ref>. In this capacity, however, the BPP is not explanatory as it cannot actually tell us why these vowels should be frequent.</p><p>We mention that depth in the neural network is helpful, with deeper embedding networks perform- ing slightly better than depth d = 0.</p><p>Finally, we identified each model's favorite com- plete vowel system of size n ( <ref type="table">Table 2)</ref>. For the BPP, this is simply the n most probable vowels. Decoding the DPP and MPP is NP-hard, but we found the best system by brute force (for small n). The dispersion in these models predicts different systems than the BPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion: Probabilistic Typology</head><p>Typology as Density Estimation? Our goal is to define a universal distribution over all possible vowel inventories. Is this appropriate? We regard this as a natural approach to typology, because it directly describes which kinds of linguistic systems are more or less common. Traditional implicational universals ("all languages with v i have v j ") are soft- ened, in our approach, into conditional probabilities such as "p(v j ∈ V | v i ∈ V ) ≈ 0.9." Here the 0.9 is not merely an empirical ratio, but a smoothed probability derived from the complete estimated distribution. It is meant to make predictions about unseen languages.</p><p>Whether human language learners exploit any properties of this distribution 10 is a separate ques- tion that goes beyond typology. <ref type="bibr" target="#b13">Jakobson (1941)</ref> did find that children acquired phoneme invento- ries in an order that reflected principles similar to dispersion ("maximum contrast") and focalization.</p><p>At any rate, we estimate the distribution given some set of attested systems that are assumed to have been drawn IID from it. One might object that this IID assumption ignores evolutionary re- lationships among the attested systems, causing our estimated distribution to favor systems that are coincidentally frequent among current human lan- guages, rather than being natural in some timeless sense. We reply that our approach is then appro- priate when the goal of typology is to estimate the distribution of actual human languages-a distri- bution that can be utilized in principle (and also in practice, as we show) to predict properties of actual languages from outside the training set.</p><p>A different possible goal of typology is a the- ory of natural human languages. This goal would require a more complex approach. One should not imagine that natural languages are drawn in a vacuum from some single, stationary distribution. Rather, each language is drawn conditionally on its parent language. Thus, one should estimate a stochastic model of the evolution of linguistic sys- tems through time, and identify "naturalness" with BPP MPP DPP changes from n − 1 changes from n − 1 changes from n − 1 n MAP inventory additions deletions MAP inventory additions deletions MAP inventory additions deletions  <ref type="table">Table 2</ref>: Highest-probability inventory of each size according to our three models (prototype-based embeddings and d = 3). The MAP configuration is computed by brute-force enumeration for small n.</p><formula xml:id="formula_11">1 i i @ @ @ @ 2 i, u u i, u i, u @ i, u i, u @ 3 i, u</formula><p>the directions in which this system tends to evolve.</p><p>Energy Minimization Approaches. The tradi- tional energy-based approach <ref type="bibr" target="#b18">(Liljencrants and Lindblom, 1972)</ref> to vowel simulation minimizes the following objective (written in our notation):</p><formula xml:id="formula_12">E(m) = 1≤i&lt;j≤m 1 ||e(v i ) − e(v j )|| 2 , (9)</formula><p>where the vectors e(v i ) ∈ R r are not spit out of a deep network, as in our case, but rather directly op- timized. <ref type="bibr" target="#b18">Liljencrants and Lindblom (1972)</ref> propose a coordinate descent algorithm to optimize E(m). While this is not in itself a probabilistic model, they generate diverse vowel systems through ran- dom restarts that find different local optima (a kind of deterministic evolutionary mechanism). We note that equation (9) assumes that the number of vowels m is given, and only encodes a notion of dispersion. <ref type="bibr" target="#b28">Roark (2001)</ref> subsequently extended equation <ref type="formula">(9)</ref> to include the notion of focalization.</p><p>Vowel Inventory Size. A fatal flaw of the tradi- tional energy minimization paradigm is that it has no clear way to compare vowel inventories of dif- ferent sizes. The problem is quite crippling since, in general, inventories with fewer vowels will have lower energy. This does not match reality-the empirical distribution over inventory sizes (shown in <ref type="figure" target="#fig_3">Figure 5)</ref> shows that the mode is actually 5 and small inventories are uncommon: no 1-vowel in- ventory is attested and only one 2-vowel inventory is known. A probabilistic model over all vowel systems must implicitly model the size of the sys- tem. Indeed, our models pit all potential inventories against each other, bestowing the extra burden to match the empirical distribution over size.</p><p>Frequency of Inventories. Another problem is the inability to model frequency. While for inven- tories of a modest size (3-5 vowels) there are very few unique attested systems, there is a plethora of attested larger vowel systems. The energy min- imization paradigm has no principled manner to tell the scientist how likely a novel system may be. Appealing again to the empirical distribution over attested vowel systems, we consider the relative diversity of systems of each size. We graph this in <ref type="figure" target="#fig_3">Figure 5</ref>. Consider all vowel systems of size 7. There are |V| 7 potential inventories, yet the empir- ical distribution is remarkably peaked. Our proba- bilistic models have the advantage in this context as well, as they naturally quantify the likelihood of an individual inventory.</p><p>Typology is a Small-Data Problem. In contrast to many common problems in applied NLP, e.g., part-of-speech tagging, parsing and machine trans- lation, the modeling of linguistic typology is fun- damentally a "small-data" problem. Out of the 7105 languages on earth, we only have linguistic annotation for 2600 of them ( <ref type="bibr" target="#b5">Comrie et al., 2013</ref>). Moreover, we only have phonetic and phonological annotation for a much smaller set of languages- between 300-500 <ref type="bibr" target="#b24">(Maddieson, 2013)</ref>. Given the paucity of data, overfitting on only those attested languages is a dangerous possibility-just because a certain inventory has never been attested, it is probably wrong to conclude that it is impossible- or even improbable-on that basis alone. By anal- ogy to language modeling, almost all sentences observed in practice are novel with respect to the training data, but we still must employ a princi- pled manner to discriminate high-probability sen- tences (which are syntactically and semantically coherent) from low-probability ones. Probabilistic modeling provides a natural paradigm for this sort of investigation-machine learning has developed well-understood smoothing techniques, e.g., reg- ularization with tuning on a held-out dev set, to avoid overfitting in a small-data scenario. apply a log-Gaussian point process, a variant of the Poisson point process, to rumor detection in Twitter. We are unaware of previous attempts to probabilistically model vowel inventory typology.</p><p>Future Work. This work lends itself to sev- eral technical extensions. One could expand the function f to more completely characterize each vowel's acoustic properties, perceptual properties, or distinctive features (footnote 7). One could gen- eralize our point process models to sample finite subsets from the continuous space of vowels (foot- note 3). One could consider augmenting the MPP with a new factor that explicitly controls the size of the vowel inventory. Richer families of point processes might also be worth exploring. For ex- ample, perhaps the vowel inventory is generated by some temporal mechanism with latent intermediate steps, such as sequential selection of the vowels or evolutionary drift of the inventory. Another possi- bility is that vowel systems tend to reuse distinctive features or even follow factorial designs, so that an inventory with creaky front vowels also tends to have creaky back vowels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>We have presented a series of point process models for the modeling of vowel system inventory typol- ogy with the goal of a mathematical grounding for research in phonological typology. All mod- els were additionally given a deep parameteriza- tion to learn representations similar to perceptual space in cognitive science. Also, we motivated our preference for probabilistic modeling in linguistic typology over previously proposed computational approaches and argued it is a more natural research paradigm. Additionally, we have introduced sev- eral novel evaluation metrics for research in vowel- system typology, which we hope will spark further interest in the area. Their performance was empiri- cally validated on the Becker-Kristal corpus, which includes data from over 200 languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The standard vowel table in IPA for the RP accent of English. The x-axis indicates the front-back spectrum and the y-axis indicates the high-low distinction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example spectrogram of the three English vowels: [i], [u] and [A]. The x-axis is time and y-axis is frequency. The first two formants F1 and F2 are marked in with colored arrows for each vowel. We used the Praat toolkit to generate the spectrogram and find the formants (Boersma et al., 2002).</figDesc><graphic url="image-2.png" coords="2,406.03,68.70,55.02,94.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Related Work in NLP.Figure 4 :</head><label>4</label><figDesc>Figure 4: Percentage of the vowel inventories (y-axis) in the Becker-Kristal corpus (Becker-Kristal, 2010) that have a given vowel (shown in IPA along the x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Histogram of the sizes of different vowel inventories in the corpus. The x-axis is the size of the vowel inventory and the y-axis is the number of inventories with that size.</figDesc></figure>

			<note place="foot" n="1"> We assume in this paper that the metric space is universal-although it would not be unreasonable to suppose that each language&apos;s vowel system has adapted to avoid confusion in the specific communicative environment of its speakers.</note>

			<note place="foot" n="4"> Most DPPs are L-ensembles (Kulesza and Taskar, 2012).</note>

			<note place="foot" n="7"> In lieu of higher formants, we could have extended the vector f (vi) to encode the binary distinctive features of the IPA vowel vi: round, tense, long, nasal, creaky, etc.</note>

			<note place="foot" n="8"> Provided that our nonlinearity in (7) is a differentiable invertible function like tanh rather than relu.</note>

			<note place="foot" n="9"> Computing cross-entropy exactly is intractable with the MPP, so we resort to an unbiased importance sampling scheme where we draw samples from the BPP and reweight according to the MPP (Liu et al., 2015).</note>

			<note place="foot" n="10"> This could happen because learners have evolved to expect the languages (the Baldwin effect), or because the languages have evolved to be easily learned (universal grammar).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The first author was funded by an NDSEG graduate fellowship, and the second author by NSF grant IIS-1423276. We would like to thank Tim Vieira and Huda Khayrallah for helpful initial feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning algorithm for Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Markov determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Hafiz Affandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Acoustic Typology of Vowel Inventories and Dispersion Theory: Insights from a Large Cross-Linguistic Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Becker-Kristal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, UCLA</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Praat, a system for doing phonetics by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulus Petrus Gerardus</forename><surname>Boersma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glot International</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EynardMehta theorem, Schur process, and their Pfaffian analogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Rains</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="291" to="317" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Comrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Introduction</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The World Atlas of Language Structures Online, Max Planck Institute for Evolutionary Anthropology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
		<ptr target="http://wals.info/chapter/s1" />
		<imprint>
			<pubPlace>Leipzig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The computational complexity of probabilistic inference using Bayesian belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Low-rank factorization of determinantal point processes pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gartrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1912" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Phonological Typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">K</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and relearning in Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<editor>David E. Rumelhart and James L. McClelland</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="282" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beitrag zur theorie des ferromagnetismus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Ising</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Physik A Hadrons and Nuclei</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="258" />
			<date type="published" when="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Kindersprache, Aphasie und allgemeine Lautgesetze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Jakobson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1941" />
		</imprint>
	</monogr>
	<note>Suhrkamp Frankfurt aM</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="419" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Determinantal point processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="123" to="286" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Course in Phonetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ladefoged</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Centage</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Sounds of the World&apos;s Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ladefoged</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Maddieson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Numerical simulation of vowel quality systems: The role of perceptual contrast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Liljencrants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Lindblom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="839" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phonetic universals in vowel systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Lindblom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Phonology</title>
		<imprint>
			<biblScope unit="page" from="13" to="44" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating the partition function by discriminance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">T</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Point process modelling of rumour dynamics in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The coincidence approach to stochastic point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odile</forename><surname>Macchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="page" from="83" to="122" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vowel quality inventories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Maddieson</surname></persName>
		</author>
		<ptr target="http://wals.info/chapter/2" />
	</analytic>
	<monogr>
		<title level="m">The World Atlas of Language Structures Online, Max Planck Institute for Evolutionary Anthropology</title>
		<editor>Matthew S. Dryer and Martin Haspelmath</editor>
		<meeting><address><addrLine>Leipzig</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mccloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wright</surname></persName>
		</author>
		<title level="m">PHOIBLE online. Leipzig: Max Planck Institute for Evolutionary Anthropology</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparison of several proposed perceptual representations of vowel spectra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">M</forename><surname>Nearey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kiefte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the XV th International Congress of Phonetic Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1005" to="1008" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved lexical acquisition through DPP-based verb clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="862" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Explaining vowel inventory tendencies via simulation: Finding a role for quantal locations and formant normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>North East Linguistic Society</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="419" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monte Carlo Statistical Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Vowel Perception and Production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Rosner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pickering</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The dispersionfocalization theory of vowel systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Jean</forename><surname>Boë</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Vallée</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Abry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="286" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward a universal law of generalization for psychological science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">4820</biblScope>
			<biblScope unit="page" from="1317" to="1323" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The quantal nature of speech: Evidence from articulatory-acoustic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">N</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Communication: A Unified View</title>
		<editor>E. E. David and P. B. Denes</editor>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1972" />
			<biblScope unit="page" from="51" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the quantal nature of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3" to="45" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cloze procedure: a new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism and Mass Communication Quarterly</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">415</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Markov Point Processes and Their Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N M</forename><surname>Van Lieshout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Imperial College Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An Introduction to Linguistic Typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viveka</forename><surname>Velupillai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Benjamins Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling and characterizing social media topics using the gamma distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connie</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EVENTS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="117" to="122" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
