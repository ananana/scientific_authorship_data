<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>MOE</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1918" to="1927"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1918</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically , we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the base-line by a large margin and achieves the state-of-the-art performance on the En-glish MS-MARCO dataset and the Chi-nese DuReader dataset, both of which are designed for MRC in real-world settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine reading comprehension (MRC), empow- ering computers with the ability to acquire knowl- edge and answer questions from textual data, is believed to be a crucial step in building a general intelligent agent <ref type="bibr" target="#b0">(Chen et al., 2016)</ref>. Recent years have seen rapid growth in the MRC community. With the release of various datasets, the MRC task has evolved from the early cloze-style test <ref type="bibr" target="#b3">(Hermann et al., 2015;</ref><ref type="bibr" target="#b4">Hill et al., 2015)</ref> to answer ex- traction from a single passage ( <ref type="bibr">Rajpurkar et al.,</ref> * This work was done while the first author was doing in- ternship at <ref type="bibr">Baidu Inc.</ref> 2016) and to the latest more complex question an- swering on web data <ref type="bibr" target="#b10">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b1">Dunn et al., 2017;</ref><ref type="bibr" target="#b2">He et al., 2017)</ref>.</p><p>Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage ( <ref type="bibr" target="#b20">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b16">Seo et al., 2016;</ref><ref type="bibr" target="#b11">Pan et al., 2017)</ref>. A significant milestone is that several MRC mod- els have exceeded the performance of human an- notators on the SQuAD dataset 1 ( <ref type="bibr" target="#b14">Rajpurkar et al., 2016</ref>). However, this success on single Wikipedia passage is still not adequate, considering the ulti- mate goal of reading the whole web. Therefore, several latest datasets <ref type="bibr" target="#b10">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b2">He et al., 2017;</ref><ref type="bibr" target="#b1">Dunn et al., 2017</ref>) attempt to design the MRC tasks in more realistic settings by involv- ing search engines. For each question, they use the search engine to retrieve multiple passages and the MRC models are required to read these passages in order to give the final answer.</p><p>One of the intrinsic challenges for such multi- passage MRC is that since all the passages are question-related but usually independently writ- ten, it's probable that multiple confusing answer candidates (correct or incorrect) exist. <ref type="table">Table 1</ref> shows an example from MS-MARCO. We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect. As is shown by <ref type="bibr" target="#b5">Jia and Liang (2017)</ref>, these confus- ing answer candidates could be quite difficult for MRC models to distinguish. Therefore, special consideration is required for such multi-passage MRC problem.</p><p>In this paper, we propose to leverage the an- swer candidates from different passages to verify the final correct answer and rule out the noisy in- correct answers. Our hypothesis is that the cor-Question: What is the difference between a mixed and pure culture? Passages: <ref type="bibr">[1]</ref> A culture is a society's total way of living and a society is a group that live in a defined territory and participate in common culture. While the answer given is in essence true, societies originally form for the express purpose to enhance . . . <ref type="bibr">[2]</ref> . . . There has been resurgence in the economic system known as capitalism during the past two decades. 4. The mixed economy is a balance between socialism and capitalism. As a result, some institutions are owned and maintained by . . . <ref type="bibr">[3]</ref> A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies. Culture on the other hand, is the lifestyle that the people in the country . . .</p><p>[4] Best Answer: A pure culture comprises a single species or strains. A mixed culture is taken from a source and may contain multiple strains or species. A contaminated culture contains organisms that derived from some place . . . <ref type="bibr">[5]</ref> . . . It will be at that time when we can truly obtain a pure culture. A pure culture is a culture consisting of only one strain. You can obtain a pure culture by picking out a small portion of the mixed culture . . . <ref type="bibr">[6]</ref> A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies. A pure culture is a culture consisting of only one strain. . . . · · · · · · Reference Answer: A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies. <ref type="table">Table 1</ref>: An example from MS-MARCO. The text in bold is the predicted answer candidate from each passage according to the boundary model. The candidate from <ref type="bibr">[1]</ref> is chosen as the final answer by this model, while the correct answer is from <ref type="bibr">[6]</ref> and can be verified by the answers from <ref type="bibr">[3]</ref>, <ref type="bibr">[4]</ref>, <ref type="bibr">[5]</ref>.</p><p>rect answers could occur more frequently in those passages and usually share some commonalities, while incorrect answers are usually different from one another. The example in <ref type="table">Table 1</ref> demonstrates this phenomenon. We can see that the answer can- didates extracted from the last four passages are all valid answers to the question and they are semanti- cally similar to each other, while the answer candi- dates from the other two passages are incorrect and there is no supportive information from other pas- sages. As human beings usually compare the an- swer candidates from different sources to deduce the final answer, we hope that MRC model can also benefit from the cross-passage answer veri- fication process.</p><p>The overall framework of our model is demon- strated in <ref type="figure" target="#fig_1">Figure 1</ref> , which consists of three mod- ules. First, we follow the boundary-based MRC models ( <ref type="bibr" target="#b16">Seo et al., 2016;</ref><ref type="bibr" target="#b20">Wang and Jiang, 2016)</ref> to find an answer candidate for each passage by identifying the start and end position of the an- swer <ref type="figure">(Figure 2)</ref>. Second, we model the mean- ings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second per- spective. Third, we conduct the answer verifica- tion by enabling each answer candidate to attend to the other candidates based on their represen- tations. We hope that the answer candidates can collect supportive information from each other ac- cording to their semantic similarities and further decide whether each candidate is correct or not.</p><p>Therefore, the final answer is determined by three factors: the boundary, the content and the cross- passage answer verification. The three steps are modeled using different modules, which can be jointly trained in our end-to-end framework.</p><p>We conduct extensive experiments on the MS- MARCO ( <ref type="bibr" target="#b10">Nguyen et al., 2016)</ref> and <ref type="bibr">DuReader (He et al., 2017</ref>) datasets. The results show that our answer verification MRC model outperforms the baseline models by a large margin and achieves the state-of-the-art performance on both datasets. <ref type="figure" target="#fig_1">Figure 1</ref> gives an overview of our multi-passage MRC model which is mainly composed of three modules including answer boundary prediction, answer content modeling and answer verification. First of all, we need to model the question and passages. Following <ref type="bibr" target="#b16">Seo et al. (2016)</ref>, we com- pute the question-aware representation for each passage (Section 2.1). Based on this representa- tion, we employ a Pointer Network ( <ref type="bibr" target="#b19">Vinyals et al., 2015)</ref> to predict the start and end position of the answer in the module of answer boundary predic- tion (Section 2.2). At the same time, with the answer content model (Section 2.3), we estimate whether each word should be included in the an- swer and thus obtain the answer representations. Next, in the answer verification module (Section 2.4), each answer candidate can attend to the other answer candidates to collect supportive informa- tion and we compute one score for each candidate   to indicate whether it is correct or not according to the verification. The final answer is determined by not only the boundary but also the answer content and its verification score (Section 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question and Passage Modeling</head><p>Given a question Q and a set of passages {P i } re- trieved by search engines, our task is to find the best concise answer to the question. First, we for- mally present the details of modeling the question and passages.</p><p>Encoding We first map each word into the vec- tor space by concatenating its word embedding and sum of its character embeddings. Then we employ bi-directional LSTMs (BiLSTM) to en- code the question Q and passages {P i } as follows:</p><p>Q-P Matching One essential step in MRC is to match the question with passages so that impor- tant information can be highlighted. We use the Attention Flow Layer ( <ref type="bibr" target="#b16">Seo et al., 2016</ref>) to conduct the Q-P matching in two directions. The similar- ity matrix S ∈ R |Q|×|P i | between the question and passage i is changed to a simpler version, where the similarity between the t th word in the question and the k th word in passage i is computed as:</p><formula xml:id="formula_0">S t,k = u Q t · u P i k (3)</formula><p>Then the context-to-question attention and question-to-context attention is applied strictly following <ref type="bibr" target="#b16">Seo et al. (2016)</ref> to obtain the question- aware passage representation {˜u{˜u P i t }. We do not give the details here due to space limitation. Next, another BiLSTM is applied in order to fuse the contextual information and get the new represen- tation for each word in the passage, which is re- garded as the match output:</p><formula xml:id="formula_1">v P i t = BiLSTM M (v P i t−1 , ˜ u P i t )<label>(4)</label></formula><p>Based on the passage representations, we intro- duce the three main modules of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer Boundary Prediction</head><p>To extract the answer span from passages, main- stream studies try to locate the boundary of the an- swer, which is called boundary model. Following ( <ref type="bibr" target="#b20">Wang and Jiang, 2016)</ref>, we employ Pointer Net- work ( <ref type="bibr" target="#b19">Vinyals et al., 2015</ref>) to compute the proba- bility of each word to be the start or end position of the span:</p><formula xml:id="formula_2">g t k = w a 1 tanh(W a 2 [v P k , h a t−1 ])<label>(5)</label></formula><formula xml:id="formula_3">α t k = exp(g t k )/ |P| j=1 exp(g t j ) (6) c t = |P| k=1 α t k v P k (7) h a t = LSTM(h a t−1 , c t )<label>(8)</label></formula><p>By utilizing the attention weights, the probabil- ity of the k th word in the passage to be the start and end position of the answer is obtained as α 1 k and α 2 k . It should be noted that the pointer network is applied to the concatenation of all passages, which is denoted as P so that the probabilities are com- parable across passages. This boundary model can be trained by minimizing the negative log proba- bilities of the true start and end indices:</p><formula xml:id="formula_4">L boundary = − 1 N N i=1</formula><p>(log α 1</p><formula xml:id="formula_5">y 1 i + log α 2 y 2 i ) (9)</formula><p>where N is the number of samples in the dataset and y 1 i , y 2 i are the gold start and end positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Content Modeling</head><p>Previous work employs the boundary model to find the text span with the maximum boundary score as the final answer. However, in our context, besides locating the answer candidates, we also need to model their meanings in order to conduct the verification. An intuitive method is to compute the representation of the answer candidates sepa- rately after extracting them, but it could be hard to train such model end-to-end. Here, we propose a novel method that can obtain the representation of the answer candidates based on probabilities. Specifically, we change the output layer of the classic MRC model. Besides predicting the boundary probabilities for the words in the pas- sages, we also predict whether each word should be included in the content of the answer. The con- tent probability of the k th word is computed as:</p><formula xml:id="formula_6">p c k = sigmoid(w c 1 ReLU(W c 2 v P i k ))<label>(10)</label></formula><p>Training this content model is also quite intu- itive. We transform the boundary labels into a con- tinuous segment, which means the words within the answer span will be labeled as 1 and other words will be labeled as 0. In this way, we define the loss function as the averaged cross entropy:</p><formula xml:id="formula_7">L content = − 1 N 1 |P| N i=1 |P | j=1 [y c k log p c k + (1 − y c k ) log(1 − p c k )]<label>(11)</label></formula><p>The content probabilities provide another view to measure the quality of the answer in addition to the boundary. Moreover, with these probabilities, we can represent the answer from passage i as a weighted sum of all the word embeddings in this passage:</p><formula xml:id="formula_8">r A i = 1 |P i | |P i | k=1 p c k [e P i k , c P i k ]<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cross-Passage Answer Verification</head><p>The boundary model and the content model focus on extracting and modeling the answer within a single passage respectively, with little considera- tion of the cross-passage information. However, as is discussed in Section 1, there could be mul- tiple answer candidates from different passages and some of them may mislead the MRC model to make an incorrect prediction. It's necessary to aggregate the information from different passages and choose the best one from those candidates. Therefore, we propose a method to enable the an- swer candidates to exchange information and ver- ify each other through the cross-passage answer verification process. Given the representation of the answer candi- dates from all passages {r A i }, each answer can- didate then attends to other candidates to collect supportive information via attention mechanism:</p><formula xml:id="formula_9">s i,j = 0, if i = j, r A i · r A j , otherwise<label>(13)</label></formula><formula xml:id="formula_10">α i,j = exp(s i,j )/ n k=1 exp(s i,k )<label>(14)</label></formula><p>˜ r</p><formula xml:id="formula_11">A i = n j=1 α i,j r A j<label>(15)</label></formula><p>Here˜rHere˜Here˜r A i is the collected verification informa- tion from other passages based on the attention weights. Then we pass it together with the orig- inal representation r A i to a fully connected layer:</p><formula xml:id="formula_12">g v i = w v [r A i , ˜ r A i , r A i ˜ r A i ]<label>(16)</label></formula><p>We further normalize these scores over all pas- sages to get the verification score for answer can- didate A i :</p><formula xml:id="formula_13">p v i = exp(g v i )/ n j=1 exp(g v j )<label>(17)</label></formula><p>In order to train this verification model, we take the answer from the gold passage as the gold an- swer. And the loss function can be formulated as the negative log probability of the correct answer:</p><formula xml:id="formula_14">L verif y = − 1 N N i=1 log p v y v i<label>(18)</label></formula><p>where y v i is the index of the correct answer in all the answer candidates of the i th instance .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Joint Training and Prediction</head><p>As is described above, we define three objectives for the reading comprehension model over multi- ple passages: 1. finding the boundary of the an- swer; 2. predicting whether each word should be included in the content; 3. selecting the best an- swer via cross-passage answer verification. Ac- cording to our design, these three tasks can share the same embedding, encoding and matching lay- ers. Therefore, we propose to train them together as multi-task learning <ref type="bibr" target="#b15">(Ruder, 2017)</ref>. The joint ob- jective function is formulated as follows:</p><formula xml:id="formula_15">L = L boundary + β 1 L content + β 2 L verif y (19)</formula><p>where β 1 and β 2 are two hyper-parameters that control the weights of those tasks.</p><p>When predicting the final answer, we take the boundary score, content score and verification score into consideration. We first extract the an- swer candidate A i that has the maximum boundary score from each passage i. This boundary score is computed as the product of the start and end prob- ability of the answer span. Then for each answer candidate A i , we average the content probabilities of all its words as the content score of A i . And we can also predict the verification score for A i using the verification model. Therefore, the final answer can be selected from all the answer candidates ac- cording to the product of these three scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To verify the effectiveness of our model on multi- passage machine reading comprehension, we con- duct experiments on the MS-MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017) datasets. Our method achieves the state-of-the-art performance on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We choose the MS-MARCO and DuReader datasets to test our method, since both of them are MS-MARCO DuReader Multiple Answers 9.93% 67.28% Multiple Spans 40.00% 56.38% One prerequisite for answer verification is that there should be multiple correct answers so that they can verify each other. Both the MS-MARCO and DuReader datasets require the human annota- tors to generate multiple answers if possible. Ta- ble 2 shows the proportion of questions that have multiple answers. However, the same answer that occurs many times is treated as one single an- swer here. Therefore, we also report the propor- tion of questions that have multiple answer spans to match with the human-generated answers. A span is taken as valid if it can achieve F1 score larger than 0.7 compared with any reference an- swer. From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader. These an- swers will provide strong signals for answer veri- fication if we can leverage them properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP ( ) and we choose the span that achieves the highest ROUGE-L score with the ref- erence answers as the gold span for training. We employ the 300-D pre-trained Glove embeddings ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>) and keep it fixed dur- ing training. The character embeddings are ran- domly initialized with its dimension as 30. For DuReader, we follow the preprocessing described in <ref type="bibr" target="#b2">He et al. (2017)</ref>.</p><p>We tune the hyper-parameters according to the Model ROUGE-L BLEU-1 FastQA Ext ( <ref type="bibr" target="#b25">Weissenborn et al., 2017)</ref> 33.67 33.93 Prediction ( <ref type="bibr" target="#b20">Wang and Jiang, 2016)</ref> 37.33 <ref type="bibr">40.72 ReasoNet (Shen et al., 2017)</ref> 38.81 39.86 R-Net ( <ref type="bibr" target="#b24">Wang et al., 2017c)</ref> 42.89 42.22 S-Net ( <ref type="bibr" target="#b18">Tan et al., 2017)</ref> 45  validation performance on the MS-MARCO de- velopment set. The hidden size is set to be 150 and we apply L2 regularization with its weight as 0.0003. The task weights β 1 , β 2 are both set to be 0.5. To train our model, we employ the Adam algorithm ( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>) with the initial learning rate as 0.0004 and the mini-batch size as 32. Exponential moving average is applied on all trainable variables with a decay rate 0.9999.</p><p>Two simple yet effective technologies are em- ployed to improve the final performance on these two datasets respectively. For MS-MARCO, ap- proximately 8% questions have the answers as Yes or No, which usually cannot be solved by ex- tractive approach <ref type="bibr" target="#b18">(Tan et al., 2017)</ref>. We address this problem by training a simple Yes/No classi- fier for those questions with certain patterns (e.g., starting with "is"). Concretely, we simply change the output layer of the basic boundary model so that it can predict whether the answer is "Yes" or "No". For DuReader, the retrieved document usually contains a large number of paragraphs that cannot be fed into MRC models directly ( <ref type="bibr" target="#b2">He et al., 2017)</ref>. The original paper employs a simple a simple heuristic strategy to select a representative paragraph for each document, while we train a paragraph ranking model for this. We will demon- strate the effects of these two technologies later. <ref type="table" target="#tab_3">Table 3</ref> shows the results of our system and other state-of-the-art models on the MS-MARCO test set. We adopt the official evaluation metrics, in- cluding ROUGE-L ( <ref type="bibr" target="#b8">Lin, 2004</ref>) and BLEU-1 ( <ref type="bibr" target="#b12">Papineni et al., 2002</ref>). As we can see, for both met- rics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human per-   <ref type="table">Table 5</ref>: Ablation study on MS-MARCO develop- ment set formance. If we ensemble the models trained with different random seeds and hyper-parameters, the results can be further improved and outperform the ensemble model in <ref type="bibr" target="#b18">Tan et al. (2017)</ref>, especially in terms of the BLEU-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on MS-MARCO</head><formula xml:id="formula_16">Model BLEU-4 ROUGE-L Match</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on DuReader</head><p>The results of our model and several baseline sys- tems on the test set of DuReader are shown in <ref type="table" target="#tab_5">Table 4</ref>. The BiDAF and Match-LSTM models are provided as two baseline systems ( <ref type="bibr" target="#b2">He et al., 2017)</ref>. Based on BiDAF, as is described in Section 3.2, we tried a new paragraph selection strategy by employing a paragraph ranking (PR) model. We can see that this paragraph ranking can boost the BiDAF baseline significantly. Finally, we im- plement our system based on this new strategy, and our system (single model) achieves further im- provement by a large margin.</p><p>Question: What is the difference between a mixed and pure culture Scores Answer Candidates: Boundary Content Verification <ref type="bibr">[1]</ref> A culture is a society's total way of living and a society is a group . . .</p><formula xml:id="formula_17">1.0 × 10 −2 1.0 × 10 −1 1.1 × 10 −1 [2]</formula><p>The mixed economy is a balance between socialism and capitalism.</p><p>1.0 × 10 −4 4.0 × 10 −2 3.2 × 10 −2 <ref type="bibr">[3]</ref> A pure culture is one in which only one kind of microbial species is . . . 5.5 × 10 −3 7.7 × 10 −2 1.2 × 10 −1 <ref type="bibr">[4]</ref> A pure culture comprises a single species or strains. A mixed . . .</p><p>2.7 × 10 −3 8.1 × 10 −2 1.3 × 10 −1 <ref type="bibr">[5]</ref> A pure culture is a culture consisting of only one strain.</p><p>5.8 × 10 −4 7.9 × 10 −2 5.1 × 10 −2 <ref type="bibr">[6]</ref> A pure culture is one in which only one kind of microbial species . . . 5.8 × 10 −3 9.1 × 10 −2 2.7 × 10 −1 . . . . . .</p><p>. . . . . . <ref type="table">Table 6</ref>: Scores predicted by our model for the answer candidates shown in <ref type="table">Table 1</ref>. Although the candidate <ref type="bibr">[1]</ref> gets high boundary and content scores, the correct answer <ref type="bibr">[6]</ref> is preferred by the verification model and is chosen as the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head><p>To get better insight into our system, we conduct in-depth ablation study on the development set of MS-MARCO, which is shown in <ref type="table">Table 5</ref>. Fol- lowing <ref type="bibr" target="#b18">Tan et al. (2017)</ref>, we mainly focus on the ROUGE-L score that is averaged case by case. We first evaluate the answer verification by ab- lating the cross-passage verification model so that the verification loss and verification score will not be used during training and testing. Then we re- move the content model in order to test the ne- cessity of modeling the content of the answer. Since we don't have the content scores, we use the boundary probabilities instead to compute the an- swer representation for verification. Next, to show the benefits of joint training, we train the bound- ary model separately from the other two models. Finally, we remove the yes/no classification in or- der to show the real improvement of our end-to- end model compared with the baseline method that predicts the answer with only the boundary model.</p><p>From <ref type="table">Table 5</ref>, we can see that the answer ver- ification makes a great contribution to the overall improvement, which confirms our hypothesis that cross-passage answer verification is useful for the multi-passage MRC. For the ablation of the con- tent model, we analyze that it will not only af- fect the content score itself, but also violate the verification model since the content probabilities are necessary for the answer representation, which will be further analyzed in Section 4.3. Another discovery is that jointly training the three mod- els can provide great benefits, which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers. At last, comparing our method with the baseline, we achieve an improvement of nearly 3 points without the yes/no classification. This significant improvement proves the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case Study</head><p>To demonstrate how each module of our model takes effect when predicting the final answer, we conduct a case study in <ref type="table">Table 6</ref> with the same ex- ample that we discussed in Section 1. For each answer candidate, we list three scores predicted by the boundary model, content model and veri- fication model respectively.</p><p>On the one hand, we can see that these three scores generally have some relevance. For exam- ple, the second candidate is given lowest scores by all the three models. We analyze that this is because the models share the same encoding and matching layers at bottom level and this relevance guarantees that the content and verification mod- els will not violate the boundary model too much. On the other hand, we also see that the verifica- tion score can really make a difference here when the boundary model makes an incorrect decision among the confusing answer candidates ( <ref type="bibr">[1]</ref>, <ref type="bibr">[3]</ref>, <ref type="bibr">[4]</ref>, <ref type="bibr">[6]</ref>). Besides, as we expected, the verifica- tion model tends to give higher scores for those an- swers that have semantic commonality with each other ( <ref type="bibr">[3]</ref>, <ref type="bibr">[4]</ref>, <ref type="bibr">[6]</ref>), which are all valid answers in this case. By multiplying the three scores, our model finally predicts the answer correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Necessity of the Content Model</head><p>In our model, we compute the answer representa- tion based on the content probabilities predicted by a separate content model instead of directly us- ing the boundary probabilities. We argue that this content model is necessary for our answer verifica- tion process. <ref type="figure">Figure 2</ref> plots the predicted content probabilities as well as the boundary probabilities  <ref type="table" target="#tab_0">The  noun  charge  unit  has  1  sense  :  1  .  a  measure  of  the  quantity  of  electricity  -LRB- determined  by  the  amount  of  an  electric  current  and  the  time  for  which  it  flows  -RRB- .  familiarity  info  :  charge  unit  used  as  a  noun  is  very  rare  .</ref> start probability end probability content probability <ref type="figure">Figure 2</ref>: The boundary probabilities and content probabilities for the words in a passage for a passage. We can see that the boundary and content probabilities capture different aspects of the answer. Since answer candidates usually have similar boundary words, if we compute the an- swer representation based on the boundary prob- abilities, it's difficult to model the real difference among different answer candidates. On the con- trary, with the content probabilities, we pay more attention to the content part of the answer, which can provide more distinguishable information for verifying the correct answer. Furthermore, the content probabilities can also adjust the weights of the words within the answer span so that unimpor- tant words (e.g. "and" and ".") get lower weights in the final answer representation. We believe that this refined representation is also good for the an- swer verification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Machine reading comprehension made rapid progress in recent years, especially for single- passage MRC task, such as SQuAD ( <ref type="bibr" target="#b14">Rajpurkar et al., 2016)</ref>. Mainstream studies ( <ref type="bibr" target="#b16">Seo et al., 2016;</ref><ref type="bibr" target="#b20">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b26">Xiong et al., 2016</ref>) treat reading comprehension as extracting answer span from the given passage, which is usually achieved by predicting the start and end position of the an- swer. We implement our boundary model sim- ilarly by employing the boundary-based pointer network ( <ref type="bibr" target="#b20">Wang and Jiang, 2016)</ref>. Another inspir- ing work is from <ref type="bibr" target="#b24">Wang et al. (2017c)</ref>, where the authors propose to match the passage against it- self so that the representation can aggregate evi- dence from the whole passage. Our verification model adopts a similar idea. However, we collect information across passages and our attention is based on the answer representation, which is much more efficient than attention over all passages. For the model training, <ref type="bibr" target="#b27">Xiong et al. (2017)</ref> argues that the boundary loss encourages exact answers at the cost of penalizing overlapping answers. There- fore they propose a mixed objective that incorpo- rates rewards derived from word overlap. Our joint training approach has a similar function. By tak- ing the content and verification loss into consid- eration, our model will give less loss for overlap- ping answers than those unmatched answers, and our loss function is totally differentiable.</p><p>Recently, we also see emerging interests in multi-passage MRC from both the academic ( <ref type="bibr" target="#b1">Dunn et al., 2017;</ref><ref type="bibr" target="#b6">Joshi et al., 2017)</ref> and indus- trial community <ref type="bibr" target="#b10">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b2">He et al., 2017)</ref>. Early studies <ref type="bibr" target="#b17">(Shen et al., 2017;</ref><ref type="bibr" target="#b24">Wang et al., 2017c</ref>) usually concat those passages and employ the same models designed for single- passage MRC. However, more and more latest studies start to design specific methods that can read multiple passages more effectively. In the as- pect of passage selection, <ref type="bibr" target="#b21">Wang et al. (2017a)</ref> in- troduced a pipelined approach that rank the pas- sages first and then read the selected passages for answering questions. <ref type="bibr" target="#b18">Tan et al. (2017)</ref> treats the passage ranking as an auxiliary task that can be trained jointly with the reading comprehension model. Actually, the target of our answer verifi- cation is very similar to that of the passage se- lection, while we pay more attention to the an- swer content and the answer verification process. Speaking of the answer verification, <ref type="bibr" target="#b23">Wang et al. (2017b)</ref> has a similar motivation to ours. They attempt to aggregate the evidence from different passages and choose the final answer from n-best candidates. However, they implement their idea as a separate reranking step after reading comprehen- sion, while our answer verification is a component of the whole model that can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an end-to-end frame- work to tackle the multi-passage MRC task . We creatively design three different modules in our model, which can find the answer boundary, model the answer content and conduct cross-passage an- swer verification respectively. All these three modules can be trained with different forms of the answer labels and training them jointly can pro- vide further improvement. The experimental re- sults demonstrate that our model outperforms the baseline models by a large margin and achieves the state-of-the-art performance on two challeng- ing datasets, both of which are designed for MRC on real web data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Encoding</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our method for multi-passage machine reading comprehension</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Q -P Matching</head><label>Q</label><figDesc></figDesc><table>Answer Boundary 
Prediction 

Answer Content 
Modeling 

Question 

í µí± " 

Passage 1 

í µí± # $ 

í µí± # $ 

í µí±(í µí± í µí±¡í µí±í µí±í µí±¡) í µí±(í µí±í µí±í µí±) 

í µí±(í µí±í µí±í µí±í µí±¡í µí±í µí±í µí±¡ ) 

Answer í µí°´3µí°´3 

⊕ 

weighted 
sum 

í µí± 5 $ 

Passage 2 

í µí± # 6 

í µí± # 6 

í µí±(í µí± í µí±¡í µí±í µí±í µí±¡) í µí±(í µí±í µí±í µí±) 

í µí±(í µí±í µí±í µí±í µí±¡í µí±í µí±í µí±¡ ) 

Answer í µí°´7µí°´7 

⊕ 

weighted 
sum 

í µí± 5 6 

Passage n 

í µí± # 8 

í µí± # 8 

í µí±(í µí± í µí±¡í µí±í µí±í µí±¡) í µí±(í µí±í µí±í µí±) 

í µí±(í µí±í µí±í µí±í µí±¡í µí±í µí±í µí±¡ ) 

Answer í µí°´:µí°´: 

⊕ 

weighted 
sum 

í µí± 5 8 

... 

Answer Verification 

í µí± 5 $ í µí±̃ 5 $ 
í µí± 5 6 í µí±̃ 5 6 
í µí± 5 8 í µí±̃ 5 8 

⊕ 

Score 1 
Score 2 
Score 3 

Attention 

Final 
Answer 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Percentage of questions that have multi- ple valid answers or answer spans</head><label>2</label><figDesc></figDesc><table>designed from real-world search engines and in-
volve a large number of passages retrieved from 
the web. One difference of these two datasets is 
that MS-MARCO mainly focuses on the English 
web data, while DuReader is designed for Chinese 
MRC. This diversity is expected to reflect the gen-
erality of our method. In terms of the data size, 
MS-MARCO contains 102023 questions, each of 
which is paired up with approximately 10 passages 
for reading comprehension. As for DuReader, it 
keeps the top-5 search results for each question 
and there are totally 201574 questions. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Performance of our method and competing models on the MS-MARCO test set</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Performance on the DuReader test set</head><label>4</label><figDesc></figDesc><table>Model 
ROUGE-L 
∆ 
Complete Model 
45.65 
-
Answer Verification 
44.38 
-1.27 
Content Modeling 
44.27 
-1.38 
Joint Training 
44.12 
-1.53 
YesNo Classification 
41.87 
-3.78 
Boundary Baseline 
38.95 
-6.70 

</table></figure>

			<note place="foot" n="1"> https://rajpurkar.github.io/SQuAD-explorer/</note>

			<note place="foot">u Q t = BiLSTM Q (u Q t−1 , [e Q t , c Q t ]) (1) u P i t = BiLSTM P (u P i t−1 , [e P i t , c P i t ]) (2) where e Q t , c Q t , e P i t , c P i t are the word-level and character-level embeddings of the t th word. u Q t and u P i t are the encoding vectors of the t th words in Q and P i respectively. Unlike previous work (Wang et al., 2017c) that simply concatenates all the passages, we process the passages independently at the encoding and matching steps.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Ba-sic Research Program of China (973 program, No. 2014CB340505) and Baidu-Peking Univer-sity Joint Project. We thank the Microsoft MS-MARCO team for evaluating our results on the anonymous test set. We also thank Ying Chen, Xuan Liu and the anonymous reviewers for their constructive criticism of the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>Au- gust 7-12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Ugur</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dureader: a chinese machine reading comprehension dataset from real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05073</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark, September 9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Memen: Multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Boyuan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Bin Cao, Deng Cai, and Xiaofei He</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Philadelphia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002-07-06" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<title level="m">An overview of multi-task learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-13" />
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">S-net: From answer extraction to answer generation for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04815</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R$ˆ3$</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00023</idno>
		<title level="m">Reinforced reader-ranker for open-domain question answering</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05116</idno>
		<title level="m">Evidence aggregation for answer re-ranking in open-domain question answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making neural QA as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">DCN+: mixed objective and deep residual coattention for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
