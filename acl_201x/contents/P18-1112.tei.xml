<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Tkachenko</surname></persName>
							<email>maksim.tkatchenko@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><forename type="middle">Cher</forename><surname>Chia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1212" to="1221"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1212</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectiv-ity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional analysis methods such as Word2Vec ( <ref type="bibr" target="#b11">Mikolov et al., 2013)</ref> and <ref type="bibr">GloVe (Pennington et al., 2014</ref>) have been critical for the success of many large-scale natural language processing (NLP) applications <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr">Socher et al., 2013;</ref><ref type="bibr" target="#b5">Goldberg, 2016)</ref>. These methods em- ploy distributional hypothesis (i.e., words used in the same contexts tend to have similar meaning) to derive distributional meaning via context predic- tion tasks and produce dense word embeddings.</p><p>While there have been active and ongoing re- search on improving word embedding methods (see Section 5), there is a relative dearth of study on the impact that an input corpus may have on the quality of the word embeddings. The previous preoccupation centers around corpus size, i.e., a larger corpus is perceived to be richer in statistical information. For instance, popular corpora include Wikipedia, Common Crawl, and Google News.</p><p>We postulate that there may be variations across corpora owing to factors that affect language use. Intuitively, the many things we write (a work email, a product review, an academic publication, etc.) may each involve certain stylistic, syntactic, and lexical choices, resulting in meaningfully dif- ferent distributions of word cooccurrences. Con- sequently, such factors may be encoded in the word embeddings, and input corpora may be dif- ferentially informative towards various NLP tasks.</p><p>In this work, we are interested in the notion of subjectivity. Some NLP tasks, such as senti- ment classification, revolve around subjective ex- pressions of likes or dislikes. Others, such as topic classification, revolve around more objective ele- ments of whether a document belongs to a topic (e.g., science, politics). Our central hypothesis is that word embeddings learnt from input corpora of contrasting levels of subjectivity perform dif- ferently when classifying sentences by sentiment, subjectivity, or topic. As the first contribution, we outline an experimental scheme to explore this hy- pothesis in Section 2, and conduct a series of con- trolled experiments in Section 3 establishing that there exists a meaningful difference between word embeddings derived from objective vs. subjective corpora. We further systematically investigate fac- tors that could potentially explain the differences.</p><p>Upon discovering from the investigation that sentiment words play a particularly important role in subjectivity-sensitive NLP tasks, such as sen- timent classification, as the second contribution, in Section 4 we develop SentiVec, a novel word embedding method infused with information from lexical resources such as a sentiment lexicon. We further identify two alternative lexical objectives: Logistic SentiVec based on discriminative logistic regression, and Spherical SentiVec based on soft clustering effect of von Mises-Fisher distributions. In Section 6, the proposed word embeddings show evident improvements on sentiment classification, as compared to the base model Word2Vec and other baselines using the same lexical resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data and Methodology</head><p>We lay out the methodology for generating word embeddings of contrasting subjectivity, whose ef- fects are tested on several text classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generating Word Embeddings</head><p>As it is difficult to precisely quantify the degree of subjectivity of a corpus, we resort to generat- ing word embeddings from two corpora that con- trast sharply in subjectivity, referring to them as the Objective Corpus and the Subjective Corpus.</p><p>Objective Corpus As virtually all contents are written by humans, an absolutely objective corpus (in the philosophical sense) may prove elu- sive. There are however exemplars where, by construction, a corpus aspires to be as objective as possible, and probably achieves that in prac- tical terms. We postulate that one such corpus is Wikipedia. Its list of policies and guidelines 1 , assiduously enforced by an editorial team, spec- ify that an article must be written from a neutral point of view, which among other things means "representing fairly, proportionately, and, as far as possible, without editorial bias, all of the sig- nificant views that have been published by reliable sources on a topic.". Moreover, it is a common resource for training distributional word embed- dings and adopted widely by the research commu- nity to solve various NLP problems. Hence, in this study, we use Wikipedia as the Objective Corpus.</p><p>Subjective Corpus By extension, one may then deem a corpus subjective if its content does not at least meet Wikipedia's neutral point of view requirement. In other words, if the content is re- plete with personal feelings and opinions. We posit that product reviews would be one such cor- pus. For instance, Amazon's Community Guide- line 2 states that "Amazon values diverse opin- ions", and that "Content you submit should be rel- evant and based on your own honest opinions and experience.". Reviews consist of expressive con- tent written by customers, and may not strive for the neutrality of an encyclopedia. We rely on a 1 https://en.wikipedia.org/wiki/ Wikipedia:List_of_policies_and_ guidelines 2 https://www.amazon.com/gp/help/ customer/display.html?nodeId=201929730 large corpus of Amazon reviews from various cat- egories (e.g., electronics, jewelry, books, and etc.) <ref type="bibr" target="#b10">(McAuley et al., 2015)</ref> as the Subjective Corpus.</p><p>Word Embeddings For the comparative anal- ysis in Section 3, we employ Word2Vec (reviewed below) to generate word embeddings from each corpus. Later on in Section 4, we will propose a new word embedding method called SentiVec.</p><p>For Word2Vec, we use the Skip-gram model to train distributional word embeddings on the Ob- jective Corpus and the Subjective Corpus respec- tively. Skip-gram aims to find word embeddings that are useful for predicting nearby words. The objective is to maximize the context probability:</p><formula xml:id="formula_0">log L(W ; C) = w∈W w ∈C(w) log P(w |w),<label>(1)</label></formula><p>where W is an input corpus and C(w) is the con- text of token w. The probability of context word w , given observed word w is defined via softmax:</p><formula xml:id="formula_1">P(w |w) = exp (v w · vw) ˆ w∈V exp (v ˆ w · vw) ,<label>(2)</label></formula><p>where v w and v w are corresponding embeddings and V is the corpus vocabulary. Though theoret- ically sound, the formulation is computationally impractical and requires tractable approximation. <ref type="bibr" target="#b11">Mikolov et al. (2013)</ref> propose two efficient pro- cedures to optimize (1): Hierarchical Softmax and Negative Sampling (NS). In this work we focus on the widely adopted NS. The intuition is that a "good" model should be able to differentiate ob- served data from noise. The differentiation task is defined using logistic regression; the goal is to tell apart real context-word pair (w , w) from ran- domly generated noise pair ( ˆ w, w). Formally,</p><formula xml:id="formula_2">log L [w',w] = log σ (v w · vw) + k i=1 log σ (−v ˆ w i · vw),<label>(3)</label></formula><p>where σ( · ) is a sigmoid function, and</p><formula xml:id="formula_3">{ ˆ w i } k i=1</formula><p>are negative samples. Summing up all the context- word pairs, we derive the NS Skip-gram objective:</p><formula xml:id="formula_4">log L word2vec (W ; C) = w∈W w ∈C(w) log L [w',w] .<label>(4)</label></formula><p>Training word embeddings with Skip-gram, we keep the same hyperparameters across all the runs: 300 dimensions for embeddings, k = 5 negative samples, and window of 5 tokens. The Objective and Subjective corpora undergo the same prepro- cessing, i.e., discarding short sentences (&lt; 5 to- kens) and rare words (&lt; 10 occurrences), remov- ing punctuation, normalizing Unicode symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Tasks</head><p>To compare word embeddings, we need a com- mon yardstick. It is difficult to define an inherent quality to word embeddings. Instead, we put them through several evaluation tasks that can leverage word embeddings and standardize their formula- tions as binary classification tasks. To boil the comparisons down to the essences of word em- beddings (which is our central focus), we rely on standardized techniques so as to attribute as much of the differences as possible to the word embed- dings. We use logistic regression for classification, and represent a text snippet (e.g., a sentence) in the feature space as the average of the word em- beddings of tokens in the snippet (ignoring out-of- vocabulary tokens). The evaluation metric is the average accuracy from 10-fold cross validation.</p><p>There are three evaluation tasks of varying de- grees of hypothetical subjectivity, as outlined be- low. Each may involve multiple datasets. Sentiment Classification Task This task clas- sifies a sentence into either positive or negative. We use two groups of datasets as follows.</p><p>The first group consists of 24 datasets from UCSD Amazon product data 3 corresponding to various product categories. Each review has a rat- ing from 1 to 5, which is transformed into pos- itive (ratings 4 or 5) or negative (ratings 1 or 2) class. For each dataset respectively, we sample 5000 sentences each from the positive and nega- tive reviews. Note that these sentences used for this evaluation task have not participated in the generation of word embeddings. Due to space constraint, in most cases we present the average accuracy across the datasets, but where appropri- ate we enumerate the results for each dataset.</p><p>The second is Cornell's sentence polarity dataset v1.0 4 (Pang and <ref type="bibr" target="#b15">Lee, 2005</ref>), made up of 5331 each of positive and negative sentences from Rotten Tomatoes movie reviews. The inclusion of this out-of-domain evaluation dataset is useful for examining whether the performance of word em- beddings from the Subjective Corpus on the first group above may inadvertently be affected by in- domain advantage arising from its Amazon origin. Subjectivity Classification Task This task classifies a sentence into subjective or objective. The dataset is Cornell's subjectivity dataset v1.0 5 , consisting of 5000 subjective sentences derived from Rotten Tomatoes (RT) reviews and 5000 ob- jective sentences derived from IMDB plot sum- maries ( <ref type="bibr" target="#b14">Pang and Lee, 2004</ref>). This task is prob- ably less sensitive to the subjectivity within word embeddings than sentiment classification, as de- termining whether a sentence is subjective or ob- jective should ideally be an objective undertaking.</p><p>Topic Classification Task We use the 20 Newsgroups dataset 6 ("bydate" version), whereby the newsgroups are organized into six subject mat- ter groupings. We extract the message body and split them into sentences. Each group's sentences then form the in-topic class, and we randomly sample an equivalent number of sentences from the remaining newsgroups to form the out-of-topic class. This results in six datasets, each correspond- ing to a binary classification task. In most cases, we present the average results, and where appro- priate we enumerate the results for each dataset. Hypothetically, this task is the least affected by the subjectivity within word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparative Analyses of Subjective vs. Objective Corpora</head><p>We conduct a series of comparative analyses under various setups. For each, we compare the perfor- mance in the evaluation tasks when using the Ob- jective Corpus and the Subjective Corpus. <ref type="table">Table 1</ref> shows the results for this series of analyses. Initial Condition Setup I seeks to answer whether there is any difference between word em- beddings derived from the Objective Corpus and the Subjective Corpus. The word embeddings were trained on the whole data respectively. Ta- ble 1 shows the corpus statistics and classification accuracies. Evidently, the Subjective word embed- dings outperform the Objective word embeddings on all the evaluation tasks. The margins are largest for sentiment classification (86.5% vs. 81.5% or +5% Amazon, and 78.2% vs. 75.4% or +2.8% on Rotten Tomatoes or RT). For subjectivity and topic classifications, the differences are smaller.  As earlier hypothesized, the sentiment classifi- cation task is more sensitive to subjectivity within word embeddings than the other tasks. Therefore, training word embeddings on a subjective corpus may confer an advantage for such tasks. On the other hand, the corpus statistics show a substan- tial difference in corpus size, which could be an alternative explanation for the outperformance by the Subjective Corpus if the larger corpus contains more informative distributional statistics.</p><p>Controlling for Corpus Size In Setup II, we keep the number of sentences in both corpora the same, by randomly downsampling sentences in the Subjective Corpus. This procedure consequently reduces the number of types and tokens (see Ta- ble 1, Setup II, Corpus Statistics). Note that the number of tokens in the Subjective corpus is now fewer than in the Objective, the latter suffers no change. Yet, even after a dramatic reduction in size, the Subjective embeddings still outperform the Objective significantly on both datasets of the sentiment classification task (+4% on Amazon and +2.5% on RT), while showing similar performance on subjectivity and topic classifications.</p><p>This bolsters the earlier observation that senti- ment classification is more sensitive to subjectiv- ity. While there is a small effect due to corpus size difference, the gap in performance between Sub- jective and Objective embeddings on sentiment classification is still significant and cannot be ex- plained away by the corpus size alone.</p><p>Controlling for Vocabulary While the Sub- jective Corpus has a much smaller vocabulary (i.e., # types), we turn a critical eye on whether its apparent advantage lies in having access to spe- cial word types that do not exist in the Objective Corpus. In Setup III, we keep the training vocabu- lary the same for both, removing the types that are <ref type="table">Table 2</ref>: Top words of misclassified sentences present in one corpus but not in the other, so that out-of-vocabulary words are ignored in the train- ing phase. <ref type="table">Table 1</ref>, Setup III, shows significant reduction in types for both corpora. Yet, the out- performance by the Subjective embeddings on the sentiment classification task still stands (+3.8% on Amazon and +2.3% on RT). Moreover, it is so for both Amazon and Rotten Tomatoes datasets, im- plying that it is not due to close in-domain sim- ilarity between the corpora used for training the word embeddings and the classification tasks.</p><p>Significant Words To get more insights on the difference between the Subjective and Objec- tive corpora, we analyze the mistakes word em- beddings make on the development folds. At this point we focus on the sentiment classification task and specifically on the Amazon data, which in- dicates the largest performance differences in the controlled experiments (see <ref type="table">Table 1</ref>, Setup III).</p><p>As words are still the main unit of informa- tion in distributional word embeddings, we extract words strongly associated with misclassified sen- tences. We employed log-odds ratio with informa- tive Dirichlet prior method ( <ref type="bibr" target="#b12">Monroe et al., 2008)</ref> to quantify this association. It is used to contrast the words in misclassified vs. correctly classified sentences, and accounts for the variance of words and their prior counts taken from a large corpus. <ref type="table">Table 2</ref> shows the top 25 words most associated with the misclassified sentences, sorted by their association scores. On average 50% of the mis- takes overlap for both word embeddings, there- fore, some of the words are included in both lists. 40 − 44% of these words carry positive or neg- ative sentiment connotations in general (see the underlined words in <ref type="table">Table 2</ref>), while other words like return or send may carry sentiment connota- tion in e-commerce context. We check if a word carries sentiment connotation using sentiment lex- icon compiled by <ref type="bibr" target="#b6">Hu and Liu (2004)</ref>, including 6789 words along with positive or negative labels.</p><p>We also observe linguistic negations (i.e., not, Don't). For instance, the word most associ- ated with the Objective-specific mistakes (exclud- ing the Subjective misclassified sentences) is not, which suggests that perhaps Subjective word em- bedding accommodates better understanding of linguistic negations, which may partially explain the difference. However, our methodology as out- lined in Section 2.2 permits exchangeable word or- der and is not intended to analyze structural inter- action between words. We focus on further anal- ysis of sentiment words, leaving linguistic nega- tions in word embeddings for future investigation.</p><p>Controlling for Sentiment Words To con- trol for the "amount" of sentiment in the Subjec- tive and Objective corpora, we use sentiment lex- icon compiled by <ref type="bibr" target="#b6">Hu and Liu (2004)</ref>. For each corpus, we create two subcorpora: With Sentiment contains only the sentences with at least one word from the sentiment lexicon, while Without Senti- ment is the complement. We match the corpora on the number of sentences, downsampling the larger corpus, train word embeddings on each subcorpus, and proceed with the classification experiments. <ref type="table" target="#tab_2">Table 3</ref> shows the results, including that of random word embeddings for reference. Sentiment lexi- con has a significant impact on the performance of sentiment and subjectivity classifications, and a smaller impact on topic classification. Without sentiment, the Subjective embeddings prove more robust, still outperforming the Objective on senti- ment classification, while the Objective performs close to random word embeddings on Amazon .</p><p>In summary, evidences from the series of con- trolled experiments support the existence of some X-factor to the Subjective embeddings, which con- fers superior performance in subjectivity-sensitive tasks such as sentiment classification.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sentiment-Infused Word Embeddings</head><p>To leverage the consequential sentiment informa- tion, we propose a family of methods, called SentiVec, for training distributional word embed- dings that are infused with information on the sen- timent polarity of words. The methods are built upon Word2Vec optimization algorithm and make use of available lexical sentiment resources such as SentiWordNet ( <ref type="bibr" target="#b0">Baccianella et al., 2010)</ref>, senti- ment lexicon by <ref type="bibr" target="#b6">Hu and Liu (2004)</ref>, and etc. SentiVec seeks to satisfy two objectives, namely context prediction and lexical category prediction:</p><formula xml:id="formula_5">log L = log L word2vec (W ; C) + λ log L lex (W, L), (5)</formula><p>where L word2vec (W ; C) is the Skip-gram objec- tive as in <ref type="formula" target="#formula_4">(4)</ref>; L lex (W, L) is a lexical objective for corpus W and lexical resource L; and λ is a trade- off parameter. Lexical resource L = {X i } n i=1 comprises of n word sets, each X i contains words of the same category. For sentiment classification, we consider positive and negative word categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Logistic SentiVec</head><p>Logistic SentiVec admits lexical resource in the form of two disjoint word sets, L = {X 1 , X 2 }, X 1 ∩ X 2 = ∅. The objective is to tell apart which word set of L word w belongs to:</p><formula xml:id="formula_6">log L lex (W, L)<label>(6)</label></formula><p>= w∈X 1 log P(w ∈ X 1 ) + w∈X 2 log P(w ∈ X 2 ).</p><p>We further tie these probabilities together, and cast the objective as a logistic regression problem:</p><formula xml:id="formula_7">P(w ∈ X 1 ) = 1 − P(w ∈ X 2 ) = σ(v w · τ ),<label>(7)</label></formula><p>where v w is a word embedding and τ is a direc- tion vector. Since word embeddings are gener- ally invariant to scaling and rotation when used as downstream feature representations, τ can be chosen randomly and fixed during training. We experiment with randomly sampled unit length di- rections. For simplicity, we also scale embedding v w to its unit length when computing v w · τ , which now equals to cosine similarity between v w and τ . When v w is completely aligned with τ , the co- sine similarity between them is 1, which maxi- mizes P(w ∈ X 1 ) and favors words in X 1 . When v w is opposite to τ , the cosine similarity equals to −1, which maximizes P(w ∈ X 2 ) and predicts vectors from X 2 . Orthogonal vectors have cosine similarity of 0, which makes both w ∈ X 1 and w ∈ X 2 equally probable. Optimizing (6) makes the corresponding word embeddings of X 1 and X 2 gravitate to the opposite semispaces and simulates clustering effect for the words of the same cate- gory, while the Word2Vec objective prevents words from collapsing to the same directions.</p><p>Optimization The objective in (6) permits simple stochastic gradient ascent optimization and can be combined with negative sampling proce- dure for Skip-gram in (5). The gradient for un- normalized embedding v w is solved as follows:</p><formula xml:id="formula_8">log L [w∈X 1 ] (D, L) v wi = (log P (x ∈ X 1 )) v wi = 1 v w 2 σ − v w · τ v w τ i v w − v wi v w · τ v w<label>(8)</label></formula><p>The optimization equation for v w , when w ∈ X 2 , can be derived analogously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spherical SentiVec</head><p>Spherical SentiVec extends Logistic SentiVec by dealing with any number of lexical categories,</p><formula xml:id="formula_9">L = {X i } n i=1</formula><p>. As such, the lexical objective takes on generic form:</p><formula xml:id="formula_10">log L lex (W, L) = n i=1 w∈X i log P (w ∈ X i ), (9)</formula><p>Each P (w ∈ X i ) defines embedding generating process. We assume each length-normalized v w for w of L is generated w.r.t. a mixture model of von Mises-Fisher (vMF) distributions. vMF is a probability distribution on a multidimensional sphere, characterized by parameters µ (mean di- rection) and κ (concentration parameter). Sam- pled points are concentrated around µ; the greater the κ, the closer the sampled points are to µ. We consider only unimodal vMF distributions, re- stricting concentration parameters to be strictly positive. Hereby, each X i ∈ L is assigned to vMF distribution parameters (µ i , κ i ) and the member- ship probabilities are defined as follows:</p><formula xml:id="formula_11">P(w ∈ X i ) = P (v w ; µ i , κ i ) = 1 Z κ i e κ i µ i ·vw ,<label>(10)</label></formula><p>where Z κ is the normalization factor. The Spherical SentiVec lexical objective forces words of every X i ∈ L to gravitate towards and concentrate around their direction mean µ i . As in Logistic SentiVec, it simulates clustering effect for the words of the same set. In comparison to the direction vector of Logistic SentiVec, mean direc- tions of Spherical SentiVec when fixed can sub- stantially influence word embeddings training and must be carefully selected. We optimize the mean directions along with the word embeddings using alternating procedure resembling K-means clus- tering algorithm. For simplicity, we keep concen- tration parameters tied, κ 1 = κ 2 = ... = κ n = κ, and treat κ as a hyperparameter of this algorithm.</p><p>Optimization We derive optimization pro- cedure for updating word embeddings assuming fixed direction means. Like Logistic SentiVec, Spherical SentiVec can be combined with the neg- ative sampling procedure of Skip-gram. The gra- dient for unnormalized word embedding v w is solved by the following equation:</p><formula xml:id="formula_12">log L [w∈X i ] (W, L) v wj = κi µij vw − vwj vw ·µ i vw vw 2<label>(11)</label></formula><p>Once word embedding v w (w ∈ X i ) is updated, we revise direction mean µ i w.r.t. maximum like- lihood estimator:</p><formula xml:id="formula_13">µi = w∈X i vw w∈X i vw .<label>(12)</label></formula><p>Updating the direction means in such a way en- sures that the lexical objective is non-decreasing. Assuming the stochastic optimization procedure for L word2vec complies with the same non- decreasing property, the proposed alternating pro- cedure converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There have been considerable research on im- proving the quality of distributional word em- beddings. <ref type="bibr" target="#b2">Bolukbasi et al. (2016)</ref> seek to de- bias word embeddings from gender stereotypes. <ref type="bibr">Rothe and Schütze (2017)</ref> incorporate WordNet lexeme and synset information. <ref type="bibr" target="#b13">Mrkšic et al. (2016)</ref> encode antonym-synonym relations. <ref type="bibr" target="#b8">Liu et al. (2015)</ref> encode ordinal relations such as hy- pernym and hyponym. <ref type="bibr" target="#b7">Kiela et al. (2015)</ref> augment Skip-gram to enforce lexical similarity or related- ness constraints, <ref type="bibr" target="#b1">Bollegala et al. (2016)</ref> modify GloVe optimization procedure for the same pur- pose. <ref type="bibr" target="#b4">Faruqui et al. (2015)</ref> employ semantic re- lations of PPDB, WordNet, FrameNet to retrofit word embeddings for various prediction tasks. We use this Retrofitting method 7 as a baseline. Socher et al. (2011) derive multi-word embed- dings for sentiment distribution prediction, while we focus on lexical distributional analysis. <ref type="bibr" target="#b9">Maas et al. (2011)</ref> and <ref type="bibr">Tang et al. (2016)</ref> use document- level sentiment annotations to fit word embed- dings, but document annotation might not always be available for distributional analysis on neutral corpora such as Wikipedia. SentiVec relies on simple sentiment lexicon instead. Refining ( <ref type="bibr">Yu et al., 2018</ref>) aligns the sentiment scores taken from lexical resource and the cosine similarity scores of corresponding word embeddings. The method generally requires fine-grained sentiment scores for the words, which may not be available in some settings. We use Refining as a baseline and adopt coarse-grained sentiment lexicon for this method. <ref type="bibr">Villegas et al. (2016)</ref> compare various distri- butional word embeddings arising from the same corpus for sentiment classification, whereas we fo- cus on the differentiation in input corpora and pro- pose novel sentiment-infused word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The objective of experiments is to study the ef- ficacy of Logistic SentiVec and Spherical SentiVec word embeddings on the aforementioned text clas- sification tasks. One natural baseline is Word2Vec, as SentiVec subsumes its context prediction objec- tive, while further incorporating lexical category prediction. We include two other baselines that can leverage the same lexical resource but in man- ners different from SentiVec, namely: Retrofitting (Faruqui et al., 2015) and Refining ( <ref type="bibr">Yu et al., 2018)</ref>. For these methods, we generate their word embeddings based on Setup III (see Section 3). All the methods were run multiple times with var- ious hyperparameters, optimized via grid-search; for each we present the best performing setting. First, we discuss the sentiment classification task. <ref type="table" target="#tab_4">Table 4</ref> shows the unfolded results for the 24 classification datasets of Amazon, as well as for Rotten Tomatoes. For each classification dataset (row), and for the Objective and Subjective em- bedding corpora respectively, the best word em- bedding methods are shown in bold. An aster- isk indicates statistically significant 8 results at 5% in comparison to Word2Vec. Both SentiVec vari- ants outperform Word2Vec in the vast majority of the cases. The degree of outperformance is higher for the Objective than the Subjective word embed- dings. This is a reasonable trend given our previ- ous findings in Section 3. As the Objective Corpus encodes less information than the Subjective Cor- pus for sentiment classification, the former is more likely to benefit from the infusion of sentiment in- formation from additional lexical resources. Note that the sentiment infusion into the word embed- dings comes from separate lexical resources, and does not involve any sentiment classification label.</p><p>SentiVec also outperforms the two baselines that benefit from the same lexical resources. Retrofitting does not improve upon Word2Vec, with the two embeddings essentially indistinguish- able (the difference is only noticeable at the sec- ond decimal point). Refining makes the word em- beddings perform worse on the sentiment classifi- cation task. One possible explanation is that Refin- ing normally requires fine-grained labeled lexicon, where the words are scored w.r.t. the sentiment scale, whereas we use sentiment lexicon of two la- bels (i.e., positive or negative). SentiVec accepts coarse-grained sentiment lexicons, and potentially could be extended to deal with fine-grained labels.</p><p>As previously alluded to, topic and subjectivity classifications are less sensitive to the subjectiv- ity within word embeddings than sentiment clas- sification. One therefore would not expect much, if any, performance gain from infusion of senti- ment information. However, such infusion should not subtract or harm the quality of word embed- dings either. <ref type="table" target="#tab_5">Table 5</ref> shows that the unfolded re- sults for topic classification on the six datasets, and the result for subjectivity classification are similar across methods. Neither the SentiVec variants, nor Retrofitting and Refining, change the subjectivity and topic classification capabilities much, which means that the used sentiment lexicon is targeted only at the sentiment subspace of embeddings.    Illustrative Changes in Embeddings To give more insights on the difference between SentiVec and Word2Vec, we show "flower" diagrams in <ref type="figure">Fig- ure 1</ref> for Logistic SentiVec and <ref type="figure">Figure 2</ref> for Spher- ical SentiVec. Each is associated with a reference word (e.g., good for <ref type="figure">Figure 1a)</ref>, and indicates rel- ative changes in cosine distances between the ref- erence word and the testing words surrounding the "flower". Every testing word is associated with a "petal" or black axis extending from the center of the circle. The "petal" length is proportional to the relative distance change in two word embeddings: κ = d SentiV ec (w ref ,w testing ) d word2vec (w ref ,w testing ) , where d SentiV ec and d word2vec are cosine distances between reference w ref and testing w testing words in SentiVec and Word2Vec embeddings correspondingly. If the dis- tance remains unchanged (κ = 1), then the "petal" points at the circumference; if the reference and testing words are closer in the SentiVec embedding than they are in Word2Vec (κ &lt; 1), the "petal" lies inside the circle; when the distance increases (κ &gt; 1), the "petal" goes beyond the circle.</p><p>The diagrams are presented for Objective Em- beddings <ref type="bibr">9</ref> . We use three reference words: good (positive), bad (negative), time (neutral); as well as three groups of testing words: green for words randomly sampled from positive lexicon (Sec- tor I-II), red for words randomly sampled from negative lexicon (Sector II-III), and gray for fre- quent neutral common nouns (Sector III-I). <ref type="figure">Figure 1</ref> shows changes produced by Logistic SentiVec. For the positive reference word <ref type="figure">(Fig- ure 1a)</ref>, the average distance to the green words is shortened, whereas the distance to the red words increases. The reverse is observed for the nega- tive reference word <ref type="figure">(Figure 1b)</ref>. This observation complies with the lexical objective (7) of Logistic SentiVec, which aims to separate the words of two different classes. Note that the gray words suffer only moderate change with respect to positive and negative reference words. For the neutral refer- ence word <ref type="figure">(Figure 1c)</ref>, the distances are only mod- erately affected across all testing groups. <ref type="figure">Figure 2</ref> shows that Spherical SentiVec tends to make embeddings more compact than Logistic SentiVec. As the former's lexical objective (9) is designed for clustering, but not for separation, we look at the comparative strength of the clustering effect on the testing words. For the positive refer- ence word <ref type="figure">(Figure 2a)</ref>, the largest clustering effect is achieved for the green words. For the negative reference word <ref type="figure">(Figure 2b</ref>), as expected, the red words are affected the most. The gray words suf- fer the least change for all the reference words.</p><p>In summary, SentiVec effectively provides an advantage for subjectivity-sensitive task such as sentiment classification, while not harming the performance of other text classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We explore the differences between objective and subjective corpora for generating word embed- dings, and find that there is indeed a difference in the embeddings' classification task performances. Identifying the presence of sentiment words as one key factor for the difference, we propose a novel method SentiVec to train word embeddings that are infused with the sentiment polarity of words derived from a separate sentiment lexicon. We further identify two lexical objectives: Logistic SentiVec and Spherical SentiVec. The proposed word embeddings show improvements in senti- ment classification, while maintaining their per- formance on subjectivity and topic classifications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Setup</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Corpus</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Corpus</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Relative changes in cosine distances in Logistic SentiVec contrasted with Word2Vec</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Corpus</head><label></label><figDesc></figDesc><table>Corpus Statistics 
Classification (Accuracy) 

# types # tokens # sentences 
Sentiment 
Subjectivity Topic 
Amazon RT 

I 
Objective 
1.34M 
1.81B 
89M 
81.5 
75.4 
90.5 
83.2 
Subjective 
1.47M 
5.49B 
313M 
86.5 
78.2 
91.1 
83.4 

II 
Objective 
1.34M 
1.81B 
89M 
81.5 
75.4 
90.5 
83.2 
Subjective 
0.59M 
1.56B 
85.5 
77.9 
90.7 
82.8 

III 
Objective 
0.29M 
1.75B 
89M 
81.6 
75.6 
90.6 
83.4 
Subjective 
1.54B 
85.4 
77.9 
90.6 
82.8 

Table 1: Controlled comparison of Objective and Subjective corpora 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>With and without sentiment 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Comparison of Sentiment-Infused Word Embeddings on Sentiment Classification Task</head><label>4</label><figDesc></figDesc><table>Corpus/Category 

Objective Embeddings 
Subjective Embeddings 

Word2Vec 
Retrofitting 
Refining 
SentiVec 
Word2Vec 
Retrofitting 
Refining 
SentiVec 
Spherical 
Logistic 
Spherical 
Logistic 
Topic 
Computers 
79.8 
79.8 
79.6 
79.6 
79.8 
79.8 
79.8 
79.8 
79.7 
79.7 
Misc 
89.8 
89.8 
89.7 
89.8 
90.0 
90.4 
90.4 
90.6 
90.4 
90.3 
Politics 
84.6 
84.6 
84.4 
84.5 
84.6 
83.8 
83.8 
83.5 
83.6 
83.5 
Recreation 
83.4 
83.4 
83.1 
83.1 
83.2 
82.6 
82.6 
82.5 
82.7 
82.8 
Religion 
84.6 
84.6 
84.5 
84.5 
84.6 
84.2 
84.2 
84.2 
84.1 
84.2 
Science 
78.2 
78.2 
78.2 
78.1 
78.3 
76.4 
76.4 
76.1 
76.7 
76.6 
Average 
83.4 
83.4 
83.2 
83.3 
83.4 
82.8 
82.8 
82.8 
82.9 
82.8 

Subjectivity 
90.6 
90.6 
90.0 
90.6 
90.6 
90.6 
90.6 
90.3 
90.7 
90.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Comparison of Word Embeddings on Subjectivity and Topic Classification Tasks</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> http://jmcauley.ucsd.edu/data/amazon/ 4 http://www.cs.cornell.edu/people/ pabo/movie-review-data/rt-polaritydata. README.1.0.txt</note>

			<note place="foot" n="5"> http://www.cs.cornell.edu/people/ pabo/movie-review-data/subjdata.README. 1.0.txt 6 http://qwone.com/ ˜ jason/20Newsgroups/</note>

			<note place="foot" n="7"> Original code is available at: https://github. com/mfaruqui/retrofitting</note>

			<note place="foot" n="8"> We use paired t-test to compute p-value.</note>

			<note place="foot" n="9"> The diagrams for Subjective Embeddings show the same trend, with the moderate changes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the National Re-search Foundation, Prime Minister's Office, Sin-gapore under its NRF Fellowship Programme (Award No. NRF-NRFF2016-07).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint word representation learning using a corpus and a semantic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Alsuhaibani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Specializing word embeddings for similarity or relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fightin&apos;words: Lexical feature selection and evaluation for identifying the content of political conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burt L Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin M</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkšic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid</forename><surname>Oséaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
