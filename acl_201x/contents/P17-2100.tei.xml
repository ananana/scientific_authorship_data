<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
						</author>
						<title level="a" type="main">Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="635" to="640"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2100</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Current Chinese social media text summa-rization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media sum-marization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention en-coder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text summarization is to produce a brief sum- mary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts ( <ref type="bibr" target="#b12">Radev et al., 2004;</ref><ref type="bibr" target="#b17">Woodsend and Lapata, 2010;</ref><ref type="bibr" target="#b1">Cheng and Lapata, 2016)</ref>. However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. There- fore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice ( <ref type="bibr" target="#b13">Rush et al., 2015;</ref><ref type="bibr" target="#b5">Hu et al., 2015)</ref>.</p><p>For extractive summarization, the selected sen- tences often have high semantic relevance to the text. However, for abstractive text summariza- tion, current models tend to produce grammatical Text: 昨晚，中联航空成都飞北京一架航班 被发现有多人吸烟。后因天气原因，飞机 备降太原机场。有乘客要求重新安检，机 长决定继续飞行，引起机组人员与未吸烟 乘客冲突。 Last night, several people were caught to smo- ke on a flight of China United Airlines from Chendu to Beijing. Later the flight temporari- ly landed on Taiyuan Airport. Some passeng- ers asked for a security check but were denied by the captain, which led to a collision betwe- en crew and passengers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN: 中联航空机场发生爆炸致多人死亡。</head><p>China United Airlines exploded in the airport, leaving several people dead.</p><p>Gold: 航班多人吸烟机组人员与乘客冲突。 Several people smoked on a flight which led to a collision between crew and passengers. and coherent summaries regardless of its semantic relevance with source texts. <ref type="figure" target="#fig_0">Figure 1</ref> shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text liter- ally, but it has low semantic relevance.</p><p>In this work, our goal is to improve the seman- tic relevance between source texts and generat- ed summaries for Chinese social media text sum- marization. To achieve this goal, we propose a Semantic Relevance Based neural model. In our model, a similarity evaluation component is intro- duced to measure the relevance of source texts and generated summaries. During training, it maxi- mizes the similarity score to encourage high se- mantic relevance between source texts and sum-maries. The representation of source texts is pro- duced by an encoder, while that of summaries is computed by a decoder. We introduce a gated at- tention encoder to better represent the source tex- t. Besides, our decoder generates summaries and provide the summary representation. Experiments show that our proposed model has better perfor- mance than baseline systems on the social media corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Chinese Abstractive Text Summarization</head><p>Current Chinese social media text summarization model is based on encoder-decoder framework. Encoder-decoder model is able to compress source texts x = {x 1 , x 2 , ..., x N } into continuous vec- tor representation with an encoder, and then gen- erate the summary y = {y 1 , y 2 , ..., y M } with a de- coder. In the previous work ( <ref type="bibr" target="#b5">Hu et al., 2015)</ref>, the encoder is a bi-directional gated recurrent neural network, which maps source texts into sentence vector {h 1 , h 2 , ..., h N }. The decoder is a uni- directional recurrent neural network, which pro- duces the distribution of output words y t with pre- vious hidden state s t−1 and word y t−1 :</p><formula xml:id="formula_0">p(y t |x) = sof tmaxf (s t−1 , y t−1 )<label>(1)</label></formula><p>where f is recurrent neural network output func- tion, and s 0 is the last hidden state of encoder h N . Attention mechanism is introduced to bet- ter capture context information of source texts ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>. Attention vector c t is represented by the weighted sum of encoder hidden states:</p><formula xml:id="formula_1">c t = N ∑ i=1 α ti h i (2) α ti = e g(st,h i ) ∑ N j=1 e g(st,h j )<label>(3)</label></formula><p>where g(s t , h i ) is a relevant score between de- coder hidden state s t and encoder hidden state h i . When predicting an output word, the decoder takes account of attention vector, which contain- s the alignment information between source texts and summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>Our assumption is that source texts and sum- maries have high semantic relevance, so our pro- posed model encourages high similarity between The encoder compresses source texts into seman- tic vectors, and the decoder generates summaries and produces semantic vectors of the generated summaries. Finally, the similarity function eval- uates the relevance between the sematic vectors of source texts and generated summaries. Our train- ing objective is to maximize the similarity score so that the generated summaries have high semantic relevance with source texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Representation</head><p>There are several methods to represent a text or a sentence, such as mean pooling of RNN output or reserving the last state of RNN. In our model, source text is represented by a gated attention en- coder ( <ref type="bibr" target="#b4">Hahn and Keller, 2016)</ref>. Every upcoming word is fed into a gated attention network, which measures its importance. The gated attention net- work outputs the important score with a feedfor- ward network. At each time step, it inputs a word vector e t and its previous context vector h t , then outputs the score β t . Then the word vector e t is multiplied by the score β t , and fed into RNN en- coder. We select the last output h N of RNN en- coder as the semantic vector of the source text V t . A natural idea to get the semantic vector of a summary is to feed it into the encoder as well. However, this method wastes much time because we encode the same sentence twice. Actually, the last output s M contains information of both source text and generated summaries. We simply com- pute the semantic vector of the summary by sub- tracting h N from s M :</p><formula xml:id="formula_2">V s = s M − h N<label>(4)</label></formula><p>Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Relevance</head><p>Our goal is to compute the semantic relevance of source text and generated summary given seman- tic vector V t and V s . Here, we use cosine simi- larity to measure the semantic relevance, which is represented with a dot product and magnitude:</p><formula xml:id="formula_3">cos(V s , V t ) = V s · V t ∥V s ∥∥V t ∥<label>(5)</label></formula><p>Source text and summary share the same language, so it is reasonable to assume that their semantic vectors are distributed in the same space. Cosine similarity is a good way to measure the distance between two vectors in the same space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Given the model parameter θ and input text x, the model produces corresponding summary y and se- mantic vector V s and V t . The objective is to mini- mize the loss function:</p><formula xml:id="formula_4">L = −p(y|x; θ) − λcos(V s , V t )<label>(6)</label></formula><p>where p(y|x; θ) is the conditional probability of summaries given source texts, and is computed by the encoder-decoder model. cos(V s , V t ) is cosine similarity of semantic vectors V s and V t . This term tries to maximize the semantic relevance between source input and target output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the evaluation of our model and show its performance on a popular so- cial media corpus. Besides, we use a case to ex- plain the semantic relevance between generated summary and source text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Our dataset is Large Scale Chinese Short Tex- t Summarization Dataset (LCSTS), which is con- structed by <ref type="bibr" target="#b5">Hu et al. (2015)</ref>. The dataset consists of more than 2.4 million text-summary pairs, con- structed from a famous Chinese social media web- site called Sina Weibo <ref type="bibr">1</ref> . It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text- summary pairs in PART II and PART III are man- ually annotated with relevant scores ranged from 1 to 5, and we only reserve pairs with scores no less than 3. Following the previous work, we use PART I as training set, PART II as development set, and PART III as test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setting</head><p>To alleviate the risk of word segmentation mis- takes (Xu and Sun, 2016), we use Chinese charac- ter sequences as both source inputs and target out- puts. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initial- ized word embedding. We tune our parameter on the development set. In our model, the embed- ding size is 400, the hidden state size of encoder- decoder is 500, and the size of gated attention net- work is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter λ is 0.0001. Both the encoder and decoder are based on LSTM unit. Follow- ing the previous work ( <ref type="bibr" target="#b5">Hu et al., 2015)</ref>, our eval- uation metric is F-score of ROUGE: ROUGE-1, ROUGE-2 and ROUGE-L (Lin and Hovy, 2003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Systems</head><p>RNN. We denote RNN as the basic sequence-to- sequence model with bi-directional GRU encoder and uni-directional GRU decoder. It is a widely used language generated framework, so it is an im- portant baseline. RNN context. RNN context is a sequence-to- sequence framework with neural attention. Atten- tion mechanism helps capture the context informa- tion of source texts. This model is a stronger base- line system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Discussions</head><p>We compare our model with above baseline sys- tems, including RNN and RNN context. We refer to our proposed Semantic Relevance Based neural model as SRB. Besides, SRB with a gated atten- tion encoder is denoted as +Attention. <ref type="table">Table 1</ref> Model ROUGE-1 ROUGE-2 ROUGE-L RNN (W) ( <ref type="bibr" target="#b5">Hu et al., 2015)</ref> 17.7 8.5 15.8 RNN (C) ( <ref type="bibr" target="#b5">Hu et al., 2015)</ref> 21.5 8.9</p><p>18.6 RNN context (W) ( <ref type="bibr" target="#b5">Hu et al., 2015)</ref> 26.8 16.1 24.1 RNN context (C) ( <ref type="bibr" target="#b5">Hu et al., 2015)</ref> 29.9 17. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27.2">RNN context + SRB (C) 32.1 18.9 29.2 +Attention (C)</head><p>33.3 20.0 30.1 <ref type="table">Table 1</ref>: Results of our model and baseline systems. Our models achieve substantial improvement of all ROUGE scores over baseline systems. <ref type="figure">(</ref>   <ref type="table">Table 2</ref>: Results of our model and state-of-the-art systems. COPYNET incorporates copying mech- anism to solve out-of-vocabulary problem, so its has higher ROUGE scores. Our model does not incorporate this mechanism currently. In the fu- ture work, we will implement this technic to fur- ther improve the performance. (Word: Word level; Char: Character level; R-1: F-score of ROUGE- 1; R-2: F-score of ROUGE-2; R-L: F-score of ROUGE-L) mantic relevance evaluation by the similarity func- tion. Therefore, SRB with gated attention encoder is able to generate summaries with high semantic relevance to source text. <ref type="figure" target="#fig_3">Figure 3</ref> is an example to show the semantic rel- evance between the source text and the summary. It shows that the main idea of the source text is about the reason why Shanghai has few giant com- pany. RNN context produces "Shanghai's giant companies" which is literally similar to the source text, while SRB generates "Shanghai has few giant companies", which is closer to the main idea in semantics. It concludes that SRB produces sum- maries with higher semantic similarity to texts. <ref type="table">Table 2</ref> summarizes the results of our model and state-of-the-art systems. COPYNET has the high- est socres, because it incorporates copying mech- anism to deals with out-of-vocabulary word prob- lem. In this paper, we do not implement this mech- anism in our model. In the future work, we will try to incorporates copying mechanism to our model to solve the out-of-vocabulary problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Abstractive text summarization has achieved suc- cessful performance thanks to the sequence-to- sequence model <ref type="bibr" target="#b15">(Sutskever et al., 2014</ref>) and at- tention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). <ref type="bibr" target="#b13">Rush et al. (2015)</ref> first used an attention-based en- coder to compress texts and a neural network lan- guage decoder to generate summaries. Follow- ing this work, recurrent encoder was introduced to text summarization, and gained better perfor- mance <ref type="bibr" target="#b8">(Lopyrev, 2015;</ref><ref type="bibr" target="#b2">Chopra et al., 2016)</ref>. To- wards Chinese texts, <ref type="bibr" target="#b5">Hu et al. (2015)</ref> built a large corpus of Chinese short text summarization. To deal with unknown word problem, <ref type="bibr" target="#b10">Nallapati et al. (2016)</ref> proposed a generator-pointer model so that the decoder is able to generate words in source texts. <ref type="bibr" target="#b3">Gu et al. (2016)</ref> also solved this issue by in- corporating copying mechanism.</p><p>Our work is also related to neural attention model. Neural attention model is first proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref>. There are many other methods to improve neural attention mod- el ( <ref type="bibr" target="#b6">Jean et al., 2015;</ref><ref type="bibr" target="#b9">Luong et al., 2015)</ref> and accel- erate the training process (Sun, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our work aims at improving semantic relevance of generated summaries and source texts for Chi- nese social media text summarization. Our model is able to transform the text and the summary in- to a dense vector, and encourage high similarity of their representation. Experiments show that our model outperforms baseline systems, and the gen- erated summary has higher semantic relevance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our Semantic Relevance Based neural model. It consists of decoder (above), encoder (below) and cosine similarity function. their representations. Figure 2 shows our proposed model. The model consists of three components: encoder, decoder and a similarity function. The encoder compresses source texts into semantic vectors, and the decoder generates summaries and produces semantic vectors of the generated summaries. Finally, the similarity function evaluates the relevance between the sematic vectors of source texts and generated summaries. Our training objective is to maximize the similarity score so that the generated summaries have high semantic relevance with source texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>With careful calculation, there are many succe- ssful Internet companies in Shanghai, but few of them becomes giant company like BAT. Th- is is also the reason why few Internet compan- ies are listed in top hundred companies of pay- ing tax. Some of them are merged, such as Eb- ay, Tudou, PPS, PPTV, Yihaodian and so on. Others are satisfied with segment market for years. Gold:为什么上海出不了互联网巨头？ Why Shanghai comes out no giant company? RNN context:上海的互联网巨头。 Shanghai's giant company. SRB:上海鲜少互联网巨头的身影。 Shanghai has few giant companies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An Example of RNN generated summary on LCSTS corpus.</figDesc></figure>

			<note place="foot" n="1"> weibo.sina.com</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ab- s/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling human reading with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LCSTS: A large scale chinese short text summarization dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangze</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1721" />
			<biblScope unit="page" from="1967" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLTNAACL</title>
		<meeting><address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating news headlines with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<idno>CoRR ab- s/1512.01712</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence rnns and beyond</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MEAD-A platform for multidocument multilingual text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blairgoldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanko</forename><surname>Arda C ¸ Elebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Drábek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyu</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahna</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Topper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Winkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asynchronous parallel learning for neural networks and structured models with dense features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic generation of story highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11" />
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dependency-based gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
