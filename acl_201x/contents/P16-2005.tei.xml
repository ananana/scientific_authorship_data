<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Domain Adaptation Regularization for Denoising Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe 6 chemin Maupertuis</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe 6 chemin Maupertuis</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Chidlovskii</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe 6 chemin Maupertuis</orgName>
								<address>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Domain Adaptation Regularization for Denoising Autoencoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="26" to="31"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Finding domain invariant features is critical for successful domain adaptation and transfer learning. However, in the case of unsupervised adaptation, there is a significant risk of overfitting on source training data. Recently, a regularization for domain adaptation was proposed for deep models by (Ganin and Lempitsky, 2015). We build on their work by suggesting a more appropriate regularization for denoising autoen-coders. Our model remains unsupervised and can be computed in a closed form. On standard text classification adaptation tasks, our approach yields the state of the art results, with an important reduction of the learning cost.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain Adaptation problem arises each time when we need to leverage labeled data in one or more related source domains, to learn a classifier for unseen data in a target domain. It has been studied for more than a decade, with applications in statistical machine translation, opinion mining, part of speech tagging, named entity recognition and document ranking <ref type="bibr" target="#b8">(Daumé and Marcu, 2006;</ref><ref type="bibr" target="#b14">Pan and Yang, 2010;</ref><ref type="bibr">Zhou and Chang, 2014)</ref>.</p><p>The idea of finding domain invariant features underpins numerous works in domain adapta- tion. A shared representation eases prediction tasks, and theoretical analyses uphold such hy- potheses <ref type="bibr" target="#b0">(Ben-David et al., 2007</ref>). For instance, <ref type="bibr" target="#b8">(Daumé and Marcu, 2006;</ref><ref type="bibr" target="#b9">Daumé, 2009)</ref> have shown that replicating features in three main sub- spaces (source, common and target) yields im- proved accuracy as the classifier can subsequently pick the most relevant common features. With the pivoting technique ( <ref type="bibr" target="#b14">Pan et al., 2010)</ref>, the bag of words features are pro- jected on a subspace that captures the relations between some central pivot features and the re- maining words. Similarly, there are several ex- tensions of topic models and matrix factorization techniques where the latent factors are shared by source and target collections <ref type="bibr" target="#b4">(Chen and Liu, 2014;</ref><ref type="bibr" target="#b6">Chen et al., 2013</ref>).</p><p>More recently, deep learning has been pro- posed as a generic solution to domain adaptation and transfer learning problems by demonstrating their ability to learn invariant features. On one hand, unsupervised models such as denoising au- toencoders (Glorot et al., 2011) or models built on word embeddings ( <ref type="bibr" target="#b3">Bollegala et al., 2015</ref>) are shown to be effective for domain adaptation. On the other hand, supervised deep models ( <ref type="bibr" target="#b13">Long et al., 2015)</ref> can be designed to select an appropri- ate feature space for classification. Adaptation to a new domain can also be performed by fine tun- ing the neural network on the target task ( <ref type="bibr" target="#b7">Chopra et al., 2013)</ref>. While such solutions perform rel- atively well, the refinement may require a signif- icant amount of new labeled data. Recent work by  has proposed a better strategy; they proposed to regularize inter- mediate layers with a domain prediction task, i.e. deciding whether an object comes from the source or target domain. This paper proposes to combine the domain pre- diction regularization idea of (Ganin and Lempit- sky, 2015) with the denoising autoencoders. More precisely, we build on stacked Marginalized De- noising Autoencoders (sMDA) <ref type="bibr" target="#b5">(Chen et al., 2012</ref>), which can be learned efficiently with a closed form solution. We show that such domain adaptation regularization keeps the benefits of the sMDA and yields results competitive to the state of the art re- sults of ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Target Regularized MDA</head><p>Stacked Denoising Autoencoders (sDA) <ref type="bibr">(Vincent et al., 2008</ref>) are multi-layer neural networks trained to reconstruct input data from partial ran- dom corruption. The random corruption, called blank-out noise or dropout, consists in randomly setting to zero some input nodes with probability p; it has been shown to act as a regularizer <ref type="bibr">(Wager et al., 2013</ref>). The sDA is composed of a set of stacked one-layer linear denoising autoen- coder components, which consider a set of N in- put documents (represented by d-dimensional fea- tures x n ) to be corrupted M times by random fea- ture dropout and then reconstructed with a linear mapping W ∈ R d×d by minimizing the squared reconstruction loss:</p><formula xml:id="formula_0">L(W) = N n=1 M m=1 ||x n − ˜ x nm W|| 2 .<label>(1)</label></formula><p>As explicit corruption comes at a high com- putational cost, ( <ref type="bibr" target="#b5">Chen et al., 2012)</ref> propose to marginalize the loss (1) by considering the limit- ing case when M → ∞ and reducing de facto the learning cost. The main advantage of this method is a closed form solution for W, which depends only on the uncorrupted inputs (x n ) and the drop- out probability. Several Marginalized Denoising Autoencoders (MDA) can be then stacked together to create a deep architecture where the representa- tions of the (l − 1) th layer serves as inputs to the l th layer 1 .</p><p>In the case of domain adaptation, the idea is to apply MDA (or sMDA) to the union of unlabeled source X s and target X t examples. Then, a stan- dard learning algorithm such as SVM or Logistic Regression is trained on the labeled source data us- ing the new feature representations (x s n W) which captures better the correlation between the source and target data.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we illustrate the effect of the MDA; it shows the relation between the word log docu- ment frequency (x-axes) and the expansion mass defined as the total mass of words transformed into word i by MDA and represented by j W ji . We can see that the mapping W learned by MDA is heavily influenced by frequent words. In fact, MDA behaves similarly to document expansion on text documents: it adds new words with a very small frequency and sometimes words with a small negative weight. As the figure shows, MDA promotes common words (despite the use of tf-idf weighting scheme) that are frequent both in source and target domains and hence aims to be domain invariant.</p><p>This is in line with the work of ( . To strengthen the invariance effect, they suggested a deep neural architecture which em- beds a domain prediction task in intermediate lay- ers, in order to capture domain invariant features. In this paper we go a step further and refine this argument by claiming that we want to be domain invariant but also to be as close as possible to the target domain distribution. We want to match the target feature distribution because it is where the classification takes place.</p><p>We therefore propose a regularization for the denoising autoencoders, in particular for MDA, with the aim to make source data resemble the tar- get data and hence to ease the adaptation.</p><p>We describe here the case of two domains, but it can be easily generalized to multiple domains. Let D be the vector of size N indicating for each document its domain, e.g. taking values of '−1' for source and '+1' for target examples. Let c be a linear classifier represented as a d dimensional vector trained to distinguish between source and target, e.g. a ridge classifier that minimizes the loss R(c, α) = ||D − Xc || 2 + α||c|| 2 .</p><p>We guide the mapping W in such a way that the denoised data points xW go towards the target side, i.e. xWc = 1 for both source and target samples. Hence, we can extend each term of the loss (1) as follows:</p><formula xml:id="formula_1">||x n − ˜ x nm W|| 2 + λ||1 − ˜ x nm Wc || 2 . (2)</formula><p>The first term here represents the reconstruction loss of the original input, like in MDA. In the sec- ond term, ˜</p><p>x mn Wc is the domain classifier pre- diction for the denoised objects forced to be close to 1, the target domain indicator, and λ &gt; 0.</p><p>Let ¯ X be the concatenation of M replicated ver- sion of the original data X, and˜Xand˜ and˜X be the matrix representation of the M corrupted versions. Tak- ing into account the domain prediction term, the loss can be written as:</p><formula xml:id="formula_2">L R (W) = || ¯ X − ˜ XW|| 2 + λ|| ¯ R − ˜ XWc || 2 ,<label>(3)</label></formula><p>where R is a vector of size N , indicating a desired regularization objective, and ¯ R its M -replicated version. Loss (3) represents a generic form to cap- ture three different ideas:</p><p>• If R = 1, the model incites the reconstructed features moving towards target specific fea- tures.</p><p>• If R = −D, the model aims to promote do- main invariant features as in ( ).</p><p>• If R = [0; 1], where 0 values are used for source data, the model penalizes the source specific features.</p><p>Learning the mapping W. (Chen et al., 2012) observed that the random corruption from equa- tion (1) could be marginalized out from the re- construction loss, yielding a unique and optimal solution. Furthermore, the mapping W can be ex- pressed in closed form as W = PQ −1 , with:</p><formula xml:id="formula_3">Q ij = S ij q i q j , if i = j, S ij q i , if i = j, P ij = S ij q j ,<label>(4)</label></formula><p>where 2 q = [1 − p, . . . , 1 − p] ∈ R d , p is the dropout probability, and S = XX T is the covari- ance matrix of the uncorrupted data X. The domain regularization term in (3) is quadratic in W, the random corruption can still be marginalized out and the expectations obtained in closed form. Indeed, the mapping W which mini- mizes the expectation of 1 M L R (W) is the solution of the following linear system 3 :</p><formula xml:id="formula_4">(P + λ(1 − p)X Rc )(I + λcc ) −1 = QW.</formula><p>(5) In (5), parameter λ controls the effect of the proposed target regularization in the MDA and the regularization on c is controlled by parame- ter α. This approach preserves the good properties of MDA, i.e. the model is unsupervised and can be computed in closed form. In addition, we can easily stack several layers together and add non- linearities between layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct unsupervised domain adaptation ex- periments on two standard collections: the Ama- zon reviews <ref type="bibr" target="#b2">(Blitzer et al., 2011</ref>) and the 20News- group ( <ref type="bibr" target="#b14">Pan and Yang, 2010</ref>) datasets.</p><p>From the Amazon dataset we consider the four most used domains: dvd (D), books (B), electron- ics (E) and kitchen (K), and adopt the settings of ( ) with the 5000 most frequent common features selected for each adaptation task and a tf-idf weighting. We then use the Logistic Regression (LR) to classify the reviews.</p><p>Our previous experiments with MDA revealed that the MDA noise probability p needs to be set to high values (e.g. 0.9). A possible explanation is that document representations are already sparse and adding low noise has no effect on the features already equal to zero. <ref type="figure" target="#fig_1">Figure 2</ref> shows the average accuracy for the twelve Amazon tasks, when we vary the noise probability p.</p><p>In addition, we observed that a single layer with a tanh activation function is sufficient to achieve top performance; stacking several layers and/or concatenating the outputs with the original features yields no improvement but increases the computational time.</p><p>The dropout probability p is fixed to 0.9 in all experiments, for both the MDA baseline and our model; we test the performance with a single layer and a tanh activation function. Stacking several layers is left for future experiments. select the LR parameters and the parameters α, λ by cross validating the classification results using only the "reconstructed" source data; for estimat- ing W we used the source with an unlabeled tar- get set (excluded at test time). This corresponds to the setting used in ( ), with the difference that they use SVM and reverse cross- validation <ref type="bibr">5</ref> . <ref type="table">Table 3</ref> shows the results for twelve adapta- tion tasks on the Amazon review dataset for the four following methods. Columns 1 and 2 show the LR classification results on the target set for the single layer MDA and the proposed target regularized MDA (MDA+TR). Column 3 reports the SVM result on the target from ( . They used a 5 layers sMDA where the 5 outputs are concatenated with input to generate 30,000 features, on which the SVM is then trained and tested (G-sMDA). Finally, column 4 shows the current state of the art results obtained with Domain-Adversarial Training of Neural Networks (DA NN) instead of SVM ( .</p><p>Despite a single layer and LR trained on the source only, the MDA baseline (80.15% on aver- age) is very close to the G-sMDA results obtained with 5 layer sMDA and 6 times larger feature set (80.18%). Furthermore, adding the target regular- ization allows to significantly outperform in many <ref type="bibr">5</ref> It consists in using self training on the target validation set and calibrating parameters on a validation set from the source labeled data.  cases the baseline and the state of the art DA NN. We note that our method has a much lower cost, as it uses the closed form solution for the recon- struction and a simple LR on the reconstructed source data, instead of domain-adversarial train- ing of deep neural networks.</p><p>We also look at the difference between the pre- viously introduced expansion mass for the MDA and MDA+TR. In the adaptation task from dvd (D) to electronics (E), the words for which the mass changed the most are the following <ref type="bibr">6</ref> : worked, to use, speakers, i have, work, mouse, bought, ca- ble, works, quality, unit, ipod, price, number , sound, card, phone, use, product, my. These words are mostly target specific and the results confirm that they get promoted by the new model. Our model favors features which are more likely to appear in target examples, while DA NN seeks domain invariant features. Despite this difference, the two approaches achieve similar results. It is surprising, and we argue that eventually both ap- proaches penalize source specific features. To test this hypothesis, we use MDA with R = [0; 1] (case 3) that penalizes source specific features and we obtain again similar performances.</p><p>Finally, we test our approach on the 20News- group adaptation tasks described in <ref type="bibr" target="#b14">(Pan and Yang, 2010)</ref>. We first filter out rare words and keep at most 10,000 features. Then, we apply both MDA and MDA+TR as above. <ref type="table">Table 3</ref> shows re- sults for ten adaptation tasks. As we can see, in all cases the target regularization (MDA+TR) helps improve the classification accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a domain adaptation regu- larization for denoising autoencoders, in particu- lar for marginalized ones. One limitation of our model is the linearity assumption for the domain classifier, but for textual data, linear classifiers are the state of the art technique. As new words and expressions become more frequent in a new do- main, the idea of using the dropout regularization that forces the reconstruction of initial objects to resemble target domain objects is rewarding. The main advantage of the new model is in the closed form solution. It is also unsupervised, as it does not require labeled target examples and yields per- formance results comparable with the current state of the art.</p><p>Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain sen- timent classification via spectral feature alignment. In Proceedings of the 19th International Conference on World Wide Web, WWW, New York, NY, USA. ACM. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Relation between log document frequency and expansion mass. One dot represents one word.</figDesc><graphic url="image-1.png" coords="2,307.28,63.80,226.77,170.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Impact of the noise parameter p on the average accuracy for the 12 Amazon adaptation tasks. Both MDA and its extension with the regularization (MDA+TR) perform better with a high dropout-out noise. Here MDA+TR is run with fixed parameters α = 100 and λ = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Accuracies of MDA, MDA+TR, G-
sMDA and DA NN on the Amazon review dataset. 
Underline indicates improvement over the base-
line MDA, bold indicates the highest value. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracies of MDA and MDA+TR on 
20Newsgroup adaptation tasks. 

</table></figure>

			<note place="foot" n="1"> Between layers, in general, a non linear function such as tanh or ReLU is applied.</note>

			<note place="foot" n="2"> In contrast to (Chen et al., 2012), we do not add a bias feature so that the domain and MDA have the same dimensionality. Experiments shown no impact on the performance.</note>

			<note place="foot" n="3"> The derivation is not included due to space limitation. 4 α ∈ [.1, 1, 50, 100, 150, 200, 300], λ ∈ [.01, .1, 1, 10].</note>

			<note place="foot" n="6"> In ascending order of the differences.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS Conference Proceedings</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing, EMNLP<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="22" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation with coupled subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>AISTATS, Fort Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain word representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Topic modeling using topics from many domains, lifelong learning and big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning, ICML Bejing</title>
		<meeting>the 31st International Conference on Machine Learning, ICML Bejing</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4683</idno>
		<title level="m">Marginalized denoising autoencoders for domain adaptation. ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leveraging multi-domain prior knowledge in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI &apos;13</title>
		<meeting>the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI &apos;13</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2071" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DLID: Deep learning for domain adaptation by interpolating between domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML</title>
		<meeting>the 30th International Conference on Machine Learning, ICML<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<title level="m">Frustratingly easy domain adaptation. CoRR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1505.07818</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML</title>
		<meeting>the 28th International Conference on Machine Learning, ICML<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
