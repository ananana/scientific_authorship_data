<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilingual Segmented Topic Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<settlement>Kyoto</settlement>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai, Seika-cho, Soraku-gun</addrLine>
									<settlement>Kyoto</settlement>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bilingual Segmented Topic Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1266" to="1276"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This study proposes the bilingual segmented topic model (BiSTM), which hierarchically models documents by treating each document as a set of segments, e.g., sections. While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al., 2009; Ni et al., 2009), consider only cross-lingual alignments between entire documents, the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments. This study also presents a method for simultaneously inferring latent topics and segmen-tation boundaries, incorporating unsuper-vised topic segmentation (Du et al., 2013) into BiSTM. Experimental results show that the proposed model significantly out-performs BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic topic models, such as probabilis- tic latent semantic analysis (PLSA) <ref type="bibr" target="#b12">(Hofmann, 1999</ref>) and latent Dirichlet allocation (LDA) ( , are generative models for documents that have been used as unsupervised frameworks to discover latent topics in document collections without prior knowledge. These topic models were originally applied to monolingual data; how- ever, various recent studies have proposed the use of probabilistic topic models in multilingual set- Most multilingual topic models, including bilin- gual LDA (BiLDA) ( <ref type="bibr" target="#b17">Mimno et al., 2009;</ref><ref type="bibr" target="#b20">Ni et al., 2009)</ref>, model a document-aligned comparable corpus, such as a collection of Wikipedia articles, where aligned documents are topically similar but are not direct translations 2 . In particular, these models assume that the documents in each tuple share the same topic distribution and that each cross-lingual topic has a language-specific word distribution.</p><p>Existing multilingual topic models consider only document-level alignments. However, most documents are hierarchically structured, i.e., a document comprises segments (e.g., sections and paragraphs) that can be aligned across languages. <ref type="figure" target="#fig_0">Figure 1</ref> shows a Wikipedia article example, which contains a set of sections. Sections 1, 2, and 3 in the English article correspond topically to sections 4, 2, and 3 in the Japanese counterpart, re-spectively. To date, such segment-level alignments have been ignored; however, we consider that such corresponding segments must share the same topic distribution. <ref type="bibr" target="#b7">Du et al. (2010)</ref> have shown that segment-level topics and their dependencies can improve model- ing accuracy in a monolingual setting. Based on that research, we expect that segment-level topics can also be useful for modeling multilingual data.</p><p>This study proposes a bilingual segmented topic model (BiSTM) that extends BiLDA to capture segment-level alignments through a hierarchical structure. In particular, BiSTM considers each document as a set of segments and models a docu- ment as a document-segment-word structure. The topic distribution of each segment (per-segment topic distribution) is generated using a Pitman- Yor process (PYP) <ref type="bibr" target="#b23">(Pitman and Yor, 1997)</ref>, in which the base measure is the topic distribution of the related document (per-document topic distri- bution). In addition, BiSTM introduces a binary variable that indicates whether two segments in different languages are aligned. If two segments are aligned, their per-segment topic distributions are shared; if they are not aligned, they are inde- pendently generated.</p><p>BiSTM leverages existing segments from a given segmentation. However, a segmentation is not always given, and a given segmentation might not be optimal for statistical modeling. Therefore, this study also presents a model, BiSTM+TS, that incorporates unsupervised topic segmentation into BiSTM. BiSTM+TS integrates point-wise bound- ary sampling into BiSTM in a manner similar to that proposed by <ref type="bibr" target="#b9">Du et al. (2013)</ref> and infers seg- mentation boundaries and latent topics jointly.</p><p>Experiments using an English-Japanese and English-French Wikipedia corpus show that the proposed models (BiSTM and BiSTM+TS) sig- nificantly outperform the standard bilingual topic model (BiLDA) in terms of perplexity, and that they improve performance in translation extrac- tion (up to +0.083 top 1 accuracy). The exper- iments also reveal that BiSTM+TS is comparable to BiSTM, which uses manually provided segmen- tation, i.e., section boundaries in Wikipedia arti- cles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bilingual LDA</head><p>This section describes the BiLDA model ( <ref type="bibr" target="#b17">Mimno et al., 2009;</ref><ref type="bibr" target="#b20">Ni et al., 2009</ref>), which we take as </p><formula xml:id="formula_0">d i (i ∈ {1, ..., D}) do 7: choose θ i ∼ Dirichlet(α) 8: for each language l ∈ {e, f } do 9: for each word w l im (m ∈ {1, ..., N l i }) do 10: choose z l im ∼ Multinomial(θ i ) 11:</formula><p>choose w l im ∼ p(w l im |z l im , ϕ l ) 12: end for 13: end for 14: end for our baseline. BiLDA is a bilingual extension of basic monolingual LDA ( ) for a document-aligned comparable corpus. While monolingual LDA assumes that each document has its own topic distribution, BiLDA assumes that aligned documents share the same topic distribu- tion and discovers latent cross-lingual topics. Algorithm 1 and <ref type="figure" target="#fig_1">Figure 2</ref> show the genera- tive process and graphical model, respectively, of BiLDA. BiLDA models a document-aligned com- parable corpus, i.e., a set of D document pairs in two languages, e and f . Each document pair d i (i ∈ {1, ..., D}) comprises aligned documents in the language e and f :</p><formula xml:id="formula_1">d i =(d e i , d f i ).</formula><p>BiLDA as- sumes that each topic k ∈ {1, ..., K} comprises the set of a discrete distribution over words for each language. Each language-specific per-topic word distribution ϕ l k (l ∈ {e, f }) is drawn from a Dirichlet distribution with the prior β l (Steps 1-5). To generate a document pair d i , the per- document topic distribution θ i is first drawn from a Dirichlet distribution with the prior α (Step 7). Thus, aligned documents d e i and d f i share the same topic distribution. Then, for each word at m ∈ {1, ..., N l i } in document d l i in language l, a latent topic assignment z l im is drawn from a multinomial </p><formula xml:id="formula_2">AS i = genAS(y i ) 13: for each set AS ig (g ∈ {1, ..., |AS i |}) do 14: choose ν ig ∼ PYP(a, b, θ i ) 15: end for 16: for each language l ∈ {e, f } do 17: for each segment s l ij (j ∈ {1, ..., S l i }) do 18: get index of s l ij in AS i : g =get idx(AS i ,s l ij ) 19: for each word w l ijm (m ∈ {1, ..., N l ij }) do 20: choose z l ijm ∼ Multinomial(ν ig ) 21: choose w l ijm ∼ p(w l ijm |z l ijm , ϕ l ) 22:</formula><p>end for 23: end for 24: end for 25: end for distribution with the prior θ i (Step 10). Later, a word w l im is drawn from a probability distribution p(w l im |z l im , ϕ l ) given the topic z l im (Step 11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bilingual Segmented Topic Model</head><p>Here, we describe BiSTM, which extends BiLDA to capture segment-level alignments. Algorithm 2 and <ref type="figure" target="#fig_2">Figure 3</ref> show the generative process and graphical model, respectively, of BiSTM. As can be seen in <ref type="figure" target="#fig_2">Figure 3</ref>, BiSTM introduces a segment- level layer between the document-and word-level layers in both languages. In other words, per- segment topic distributions for each language, ν e and ν f , are introduced between per-document topic distributions θ and topic assignments for words, z e and z f . In addition, BiSTM incorpo- rates binary variables y to represent segment-level alignments.</p><p>Each document d l i in a pair of aligned doc- uments d i is divided into S l i segments:</p><formula xml:id="formula_3">d l i = ∪ S l i j=1 s l ij .</formula><p>BiSTM makes the same assumption for per-topic word distributions as BiLDA, i.e., ϕ l k are language-specific and drawn from Dirichlet distri- butions (Steps 1-5).</p><p>In the generative process for a document pair d i , the per-document topic distribution θ i is first drawn in the same way as in BiLDA (Step 7). Thus, in BiSTM, each document pair shares the same topic distribution.</p><p>Then, if segment-level alignments are not given, y i are generated ( <ref type="bibr">Steps 8-11)</ref>. We assume that each document pair d i has a probability γ i that indicates comparability between segments across languages. γ i is drawn from a Beta distribution with the priors η 0 and η 1 (Step 9). Then, each of y i is drawn from a Bernoulli distribution with the prior γ i (Step 10). Here, y ijj ′ = 1 if and only if s e ij and s f ij ′ are aligned; otherwise, y ijj ′ = 0. Note that if segment-level alignments are observed, then Steps 8-11 are skipped. Later, a set of aligned segment sets AS i is generated based on y i (Step 12). For example, given</p><formula xml:id="formula_4">d e i = {s e i1 , s e i2 }, d f i = {s f i1 , s f i2 , s f i3 }</formula><p>, y i11 and y i12 are 1, and the other y's are 0,</p><formula xml:id="formula_5">AS i = {AS i1 = {s e i1 , s f i1 , s f i2 }, AS i2 = {s e i2 }, AS i3 = {s f i3 }} is generated in</formula><p>Step 12. Then, for each aligned segment set AS ig (g ∈ {1, ..., |AS i |}), the per-segment topic distribution ν ig is obtained from a Pitman-Yor process with the base measure θ i , the concentration parame- ter a, and the discount parameter b (Step 14). Through Steps 12-15, aligned segments indicated by y share the same per-segment topic distribu- tion. For instance, s e i1 , s f i1 , and s f i2 have the same topic distribution ν i1 ∼ PYP(a, b, θ i ) in the above example.</p><p>Then, for each word at m ∈ {1, ..., N l ij } in segment s l ij in document d l i in language l, a la- tent topic assignment z l ijm is drawn from a multi- nomial distribution with the prior ν ig <ref type="bibr">(Step 20)</ref>, where g denotes the index of the element set of AS i that includes the segment s l ij , e.g., g for s f i2 is 1. Subsequently, a word w l ijm is drawn based on the assigned topic z l ijm and the language-specific per-topic word distribution ϕ l in the same manner as in BiLDA (Step 21). <ref type="table">Table count</ref> of topic k in the CRP for ali- gned segment set g in document pair i.</p><formula xml:id="formula_6">t igk</formula><formula xml:id="formula_7">t ig K-dimensional vector, where k-th value is t igk . t ig·</formula><p>Total table count in aligned segment set g in document pair i, i.e., ∑ k t igk . n igk Total number of words with topic k in al- igned segment set g in document pair i. n ig· Total number of words in aligned segme- nt set g in document pair i, i.e., <ref type="table">Table 1</ref>: Statistics used in our Inference</p><formula xml:id="formula_8">∑ k n igk . M l kw Total number of word w with topic k in language l. M l k |W l |-dimensional vector, where w-th value is M l kw .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inference for BiSTM</head><p>In inference, we find the set of latent variables θ, ν, z, and ϕ that maximizes their posterior probability given the model parameters α, β and observations w, y, i.e., p(θ, ν, z, ϕ|α, β, w, y).</p><p>Here, a language-dependent variable without a su- perscript denotes both of the variable in language e and that in f , e.g., z = {z e , z f }. Unfortu- nately, as in other probabilistic topic models, such as LDA and BiLDA, we cannot compute this pos- terior using an exact inference method. This sec- tion presents an approximation method for BiSTM based on blocked Gibbs sampling, inspired by <ref type="bibr" target="#b9">Du et al. (2013)</ref>. In our inference, the hierarchy in BiSTM, i.e., the generation of ν and z, is explained by the Chinese restaurant process (CRP), through which the parameters θ, ν, and ϕ are integrated out, and the statistics on table counts in the CRP, t, are introduced. <ref type="table">Table 1</ref> lists all statistics used in our inference, where W l denotes a vocabulary set in language l. Moreover, to accelerate con- vergence, we introduce an auxiliary binary vari- able δ l ijm for w l ijm , indicating whether w l ijm is the first customer on a table (δ l ijm = 1) or not (δ l ijm = 0), and t igk is computed based on δ in the same manner as in :</p><formula xml:id="formula_9">t igk = ∑ s l ij ∈AS ig N l ij ∑ m=1 δ l ijm I(z l ijm = k), where I(x)</formula><p>is a function that returns 1 if the condition x is true and 0 otherwise. Our inference groups z l ijm and δ l ijm (each group is called a "block") and jointly samples them.</p><p>Moreover, if y is not observed, our inference al- ternates two different kinds of blocks, (z l ijm , δ l ijm ) and y ijj ′ . In each sampling, individual variables are resampled, conditioned on all other variables. In the following, we describe each sampling stage.</p><formula xml:id="formula_10">Sampling (z, δ):</formula><p>The joint posterior distribution of z, w, and δ is induced in a manner similar to that in <ref type="bibr" target="#b7">Du et al. (2010;</ref></p><formula xml:id="formula_11">: p(z, w, δ|α, β, a, b, y) = D ∏ i=1 ( Beta K (α + ∑ AS i t ig ) Beta K (α) ∏ AS i ( (b|a) t ig· (b) n ig· K ∏ k=1 S ( n igk , t igk , a ) ( n igk t igk ) −1 ) ) K ∏ k=1 ( Beta W e (β e + M e k ) Beta W e (β e ) Beta W f (β f + M f k ) Beta W f (β f ) ) ,</formula><p>where Beta K (·) and Beta W l (·) are K-and |W l |- dimensional beta functions, respectively, (b|a) n is the Pochhammer symbol 3 , and (b) n is given by (b|1) n . S(n, m, a) is a generalized Stirling num- ber of the second kind ( <ref type="bibr" target="#b13">Hsu and Shiue, 1998</ref>), which is given by the linear recursion S(n + 1, m, a) = S(n, m − 1, a) + (n − ma)S(n, m, a).</p><p>To reduce computational cost, the Stirling num- bers are preliminarily calculated in a logarithm format <ref type="bibr" target="#b3">(Buntine and Hutter, 2012)</ref>. Then, the cached values are used in our sampling.</p><p>The joint conditional distributions of z l ijm and δ l ijm are obtained from the above joint distribution using Bayes' rule:</p><formula xml:id="formula_12">p(z l ijm = k, δ l ijm = 1|z −z l ijm , w, δ −δ l ijm , α, β, a, b, y) = β l w l ijm + M l kw l ijm ∑ w∈W l (β l w + M l kw ) α k + ∑ AS i t igk ∑ K k=1 (α k + ∑ AS i t igk ) b + at ig ′ · b + n ig ′ · S(n ig ′ k + 1, t ig ′ k + 1, a) S(n ig ′ k , t ig ′ k , a) t ig ′ k + 1 n ig ′ k + 1 , p(z l ijm = k, δ l ijm = 0|z −z l ijm , w, δ −δ l ijm , α, β, a, b, y) = β l w l ijm + M l kw l ijm ∑ w∈W l (β l w + M l kw ) 1 b + n ig ′ · S(n ig ′ k + 1, t ig ′ k , a) S(n ig ′ k , t ig ′ k , a) n ig ′ k + 1 − t ig ′ k n ig ′ k + 1 ,</formula><p>where s l ij is included in AS ig ′ .</p><p>Sampling y:</p><p>In our inference, each aligned segment set cor- responds to a restaurant in the CRP. We regard the sampling of y ijj ′ as the choice of splitting or merging restaurant(s) in a manner similar to that in the sampling of segmentation boundaries in <ref type="bibr" target="#b9">Du et al. (2013)</ref>. In particular, if y ijj ′ = 0, then one aligned segment set AS m is split into two aligned segment sets AS l and AS r , where AS l , AS r , and AS m include s e ij , s f ij ′ , and both, re- spectively. If y ijj ′ = 1, then AS</p><note type="other">l and AS r are merged to AS m . For simplicity, our inference specifies AS l and AS r based on the current y as follows: if AS i (s e ij ) = AS i (s f ij ′ ), then AS l = {s e ij } ∪ AS f i (s e ij ) \ {s f ij ′ } and AS r = {s f ij ′ } ∪ AS e i (s f ij ′ )\{s e ij }; otherwise, AS l = AS i (s e ij ) and AS r = AS i (s f ij ′ ). Here, AS i (j) is the element set of AS i that includes the segment j, and AS l i (j) is the set of segments in language l included in AS i (j). For example, in the example in Section 3, AS i (s f i1 ) = AS i1 = {s e i1 , s f i1 , s f i2 }, AS e i (s f i1 ) = {s e i1 }, and AS</note><formula xml:id="formula_13">f i (s f i1 ) = {s f i1 , s f i2 }. In addition, if y i11 = 0, then AS m = {s e i1 , s f i1 , s f i2 } is split into AS l = {s e i1 } ∪ AS f i (s e i1 ) \ {s f i1 } = {s e i1 , s f i2 } and AS r = {s f i1 } ∪ AS e i (s f i1 ) \ {s e i1 } = {s f i1 }. If y i23 = 1, then AS l = AS i (s e i2 ) = {s e i2 } and AS r = AS i (s f i3 ) = {s f i3 } are merged to AS m = {s e i2 , s f i3 }.</formula><p>The conditional distributions of y ijj ′ are as follows: p(y ijj ′ = 0|y −y ijj ′ , z, w, δ, α, a, b, η 0 , η 1 )</p><formula xml:id="formula_14">∝ η 0 + c i0 η 0 + η 1 + c i0 + c i1 Beta K ( α + ∑ AS i t ig ) ∏ g∈{AS l ,ASr} (b|a) t ig· (b) n ig· K ∏ k=1 S(n igk , t igk , a), p(y ijj ′ = 1|y −y ijj ′ , z, w, t \ T, α, a, b, η 0 , η 1 ) ∝ ∑ T ( η 1 + c i1 η 0 + η 1 + c i0 + c i1 Beta K ( α + ∑ AS i t ig ) (b|a) t i,ASm,· (b) n i,ASm,· K ∏ k=1 S(n i,ASm,k , t i,ASm,k , a) ) ,</formula><p>where T is the set of t igk such that for either or both of AS l and AS r , t igk = 1. c i0 and c i1 are the total number of y i 's whose values are 0 and that of y i 's whose values are 1, respectively. Note that we change y i 's that relate to the selected action (merging or splitting), in addition to y ijj ′ to maintain consistency between y and the aligned segment sets.</p><p>Inference of θ, ν, ϕ:</p><p>Although our inference does not directly estimate θ, ν, and ϕ, these variables can be inferred from the following posterior expected values via Algorithm 3 Generative Process for Segments 1:</p><formula xml:id="formula_15">for each document d l i (i ∈ {1, ..., D}) do 2: choose π l i ∼ Beta(λ 0 ,λ 1 ) 3: for each passage u l ih (h ∈ {1, ..., U l i }) do 4: choose ρ l ih ∼ Bernoulli(π l i ) 5</formula><note type="other">: end for 6: s l i = concatenate(u l i , ρ l i ) 7: end for sampling:</note><formula xml:id="formula_16">ˆ θ ik = E z i ,t i |w i ,α,β,a,b,y [ α k + ∑ AS i t igk ∑ K k=1 (α k + ∑ AS i t igk ) ] , ˆ ν igk = E z i ,t i |w i ,α,β,a,b,y [ n igk − at igk b + n ig· + θ ik at ig· + b b + n ig· ] , ˆ ϕ l kw = E z,t|w,α,β,a,b,y [ β l w + M l kw ∑ w ′ ∈W l (β l w ′ + M l kw ′ )</formula><p>] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Integration of Topic Segmentation into BiSTM (BiSTM+TS)</head><p>To infer segmentation boundaries simultaneously with cross-lingual topics, we integrate the unsu- pervised Bayesian topic segmentation method pro- posed by <ref type="bibr" target="#b9">Du et al. (2013)</ref> into the proposed BiSTM (BiSTM+TS).</p><p>We assume that each segment is a sequence of topically-related passages. In particular, we con- sider a sentence as a passage. Our segmenta- tion model defines a segment in document d l i by a boundary indicator variable ρ l ih for each pas-  <ref type="bibr">(</ref>Step 6).</p><formula xml:id="formula_17">sage u l ih (h ∈ {1, ..., U l i }); ρ l ih is 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inference for BiSTM+TS</head><p>Our inference for BiSTM+TS alternates three dif- ferent kinds of blocks, sampling of ρ and sam- plings for BiSTM ((z, δ) and y). The conditional distribution of ρ comprises the Gibbs probability for splitting one segment s m into two segments s r and s l by placing the boundary after u l ih (ρ l ih = 1) and that for merging s r and s l to s m by removing the boundary after u l ih (ρ l ih = 0). These probabilities are estimated in the same manner as the conditional probabilities of y ijj ′ , where y (y ijj ′ = 0, 1), AS l , AS r , AS m , η 0 , and η 1 are replaced with ρ (ρ l ih = 1, 0), s l , s r , s m , λ 1 , and λ 0 , respectively, and the statistics t and n are summed for every segment rather than for every aligned segment set (see Equation <ref type="formula">(6)</ref> and <ref type="formula">(9)</ref> in <ref type="bibr" target="#b9">Du et al. (2013)</ref>).</p><p>Our inference assumes that sampling ρ does not depend on aligned segments in the other lan- guage, i.e., y 4 . After splitting or merging, we set the y's of s m , s l , and s r as follows: if s m is split into s l and s r , then AS(s l ) = AS(s m ) and AS(s r ) = AS(s m ); if s l and s r are merged to s m , then AS(s m ) = AS(s l ) ∪ AS(s r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We evaluated the proposed models in terms of perplexity and performance in translation pair extraction, which is a well-known application that uses a bilingual topic model. We used a document-aligned comparable corpus comprising 3,995 document pairs, each of which is a Japanese Wikipedia article in the Kyoto Wiki Corpus 5 and its corresponding English Wikipedia article 6 . Note that the English articles were collected from the English Wikipedia database dump (2 June 2015) <ref type="bibr">7</ref> based on inter-language links, even though the original Kyoto Wiki corpus is a parallel corpus, in which each sentence in the Japanese articles is manually translated into English. Thus, our ex- perimental data is not a parallel corpus. We ex- tracted texts from the collected English articles using an open-source script <ref type="bibr">8</ref> . All Japanese and English texts were segmented using MeCab <ref type="bibr">9</ref> and TreeTagger <ref type="bibr">10 (Schmid, 1994)</ref>, respectively. Then, function words were removed, and the remaining words were lemmatized to reduce data sparsity.</p><p>For translation extraction experiments, we au- tomatically created a gold-standard translation set according to <ref type="bibr" target="#b16">Liu et al. (2013)</ref>. We first com- puted p(w e |w f ) and p(w f |w e ) by running IBM Model 4 on the original Kyoto Wiki corpus, which is a parallel corpus, using GIZA++ <ref type="bibr" target="#b22">(Och and Ney, 2003)</ref>, and then extracted word pairs ( ˆ w e , ˆ w f ) that satisfy both of the following con- ditions: ˆ w e = argmax w e p(w e |w f = ˆ w f ) andˆw andˆ andˆw f = argmax w f p(w f |w e = ˆ w e ). Finally, we eliminated word pairs that do not appear in the document pairs in the document-aligned compa- rable corpus. We used all 7,930 Japanese words in the resulting gold-standard set as the evaluation input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Competing Methods</head><p>We compared the proposed models (BiSTM and BiSTM+TS) with a standard bilingual topic model (BiLDA). BiSTM considers each section in Wikipedia articles as a segment. Note that align- ments between sections are not given in our exper- imental data. Thus, y is inferred in both BiSTM and BiSTM+TS.</p><p>As in the proposed models, BiLDA was trained using Gibbs sampling ( <ref type="bibr" target="#b17">Mimno et al., 2009;</ref><ref type="bibr" target="#b20">Ni et al., 2009;</ref><ref type="bibr" target="#b29">Vuli´cVuli´c et al., 2015</ref>). In the training of each model, each variable was first initialized. Here, z l ijm is randomly initialized to an integer be- tween 1 and K, and each of δ l ijm , y ijj ′ , and ρ l ih is randomly initialized to 0 or 1. We then performed 10,000 Gibbs iterations. We used the symmetric prior α k = 50/K and β l w = 0.01 over θ and ϕ l , respectively, in accordance with Vuli´c <ref type="bibr" target="#b27">Vuli´c et al. (2011)</ref>. The hyperparameters a, b, λ 0 , and λ 1 were set to 0.2, 10, 0.1, and 0.1, respectively, in accor- dance with <ref type="bibr" target="#b7">Du et al. (2010;</ref>. Both η 0 and η 1 were set to 0.2 as a result of preliminary exper- iments. We used several values of K to measure the impact of topic size: we used K = 100 and K = 400 in accordance with <ref type="bibr" target="#b16">Liu et al. (2013)</ref> in addition to the suggested value K = 2, 000 in Vuli´c <ref type="bibr" target="#b27">Vuli´c et al. (2011)</ref>.</p><p>In the translation extraction experiments, Model K=100 K=400 K=2,000 BiLDA 693.6 530.7 479.9 BiSTM 520.1 429.3 394.6 BiSTM+TS 537. <ref type="bibr">5</ref> 445.3 411.8 <ref type="table">Table 2</ref>: Test Set Perplexity we used two translation extraction methods, i.e., Cue (Vuli´cVuli´c et al., 2011) and Liu ( <ref type="bibr" target="#b16">Liu et al., 2013)</ref>. Both methods first infer cross- lingual topics for words using a bilingual topic model (BiLDA/BiSTM/BiSTM+TS) and then extract word pairs (w e , w f ) with a high value of the probability p(w e |w f ) de- fined by the inferred topics. Cue calculates</p><formula xml:id="formula_18">p(w e |w f ) = ∑ K k=1 p(w e |k)p(k|w f ), where p(k|w) ∝ p(w|k) ∑ K k=1 p(w|k)</formula><p>and p(w|k) = ϕ kw .</p><p>Liu first converts a document-aligned com- parable corpus into a topic-aligned parallel corpus according to the topics of words and computes p(w e |w f , k) by running IBM Model 1 on the parallel corpus.</p><formula xml:id="formula_19">Liu then calcu- lates p(w e |w f ) = ∑ K k=1 p(w e |w f , k)p(k|w f ).</formula><p>Hereafter, a bilingual topic model used in an extraction method is shown in parentheses, e.g., Cue(BiLDA) denotes Cue with BiLDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>We evaluated the predictive performance of each model by computing the test set perplexity based on 5-fold cross validation. A lower perplexity in- dicates better generalization performance. <ref type="table">Table  2</ref> shows the perplexity of each model. As can be seen, BiSTM and BiSTM+TS are better than BiLDA in terms of perplexity.</p><p>We measured the performance of translation ex- traction with top N accuracy (ACC N ), the number of test words whose top N translation candidates contain a correct translation over the total num- ber of test words <ref type="bibr">(7,</ref><ref type="bibr">930)</ref>. <ref type="table" target="#tab_2">Table 3</ref> summarizes ACC 1 and ACC 10 for each model. As can be seen, Cue/Liu(BiSTM) and Cue/Liu(BiSTM+TS) significantly outperform Cue/Liu(BiLDA) (p &lt; 0.01 in the sign test). This indicates that BiSTM and BiSTM+TS improve the performance of trans- lation extraction for both the Cue and Liu methods by assigning more suitable topics.</p><p>Both experiments prove that capturing segment- level alignments is effective for modeling bilin- gual data. In addition, these experiments show that BiSTM+TS is comparable with BiSTM, indicat-</p><formula xml:id="formula_20">ACC 1 Method K=100 K=400 K=2,000</formula><p>Cue <ref type="formula">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="195">174</head><p>Inference y = 0 43 1132 ing that the proposed model could yield a signifi- cant benefit even if the boundaries of segments are unknown. <ref type="table" target="#tab_2">Tables 2 and 3</ref> show that a larger topic size yields better performance for each model. Fur- thermore, Liu outperforms Cue regardless of the choice of bilingual topic models, which is con- sistent with previously reported results ( <ref type="bibr" target="#b16">Liu et al., 2013)</ref>. The results of our experiments demonstrate that the proposed models have the same tendencies as BiLDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Inferred Segment-level Alignments</head><p>We created a reference set to evaluate segment- level alignments y inferred by BiSTM (K=2,000). We randomly selected 100 document pairs from the comparable corpus and then manually iden- tified cross-lingual alignments between sections. <ref type="table" target="#tab_3">Table 4</ref> shows the distribution of inferred y values and that of y values in the reference set. As can be seen, the accuracy of y is 0.859 <ref type="bibr">(1,327/1,544)</ref>.</p><p>The majority of false negatives (121/174) are sections that are not parallel but correspond par- tially. An example is the alignment between the Model Japanese article English article BiSTM 4.8 2.9 BiSTM+TS 10.6 4.1 <ref type="table">Table 5</ref>: Average Number of Segments Japanese section "history" and the English sec- tion "Bujutsu (old type of Budo)" in the "Budo (a Japanese martial art)" article pair, where a part of the English section "Bujutsu" is described in the Japanese section "history." Such errors might not necessarily have a negative effect, because partial alignments can be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Inferred Segmentation Boundaries</head><p>This section compares segment boundaries in- ferred by BiSTM+TS (K=2,000) with section boundaries in the original articles, which have been referred to by BiSTM. The recall of BiSTM+TS for the original section boundaries is 0.727. This indicates that the unsupervised segmentation in BiSTM+TS finds drastic topical changes, i.e., section boundaries, with high recall. <ref type="table">Table 5</ref> shows the average number of seg- ments per article for each model. As can be seen, BiSTM+TS divides an article into segments smaller than the original sections. This seems to be reasonable, because some original sections in- clude multiple topics. However, <ref type="table" target="#tab_2">Tables 2 and 3</ref> show that inferred boundaries do not work better than section boundaries. One reason for that is that some errors are caused by a sparseness prob- lem, when BiSTM+TS separates an article into ex- tremely fine-grained segments. In addition, <ref type="table">Table  5</ref> reveals that BiSTM+TS increases the gap be- tween languages. Thus, segmentation with a com- parable granularity between languages might be favorable for the proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effectiveness for an English-French Wikipedia Corpus</head><p>We evaluated BiLDA, BiSTM, and BiSTM+TS in terms of perplexity and performance in translation extraction on an English-French Wikipedia corpus to verify the effectiveness of the proposed models for language pairs other than English-Japanese. The settings, e.g., parameters, for each model are the same as in Section 5. Note that we report only the performances of each model with K = 2, 000, because all models achieved the best performances when K = 2, 000.  We collected French articles that correspond to the English articles used in the experiments in Section 5, from the French Wikipedia database dump (2 June 2015) based on inter-language links. As a result, our English-French corpus comprises 3,159 document pairs. The French articles were preprocessed in the same manner as the English ar- ticles: text extraction using the open-source script, segmentation using TreeTagger, removal of func- tion words, and lemmatization.</p><p>We created a gold-standard translation set for translation extraction experiments using Google Translate service 11 in a manner similar to that in <ref type="bibr" target="#b11">Gouws et al. (2015)</ref> and <ref type="bibr" target="#b6">Coulmance et al. (2015)</ref>, translating the French words in our corpus us- ing Google Translate, and then eliminating word pairs that do not appear in the document pairs in our corpus. We used the top 1,000 most frequent French words in the resulting gold-standard set as the evaluation input. <ref type="table" target="#tab_5">Table 6</ref> summarizes ACC 1 , ACC 10 , and per- plexity. It shows that the proposed models are ef- fective also for the English-French Wikipedia cor- pus. BiSTM and BiSTM+TS outperform BiLDA in terms of perplexity and performance of transla- tion extraction, and BiSTM+TS works well even if the boundaries of segments are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Multilingual topic models other than BiLDA (Sec- tion 2) have been proposed for document-aligned comparable corpora. <ref type="bibr" target="#b10">Fukumasu et al. (2012)</ref> ap- plied SwitchLDA ( <ref type="bibr" target="#b19">Newman et al., 2006</ref>) and Cor- respondence LDA , which were originally intended to work with multimodal data, such as annotated image data, to modeling multilingual text data. They also proposed a sym- metric version of Correspondence LDA. <ref type="bibr" target="#b24">Platt et al. (2010)</ref> projected monolingual models based on PLSA or Principal Component Analysis into a shared multilingual space with the constraint that document pairs must map to similar locations. <ref type="bibr" target="#b14">Hu et al. (2014)</ref> proposed a multilingual tree-based topic model that uses a hierarchical bilingual dic- tionary in addition to document alignments. Note that these models do not consider segment-level alignments.</p><p>There are several multilingual topic models tai- lored for data other than a document-aligned com- parable corpus, including bilingual topic mod- els for word alignment and machine translation on parallel sentence pairs ( <ref type="bibr" target="#b32">Zhao and Xing, 2006;</ref><ref type="bibr" target="#b33">Zhao and Xing, 2008)</ref>. Some models have mined multilingual topics from unaligned text data by bridging the gap between different lan- guages using a bilingual dictionary <ref type="bibr" target="#b15">(Jagarlamudi and Daumé III, 2010;</ref><ref type="bibr" target="#b31">Zhang et al., 2010;</ref><ref type="bibr" target="#b18">Negi, 2011)</ref>. Boyd-Graber and <ref type="bibr" target="#b2">Blei (2009)</ref> used parallel sentences in combination with a bilingual dictio- nary. However, these models have the drawback that they require a parallel corpus or a bilingual dictionary in advance, which cannot be obtained for some language pairs or domains.</p><p>In a monolingual setting, some topic models that consider segment-level topics have been pro- posed. <ref type="bibr" target="#b7">Du et al. (2010)</ref> considered a document as a set of segments and generated each per-segment topic distribution from the topic distribution of the related document through a Pitman-Yor process. Others have considered a document as a sequence of segments. <ref type="bibr">Cheng et al. (2009)</ref> reflected the un- derlying sequences of segments' topics by posit- ing a permutation distribution over a document. <ref type="bibr" target="#b30">Wang et al. (2011)</ref> modeled topical sequences in documents with a latent first-order Markov chain, and <ref type="bibr" target="#b8">Du et al. (2012)</ref> generated each per-segment topic distribution from the topic distribution of its document and that of its previous segment. Note that none of these models have been extended to a multilingual setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper, we proposed BiSTM, which models a document hierarchically and deals with segment- level alignments. BiSTM assigns the same topic distribution to both aligned documents and aligned segments. We also presented an extended model, BiSTM+TS, that infers segmentation boundaries in addition to latent topics by incorporating unsu- pervised topic segmentation ( <ref type="bibr" target="#b9">Du et al., 2013)</ref>. Our experimental results show that capturing segment- level alignments improves perplexity and transla- tion extraction performance, and that BiSTM+TS yields a significant benefit even if the boundaries of segments are not given.</p><p>This paper presented an extension to BiLDA, but hierarchical structures can also be incorporated into other bilingual topic models (Section 7). As future work, we would like to verify the effec- tiveness of the proposed models for other datasets or other cross-lingual tasks, such as cross-lingual document classification ( <ref type="bibr" target="#b20">Ni et al., 2009;</ref><ref type="bibr" target="#b24">Platt et al., 2010;</ref><ref type="bibr" target="#b21">Ni et al., 2011;</ref>) and cross-lingual information retrieval <ref type="bibr" target="#b28">(Vuli´cVuli´c et al., 2013</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Wikipedia Article Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical Model of BiLDA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graphical Model of BiSTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Performance of Translation Extraction</head><label>3</label><figDesc></figDesc><table>Reference y = 1 Reference y = 0 

Inference y = 1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Distribution of Segment-level Align-
ments 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Performance on an English-French 
Wikipedia Corpus (K = 2, 000) 

</table></figure>

			<note place="foot" n="1"> In this work, we deal with a bilingual setting, but our approach can be extended straightforwardly to apply to more than two languages. 2 In this study, we focus on models for a document-aligned comparable corpus. We describe other types of multilingual topic models and their limitations in Section 7.</note>

			<note place="foot" n="3"> (b|a)n = ∏ n−1 t=0 (b + ta).</note>

			<note place="foot" n="4"> We leave a bilingual extension of the topic segmentation, i.e., incorporation of y, for future work. 5 http://alaginrc.nict.go.jp/ WikiCorpus/index_E.html 6 We filtered out the Japanese articles that do not have corresponding English articles. 7 http://dumps.wikimedia.org/enwiki/ 8 https://github.com/attardi/ wikiextractor/</note>

			<note place="foot" n="9"> http://taku910.github.io/mecab/ 10 http://www.cis.uni-muenchen.de/ ˜ schmid/tools/TreeTagger/</note>

			<note place="foot" n="11"> http://translate.google.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Atsushi Fujita for valuable comments on earlier versions of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling Annotated Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilingual Topic Models for Unaligned Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1007.0296.pdf" />
		<title level="m">A Bayesian View of the Poisson-Dirichlet Process</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global Models of Document Structure using Latent Permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Harr Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="371" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sampling Table Configurations for the Hierarchical Poisson-Dirichlet Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="296" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transgram, Fast Cross-lingual Word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1109" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Segmented Topic Model Based on the Twoparameter Poisson-Dirichlet Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="5" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modelling Sequential Text with an Adaptive Topic Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="535" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic Segmentation with a Structured Topic Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Symmetric Correspondence Topic Models for Multilingual Text Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Fukumasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1286" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic Latent Semantic Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Unified Approach to Generalized Stirling Numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter Jau-Shyong</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="366" to="384" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Polylingual Tree-Based Topic Models for Translation Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1166" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting Multilingual Topics from Unaligned Comparable Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd European Conference on Advances in Information Retrieval</title>
		<meeting>the 32nd European Conference on Advances in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="444" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Topic Models + Word Alignment = A Flexible Framework for Extracting Bilingual Dictionary from Comparable Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Polylingual Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="880" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining Bilingual Topic Hierarchies from Unaligned Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Negi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="992" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical Entitytopic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="680" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mining Multilingual Topics from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International World Wide Web Conference</title>
		<meeting>the 18th International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1155" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross Lingual Text Classification by Mining Multilingual Topics from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fourth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Two-Parameter Poisson-Dirichlet Distribution Derived from a Stable Subordinator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="855" to="900" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Translingual Document Representations from Discriminative Projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic Part-of-Speech Tagging Using Decision Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on New Methods in Language Processing</title>
		<meeting>the International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge Transfer Across Multilingual Corpora via Latent Topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wim De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining</title>
		<meeting>the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="549" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying Word Translations from Comparable Corpora Using Latent Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-Language Information Retrieval Models Based on Latent Topic Models Trained with Document-Aligned Comparable Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Probabilistic Topic Modeling in Multilingual Settings: An Overview of Its Methodology and Applications. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="111" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structural Topic Model for Latent Topical Structure Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1526" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-Lingual Latent Topic Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1128" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BiTAM: Bilingual Topic AdMixture Models for Word Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1689" to="1696" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
