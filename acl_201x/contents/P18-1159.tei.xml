<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1715" to="1724"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1715</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select two-stage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components , especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Teaching machines to read and comprehend hu- man languages is a long-standing objective in nat- ural language processing. In order to evaluate this ability, reading comprehension (RC) is designed to answer questions through reading relevant pas- sages. In recent years, RC has attracted intense in- terest. Various advanced neural models have been proposed along with newly released datasets <ref type="bibr" target="#b8">(Hermann et al., 2015;</ref><ref type="bibr" target="#b15">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b6">Dunn et al., 2017;</ref><ref type="bibr" target="#b5">Dhingra et al., 2017b;</ref><ref type="bibr" target="#b7">He et al., 2017</ref>). Daiquiris are a family of cocktails whose main ingredients are rum and lime juice. P4 A homemade Cuba Libre Preparation</p><p>To make a Cuba Libre properly, fill a highball glass with ice and half fill with cola.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P5 The difference between the Cuba</head><p>Libre and Rum is a lime wedge at the end. The key information is marked in italic, which should be combined from different text pieces to select the correct answer "Cuba Libre".</p><p>Most existing approaches mainly focus on mod- eling the interactions between questions and pas- sages ( <ref type="bibr" target="#b4">Dhingra et al., 2017a;</ref><ref type="bibr" target="#b17">Seo et al., 2017;</ref>, paying less attention to infor- mation concerning answer candidates. However, when human solve this problem, we often first read each piece of text, collect some answer candi- dates, then focus on these candidates and combine their information to select the final answer. This collect-then-select process can be more significant in open-domain scenarios, which require the com- bination of candidates from multiple passages to answer one single question. This phenomenon is illustrated by the example in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>With this motivation, we formulate an extract- then-select two-stage architecture to simulate the above procedure. The architecture contains two components: (1) an extraction model, which gen- erates answer candidates, (2) a selection model, which combines all these candidates and finds out the final answer. However, answer candidates to be focused on are often unobservable, as most RC datasets only provide golden answers. Therefore, we treat candidate extraction as a latent variable and train these two stages jointly with reinforce- ment learning (RL).</p><p>In conclusion, our work makes the following contributions:</p><p>1. We formulate open-domain reading compre- hension as a two-stage procedure, which first ex- tracts answer candidates and then selects the final answer. With joint training, we optimize these two correlated stages as a whole.</p><p>2. We propose a novel answer selection model, which combines the information from all the ex- tracted candidates using an attention-based corre- lation matrix. As shown in experiments, the infor- mation fusion is greatly helpful for answer selec- tion.</p><p>3. With the two-stage framework and the joint training strategy, our method significantly sur- passes the state-of-the-art performance on two challenging public RC datasets Quasar-T ( <ref type="bibr" target="#b5">Dhingra et al., 2017b</ref>) and SearchQA ( <ref type="bibr" target="#b6">Dunn et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years, reading comprehension has made remarkable progress in methodology and dataset construction. Most existing approaches mainly focus on modeling sophisticated interactions be- tween questions and passages, then use the pointer networks ( <ref type="bibr" target="#b21">Vinyals et al., 2015)</ref> to directly model the answers ( <ref type="bibr" target="#b4">Dhingra et al., 2017a;</ref><ref type="bibr" target="#b22">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b17">Seo et al., 2017;</ref>. These methods prove to be effective in existing close-domain datasets ( <ref type="bibr" target="#b8">Hermann et al., 2015;</ref><ref type="bibr" target="#b9">Hill et al., 2015;</ref><ref type="bibr" target="#b15">Rajpurkar et al., 2016)</ref>.</p><p>More recently, open-domain RC has attracted increasing attention <ref type="bibr" target="#b14">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b6">Dunn et al., 2017;</ref><ref type="bibr" target="#b5">Dhingra et al., 2017b;</ref><ref type="bibr" target="#b7">He et al., 2017)</ref> and raised new challenges for question answer- ing techniques. In these scenarios, a question is paired with multiple passages, which are often collected by exploiting unstructured documents or web data. Aforementioned approaches often rely on recurrent neural networks and sophisticated at- tentions, which are prohibitively time-consuming if passages are concatenated altogether. There- fore, some work tried to alleviate this problem in a coarse-to-fine schema. <ref type="bibr" target="#b23">Wang et al. (2018a)</ref> com- bined a ranker for selecting the relevant passage and a reader for producing the answer from it. However, this approach only depended on one pas- sage when producing the answer, hence put great demands on the precisions of both components. Worse still, this framework cannot handle the sit- uation where multiple passages are needed to an- swer correctly. In consideration of evidence aggre- gation, <ref type="bibr" target="#b24">Wang et al. (2018b)</ref> proposed a re-ranking method to resolve the above issue. However, their re-ranking stage was totally isolated from the can- didate extraction procedure. Being different from the re-ranking perspective, we propose a novel se- lection model to combine the information from all the extracted candidates. Moreover, with rein- forcement learning, our candidate extraction and answer selection models can be learned in a joint manner. <ref type="bibr" target="#b20">Trischler et al. (2016)</ref> also proposed a two-step extractor-reasoner model, which first ex- tracted K most probable single-token answer can- didates and then compared the hypotheses with all the sentences in the passage. However, in their work, each candidate was considered isolat- edly, and their objective only took into account the ground truths compared with our RL treatment.</p><p>The training strategy employed in our paper is reinforcement learning, which is inspired by recent work exploiting it into question answer- ing problem. The above mentioned coarse-to-fine framework ( <ref type="bibr" target="#b3">Choi et al., 2017;</ref><ref type="bibr" target="#b23">Wang et al., 2018a</ref>) treated sentence selection as a latent variable and jointly trained the sentence selection module with the answer generation module via RL. <ref type="bibr" target="#b18">Shen et al. (2017)</ref> modeled the multi-hop reasoning proce- dure with a termination state to decide when it is adequate to produce an answer. RL is suitable to capture this stochastic behavior. <ref type="bibr" target="#b10">Hu et al. (2018)</ref> merely modeled the extraction process, using F1 as rewards in addition to maximum likelihood es- timation. RL was utilized in their training process, as the F1 measure is not differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two-stage RC Framework</head><p>In this work, we mainly consider the open-domain extractive reading comprehension. In this sce- nario, a given question Q is paired with mul- tiple passages P = {P 1 , P 2 , ..., P N }, based on which we aim to find out the answer A. Moreover, the golden answers are almost subspans shown in some passages in P. Our main framework con- sists of two parts, which are: (1) extracting answer candidates C = {C 1 , C 2 , ..., C M } from passages P and <ref type="formula" target="#formula_1">(2)</ref> selecting the final answer A from candi- dates C. This process is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We design different models for each part and optimize them as a whole with joint reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Extraction</head><p>We build candidate set C by independently ex- tracting K candidates from each passage P i ac- cording to the following distribution:</p><formula xml:id="formula_0">p(C|Q, P) = N i p({C ij } K j=1 |Q, P i ) C = N i=1 {C ij } K j=1 (1)</formula><p>where C ij denotes the jth candidate extracted from the ith passage. K is set as a constant num- ber in our formulation. Taking K as 2 for an ex- ample, we denote each probability shown on the right side of Equation 1 through sampling without replacement:</p><formula xml:id="formula_1">p({C i1 , C i2 }) = p(C i1 )p(C i2 )/(1 − p(C i1 )) + p(C i1 )p(C i2 )/(1 − p(C i2 ))<label>(2)</label></formula><p>where we neglect Q, P i to abbreviate the condi- tional distributions in Equation 1.</p><p>Consequently, the basic block of our candidate extraction stage turns out to be the distribution of each candidate P (C ij |Q, P i ). In the rest of this subsection, we will elaborate on the model archi- tecture concerning candidate extraction, which is displayed in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><formula xml:id="formula_2">Question &amp; Passage Representation Firstly, we embed the question Q = {x k Q } l Q k=1</formula><p>and its rele- vant passage P = {x t P } l P t=1 ∈ P with word vectors to form Q ∈ R dw×l Q and P ∈ R dw×l P respec- tively, where d w is the dimension of word embed- dings, l Q and l P are the length of Q and P .</p><p>We then feed Q and P to a bidirectional LSTM to form their contextual representations</p><formula xml:id="formula_3">H Q ∈ R d h ×l Q and H P ∈ R d h ×l P : H Q = BiLSTM(Q) H P = BiLSTM(P)<label>(3)</label></formula><p>Question &amp; Passage Interaction Modeling the interactions between questions and passages is a critical step in reading comprehension. Here, we adopt the attention mechanism similar to ( <ref type="bibr" target="#b11">Lee et al., 2016</ref>) to generate question-dependent pas- sage representation</p><formula xml:id="formula_4">H P . Assume H Q = {h k Q } l Q k=1 , H P = {h t P } l P t=1</formula><p>, we have:</p><formula xml:id="formula_5">α tk = e h k Q ·h t P l Q k=1 e h k Q ·h t P 1 ≤ k ≤ l Q , 1 ≤ t ≤ l P h t P = l Q k=1 α tk h k Q 1 ≤ t ≤ l P H P ={ h t P } l P t=1</formula><p>(4) After concatenating two kinds of passage rep- resentations H P and H P , we use another bidirec- tional LSTM to get the final representation of ev- ery position in passage P as G P ∈ R dg×l P :</p><formula xml:id="formula_6">G P = BiLSTM([H P ; H P ])<label>(5)</label></formula><p>Candidate Scoring Then we use two linear transformations w b ∈ R 1×dg and w e ∈ R 1×dg to calculate the begin and the end scores for each po- sition:</p><formula xml:id="formula_7">{b t P } l Q t=1 = b P = w b G P {e t P } l Q t=1 = e P = w e G P<label>(6)</label></formula><p>At last, we model the probability of every sub- span in passage P as a candidate C = {x t P } Ce t=C b according to its begin and end position:</p><formula xml:id="formula_8">p(C|Q, P ) = exp(b C b P + e Ce P ) l P k=1 l P t=k exp(b k P + e t P )<label>(7)</label></formula><p>In this definition, the probabilities of all the valid answer candidates are already normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Selection</head><p>As the second part of our framework, the answer selection model finds out the most probable an- swer by calculating p(C|Q, P, C) for each candi- date C ∈ C. The model architecture is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Notably, selection model receives candidate set C as additional information. This more focused information allows the model to combine evi- dences from all the candidates, which would be useful for selecting the best answer.</p><p>For ease of understanding, we briefly describe the selection stage as follows. After being ex- tracted from a single passage, a candidate borrows information from other candidates across different passages. With this global information, the pas- sage is reread to confirm the correctness of the candidate further. The following are details about the selection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Representation</head><p>Questions are funda- mental for finding out the correct answer. As did for the extraction model, we embed the question Q with word vectors to form Q ∈ R dw×l Q . Then we use a bidirectional LSTM to establish its con- textual representation:</p><formula xml:id="formula_9">S q = BiLSTM(Q)<label>(8)</label></formula><p>A max-pooling operation across all the positions is followed to get the condensed vector represen- tation: Passage Representation Assume the candidate C is extracted from the passage P ∈ P. To be informed of C, we first build the representation of P . For every word in P , three kinds of features are utilized:</p><formula xml:id="formula_10">r q = MaxPooling(S q )<label>(9)</label></formula><p>• Word embedding: each word expresses its basic feature with the word vector.</p><p>• Common word: the feature has value 1 when the word occurs in the question, otherwise 0.</p><p>• Question independent representation: the condensed representation r q .</p><p>With these features, information not only in Q but also in P is considered. By concatenating them, we get r t P corresponding to every position t in pas- sage P . Then with another bidirectional LSTM, we fuse these features to form the contextual rep- resentation of P as S P ∈ R ds×l P :</p><formula xml:id="formula_11">R P = {r t P } l P t=1 S P = BiLSTM(R P )<label>(10)</label></formula><p>Candidate Representation Candidates provide more focused information for answer selection. Therefore, for each candidate, we first build its in- dependent representation according to its position in the passage, then construct candidates fused representation through combination of other cor- related candidates.</p><p>Given the candidate C = {x t P } Ce t=C b in the pas- sage P , we extract its corresponding span from S P = {s t P } l P t=1 to form S C = {s t P } Ce t=C b</p><p>as its contextual encoding. Moreover, we calculate its condensed vector representation through its begin and end positions:</p><formula xml:id="formula_12">r C = tanh(W b s C b P + W e s Ce P )<label>(11)</label></formula><p>where</p><formula xml:id="formula_13">W b ∈ R dc×ds , W e ∈ R dc×ds .</formula><p>To model the interactions among all the answer candidates, we calculate the correlations of the candidate C, which is assumed to be indexed by j in C, with others {C m } M m=1,m =j via attention mechanism:</p><formula xml:id="formula_14">V jm = w v tanh(W c r C + W o r Cm )<label>(12)</label></formula><p>where W c ∈ R dc×dc , W o ∈ R dc×dc and w v ∈ R 1×dc are linear transformations to capture the in- tensity of each interaction.</p><p>In this way, we form a correlation matrix V ∈ R M ×M , where M is the total number of candi- dates. With the correlation matrix, for the can- didate C, we normalize its interactions via a sof tmax operation, which emphasizes the influ- ence of stronger interactions:</p><formula xml:id="formula_15">α m = e V jm M m=1,m =j e V jm<label>(13)</label></formula><p>To take into account different influences of all the other candidates, it is sensible to generate a candidates fused representation according to the above normalized interactions:</p><formula xml:id="formula_16">r C = M m=1,m =j α m r Cm<label>(14)</label></formula><p>In this formulation, all the other candidates con- tribute their influences to the fused representation by their interactions with C, thus information from different passages is gathered altogether. In our experiments, this kind of information fusion is the key point for performance improvements.</p><p>Passage Advanced Representation As more focused information of the candidate C is avail- able, we are provided with a better way to confirm its correctness by rereading its corresponding pas- sage P . Specifically, we equip each position t in P with following advanced features:</p><p>• Passage contextual representation: the for- mer passage representation s t P .</p><p>• Candidate-dependent passage representation:</p><p>replace H Q with S C and H P with S P in Equation 4 to model the interactions between candidates and passages to form s t P .</p><p>• Candidate related distance feature: the rela- tive distance to the candidate C can be a ref- erence of the importance of each position.</p><p>• Candidate independent representation: use r C to consider the concerned candidate C.</p><p>• Candidates fused representation: use r C to consider all the other candidates interacting with the concerned candidate C.</p><p>With these features, we capture the information from the question, the passages and all the candi- dates. By concatenating them, we get u t P in every position in the passage P . Combining these fea- tures with a bidirectional LSTM, we get:</p><formula xml:id="formula_17">U P = {u t P } l P t=1 F P = BiLSTM(U P )<label>(15)</label></formula><p>Answer Scoring At last, the max pooling of each dimension of F P is performed, resulting in a condensed vector representation, which contains all the concerned information in a candidate:</p><formula xml:id="formula_18">z C = MaxPooling(F P )<label>(16)</label></formula><p>The final score of this candidate as the answer is calculated via a linear transformation, which is then normalized across all the candidates:</p><formula xml:id="formula_19">s = w z z C p(C|Q, P, C) = e s M k=1 e s k<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Training with RL</head><p>In our formulation, the answer candidate set in- fluences the result of answer selection to a large extent. However, with only golden answers pro- vided in the training data, it is not apparent which candidates should be considered further. To alleviate the above problem, we treat candi- date extraction as a latent variable, jointly train the extraction model and the selection model with re- inforcement learning. Formally, in the extraction and selection stages, two kinds of actions are mod- eled. The action space for the extraction model is to select from different candidate sets, which is formulated by Equation 1. The action space for the selection model is to select from all extracted candidates, which is formulated by Equation 17. Our goal is to select the final answer that leads to a high reward. Inspired by <ref type="bibr" target="#b23">Wang et al. (2018a)</ref>, we define the reward of a candidate to reflect its accordance with the golden answer:</p><formula xml:id="formula_20">r(C, A) =    2 if C == A f 1(C, A) else if C ∩ A = ∅ −1 else (18)</formula><p>where f 1(., .) ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> is the function to measure word-level F1 score between two sequences. In- corporating this reward can alleviate the overstrict requirements set by traditional maximum likeli- hood estimation as well as keep consistent with our evaluation methods in experiments.</p><p>The learning objective becomes to maximize the expected reward modeled by our framework, where θ stands for all the parameters involved: Each question is paired with 100 sentence-level passages retrieved from ClueWeb09 ( <ref type="bibr" target="#b1">Callan et al., 2009</ref>) based on Lucene.</p><formula xml:id="formula_21">L(θ) = −E C∼P (C|Q,P) [E C∼P (C|Q,P,C) r(C, A)] = −E C∼P (C|Q,P) [ C P (C|Q, P, C)r(C, A)]</formula><p>SearchQA ( <ref type="bibr" target="#b6">Dunn et al., 2017</ref>) starts from exist- ing question-answer pairs, which are crawled from J!Archive, and is augmented with text snippets re- trieved by Google, resulting in more than 140,000 question-answer pairs with each pair having 49.6 snippets on average.</p><p>The detailed statistics of these two datasets is shown in <ref type="table">Table 2.</ref> #q(train) #q(dev) #q <ref type="table" target="#tab_1">(test)  #p  Quasar-T  28,496  3,000  3,000  100  SearchQA  99,811  13,893  27,247  50   Table 2</ref>:</p><p>The statistics of our experimental datasets. #q represents the number of questions for each split of the datasets. #p is the number of passages for each question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Settings</head><p>We initialize word embeddings with the 300- dimensional Glove vectors 1 . All the bidirectional LSTMs hold 1 layer and 100 hidden units. All the linear transformations take the size of 100 as output dimension. The common word feature and the candidate related distance feature are embed- ded with vectors of dimension 4 and 50 respec- tively. By default, we set K as 2 in Equation 1, which means each passage generates two candi- dates based on the extraction model.</p><p>For ease of training, we first initialize our mod- els by maximum likelihood estimation and fine- tune them with RL. The similar training strategy is commonly employed when RL process is involved ( <ref type="bibr" target="#b16">Ranzato et al., 2015;</ref><ref type="bibr" target="#b12">Li et al., 2016a;</ref><ref type="bibr" target="#b10">Hu et al., 2018)</ref>. To pre-train the extraction model, we only use passages containing ground truths as training data. The log likelihood of Equation 7 is taken as the training objective for each question and pas- sage pair. After pre-training the extraction model, we use it to generate two top-scoring candidates from each passage, forming the training data to pre-train our selection model, and maximize the log likelihood of the Equation 17 as our second objective. In pre-training, we use the batch size of 30 for the extraction model, 20 for the selection model and RMSProp <ref type="bibr" target="#b19">(Tieleman and Hinton, 2012)</ref> with an initial learning rate of 2e-3. In fine-tuning with RL, we use the batch size of 5 and RMSProp with an initial learning rate of 1e-4. Also, we use a dropout rate of 0.1 in each training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>In addition to results of previous work, we add two baselines to demonstrate the effectiveness of our framework. The first baseline only applies the extraction model to score the answers, which is aimed at explaining the importance of the se- lection model. The second one only uses the pre-trained extraction model and selection model <ref type="bibr" target="#b4">Dhingra et al., 2017a)</ref> 26.4 26.4 - - BIDAF ( <ref type="bibr" target="#b17">Seo et al., 2017)</ref> 25.9 28.5 28.6 34.6 AQA ( <ref type="bibr" target="#b0">Buck et al., 2018)</ref> - - 38.7 45.6 R 3 <ref type="figure" target="#fig_0">(Wang et al., 2018a)</ref> 35.3 41.7 49.0 55.3 Re-Ranker ( <ref type="bibr" target="#b24">Wang et al., 2018b</ref></p><formula xml:id="formula_22">Quasar-T SearchQA EM F1 EM F1 GA (</formula><note type="other">) Strength-Based Re-Ranker (Probability) 36.1 42.4 50.4 56.5 Strength-Based Re-Ranker (Counting) 37.1 46.7 54.2 61.6 Coverage-Based Re-Raner 40.6 49.1 53.6 60.6 Full Re-Ranker 42.3 49.6 57.0 63.2 Our Methods Extraction Model 35.4 41.6 44.7 51.2 Extraction + Selection (Isolated Training) 41.6 49.5 49.7 56.6 Extraction + Selection (Joint Training)</note><p>45.9 53.9 58.3 64.2 <ref type="table">Table 3</ref>: Experimental results on the test set of Quasar-T and SearchQA. Full re-ranker is the ensemble of three different re-rankers in ( <ref type="bibr" target="#b24">Wang et al., 2018b</ref>).</p><p>to illustrate the benefits from our joint training schema.</p><p>The often used evaluation metrics for extractive RC are exact match (EM) and F1 ( <ref type="bibr" target="#b15">Rajpurkar et al., 2016)</ref>. The experimental results on Quasar-T and SearchQA are shown in <ref type="table">Table 3</ref>.</p><p>As seen from the results on Quasar-T, our quite simple extraction model alone almost reaches the state-of-the-art result compared with other meth- ods without re-rankers. The combination of the extraction and selection models exceeds our ex- traction baseline by a great margin, and also re- sults in performance surpassing the best single re- ranker in ( <ref type="bibr" target="#b24">Wang et al., 2018b</ref>). This result illus- trates the necessity of introducing the selection model, which incorporates information from all the candidates. In the end, by joint training with RL, our method produces better performance even compared with the ensemble of three different re- rankers.</p><p>On SearchQA, we find that our extraction model alone performs not that well compared with the state-of-the-art model without re-rankers. How- ever, the improvement brought by our selection model isolatedly or jointly trained still demon- strates the importance of our two-stage frame- work. Not surprisingly, comparing the results, our isolated training strategy still lags behind the sin- gle re-ranker proposed in ( <ref type="bibr" target="#b24">Wang et al., 2018b)</ref>, partly because of the deficiency with our extrac- tion model. However, uniting our extraction and selection models with RL makes up the dispar- ity, and the performance surpasses the ensemble of three different re-rankers, let alone the result of  any single re-ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Further Analysis</head><p>Effect of Features in Selection Model As the incorporation of the selection model improves the overall performance significantly, we conduct ab- lation analysis on the Quasar-T to prove the effec- tiveness of its major components. As shown in Ta- ble 4, all these components modeling the selection procedure play important roles in our final archi- tecture. Specifically, introducing the independent repre- sentation of the question and its common words with the passage seems an efficient way to con- sider the information of questions, which is con- sistent with previous work ( <ref type="bibr" target="#b13">Li et al., 2016b;</ref>.</p><p>As for features related to candidates, the incor- poration of the candidate independent information Q Cocktails : Rum , lime , and cola drink make a . A Cuba Libre P1</p><p>In Nicaragua , when it is mixed using Flor de Ca a -LRB-the national brand of rum -RRB-and cola , it is called a Nica Libre . P2</p><p>The drink ... Daiquiri The custom of mixing lime with rum for a cooling drink on a hot Cuban day has been around a long time .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P3</head><p>If you only learn to make two cocktails , the Manhattan should be one of them .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P4</head><p>Daiquiri Cocktail recipe for a Daiquiri , a classic rum and lime drink that every bartender should know .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P5</head><p>Hemingway Special Daiquiri : Daiquiris are a family of cocktails whose main ingredients are rum and lime juice . P6</p><p>In the Netherlands the drink is commonly called Baco , from the two ingredients of Bacardi rum and cola . P7</p><p>A homemade Cuba Libre Preparation To make a Cuba Libre prop- erly , fill a highball glass with ice and half fill with cola .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P8</head><p>Bacardi Cocktail Cocktail recipe for a Bacardi Cocktail , a clas- sic cocktail of Bacardi rum , lemon or lime juice and grenadine Roy Rogers -LRB-non-alcoholic -RRB-Cocktail recipe for a Roy Rogers , P9</p><p>Margarita Cocktail recipe for a Margarita , a popular refreshing tequila and lime drink for summer . P10 The difference between the Cuba Libre and Rum is a lime wedge at the end . contributes to the final result more or less. These features include candidate-dependent passage rep- resentation, candidate independent representation and candidate related distance feature. Most importantly, the candidates fused repre- sentation, which combines the information from all the candidates, demonstrates its indispensable role in candidate modeling, with a performance drop of nearly 8% when discarded. This phe- nomenon also verifies the necessity of our extract- then-select procedure, showing the importance of combining information scattered in different text pieces when picking out the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example for Candidates Fused Representation</head><p>We conduct a case study to demonstrate the im- portance of candidates fused information further. In <ref type="table" target="#tab_4">Table 5</ref>, each candidate only partly matches the description of the question in its independent con- text. To correctly answer the question, informa- tion in P 7 and P 10 should be combined. In exper- iments, our selection model provides the correct answer, while the wrong candidate "Daiquiri", a different kind of cocktail, is selected if candidates fused representation is discarded. The attention map established when modeling the fusion of can- didates (corresponding to Equation 13) in this ex- ample is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, in which we can see the interactions among all the candidates from    <ref type="figure">figure,</ref> it is obvious that the interaction of "Cuba Libre" in P 7 and P 10 is the key point to answer the question correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Candidate Number</head><p>The candidate ex- traction stage takes an important role to decide what information should be focused on further. Therefore, we also test the influence of different K when extracting candidates from each passage.</p><p>The results are shown in <ref type="table" target="#tab_6">Table 6</ref>. Taking K = 1 degrades the performance, which conforms to the expectation, as the correct candidates become less in this stricter situation. However, taking K = 3 can not improve the performance further. Al- though a larger K means a higher possibility to include good answers, it raises more challenges for the selection model to pick out the correct one from candidates with more varieties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we formulate the problem of RC as a two-stage process, which first generates candi- dates with an extraction model, then selects the final answer by combining the information from all the candidates. Furthermore, we treat can- didate extraction as a latent variable and jointly train these two stages with RL.</p><p>Experiments on public open-domain RC datasets Quasar-T and SearchQA show the necessity of introducing the selection model and the effectiveness of fusing candidates information when modeling. More- over, our joint training strategy leads to significant improvements in performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two-stage RC Framework. The first part extracts candidates (denoted with circles) from all the passages. The second part establishes interactions among all these candidates to select the final answer. The different gray scales of dashed lines between candidates represent different intensities of interactions.</figDesc><graphic url="image-1.png" coords="3,176.00,71.71,74.31,71.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Candidate Extraction Model Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Answer Selection Model Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Following REINFORCE algorithm, we approx- imate the gradient of the above objective with a sampled candidate set, C ∼ P (C|Q, P), resulting in the following form: L(θ) ≈ − C P (C|Q, P, C)r(C, A) −−logP (C|Q, P)[ C P (C|Q, P, C)r(C, A)] (20) 4 Experiments 4.1 Datasets We evaluate our models on two publicly available open-domain RC datasets, which are commonly adopted in related work. Quasar-T (Dhingra et al., 2017b) consists of 43,000 open-domain trivia questions and corresponding answers obtained from various internet sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The attention map generated when modeling candidates fused representations for the example in Table 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The answer candidates are in a bold font. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Ablation results concerning the selec-
tion model on the test set of Quasar-T. Obviously, 
candidates fused representation is the most evident 
feature when modeling the answer selection pro-
cedure. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>An example from Quasar-T to illustrate 
the necessity of fused information. Candidates ex-
tracted from passages are in a bold font. To cor-
rectly answer the question, information in P 7 and 
P 10 should be combined. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>Quasar-T 
EM 
F1 

K=1 
43.9 52.4 
K=2 
45.9 53.9 
K=3 
45.8 53.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Different number of extracted candidates 
results in different final performance on the test set 
of Quasar-T. 

different passages. In this </table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Basic Research Program of China (973 program, No. 2014CB340505). We thank Ying Chen and anony-mous reviewers for valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ask the right questions: Active question reformulation with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Clueweb09 data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coarse-to-fine question answering for long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="209" to="220" />
		</imprint>
	</monogr>
	<note>Alexandre Lacoste, and Jonathan Berant</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gatedattention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quasar: Datasets for question answering by search and reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dureader: a chinese machine reading comprehension dataset from real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05073</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dataset and neural recurrent sequence labeling model for opendomain factoid question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06275</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 colocated with the 30th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural language comprehension with the epireader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="128" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">R3: Reinforced reader-ranker for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evidence aggregation for answer re-ranking in open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
