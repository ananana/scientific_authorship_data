<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Minimum Risk Training for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Minimum Risk Training for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1683" to="1692"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differ-entiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, end-to-end neural machine transla- tion (NMT) <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> has attracted increasing attention from the com- munity. Providing a new paradigm for machine translation, NMT aims at training a single, large neural network that directly transforms a source- language sentence to a target-language sentence without explicitly modeling latent structures (e.g., word alignment, phrase segmentation, phrase re- ordering, and SCFG derivation) that are vital in conventional statistical machine translation (SMT) ( <ref type="bibr" target="#b2">Brown et al., 1993;</ref><ref type="bibr" target="#b12">Koehn et al., 2003;</ref><ref type="bibr" target="#b3">Chiang, 2005)</ref>.</p><p>Current NMT models are based on the encoder- decoder framework ( <ref type="bibr" target="#b4">Cho et al., 2014;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014)</ref>, with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language sen- tence. While early efforts encode the input into a * Corresponding author: Yang Liu. fixed-length vector, <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> advo- cate the attention mechanism to dynamically gen- erate a context vector for a target word being gen- erated.</p><p>Although NMT models have achieved results on par with or better than conventional SMT, they still suffer from a major drawback: the models are op- timized to maximize the likelihood of training data instead of evaluation metrics that actually quantify translation quality. <ref type="bibr" target="#b19">Ranzato et al. (2015)</ref> indicate two drawbacks of maximum likelihood estimation (MLE) for NMT. First, the models are only ex- posed to the training distribution instead of model predictions. Second, the loss function is defined at the word level instead of the sentence level.</p><p>In this work, we introduce minimum risk train- ing (MRT) for neural machine translation. The new training objective is to minimize the expected loss (i.e., risk) on the training data. MRT has the following advantages over MLE:</p><p>1. Direct optimization with respect to evalu- ation metrics: MRT introduces evaluation metrics as loss functions and aims to mini- mize expected loss on the training data.</p><p>2. Applicable to arbitrary loss functions: our approach allows arbitrary sentence-level loss functions, which are not necessarily differen- tiable.</p><p>3. Transparent to architectures: MRT does not assume the specific architectures of NMT and can be applied to any end-to-end NMT sys- tems.</p><p>While MRT has been widely used in conven- tional SMT <ref type="bibr" target="#b17">(Och, 2003;</ref><ref type="bibr" target="#b21">Smith and Eisner, 2006;</ref><ref type="bibr" target="#b8">He and Deng, 2012</ref>) and deep learning based MT ( <ref type="bibr" target="#b6">Gao et al., 2014)</ref>, to the best of our knowledge, this work is the first effort to introduce MRT into end-to-end NMT. Experiments on a variety of language pairs <ref type="bibr">(Chinese-English, English-French, and English-German)</ref> show that MRT leads to sig- nificant improvements over MLE on a state-of- the-art NMT system ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Given a source sentence x = x 1 , . . . , x m , . . . , x M and a target sentence y = y 1 , . . . , y n , . . . , y N , end-to-end NMT directly models the translation probability:</p><formula xml:id="formula_0">P (y|x; θ) = N n=1 P (y n |x, y &lt;n ; θ),<label>(1)</label></formula><p>where θ is a set of model parameters and y &lt;n = y 1 , . . . , y n−1 is a partial translation. Predicting the n-th target word can be modeled by using a recurrent neural network:</p><formula xml:id="formula_1">P (y n |x, y &lt;n ; θ) ∝ exp q(y n−1 , z n , c n , θ) ,<label>(2)</label></formula><p>where z n is the n-th hidden state on the target side, c n is the context for generating the n-th tar- get word, and q(·) is a non-linear function. Cur- rent NMT approaches differ in calculating z n and c n and defining q(·). Please refer to <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) for more details. Given a set of training examples D = {{x (s) , y (s) } S s=1 , the standard training objective is to maximize the log-likelihood of the training data:</p><formula xml:id="formula_2">ˆ θ MLE = argmax θ L(θ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">L(θ) = S s=1 log P (y (s) |x (s) ; θ) (4) = S s=1 N (s) n=1 log P (y (s) n |x (s) , y (s) &lt;n ; θ). (5)</formula><p>We use N (s) to denote the length of the s-th target sentence y (s) . The partial derivative with respect to a model parameter θ i is calculated as</p><formula xml:id="formula_4">∂L(θ) ∂θ i = S s=1 N (s) n=1 ∂P (y (s) n |x (s) , y (s) &lt;n ; θ)/∂θ i P (y (s) n |x (s) , y (s) &lt;n ; θ)</formula><p>. (6) <ref type="bibr" target="#b19">Ranzato et al. (2015)</ref> point out that MLE for end-to-end NMT suffers from two drawbacks. First, while the models are trained only on the training data distribution, they are used to generate target words on previous model predictions, which can be erroneous, at test time. This is referred to as exposure bias <ref type="bibr" target="#b19">(Ranzato et al., 2015)</ref>. Second, MLE usually uses the cross-entropy loss focus- ing on word-level errors to maximize the proba- bility of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>) and TER ( <ref type="bibr" target="#b22">Snover et al., 2006</ref>).</p><p>As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evalu- ation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Minimum Risk Training for Neural Machine Translation</head><p>Minimum risk training (MRT), which aims to minimize the expected loss on the training data, has been widely used in conventional SMT <ref type="bibr" target="#b17">(Och, 2003;</ref><ref type="bibr" target="#b21">Smith and Eisner, 2006;</ref><ref type="bibr" target="#b8">He and Deng, 2012</ref>) and deep learning based MT ( <ref type="bibr" target="#b6">Gao et al., 2014</ref>). The basic idea is to introduce evaluation metrics as loss functions and assume that the opti- mal set of model parameters should minimize the expected loss on the training data. Let x (s) , y (s) be the s-th sentence pair in the training data and y be a model prediction. We use a loss function ∆(y, y (s) ) to measure the discrep- ancy between the model prediction y and the gold- standard translation y (s) . Such a loss function can be negative smoothed sentence-level evalua- tion metrics such as BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>), NIST <ref type="bibr" target="#b5">(Doddington, 2002</ref>), TER ( <ref type="bibr" target="#b22">Snover et al., 2006</ref>), or METEOR ( <ref type="bibr" target="#b13">Lavie and Denkowski, 2009</ref>) that have been widely used in machine translation evaluation. Note that a loss function is not param- eterized and thus not differentiable.</p><p>In MRT, the risk is defined as the expected loss with respect to the posterior distribution:</p><formula xml:id="formula_5">R(θ) = S s=1 E y|x (s) ;θ ∆(y, y (s) ) (7) = S s=1 y∈Y(x (s) ) P (y|x (s) ; θ)∆(y, y (s) ),<label>(8)</label></formula><p>where Y(x (s) ) is a set of all possible candidate translations for x (s) . <ref type="table">Table 1</ref>: Example of minimum risk training. x (s) is an observed source sentence, y (s) is its corresponding gold-standard translation, and y 1 , y 2 , and y 3 are model predictions. For simplicity, we suppose that the full search space contains only three candidates. The loss function ∆(y, y (s) ) measures the difference between model prediction and gold-standard. The goal of MRT is to find a distribution (the last column) that correlates well with the gold-standard by minimizing the expected loss.</p><formula xml:id="formula_6">∆(y, y (s) ) P (y|x (s) ; θ) y 1 −1.0 0.2 0.3 0.5 0.7 y 2 −0.3 0.5 0.2 0.2 0.1 y 3 −0.5 0.3 0.5 0.3 0.2 E y|x (s) ;θ [∆(y, y (s) )] −0.50 −0.61 −0.71 −0.83</formula><p>The training objective of MRT is to minimize the risk on the training data:</p><formula xml:id="formula_7">ˆ θ MRT = argmin θ R(θ) .<label>(9)</label></formula><p>Intuitively, while MLE aims to maximize the likelihood of training data, our training objective is to discriminate between candidates. For example, in <ref type="table">Table 1</ref>, suppose the candidate set Y(x (s) ) con- tains only three candidates: y 1 , y 2 , and y 3 . Ac- cording to the losses calculated by comparing with the gold-standard translation y (s) , it is clear that y 1 is the best candidate, y 3 is the second best, and y 2 is the worst: y 1 &gt; y 3 &gt; y 2 . The right half of <ref type="table">Table 1</ref> shows four models. As model 1 (column 3) ranks the candidates in a reverse order as com- pared with the gold-standard (i.e., y 2 &gt; y 3 &gt; y 1 ), it obtains the highest risk of −0.50. Achieving a better correlation with the gold-standard than model 1 by predicting y 3 &gt; y 1 &gt; y 2 , model 2 (column 4) reduces the risk to −0.61. As model 3 (column 5) ranks the candidates in the same or- der with the gold-standard, the risk goes down to −0.71. The risk can be further reduced by con- centrating the probability mass on y 1 (column 6). As a result, by minimizing the risk on the training data, we expect to obtain a model that correlates well with the gold-standard.</p><p>In MRT, the partial derivative with respect to a model parameter θ i is given by</p><formula xml:id="formula_8">∂R(θ) ∂θ i = S s=1 E y|x (s) ;θ ∆(y, y (s) ) × N (s) n=1 ∂P (y n |x (s) , y &lt;n ; θ)/∂θ i P (y n |x (s) , y &lt;n ; θ)</formula><p>. <ref type="formula" target="#formula_0">(10)</ref> Since Eq. <ref type="formula" target="#formula_0">(10)</ref> suggests there is no need to dif- ferentiate ∆(y, y (s) ), MRT allows arbitrary non- differentiable loss functions. In addition, our ap- proach is transparent to architectures and can be applied to arbitrary end-to-end NMT models.</p><p>Despite these advantages, MRT faces a major challenge: the expectations in Eq. <ref type="formula" target="#formula_0">(10)</ref> are usu- ally intractable to calculate due to the exponential search space of Y(x (s) ), the non-decomposability of the loss function ∆(y, y (s) ), and the context sensitiveness of NMT.</p><p>To alleviate this problem, we propose to only use a subset of the full search space to approxi- mate the posterior distribution and introduce a new training objective:</p><formula xml:id="formula_9">˜ R(θ) = S s=1 E y|x (s) ;θ,α ∆(y, y (s) ) (11) = S s=1 y∈S(x (s) ) Q(y|x (s) ; θ, α)∆(y, y (s) ), (12)</formula><p>where</p><formula xml:id="formula_10">S(x (s) ) ⊂ Y(x (s)</formula><p>) is a sampled subset of the full search space, and Q(y|x (s) ; θ, α) is a dis- tribution defined on the subspace S(x (s) ):</p><formula xml:id="formula_11">Q(y|x (s) ; θ, α) = P (y|x (s) ; θ) α y ∈S(x (s) ) P (y |x (s) ; θ) α . (13)</formula><p>Note that α is a hyper-parameter that controls the sharpness of the Q distribution <ref type="bibr" target="#b17">(Och, 2003)</ref>.</p><p>Algorithm 1 shows how to build S(x (s) ) by sampling the full search space. The sampled sub- set initializes with the gold-standard translation (line 1). Then, the algorithm keeps sampling a tar- get word given the source sentence and the partial translation until reaching the end of sentence (lines 3-16). Note that sampling might produce dupli- cate candidates, which are removed when building Input: the s-th source sentence in the training data x (s) , the s-th target sentence in the training data y <ref type="bibr">(s)</ref> , the set of model parameters θ, the limit on the length of a candidate translation l, and the limit on the size of sampled space k. Output: sampled space S(x (s) ). the subspace. We find that it is inefficient to force the algorithm to generate exactly k distinct candi- dates because high-probability candidates can be sampled repeatedly, especially when the probabil- ity mass highly concentrates on a few candidates. In practice, we take advantage of GPU's parallel architectures to speed up the sampling. <ref type="bibr">1</ref> Given the sampled space, the partial derivative with respect to a model parameter θ i of˜Rof˜ of˜R(θ) is given by</p><formula xml:id="formula_12">∂ ˜ R(θ) ∂θ i = α S s=1 E y|x (s) ;θ,α ∂P (y|x (s) ; θ)/∂θ i P (y|x (s) ; θ) × ∆(y, y (s) ) − E y |x (s) ;θ,α [∆(y , y (s) )]</formula><p>. <ref type="formula" target="#formula_0">(14)</ref> Since |S(x (s) )| |Y(x (s) )|, the expectations in Eq. <ref type="formula" target="#formula_0">(14)</ref> can be efficiently calculated by ex- plicitly enumerating all candidates in S(x (s) ). In our experiments, we find that approximating the full space with 100 samples (i.e., k = 100) works very well and further increasing sample size does not lead to significant improvements (see Section 4.3). <ref type="bibr">1</ref> To build the subset, an alternative to sampling is com- puting top-k translations. We prefer sampling to comput- ing top-k translations for efficiency: sampling is more effi- cient and easy-to-implement than calculating k-best lists, es- pecially given the extremely parallel architectures of GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluated our approach on three transla- tion tasks: Chinese-English, English-French, and English-German. The evaluation metric is BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>) as calculated by the multi-bleu.perl script.</p><p>For Chinese-English, the training data consists of 2.56M pairs of sentences with 67.5M Chinese words and 74.8M English words, respectively. We used the NIST 2006 dataset as the validation set (hyper-parameter optimization and model selec- tion) and the <ref type="bibr">NIST 2002</ref><ref type="bibr">NIST , 2003</ref><ref type="bibr">NIST , 2004</ref><ref type="bibr">NIST , 2005</ref><ref type="bibr">NIST , and 2008</ref>   We compare our approach with two state-of- the-art SMT and NMT systems:</p><p>1. MOSES ( <ref type="bibr" target="#b11">Koehn and Hoang, 2007)</ref>: a phrase- based SMT system using minimum error rate training <ref type="bibr" target="#b17">(Och, 2003</ref>).</p><p>2. RNNSEARCH ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>: an attention-based NMT system using maxi- mum likelihood estimation.</p><p>MOSES uses the parallel corpus to train a phrase-based translation model and the target part to train a 4-gram language model using the SRILM toolkit <ref type="bibr" target="#b23">(Stolcke, 2002</ref>). <ref type="bibr">2</ref> The log-linear model Moses uses is trained by the minimum error rate training (MERT) algorithm <ref type="bibr" target="#b17">(Och, 2003</ref>) that directly optimizes model parameters with respect to evaluation metrics.</p><p>RNNSEARCH uses the parallel corpus to train an attention-based neural translation model using the maximum likelihood criterion.</p><p>On top of RNNSEARCH, our approach replaces MLE with MRT. We initialize our model with the RNNsearch50 model ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>). We set the vocabulary size to 30K for Chinese-English and English-French and 50K for English-German. The beam size for decoding is 10. The default loss function is negative smoothed sentence-level BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of α</head><p>The hyper-parameter α controls the smoothness of the Q distribution (see Eq. <ref type="formula" target="#formula_0">(13)</ref>). As shown in <ref type="bibr">2</ref> It is possible to exploit larger monolingual corpora for both MOSES and RNNSEARCH <ref type="bibr" target="#b7">(Gulcehre et al., 2015;</ref><ref type="bibr" target="#b20">Sennrich et al., 2015</ref>). We leave this for future work.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Sample Size</head><p>For efficiency, we sample k candidate translations from the full search space Y(x (s) ) to build an approximate posterior distribution Q (Section 3). <ref type="figure" target="#fig_3">Figure 2</ref> shows the effect of sample size k on the Chinese-English validation set. It is clear that BLEU scores consistently rise with the increase of k. However, we find that a sample size larger than 100 (e.g., k = 200) usually does not lead to signi- ficant improvements and increases the GPU mem- ory requirement. Therefore, we set k = 100 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Loss Function</head><p>As our approach is capable of incorporating evalu- ation metrics as loss functions, we investigate the effect of different loss functions on BLEU, TER and NIST scores on the Chinese-English valida- tion set. As shown in <ref type="table" target="#tab_1">Table 2</ref>, negative smoothed sentence-level BLEU (i.e, −sBLEU) leads to sta- tistically significant improvements over MLE (p &lt; 0.01). Note that the loss functions are all defined at the sentence level while evaluation metrics are cal- culated at the corpus level. This discrepancy might explain why optimizing with respect to sTER does not result in the lowest TER on the validation set. As −sBLEU consistently improves all evaluation metrics, we use it as the default loss function in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison of Training Time</head><p>We used a cluster with 20 Telsa K40 GPUs to train the NMT model. For MLE, it takes the cluster about one hour to train 20,000 mini-batches, each of which contains 80 sentences. The training time for MRT is longer than MLE: 13,000 mini-batches can be processed in one hour on the same cluster. <ref type="figure" target="#fig_5">Figure 3</ref> shows the learning curves of MLE and MRT on the validation set. For MLE, the BLEU score reaches its peak after about 20 hours and then keeps going down dramatically. Initializing with the best MLE model, MRT increases BLEU scores dramatically within about 30 hours. <ref type="bibr">3</ref> Af- terwards, the BLEU score keeps improving grad- ually but there are slight oscillations. <ref type="table" target="#tab_3">Table 3</ref> shows BLEU scores on Chinese-English datasets. For RNNSEARCH, we follow Luong <ref type="bibr">3</ref> Although it is possible to initialize with a randomized model, it takes much longer time to converge.     <ref type="table" target="#tab_2">Table 4</ref>: Case-insensitive TER scores on Chinese-English translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results on Chinese-English Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Comparison of BLEU Scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Comparison of TER Scores</head><p>MLE vs. MRT &lt; = &gt; evaluator 1 54% 24% 22% evaluator 2 53% 22% 25% and RNNSEARCH with MRT on the Chinese- English test set with respect to input sentence lengths. While MRT consistently improves over MLE for all lengths, it achieves worse translation performance for sentences longer than 60 words.</p><p>One reason is that RNNSEARCH tends to pro- duce short translations for long sentences. As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, both MLE and MRE gen- erate much shorter translations than MOSES. This results from the length limit imposed by RNNSEARCH for efficiency reasons: a sentence in the training set is no longer than 50 words. This limit deteriorates translation performance because the sentences in the test set are usually longer than 50 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.4">Subjective Evaluation</head><p>We also conducted a subjective evaluation to vali- date the benefit of replacing MLE with MRT. Two human evaluators were asked to compare MLE and MRT translations of 100 source sentences ran- domly sampled from the test sets without know- ing from which system a candidate translation was generated. <ref type="table" target="#tab_4">Table 5</ref> shows the results of subjective evalua- tion. The two human evaluators made close judge- ments: around 54% of MLE translations are worse than MRE, 23% are equal, and 23% are better. <ref type="table">Table 6</ref> shows some example translations. We find that MOSES translates a Chinese string "yi wei fuze yu pingrang dangju da jiaodao de qian guowuyuan guanyuan" that requires long-distance reordering in a wrong way, which is a notorious challenge for statistical machine translation. In contrast, RNNSEARCH-MLE seems to overcome this problem in this example thanks to the capa- bility of gated RNNs to capture long-distance de- pendencies. However, as MLE uses a loss func- tion defined only at the word level, its translation lacks sentence-level consistency: "chinese" oc- curs twice while "two senate" is missing. By opti- mizing model parameters directly with respect to sentence-level BLEU, RNNSEARCH-MRT seems to be able to generate translations more consis- tently at the sentence level. <ref type="table" target="#tab_6">Table 7</ref> shows the results on English-French trans- lation. We list existing end-to-end NMT systems that are comparable to our system. All these sys- tems use the same subset of the WMT 2014 train- ing corpus and adopt MLE as the training crite- rion. They differ in network architectures and vo- cabulary sizes. Our RNNSEARCH-MLE system achieves a BLEU score comparable to that of <ref type="bibr" target="#b9">Jean et al. (2015)</ref>. RNNSEARCH-MRT achieves the highest BLEU score in this setting even with a vo- cabulary size smaller than <ref type="bibr" target="#b16">Luong et al. (2015b)</ref> and <ref type="bibr" target="#b24">Sutskever et al. (2014)</ref>. Note that our ap- proach does not assume specific architectures and can in principle be applied to any NMT systems. <ref type="table" target="#tab_7">Table 8</ref> shows the results on English-German translation. Our approach still significantly out-Source meiguo daibiao tuan baokuo laizi shidanfu daxue de yi wei zhongguo zhuanjia , liang ming canyuan waijiao zhengce zhuli yiji yi wei fuze yu pingrang dangju da jiaodao de qian guowuyuan guanyuan . Reference the us delegation consists of a chinese expert from the stanford university , two senate foreign affairs policy assistants and a former state department official who was in charge of dealing with pyongyang authority . MOSES the united states to members of the delegation include representatives from the stanford university , a chinese expert , two assistant senate foreign policy and a responsible for dealing with pyongyang before the officials of the state council . RNNSEARCH-MLE the us delegation comprises a chinese expert from stanford university , a chinese foreign office assistant policy assistant and a former official who is responsible for dealing with the pyongyang authorities . RNNSEARCH-MRT the us delegation included a chinese expert from the stanford university , two senate foreign policy assistants , and a former state department official who had dealings with the pyongyang authorities . <ref type="table">Table 6</ref>: Example Chinese-English translations. "Source" is a romanized Chinese sentence, "Refer- ence" is a gold-standard translation. "MOSES" and "RNNSEARCH-MLE" are baseline SMT and NMT systems. "RNNSEARCH-MRT" is our system.   performs MLE and achieves comparable results with state-of-the-art systems even though <ref type="bibr" target="#b15">Luong et al. (2015a)</ref> used a much deeper neural network. We believe that our work can be applied to their architecture easily. Despite these significant improvements, the margins on English-German and English-French datasets are much smaller than Chinese-English. We conjecture that there are two possible rea- sons. First, the Chinese-English datasets contain four reference translations for each sentence while both English-French and English-German datasets only have single references. Second, Chinese and English are more distantly related than English, French and German and thus benefit more from MRT that incorporates evaluation metrics into op- timization to capture structural divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.5">Example Translations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Results on English-French Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Results on English-German Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work originated from the minimum risk train- ing algorithms in conventional statistical machine translation <ref type="bibr" target="#b17">(Och, 2003;</ref><ref type="bibr" target="#b21">Smith and Eisner, 2006;</ref><ref type="bibr" target="#b8">He and Deng, 2012)</ref>. Och (2003) describes a smoothed error count to allow calculating gradi- ents, which directly inspires us to use a param- eter α to adjust the smoothness of the objective function. As neural networks are non-linear, our approach has to minimize the expected loss on the sentence level rather than the loss of 1-best translations on the corpus level. Smith and Eis- ner (2006) introduce minimum risk annealing for training log-linear models that is capable of grad- ually annealing to focus on the 1-best hypothe- sis. <ref type="bibr" target="#b8">He et al. (2012)</ref> apply minimum risk training to learning phrase translation probabilities. <ref type="bibr" target="#b6">Gao et al. (2014)</ref> leverage MRT for learning continu- ous phrase representations for statistical machine translation. The difference is that they use MRT to optimize a sub-model of SMT while we are in- terested in directly optimizing end-to-end neural translation models.</p><p>The Mixed Incremental Cross-Entropy Rein- force (MIXER) algorithm ( <ref type="bibr" target="#b19">Ranzato et al., 2015)</ref> is in spirit closest to our work. Building on the REINFORCE algorithm proposed by <ref type="bibr">Williams (1992)</ref>, MIXER allows incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy. The major dif- ference is that <ref type="bibr" target="#b19">Ranzato et al. (2015)</ref> leverage rein- forcement learning while our work resorts to mini- mum risk training. In addition, MIXER only sam- ples one candidate to calculate reinforcement re- ward while MRT generates multiple samples to calculate the expected risk. <ref type="figure" target="#fig_3">Figure 2</ref> indicates that multiple samples potentially increases MRT's ca- pability of discriminating between diverse candi- dates and thus benefit translation quality. Our ex- periments confirm their finding that taking evalu- ation metrics into account when optimizing model parameters does help to improve sentence-level text generation.</p><p>More recently, our approach has been suc- cessfully applied to summarization <ref type="bibr" target="#b0">(Ayana et al., 2016)</ref>. They optimize neural networks for head- line generation with respect to ROUGE <ref type="bibr" target="#b14">(Lin, 2004</ref>) and also achieve significant improvements, confirming the effectiveness and applicability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented a framework for minimum risk training in end-to-end neural ma- chine translation. The basic idea is to minimize the expected loss in terms of evaluation metrics on the training data. We sample the full search space to approximate the posterior distribution to improve efficiency. Experiments show that MRT leads to significant improvements over maximum likelihood estimation for neural machine trans- lation, especially for distantly-related languages such as Chinese and English.</p><p>In the future, we plan to test our approach on more language pairs and more end-to-end neural MT systems. It is also interesting to extend mini- mum risk training to minimum risk annealing fol- lowing <ref type="bibr" target="#b21">Smith and Eisner (2006)</ref>. As our approach is transparent to loss functions and architectures, we believe that it will also benefit more end-to-end neural architectures for other NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>S(x (s) ) ← {y (s) }; // the gold-standard translation is included 2 i ← 1; 3 while i ≤ k do 4 y ← ∅; // an empty candidate translation 5 n ← 1; 6 while n ≤ l do 7 y ∼ P (yn|x (s) , y&lt;n; θ); // sample the n-th target word 8 y ← y ∪ {y}; 9 if y = EOS then 10 break; // terminate if reach the end of sentence 11 end 12 n ← n + 1; 13 end 14 S(x (s) ) ← S(x (s) ) ∪ {y}; 15 i ← i + 1; 16 end Algorithm 1: Sampling the full search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>datasets as test sets. For English-French, to compare with the results reported by previous work on end-to-end NMT (Sutskever et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015b), we used the same subset of the WMT 2014 training cor- pus that contains 12M sentence pairs with 304M English words and 348M French words. The con- catenation of news-test 2012 and news-test 2013 serves as the validation set and news-test 2014 as the test set. For English-German, to compare with the results reported by previous work (Jean et al., 2015; Luong et al., 2015a), we used the same sub- set of the WMT 2014 training corpus that contains 4M sentence pairs with 91M English words and 87M German words. The concatenation of news- test 2012 and news-test 2013 is used as the valida- tion set and news-test 2014 as the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Effect of α on the Chinese-English validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of sample size on the ChineseEnglish validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 ,</head><label>1</label><figDesc>Figure 1, we find that α has a critical effect on BLEU scores on the Chinese-English validation set. While α = 1 × 10 −1 deceases BLEU scores dramatically, α = 5 × 10 −3 improves translation quality significantly and consistently. Reducing α further to 1 × 10 −4 , however, results in lower BLEU scores. Therefore, we set α = 5 × 10 −3 in the following experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of training time on the Chinese-English validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: BLEU scores on the Chinese-English test set over various input sentence lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of output sentences lengths on the Chinese-English test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Effect of loss function on the Chinese- English validation set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 gives</head><label>4</label><figDesc></figDesc><table>TER scores on Chinese-English 
datasets. The loss function used in MRT is 
−sBLEU. MRT still obtains dramatic improve-
ments over MOSES and RNNSEARCH with MLE 
as the training criterion (up to -10.27 and -8.32 
TER points, respectively) across all test sets. All 
the improvements are statistically significant. 

4.6.3 BLEU Scores over Sentence Lengths 
Figure 4 shows the BLEU scores of translations 
generated by MOSES, RNNSEARCH with MLE, </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Case-insensitive BLEU scores on Chinese-English translation.</head><label>3</label><figDesc></figDesc><table>System 
Training MT06 MT02 MT03 MT04 MT05 MT08 
MOSES 
MERT 
59.22 62.97 62.44 61.20 63.44 62.36 

RNNSEARCH 
MLE 
60.74 58.94 60.10 58.91 61.74 64.52 
MRT 
52.86 52.87 52.17 51.49 53.42 57.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Subjective evaluation of MLE and MRT on Chinese-English translation.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Comparison with previous work on English-French translation. The BLEU scores are case-
sensitive. "PosUnk" denotes Luong et al. (2015b)'s technique of handling rare words. 

System 
Architecture 
Training BLEU 
Existing end-to-end NMT systems 
Jean et al. (2015) 
gated RNN with search 

MLE 

16.46 
Jean et al. (2015) 
gated RNN with search + PosUnk 
18.97 
Jean et al. (2015) 
gated RNN with search + LV + PosUnk 
19.40 
Luong et al. (2015a) LSTM with 4 layers + dropout + local att. + PosUnk 
20.90 
Our end-to-end NMT systems 

this work 
gated RNN with search 
MLE 
16.45 
gated RNN with search 
MRT 
18.02 
gated RNN with search + PosUnk 
MRT 
20.45 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Comparison with previous work on English-German translation. The BLEU scores are case-
sensitive. </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was done while Shiqi Shen and Yong Cheng were visiting Baidu. Maosong Sun and Hua Wu are supported by the 973 Program <ref type="bibr">(2014CB340501 &amp; 2014CB34505)</ref>. Yang Liu is supported by the National Natural Science Foun-dation of China ( <ref type="bibr">No.61522204 and No.61432013)</ref> and the 863 <ref type="bibr">Program (2015AA011808)</ref>. This re-search is also supported by the Singapore National Research Foundation under its International Re-search Centre@Singapore Funding Initiative and administered by the IDM Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01904</idno>
		<title level="m">Neural headline generation with minimum risk training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguisitics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT</title>
		<meeting>HLT</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen Tao Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum expected bleu training of phrase and lexicon translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factored translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The mereor metric for automatic evaluation of machine translation. Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732v1</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA</title>
		<meeting>AMTA</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Srilm-am extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Willams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
