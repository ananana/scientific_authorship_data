<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast Unified Model for Parsing and Sentence Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
							<email>sbowman@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Stanford AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
							<email>jgauthie@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Stanford AI Lab</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Stanford Symbolic Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
							<email>arastogi@stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Stanford AI Lab</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Stanford Electrical Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
							<email>rgupta93@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Stanford AI Lab</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Stanford NLP Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Stanford AI Lab</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Stanford Linguistics</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Stanford Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cgpotts@stanford</forename><surname>Edu</surname></persName>
						</author>
						<title level="a" type="main">A Fast Unified Model for Parsing and Sentence Understanding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1466" to="1477"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they usually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25× over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A wide range of current models in NLP are built around a neural network component that produces vector representations of sentence meaning (e.g., <ref type="bibr" target="#b34">Tai et al., 2015)</ref>. This com- ponent, the sentence encoder, is generally formu- lated as a learned parametric function from a se- quence of word vectors to a sentence vector, and this function can take a range of different forms. Common sentence encoders include sequence- based recurrent neural network models (RNNs, see <ref type="figure">Figure 1a</ref>) with Long Short-Term Memory (LSTM, * The first two authors contributed equally. Figure 1: An illustration of two standard designs for sentence encoders. The TreeRNN, unlike the sequence-based RNN, requires a substantially dif- ferent connection structure for each sentence, mak- ing batched computation impractical. <ref type="bibr" target="#b17">Hochreiter and Schmidhuber, 1997)</ref>, which ac- cumulate information over the sentence sequen- tially; convolutional neural networks <ref type="bibr" target="#b22">(Kalchbrenner et al., 2014;</ref>, which accu- mulate information using filters over short local se- quences of words or characters; and tree-structured recursive neural networks <ref type="bibr">(TreeRNNs, Goller and Küchler, 1996;</ref><ref type="bibr" target="#b30">Socher et al., 2011a</ref>, see <ref type="figure">Figure 1b</ref>), which propagate information up a binary parse tree.</p><p>Of these, the TreeRNN appears to be the prin- cipled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure <ref type="bibr">(Dowty, 2007, i.a.)</ref>. TreeRNNs have shown promise <ref type="bibr" target="#b34">(Tai et al., 2015;</ref><ref type="bibr" target="#b24">Li et al., 2015;</ref><ref type="bibr" target="#b4">Bowman et al., 2015b</ref>), but have  largely been overlooked in favor of sequence- based RNNs because of their incompatibility with batched computation and their reliance on external parsers. Batched computation-performing syn- chronized computation across many examples at once-yields order-of-magnitude improvements in model run time, and is crucial in enabling neural networks to be trained efficiently on large datasets. Because TreeRNNs use a different model structure for each sentence, as in <ref type="figure">Figure 1</ref>, efficient batching is impossible in standard implementations. Partly to address efficiency problems, standard TreeRNN models commonly only operate on sentences that have already been processed by a syntactic parser, which slows and complicates the use of these mod- els at test time for most applications.</p><p>This paper introduces a new model to address both these issues: the Stack-augmented Parser- Interpreter Neural Network, or SPINN, shown in <ref type="figure" target="#fig_1">Figure 2</ref>. SPINN executes the computations of a tree-structured model in a linearized sequence, and can incorporate a neural network parser that pro- duces the required parse structure on the fly. This design improves upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it sup- ports batched computation for both parsed and un- parsed sentences, yielding dramatic speedups over standard TreeRNNs. Finally, it supports a novel tree-sequence hybrid architecture for handling lo- cal linear context in sentence interpretation. This model is a basically plausible model of human sen- tence processing and yields substantial accuracy gains over pure sequence-or tree-based models.</p><p>We evaluate SPINN on the Stanford Natural Lan- guage Inference entailment task <ref type="bibr">(SNLI, Bowman et al., 2015a)</ref>, and find that it significantly out- performs other sentence-encoding-based models, even with a relatively simple and underpowered implementation of the built-in parser. We also find that SPINN yields speed increases of up to 25× over a standard TreeRNN implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There is a fairly long history of work on building neural network-based parsers that use the core op- erations and data structures from transition-based parsing, of which shift-reduce parsing is a variant <ref type="bibr" target="#b16">(Henderson, 2004;</ref><ref type="bibr" target="#b12">Emami and Jelinek, 2005;</ref><ref type="bibr" target="#b37">Titov and Henderson, 2010;</ref><ref type="bibr" target="#b6">Chen and Manning, 2014;</ref><ref type="bibr" target="#b5">Buys and Blunsom, 2015;</ref><ref type="bibr" target="#b10">Dyer et al., 2015;</ref><ref type="bibr" target="#b23">Kiperwasser and Goldberg, 2016)</ref>. In addition, there has been recent work proposing models designed pri- marily for generative language modeling tasks that use this architecture as well ( <ref type="bibr" target="#b11">Dyer et al., 2016)</ref>. To our knowledge, SPINN is the first model to use this architecture for the pur- pose of sentence interpretation, rather than parsing or generation. <ref type="bibr">Socher et al. (2011a,b)</ref> present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods re- quire an expensive search process at test time. Our model presents a much faster alternative approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our model: SPINN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: Shift-reduce parsing</head><p>SPINN is inspired by shift-reduce parsing <ref type="bibr" target="#b0">(Aho and Ullman, 1972)</ref>, which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language pars- ing (e.g., <ref type="bibr" target="#b29">Shieber, 1983;</ref><ref type="bibr" target="#b26">Nivre, 2003)</ref>.</p><p>A shift-reduce parser accepts a sequence of input tokens x = (x 0 , . . . , x N −1 ) and consumes transitions a = (a 0 , . . . , a T −1 ), where each a t ∈ {, } specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It pro- ceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binary-branching tree structure over N words, this requires T = 2N − 1 transitions through a total of T + 1 states.</p><p>The parser uses two auxiliary data structures: a stack S of partially completed subtrees and a buffer B of tokens yet to be parsed. The parser is initialized with the stack empty and the buffer containing the tokens x of the sentence in order. Let S, B = ∅, x denote this starting state. It next proceeds through the transition sequence, where each transition a t selects one of the two following operations. Below, the | symbol denotes the cons (concatenation) operator. We arbitrarily choose to always cons on the left in the notation below. : S, x | B → x | S, B. This operation pops an element from the buffer and pushes it on to the top of the stack.</p><p>: x | y | S, B → (x, y) | S, B. This operation pops the top two elements from the stack, merges them, and pushes the result back on to the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composition and representation</head><p>SPINN is based on a shift-reduce parser, but it is designed to produce a vector representation of a sentence as its output, rather than a tree as in stan- dard shift-reduce parsing. It modifies the shift- reduce formalism by using fixed length vectors to represent each entry in the stack and the buffer. Correspondingly, its operation combines two vector representations from the stack into an- other vector using a neural network function.</p><p>The composition function When a op- eration is performed, the vector representations of two tree nodes are popped off of the stack and fed into a composition function, which is a neural net- work function that produces a representation for a new tree node that is the parent of the two popped nodes. This new node is pushed on to the stack. The TreeLSTM composition function <ref type="bibr" target="#b34">(Tai et al., 2015</ref>) generalizes the LSTM neural network layer to tree-rather than sequence-based inputs, and it shares with the LSTM the idea of representing in- termediate states as a pair of an active state repre- sentation h and a memory representation c. Our version is formulated as:</p><formula xml:id="formula_0">             i f l f r o g              =              σ σ σ σ tanh              W comp         h 1 s h 2 s e         + b comp (1) c = f l c 2 s + f r c 1 s + i g (2) h = o c (3)</formula><p>where σ is the sigmoid activation function, is the elementwise product, the pairs h 1 s , c 1 s and h 2 s , c 2 s are the two input tree nodes popped off the stack, and e is an optional vector-valued input argument which is either empty or comes from an external source like the tracking LSTM (see Section 3.3). The result of this function, the pair h, c, is placed back on the stack. Each vector-valued variable listed is of dimension D except e, of the indepen- dent dimension D tracking .</p><p>The stack and buffer The stack and the buffer are arrays of N elements each (for sentences of up to N words), with the two D-dimensional vectors h and c in each element.</p><p>Word representations We use word represen- tations based on the 300D vectors provided with GloVe ( <ref type="bibr" target="#b27">Pennington et al., 2014</ref>). We do not update these representations during training. Instead, we use a learned linear transformation to map each in- put word vector x GloVe into a vector pair h, c that is stored in the buffer:</p><formula xml:id="formula_1">(4) h c = W wd x GloVe + b wd</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The tracking LSTM</head><p>In addition to the stack, buffer, and composition function, our full model includes an additional component: the tracking LSTM. This is a simple sequence-based LSTM RNN that operates in tan- dem with the model, taking inputs from the buffer and stack at each step. It is meant to maintain a low-resolution summary of the portion of the sen- tence that has been processed so far, which is used for two purposes: it supplies feature representa- tions to the transition classifier, which allows the model to stand alone as a parser, and it additionally supplies a secondary input e to the composition function-see (1)-allowing context information to enter the construction of sentence meaning and forming what is effectively a tree-sequence hybrid model.</p><p>The tracking LSTM's inputs (yellow in <ref type="figure" target="#fig_1">Figure 2</ref>) are the top element of the buffer h 1 b (which would be moved in a operation) and the top two elements of the stack h 1 s and h 2 s (which would be composed in a operation).</p><p>Why a tree-sequence hybrid? Lexical ambigu- ity is ubiquitous in natural language. Most words have multiple senses or meanings, and it is gener- ally necessary to use the context in which a word occurs to determine which of its senses or mean- ings is meant in a given sentence. Even though TreeRNNs are more effective at composing mean- ings in principle, this ambiguity can give simpler sequence-based sentence-encoding models an ad- vantage: when a sequence-based model first pro- cesses a word, it has direct access to a state vec- tor that summarizes the left context of that word, which acts as a cue for disambiguation. In con- trast, when a standard tree-structured model first processes a word, it only has access to the con- stituent that the word is merging with, which is often just a single additional word. Feeding a con- text representation from the tracking LSTM into the composition function is a simple and efficient way to mitigate this disadvantage of tree-structured models. Using left linear context to disambiguate is also a plausible model of human interpretation.</p><p>It would be straightforward to augment SPINN to support the use of some amount of right-side context as well, but this would add complexity to the model that we think is largely unnecessary: humans are very effective at understanding the be- ginnings of sentences before having seen or heard the ends, suggesting that it is possible to get by without the unavailable right-side context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parsing: Predicting transitions</head><p>For SPINN to operate on unparsed inputs, it needs to produce its own transition sequence a rather than relying on an external parser to supply it as part of the input. To do this, the model predicts a t at each step using a simple two-way softmax classifier whose input is the state of the tracking LSTM:</p><formula xml:id="formula_2">(5) p a = softmax(W trans h tracking + b trans )</formula><p>The above model is nearly the simplest viable im- plementation of a transition decision function. In contrast, the decision functions in state-of-the-art transition-based parsers tend to use significantly richer feature sets as inputs, including features con- taining information about several upcoming words on the buffer. The value h tracking is a function of only the very top of the buffer and the top two stack elements at each timestep. At test time, the model uses whichever transi- tion (i.e., or ) is assigned a higher (unnormalized) probability. The prediction func- tion is trained to mimic the decisions of an external parser. These decisions are used as inputs to the model during training. For SNLI, we use the bi- nary Stanford PCFG Parser parses that are included with the corpus. We did not find scheduled sam- pling ( )-having the model use its own transition decisions sometimes at training time-to help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation issues</head><p>Representing the stack efficiently A naïve im- plementation of SPINN needs to handle a size O(N ) stack at each timestep, any element of which may be involved in later computations. A naïve backpropagation implementation would then re- quire storing each of the O(N ) stacks for a back- ward pass, leading to a per-example space require- ment of O(NT D) floats. This requirement is pro- hibitively large for significant batch sizes or sen- tence lengths N. Such a naïve implementation would also require copying a largely unchanged stack at each timestep, since each or operation writes only one new representation to the top of the stack.</p><p>We propose a space-efficient stack representa- tion inspired by the zipper technique <ref type="bibr" target="#b18">(Huet, 1997</ref>) that we call thin stack. For each input sentence, we Algorithm 1 The thin stack algorithm 1: function S(bufferTop, a, t, S, Q) Each row S[t] (for 0 &lt; t ≤ T) represents the top of the actual stack at timestep t. At each timestep we can a new element onto the stack, or the top two elements of the stack into a single ele- ment. To shift an element from the buffer to the top of the stack at timestep t, we simply write it into the location S <ref type="bibr">[t]</ref>. In order to perform the operation, we need to retrieve the top two elements of the actual stack. We maintain a queue Q of pointers into S which contains the row indices of S which are still present in the actual stack. The top two elements of the stack can be found by us- ing the final two pointers in the queue Q. These retrieved elements are used to perform the operation, which modifies Q to mark that some rows of S have now been replaced in the actual stack. Algorithm 1 describes the full mechanics of a stack feedforward in this compressed representa- tion. It operates on the single T × D matrix S and a backpointer queue Q. <ref type="table">Table 1</ref> shows an example run.</p><p>This stack representation requires substantially less space. It stores each element involved in the feedforward computation exactly once, meaning that this representation can still support efficient backpropagation. Furthermore, all of the updates to S and Q can be performed batched and in-place on a GPU, yielding substantial speed gains over both a more naïve SPINN implementation and a standard TreeRNN implementation. We describe speed results in Section 3.7.</p><p>Preparing the data At training time, SPINN re- quires both a transition sequence a and a token sequence x as its inputs for each sentence. The token sequence is simply the words in the sentence in order. a can be obtained from any constituency parse for the sentence by first converting that parse into an unlabeled binary parse, then linearizing it (with the usual in-order traversal), then taking each <ref type="table">Table 1</ref>: The thin-stack algorithm operating on the input sequence x = (Spot, sat, down) and the transition sequence shown in the rightmost column. S[t] shows the top of the stack at each step t. The last two elements of Q (underlined) specify which rows t would be involved in a operation at the next step.</p><formula xml:id="formula_3">t S[t] Q t a t 0 1 Spot 1 2 sat 1 2 3 down 1 2 3 4 (sat down) 1 4 5 (Spot (sat down)) 5</formula><p>word token as a transition and each ')' as a transition, as here: Handling variable sentence lengths For any sentence model to be trained with batched com- putation, it is necessary to pad or crop sentences to a fixed length. We fix this length at N = 25 words, longer than about 98% of sentences in SNLI. Tran- sition sequences a are cropped at the left or padded at the left with s. Token sequences x are then cropped or padded with empty tokens at the left to match the number of s added or removed from a, and can then be padded with empty tokens at the right to meet the desired length N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">TreeRNN-equivalence</head><p>Without the addition of the tracking LSTM, SPINN (in particular the SPINN-PI-NT variant, for parsed input, no tracking) is precisely equivalent to a con- ventional tree-structured neural network model in the function that it computes, and therefore it also has the same learning dynamics. In both, the rep- resentation of each sentence consists of the repre- sentations of the words combined recursively using a TreeRNN composition function (in our case, the TreeLSTM function). SPINN, however, is dramat- ically faster, and supports both integrated parsing and a novel approach to context through the track- ing LSTM. Batch size Feedforward time (sec <ref type="bibr">)</ref> Thin-stack GPU CPU ( <ref type="bibr" target="#b20">Irsoy and Cardie, 2014</ref>) RNN <ref type="figure">Figure 3</ref>: Feedforward speed comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Inference speed</head><p>In this section, we compare the test-time speed of our SPINN-PI-NT with an equivalent TreeRNN implemented in the conventional fashion and with a standard RNN sequence model. While the full models evaluated below are implemented and trained using Theano <ref type="bibr">(Theano Development Team, 2016)</ref>, which is reasonably efficient but not perfect for our model, we wish to compare well-optimized implementations of all three models. To do this, we reimplement the feedforward1 of SPINN-PI-NT and an LSTM RNN baseline in C++/CUDA, and compare that implementation with a CPU-based C++/Eigen TreeRNN implementation from Irsoy and Cardie (2014), which we modified to perform exactly the same computations as SPINN-PI-NT.2 TreeRNNs like this can only operate on a single example at a time and are thus poorly suited for GPU computation. Each model is restricted to run on sentences of 30 tokens or fewer. We fix the model dimension D and the word embedding dimension at 300. We run the CPU performance test on a 2.20 GHz 16-core Intel Xeon E5-2660 processor with hyperthreading enabled. We test our thin-stack implementation and the RNN model on an NVIDIA Titan X GPU. <ref type="figure">Figure 3</ref> compares the sentence encoding speed of the three models on random input data. We ob- serve a substantial difference in runtime between the CPU and thin-stack implementations that in- creases with batch size. With a large but practical 1We chose to reimplement and evaluate only the feedfor- ward/inference pass, as inference speed is the relevant perfor- mance metric for most practical applications.</p><p>2The original code for Irsoy &amp; Cardie's model is avail- able at https://github.com/oir/deep-recursive. Our optimized C++/CUDA models and the Theano source code for the full SPINN are available at https://github.com/ stanfordnlp/spinn. batch size of 512, the largest on which we tested the TreeRNN, our model is about 25× faster than the standard CPU implementation, and about 4× slower than the RNN baseline.</p><p>Though this experiment only covers SPINN- PI-NT, the results should be similar for the full SPINN model: most of the computation involved in running SPINN is involved in populating the buffer, applying the composition function, and manipulating the buffer and the stack, with the low-dimensional tracking and parsing components adding only a small additional load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NLI Experiments</head><p>We evaluate SPINN on the task of natural lan- guage inference (NLI, a.k.a. recognizing textual entailment, or RTE; <ref type="bibr" target="#b8">Dagan et al., 2006</ref>). NLI is a sentence pair classification task, in which a model reads two sentences (a premise and a hypothesis), and outputs a judgment of entailment, contradic- tion, or neutral, reflecting the relationship between the meanings of the two sentences. Below is an ex- ample sentence pair and judgment from the SNLI corpus which we use in our experiments:</p><p>Premise: Girl in a red coat, blue head wrap and jeans is making a snow angel. Hypothesis: A girl outside plays in the snow. Label: entailment SNLI is a corpus of 570k human-labeled pairs of scene descriptions like this one. We use the stan- dard train-test split and ignore unlabeled exam- ples, which leaves about 549k examples for train- ing, 9,842 for development, and 9,824 for testing. SNLI labels are roughly balanced, with the most frequent label, entailment, making up 34.2% of the test set.</p><p>Although NLI is framed as a simple three-way classification task, it is nonetheless an effective way of evaluating the ability of a model to ex- tract broadly informative representations of sen- tence meaning. In order for a model to perform reliably well on NLI, it must be able to represent and reason with the core phenomena of natural lan- guage semantics, including quantification, corefer- ence, scope, and several types of ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Applying SPINN to SNLI</head><p>Creating a sentence-pair classifier To classify an SNLI sentence pair, we run two copies of SPINN with shared parameters: one on the premise sen- tence and another on the hypothesis sentence. We then use their outputs (the h states at the top of each Param.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range Strategy</head><p>RNN SP.-PI-NT SP.-PI SP.</p><p>Initial LR 2 × 10 −4 -2 × 10 −2 5 × 10 −3 3 × 10 −4 7 × 10 −3 2 × 10 −3 L2 regularization λ 8 × 10 −7 -3 × 10 −5 4 × 10 −6 3 × 10 −6 2 × 10 −5 3 × 10 −5 Transition cost α 0.5-4.0 <ref type="table" target="#tab_2">- - - 3.9  Embedding transformation dropout  80-95%  - 83%  92%  86%  Classifier MLP dropout  80-95%  94%  94%  93%  94%  Tracking LSTM size D tracking  24-128  - - 61  79  Classifier MLP layers  1-3  2  2  2  1   Table 2</ref>: Hyperparameter ranges and values. Range shows the hyperparameter ranges explored during random search. Strategy indicates whether sampling from the range was uniform, or log-uniform. Dropout parameters are expressed as keep rates rather than drop rates.</p><p>stack at time t = T) to construct a feature vector</p><p>x classifier for the pair. This feature vector consists of the concatenation of these two sentence vec- tors, their difference, and their elementwise prod- uct (following Mou et al., 2016):</p><formula xml:id="formula_4">(6) x classifier =            h premise h hypothesis h premise − h hypothesis h premise h hypothesis           </formula><p>This feature vector is then passed to a series of 1024D ReLU neural network layers (i.e., an MLP; the number of layers is tuned as a hyperparameter), then passed into a linear transformation, and then finally passed to a softmax layer, which yields a distribution over the three labels.</p><p>The objective function Our objective combines a cross-entropy objective L s for the SNLI classifi- cation task, cross-entropy objectives L t p and L t h for the parsing decision for each of the two sentences at each step t, and an L2 regularization term on the trained parameters. The terms are weighted using the tuned hyperparameters α and λ:</p><formula xml:id="formula_5">(7) L m =L s + α T −1 t=0 (L t p + L t h ) + λ θ 2 2</formula><p>Initialization, optimization, and tuning We initialize the model parameters using the nonpara- metric strategy of <ref type="bibr" target="#b15">He et al. (2015)</ref>, with the excep- tion of the softmax classifier parameters, which we initialize using random uniform samples from [−0.005, 0.005].</p><p>We use minibatch SGD with the RMSProp op- timizer <ref type="bibr" target="#b36">(Tieleman and Hinton, 2012</ref>) and a tuned starting learning rate that decays by a factor of 0.75 every 10k steps. We apply both dropout <ref type="bibr" target="#b32">(Srivastava et al., 2014</ref>) and batch normalization <ref type="bibr" target="#b19">(Ioffe and Szegedy, 2015)</ref> to the output of the word em- bedding projection layer and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.</p><p>We train each model for 250k steps in each run, using a batch size of 32. We track each model's performance on the development set during train- ing and save parameters when this performance reaches a new peak. We use early stopping, eval- uating on the test set using the parameters that perform best on the development set.</p><p>We use random search to tune the hyperparame- ters of each model, setting the ranges for search for each hyperparameter heuristically (and validating the reasonableness of the ranges on the develop- ment set), and then launching eight copies of each experiment each with newly sampled hyperparam- eters from those ranges. <ref type="table">Table 2</ref> shows the hyper- parameters used in the best run of each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models evaluated</head><p>We evaluate four models. The four all use the sentence-pair classifier architecture described in Section 4.1, and differ only in the function com- puting the sentence encodings. First, a single- layer LSTM RNN (similar to that of Bowman et al., 2015a) serves as a baseline encoder. Next, the minimal SPINN-PI-NT model (equivalent to a TreeLSTM) introduces the SPINN model design. SPINN-PI adds the tracking LSTM to that design. Finally, the full SPINN adds the integrated parser.</p><p>We compare our models against several base- lines, including the strongest published non-neural network-based result from <ref type="bibr" target="#b3">Bowman et al. (2015a)</ref> and previous neural network models built around several types of sentence encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Params. Trans. acc. (%) Train acc. (%) Test acc. (%)</head><p>Previous non-NN results Lexicalized classifier <ref type="bibr" target="#b3">(Bowman et al., 2015a</ref>   <ref type="table" target="#tab_2">Table 3</ref> shows our results on SNLI. For the full SPINN, we also report a measure of agreement be- tween this model's parses and the parses included with SNLI, calculated as classification accuracy over transitions averaged across timesteps. We find that the bare SPINN-PI-NT model per- forms little better than the RNN baseline, but that SPINN-PI with the added tracking LSTM performs well. The success of SPINN-PI, which is the hy- brid tree-sequence model, suggests that the tree- and sequence-based encoding methods are at least partially complementary, with the sequence model presumably providing useful local word disam- biguation. The full SPINN model with its rela- tively weak internal parser performs slightly less well, but nonetheless robustly exceeds the perfor- mance of the RNN baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Both SPINN-PI and the full SPINN significantly outperform all previous sentence-encoding mod- els. Most notably, these models outperform the tree-based CNN of <ref type="bibr" target="#b25">Mou et al. (2016)</ref>, which also uses tree-structured composition for local feature extraction, but uses simpler pooling techniques to build sentence features in the interest of effi- ciency. Our results show that a model that uses tree-structured composition fully (SPINN) outper- forms one which uses it only partially (tree-based CNN), which in turn outperforms one which does not use it at all (RNN).</p><p>The full SPINN performed moderately well at reproducing the Stanford Parser's parses of the SNLI data at a transition-by-transition level, with 92.4% accuracy at test time.3 However, its transi- 3Note that this is scoring the model against automatic tion prediction errors are fairly evenly distributed across sentences, and most sentences were as- signed partially invalid transition sequences that either left a few words out of the final representa- tion or incorporated a few padding tokens into the final representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>The use of tree structure improves the performance of sentence-encoding models for SNLI. We suspect that this improvement is largely due to the more ef- ficient learning of accurate generalizations overall, and not to any particular few phenomena. How- ever, some patterns are identifiable in the results.</p><p>While all four models under study have trouble with negation, the tree-structured SPINN models do quite substantially better on these pairs. This is likely due to the fact that parse trees make the scope of any instance of negation (the portion of the sentence's content that is negated) relatively easy to identify and separate from the rest of the sentence. For test set sentence pairs like the one below where negation (not or n't) does not appear in the premise but does appear in the hypothesis, the RNN shows 67% accuracy, while all three tree- structured models exceed 73%. Only the RNN got the below example wrong:</p><p>Premise: The rhythmic gymnast completes her floor exer- cise at the competition. Hypothesis: The gymnast cannot finish her exercise. Label: contradiction Note that the presence of negation in the hypothesis is correlated with a label of contradiction in SNLI, but not as strongly as one might intuit-only 45% of these examples in the test set are labeled as parses, not a human-judged gold standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>contradictions.</head><p>In addition, it seems that tree-structured mod- els, and especially the tree-sequence hybrid mod- els, are more effective than RNNs at extracting in- formative representations of long sentences. The RNN model falls off in test accuracy more quickly with increasing sentence length than SPINN-PI- NT, which in turn falls of substantially faster than the two hybrid models, repeating a pattern seen more dramatically on artificial data in <ref type="bibr" target="#b4">Bowman et al. (2015b)</ref>. On pairs with premises of 20 or more words, the RNN's 76.7% accuracy, while SPINN-PI reaches 80.2%. All three SPINN mod- els labeled the following example correctly, while the RNN did not:</p><p>Premise: A man wearing glasses and a ragged costume is playing a Jaguar electric guitar and singing with the accompaniment of a drummer. Hypothesis: A man with glasses and a disheveled outfit is playing a guitar and singing along with a drummer. Label: entailment We suspect that the hybrid nature of the full SPINN model is also responsible for its surpris- ing ability to perform better than an RNN baseline even when its internal parser is relatively ineffec- tive at producing correct full-sentence parses. It may act somewhat like the tree-based CNN, only with access to larger trees: using tree structure to build up local phrase meanings, and then using the tracking LSTM, at least in part, to combine those meanings.</p><p>Finally, as is likely inevitable for models evalu- ated on SNLI, all four models under study did sev- eral percent worse on test examples whose ground truth label is neutral than on examples of the other two classes. Entailment-neutral and neu- tral-contradiction confusions appear to be much harder to avoid than entailment-contradiction con- fusions, where relatively superficial cues might be more readily useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>We introduce a model architecture (SPINN-PI-NT) that is equivalent to a TreeLSTM, but an order of magnitude faster at test time. We expand that archi- tecture into a tree-sequence hybrid model (SPINN- PI), and show that this yields significant gains on the SNLI entailment task. Finally, we show that it is possible to exploit the strengths of this model without the need for an external parser by inte- grating a fast parser into the model (as in the full SPINN), and that the lack of external parse infor- mation yields little loss in accuracy.</p><p>Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b28">Rocktäschel et al., 2016)</ref>, despite its demon- strated effectiveness on the SNLI task.4 However, we expect that it should be possible to produc- tively combine our model with soft attention to reach state-of-the-art performance.</p><p>Our tracking LSTM uses only simple, quick-to- compute features drawn from the head of the buffer and the head of the stack. It is plausible that giv- ing the tracking LSTM access to more information from the buffer and stack at each step would allow it to better represent the context at each tree node, yielding both better parsing and better sentence encoding. One promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by <ref type="bibr" target="#b10">Dyer et al. (2015)</ref>.</p><p>For a more ambitious goal, we expect that it should be possible to implement a variant of SPINN on top of a modified stack data structure with differentiable and operations (as in <ref type="bibr" target="#b14">Grefenstette et al., 2015;</ref><ref type="bibr" target="#b21">Joulin and Mikolov, 2015</ref>). This would make it possible for the model to learn to parse using guidance from the se- mantic representation objective, which currently is blocked from influencing the key parsing param- eters by our use of hard / decisions. This change would allow the model to learn to pro- duce parses that are, in aggregate, better suited to supporting semantic interpretation than those sup- plied in the training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>t</head><label></label><figDesc>buffer down sat stack cat the composition tracking transition down sat the cat composition tracking transition down sat the cat tracking (a) The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down. 'Tracking', 'transition', and 'composition' are neural network layers. Gray arrows indicate connections which are blocked by a gating function.= 7 = T (the cat) (sat down) output to model for semantic task (b) The fully unrolled SPINN for the cat sat down, with neural network layers omitted for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two views of the Stack-augmented Parser-Interpreter Neural Network (SPINN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>represent the stack with a single T × D matrix S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Unlabeled binary parse:</head><label></label><figDesc>( ( the cat ) ( sat down ) ) x: the, cat, sat, down a: , , , , , ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on SNLI 3-way inference classification. Params. is the approximate number of trained 
parameters (excluding word embeddings for all models). Trans. acc. is the model's accuracy in predicting 
parsing transitions at test time. Train and test are SNLI classification accuracy. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge financial support from a Google Faculty Research Award, the Stanford Data Science Initiative, and the National Science Foundation un-der grant nos. BCS 1456077 and IIS 1514268. Some of the Tesla K40s used for this research were donated by the NVIDIA Corporation. We also thank Kelvin Guu, Noah Goodman, and many others in the Stanford NLP group for helpful com-ments.</p><p>4Attention-based models like <ref type="bibr" target="#b28">Rocktäschel et al. (2016)</ref>, <ref type="bibr" target="#b39">Wang and Jiang (2016)</ref>, and the unpublished <ref type="bibr" target="#b7">Cheng et al. (2016)</ref> have shown accuracies as high as 86.3% on SNLI, but are more narrowly engineered to suit the task and do not yield sentence encodings.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The theory of parsing, translation, and compiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 29. Montréal</title>
		<meeting><address><addrLine>Québec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tree-structured composition in neural networks without treestructured architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches</title>
		<meeting>the 2015 NIPS Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches<address><addrLine>Montréal, Québec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative incremental dependency parsing with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="863" to="869" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
	<note>The PASCAL recognising textual entailment challenge</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compositionality as an empirical problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dowty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Brown University Conference on Direct Compositionality</title>
		<meeting>the Brown University Conference on Direct Compositionality</meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neural syntactic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="195" to="227" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Küchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
		<meeting>the IEEE International Conference on Neural Networks<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 29. Montréal</title>
		<meeting><address><addrLine>Québec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The zipper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of functional programming</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="549" to="554" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 29</title>
		<meeting><address><addrLine>Montréal, Québec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00375</idno>
		<title level="m">Easy-first dependency parsing with hierarchical tree LSTMs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Men</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Kočisk`y, and Phil Blunsom</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentence disambiguation by a shift-reduce parsing technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 21st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff Chiung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<editor>Lise Getoor and Tobias Scheffer</editor>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Machine Learning</title>
		<meeting><address><addrLine>Coursera</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A latent variable model for generative dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Parsing Technology</title>
		<editor>Harry Bunt, Paola Merlo, and Joakim Nivre</editor>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="35" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 29. Montréal</title>
		<meeting><address><addrLine>Québec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Top-down tree long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="310" to="320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
