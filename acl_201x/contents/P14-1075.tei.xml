<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perplexity on Reduced Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Kobayashi</surname></persName>
							<email>hakobaya@yahoo-corp.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<addrLine>9-7-1 Akasaka, Minato-ku</addrLine>
									<postCode>107-6211</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Perplexity on Reduced Corpora</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="797" to="806"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper studies the idea of removing low-frequency words from a corpus, which is a common practice to reduce computational costs, from a theoretical standpoint. Based on the assumption that a corpus follows Zipf&apos;s law, we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary. In addition , we show an approximate behavior of each formula under certain conditions. We verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Removing low-frequency words from a corpus (often called cutoff) is a common practice to save on the computational costs involved in learning language models and topic models. In the case of language models, we often have to remove low-frequency words because of a lack of com- putational resources, since the feature space of k- grams tends to be so large that we sometimes need cutoffs even in a distributed environment <ref type="bibr" target="#b4">(Brants et al., 2007</ref>). In the case of topic models, the in- tuition is that low-frequency words do not make a large contribution to the statistics of the models. Actually, when we try to roughly analyze a corpus with topic models, a reduced corpus is enough for the purpose <ref type="bibr" target="#b20">(Steyvers and Griffiths, 2007)</ref>.</p><p>A natural question arises: How many low- frequency words can we remove while maintain- ing sufficient performance? Or more generally, by how much can we reduce a corpus/model us- ing a certain strategy and still keep a sufficient level of performance? There have been many stud-ies addressing the question as it pertains to differ- ent strategies <ref type="bibr" target="#b21">(Stolcke, 1998;</ref><ref type="bibr" target="#b5">Buchsbaum et al., 1998;</ref><ref type="bibr" target="#b11">Goodman and Gao, 2000</ref>; <ref type="bibr" target="#b9">Gao and Zhang, 2002;</ref><ref type="bibr" target="#b14">Ha et al., 2006;</ref><ref type="bibr" target="#b15">Hirsimaki, 2007;</ref><ref type="bibr" target="#b7">Church et al., 2007)</ref>. Each of these studies experimen- tally discusses trade-off relationships between the size of the reduced corpus/model and its perfor- mance measured by perplexity, word error rate, and other factors. To our knowledge, however, there is no theoretical study on the question and no evidence for such a trade-off relationship, es- pecially for topic models.</p><p>In this paper, we first address the question from a theoretical standpoint. We focus on the cutoff strategy for reducing a corpus, since a cutoff is simple but powerful method that is worth study- ing; as reported in <ref type="bibr" target="#b11">(Goodman and Gao, 2000</ref>; <ref type="bibr" target="#b9">Gao and Zhang, 2002</ref>), a cutoff is competitive with sophisticated strategies such as entropy prun- ing. As the basis of our theory, we assume Zipf's law <ref type="bibr" target="#b23">(Zipf, 1935)</ref>, which is an empirical rule repre- senting a long-tail property of words in a corpus. Our approach is essentially the same as those in physics, in the sense of constructing a theory while believing experimentally observed results. For ex- ample, we can derive the distance to the landing point of a ball thrown up in the air with initial speed v 0 and angle θ as v 0 2 sin(2θ)/g by believ- ing in the experimentally observed gravity acceler- ation g. In a similar fashion, we will try to clarify the trade-off relationship by believing Zipf's law. The rest of the paper is organized as follows. In Section 2, we define the notation and briefly ex- plain Zipf's law and perplexity. In Section 3, we theoretically derive the trade-off formulae of the cutoff for unigram models, k-gram models, and topic models, each of which represents its per- plexity with respect to a reduced vocabulary, un- der the assumption that the corpus follows Zipf's law. In addition, we show an approximate behav- ior of each formula under certain conditions. In Section 4, we verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on several real corpora. Sec- tion 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let us consider a corpus w := w 1 · · · w N of cor- pus size N and vocabulary size W . We use an abridged notation {w} := {w ∈ w} to repre- sent the vocabulary of w. Clearly, N = |w| and W = |{w}| hold. When w has additional nota- tions, N and W inherit them. For example, we will use N ′ as the size of w ′ without its definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Power law and Zipf's law</head><p>A power law is a mathematical relationship be- tween two quantities x and y, where y is propor- tional to the c-th power of x, i.e., y ∝ x c , and c is a real number. Zipf's law <ref type="bibr" target="#b23">(Zipf, 1935</ref>) is a power law discovered on real corpora, wherein for any word w ∈ w in a corpus w, its frequency (or word count) f (w) is inversely proportional to its frequency ranking r(w), i.e.,</p><formula xml:id="formula_0">f (w) = C r(w) .</formula><p>Here, f (w) := |{w ′ ∈ w | w ′ = w}|, and r(w) := |{w ′ ∈ w | f (w ′ ) ≥ f (w)}|. From the definition, the constant C is the maximum fre- quency in the corpus. Taking the natural loga- rithms ln(·) of both sides of the above equation, we find that its plot becomes linear on a log-log graph of r(w) and f (w). In fact, the result based on a statistical test in <ref type="bibr" target="#b8">(Clauset et al., 2009)</ref> reports that the frequencies of words in a corpus com- pletely follow a power law, whereas many datasets with long-tail properties, such as networks, actu- ally do not follow power laws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Perplexity</head><p>Perplexity is a widely used evaluation measure of k-gram models and topic models. Let p be a pre- dictive distribution over words, which was learned from a training corpus w based on a certain model. Formally, perplexity PP is defined as the geomet- ric mean of the inverse of the per-word likelihood on the held-out test corpus w τ , i.e.,</p><formula xml:id="formula_1">PP := ( ∏ w∈wτ 1 p(w) ) 1 Nτ .</formula><p>Intuitively, PP means how many possibilities one has for estimating the next word in a test cor- pus. According to the definition, a lower perplex- ity means better generalization performance of p. Another well-known evaluation measure is cross- entropy. Since cross-entropy is easily calculated as log 2 PP, we can apply many of the results of this paper to cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Perplexity on Reduced Corpora</head><p>Now let us consider what a cutoff is. In our study, we simply define a corpus that has been reduced by removing low-frequency words from the origi- nal corpus with a certain threshold. Formally, we say w ′ is a corpus reduced from the original cor- pus w, if w ′ is the longest subsequence of w such that max w ′ ∈w ′ r(w ′ ) = W ′ . Note that a sub- sequence can include gaps in contrast to a sub- string. For example, supposing we have a corpus w = abcaba with a vocabulary {w} = {a, b, c}, w ′ 1 = ababa is a reduced corpus, while w ′ 2 = aba and w ′ 3 = acaa are not.</p><p>After learning a distribution p ′ from a re- duced corpus w ′ , we need to infer the distri- bution p learned from the original corpus w. Here, we use constant restoring (defined below), which assumes the frequencies of the reduced low- frequency words are a constant. Definition 1 (Constant Restoring). Given a pos- itive constant λ, a distribution p ′ over a reduced corpus w ′ , and a corpus w, we say thatˆpthatˆ thatˆp is a λ-restored distribution of p ′ from w ′ to w, if ∑ w∈{w}ˆpw∈{w}ˆ w∈{w}ˆp(w) = 1, and for any w ∈ w,</p><formula xml:id="formula_2">ˆ p(w) ∝ { p ′ (w) (w ∈ w ′ ) λ (w / ∈ w ′ ).</formula><p>Constant restoring is similar to the additive smoothing defined byˆpbyˆ byˆp(w) ∝ p ′ (w) + λ, which is used to solve the zero-frequency problem of lan- guage models <ref type="bibr" target="#b6">(Chen and Goodman, 1996</ref>). The only difference is the addition of a constant λ only to zero-frequency words. We think con- stant restoring is theoretically natural in our set- ting, since we can derive the above equation by letting each frequency of reduced words be λN ′ and defining a restored frequency function as fol- lows:</p><formula xml:id="formula_3">ˆ f (w) = { f (w) (w ∈ w ′ ) λN ′ (w / ∈ w ′ ).</formula><p>Informally, constant restoring involves padding the vocabulary, while additive smoothing involves padding the corpus. Smoothing should be carried out after restoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perplexity of Unigram Models</head><p>Let us consider the perplexity of a unigram model learned from a reduced corpus. In unigram mod- els, a predictive distribution p ′ on a reduced cor- pus w ′ can be simply calculated as p ′ (w ′ ) = f (w ′ )/N ′ . We shall start with an analysis of training-set perplexity, since we can derive an ex- act formula for it, which will give us a sufficient idea for making an approximate analysis of test- set perplexity.</p><formula xml:id="formula_4">LetˆPPLetˆ LetˆPP 1 := ( ∏ w∈w 1 ˆ p(w) ) 1</formula><p>N be the perplexity of a λ-restored distributionˆpdistributionˆ distributionˆp on a unigram model.</p><p>The next lemma gives the optimal restoring con- stant λ * minimizingˆPPminimizingˆ minimizingˆPP 1 .</p><p>Lemma 2. For any λ-restored distributionˆpdistributionˆ distributionˆp of a distribution p ′ from a reduced corpus w ′ to the original corpus w, its perplexity is minimized by</p><formula xml:id="formula_5">λ * = N − N ′ (W − W ′ )N ′ .</formula><p>Proof. Let w R be the longest subsequence such that min w ′ ∈w ′ r(w ′ ) = W ′ + 1. Since w R is the remainder of w ′ , N R = N − N ′ and W R = W − W ′ hold. After substituting the normalized form ofˆpofˆ ofˆp of Definition 1 intô PP 1 , we havê</p><formula xml:id="formula_6">havê PP 1 = ( ∏ w ′ ∈w ′ 1 ˆ p(w ′ ) ∏ w R ∈w R 1 ˆ p(w R ) ) 1 N = ( ∏ w ′ ∈w ′ 1 + W R λ p ′ (w ′ ) ∏ w R ∈w R 1 + W R λ λ ) 1 N = 1 + W R λ λ N R N ( ∏ w ′ ∈w ′ 1 p ′ (w ′ ) ) 1 N .</formula><p>We obtain the optimal smoothing factor λ * when</p><formula xml:id="formula_7">∂ ∂λˆPP ∂λˆ ∂λˆPP 1 ∝ ∂ ∂λ (1 + W R λ)/λ N R N = 0.</formula><p>By using a similar argument to the one in the above lemma, we can obtain the optimal constant of additive smoothing as</p><formula xml:id="formula_8">λ * ≈ N −N ′ W N ′ , when N is sufficiently large.</formula><p>The next theorem gives the exact formula of the training-set perplexity of a unigram model learned from a reduced corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3. For any distribution p ′ on a unigram model learned from a corpus w ′ reduced from the original corpus w following Zipf's law, the per- plexityˆPPplexityˆ plexityˆPP 1 of the</head><formula xml:id="formula_9">λ * -restored distributionˆpdistributionˆ distributionˆp of p ′ from w ′ to w is calculated byˆPP byˆ byˆPP 1 (W ′ ) =H(W ) exp ( B(W ′ ) H(W ) ) ( W − W ′ H(W ) − H(W ′ ) ) 1− H(W ′ ) H(W ) ,</formula><p>where</p><formula xml:id="formula_10">H(X) := ∑ X x=1 1 x and B(X) := ∑ X x=1 ln x x .</formula><p>Proof. We expand the first part ofˆPPofˆ ofˆPP 1 in the proof of Lemma 2 using λ * as follows:</p><formula xml:id="formula_11">1 + W R λ * λ * N R N = ( 1 + N R N ′ ) ( W R N ′ N R ) N R N = ( N N ′ ) ( (W − W ′ )N ′ N − N ′ ) 1− N ′ N .</formula><p>The second part ofˆPPofˆ ofˆPP 1 is as follows:</p><formula xml:id="formula_12">( ∏ w ′ ∈w ′ 1 p ′ (w ′ ) ) 1 N = ∏ w ′ ∈{w ′ } ( 1 p ′ (w ′ ) ) f (w ′ ) N = W ′ ∏ r=1 ( rN ′ C ) C rN = W ′ ∏ r=1 ( N ′ C ) C rN W ′ ∏ r=1 r C rN = ( N ′ C )N ′ N exp ( C N W ′ ∑ r=1 ln r r )</formula><p>.</p><p>We obtain the objective formula by putting the above two formulae together with N = CH(W ) and N ′ = CH(W ′ ), which are derived from Zipf's law.</p><p>The functions H(X) and B(X) are the X-th partial sum of the harmonic series and Bertrand series (special form), respectively. An approxima- tion by definite integrals yields H(X) ≈ ln X +γ, where γ is the Euler-Mascheroni constant, and B(X) ≈ 1 2 ln 2 X. We may omit γ from the ap- proximate analysis. Now let us consider an approximate form ofˆPP ofˆ ofˆPP 1 (W ′ ) in Theorem 3. For further discussion, we define the last part ofˆPPofˆ ofˆPP 1 (W ′ ) as follows:</p><formula xml:id="formula_13">F (W, W ′ ) := ( W − W ′ H(W ) − H(W ′ ) ) 1− H(W ′ ) H(W ) .</formula><p>Since W ′ = δW holds for an appropriate ratio δ, we have</p><formula xml:id="formula_14">F (W, δW ) = ( W − δW H(W ) − H(δW ) ) 1− H(δW ) H(W ) ≈ ( W − δW ln W − ln (δW ) ) 1− ln (δW ) ln W = ( W (1 − δ) − ln δ ) − ln δ ln W → 1 δ (W → ∞).</formula><p>Therefore, when W is sufficiently large, we can use</p><formula xml:id="formula_15">F (W, W ′ ) ≈ W W ′ , since F (W, δW ) ≈ 1</formula><p>δ holds for any ratio δ : 0 &lt; δ &lt; 1. Using this fact, we obtain an approximate formulã PP 1 ofˆPPofˆ ofˆPP 1 as follows:</p><formula xml:id="formula_16">˜ PP 1 (W ′ ) = ln W exp ( ln 2 W ′ 2 ln W ) W W ′ = √ W ln W exp (ln W ′ − ln W ) 2 2 ln W .</formula><p>The complexity of˜PPof˜ of˜PP 1 is quasi-polynomial, i.e.,</p><formula xml:id="formula_17">˜ PP 1 (W ′ ) = O(W ′ ln W ′ )</formula><p>, which behaves as a quadratic function on a log-log graph. Since˜PP Since˜</p><formula xml:id="formula_18">Since˜PP 1 (W ′ ) is convex, i.e., ∂ 2 ∂W ′2˜PP ′2˜ ′2˜PP 1 (W ′ ) &gt; 0, and its gradient ∂ ∂W ′ ˜ PP 1 (W ′ )</formula><p>is zero when W ′ = W , we infer that low-frequency words may not largely contribute to the statistics.</p><p>Considering the special case of W ′ = W , we obtain the perplexity PP 1 of the unigram model learned from the original corpus w as</p><formula xml:id="formula_19">PP 1 = H(W ) exp ( B(W ) H(W ) ) ≈ √ W ln W.</formula><p>Interestingly, PP 1 is approximately expressed as a simple elementary function of vocabulary size W . This suggests that models learned from cor- pora with the same vocabulary size theoretically have the same perplexity. For the test-set perplexity, we assume that both the training corpus w and test corpus w τ are gen- erated from the same distribution based on Zipf's law. This assumption is natural, considering the situation of an in-domain test or cross validation test. Let w τ ′ be the longest subsequence of w τ such that for any w ∈ w τ ′ , w ∈ w ′ holds. For-</p><formula xml:id="formula_20">mally, we assume p ′ (w) ≈ p τ ′ (w) for any w ∈ w ′ τ when W τ &gt; W ′ , where p τ ′ is the true distribu- tion over w τ ′ .</formula><p>Using similar arguments to those of Lemma 2 and Theorem 3 for w τ , we obtain an approximation formula for the test-set perplex- ity, where we simply substitute W and W ′ in the exact formula for the training-set perplexity with W τ and W τ ′ , respectively. For simplicity, we will only consider training-set perplexity from now on, since we can make a similar argument for the test- set perplexity in the later analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Perplexity of k-gram Models</head><p>Here, we will consider the perplexity of a k-gram model learned from a reduced corpus as a standard extension of a unigram model. Our theory only assumes that the corpus is generated on the basis of Zipf's law. Thus, we can use a simple model where k-grams are calculated from a random word sequence based on Zipf's law. This model seems to be stupid, since we can easily notice that the bigram "is is" is quite frequent, and the two bi- grams "is a" and "a is" have the same frequency. However, the experiments described later uncov- ered the fact that the model can roughly capture the behavior of real corpora.</p><p>The frequency f k of k-gram word w k ∈ w k in the model is represented by the following formula:</p><formula xml:id="formula_21">f k (w k ) = C k g k (r k (w k )) ,</formula><p>where C k is the maximal frequency in k-grams, r k is the frequency ranking of w k over k-grams, and g k expresses the frequency decay in k-grams. For example, the decay function g 2 of bigrams is as follows:</p><formula xml:id="formula_22">(g 2 (i)) i := (g 2 (1), g 2 (2), g 2 (3), · · · ) = (1 · 1, 1 · 2, 2 · 1, 1 · 3, 3 · 1, · · · ) = (1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, · · · ).</formula><p>This is an inverse of the sum of Piltz's divisor functions d 2 (n) := ∑ i 1 ·i 2 =n 1, which represents the number of divisors of an integer n (cf. <ref type="bibr" target="#b19">(OEIS, 2001)</ref>). In general, we formally define g k through its inverse:</p><formula xml:id="formula_23">g −1 k (ℓ) := S k (ℓ), where S k (ℓ) := ∑ ℓ n=1 d k (n) and d k (n) := ∑ i 1 ·i 2 ···i k =n 1. Since (g k (i)) i</formula><p>is a sorted sequence of the elements of the k-th tensor power of vector (1, · · · , W ), we can calculate the maximum frequency C k as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4. For any corpus w following Zipf's law, the maximum frequency of k-grams in our model is calculated by</head><formula xml:id="formula_24">C k = N − (k − 1)D (H(W )) k ,</formula><p>where D denotes the number of documents in w.</p><p>Proof. We use</p><formula xml:id="formula_25">∑ w k f k (w k ) = C k ( ∑ w 1/r(w)) k .</formula><p>The sum S k (ℓ) of Piltz's divisor functions can be approximated by ℓP k (ln ℓ), where P k (x) is a polynomial of degree k − 1 with respect to x, and the main term of ℓP k (ln ℓ) is given by the following residue Res s=1</p><formula xml:id="formula_26">ζ k (s)x s s</formula><p>, where ζ(s) is the Riemann zeta function ( <ref type="bibr" target="#b17">Li, 2005)</ref>. Using this fact, we obtain an approximation ln (g −1 k (ℓ)) ≈ ln ℓ + O(ln (ln ℓ)) ≈ ln ℓ, when ℓ is sufficiently large. Thus, when the corpus is sufficiently large, we can see that the behavior of f k is roughly linear on a log-log graph, i.e.,</p><formula xml:id="formula_27">f k (w k ) ∝ r k (w k ) −1 , since if g −1 k (ℓ) ∝ ℓ c holds, then f k (r) ∝ (g k (r)) −1 ∝ r − 1 c holds.</formula><p>Unfortunately, however, most corpora in the real world are not so large that the above- mentioned relation holds. Actually, <ref type="bibr" target="#b13">Ha et al. (Ha et al., 2002;</ref><ref type="bibr" target="#b14">Ha et al., 2006</ref>) experimentally found that although a k-gram corpus roughly follows a power law even when k &gt; 1, its exponent is smaller than 1 (for Zipf's law). They pointed out that the exponent of bigrams is about 0.66, and that of 5-grams is about 0.59 in the Wall Street Journal corpus (WSJ87). Believing their claim that there exists a constant π k such that f k (w k ) ∝ r k (w k ) −π k , we estimated the exponent of k-grams in an actual situation in the form of the following lemma.</p><p>Lemma 5. Assuming that f k (w k ) ∝ r k (w k ) −π k holds for any k-gram word w k ∈ w k in a corpus w following Zipf's law, the optimal exponent in our model based on the least squares criterion is calculated by</p><formula xml:id="formula_28">π k = ln W (k − 1) ln (ln W ) + ln W .</formula><p>Proof. We find the optimal exponent π k by mini- mizing the sum of squared errors between the gra- dients of g −1 k (r) and r 1 π k on a log-log graph:</p><formula xml:id="formula_29">∫ { ∂ ∂y (y + ln P k (y)) − ∂ ∂y ( 1 π k y )} 2 dy,</formula><p>where y = ln r.</p><p>In the case of unigrams (k = 1), the formula exactly represents Zipf's law. In the case of k- grams (k &gt; 1), we found that the formula ap- proaches Zipf's law when W approaches infinity, i.e., lim W →∞ π k = 1.</p><p>Let us consider the perplexity of a k-gram model learned from a reduced corpus. We im- mediately obtain the following corollary using Lemma 5.</p><p>Corollary 6. For any distribution p ′ on a k-gram model learned from a corpus w ′ reduced from the original corpus w following Zipf's law, assuming that f k (w k ) ∝ r k (w k ) −π k holds for any k-gram word w k ∈ w k and the optimal exponent π k in Lemma 5, the perplexityˆPPperplexityˆ perplexityˆPP k of the λ * -restored distributionˆpdistributionˆ distributionˆp of p ′ from w ′ to w is calculated byˆPP</p><formula xml:id="formula_30">byˆ byˆPP k (W ′ ) =H π k (W ) exp ( B π k (W ′ ) H π k (W ) ) ( W − W ′ H π k (W ) − H π k (W ′ ) ) 1− Hπ k (W ′ ) Hπ k (W ) ,</formula><p>where</p><formula xml:id="formula_31">H a (X) := ∑ X x=1 1 x a and B a (X) := ∑ X x=1</formula><p>a ln x x a . H a (X) is the X-th partial sum of the P-series or hyper-harmonic series, which is a generaliza- tion of the harmonic series H(X). B a (X) is the X-th partial sum of the Bertrand series (another special form of B(X)). When 0 &lt; a &lt; 1, we can easily calculatê PP k (W ′ ) by using the following approximations:</p><formula xml:id="formula_32">H a (X) ≈ (X + 1) 1−a − 1 1 − a B a (X) ≈ a 1 − a (X + 1) 1−a ln(X + 1) − a (1 − a) 2 (X + 1) 1−a + a (1 − a) 2 .</formula><p>By putting the approximations of H a (X) and B a (X) into the formula of Corollary 6, we ob-</p><formula xml:id="formula_33">tain an approximationˆPPapproximationˆ approximationˆPP k (W ′ ) ≈ O(W ′ W ′1−π k ).</formula><p>This implies thatˆPPthatˆ thatˆPP k (W ′ ) is approximately linear on a log-log graph, when π k is close to 1, i.e., k is relatively small and W is sufficiently large. Note that we must use the approximation of H(X), not H a (X), when a = 1.</p><p>The fact that the frequency of k-grams follows a power law leads us to an additional convenient property, since the process of generating a cor- pus in our theory can be treated as a variant of the coupon collector's problem. In this problem, we consider how many trials are needed for col- lecting all coupons whose occurrence probabilities follow some stable distribution. According to a well-known result about power law distributions (Boneh and Papanicolaou, 1996), we need a cor- pus of size kW k 1−π k ln W when π k &lt; 1, and W ln 2 W when π k = 1 for collecting all of the k-grams, the number of which is W k . Using results in (Atso- nios et al., 2011), we can easily obtain a lower and upper bound of the actual vocabulary size˜Wsize˜ size˜W k of k-grams from the corpus size N and vocabulary size W as˜W as˜ as˜W k ≥ (π k + 1)</p><formula xml:id="formula_34">( 1 − e − (1−π k )N W k −1 −ln W k −1 W k ) ˜ W k ≤ π k π k − 1 ( N H π k (W k ) ) 1 π k − N W 1−π k (π k − 1)H π k (W k ) .</formula><p>This means that we can determine the rough sparseness of k-grams and adjust some of the pa- rameters such as the gram size k in learning statis- tical language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Perplexity of Topic Models</head><p>In this section, we consider the perplexity of the widely used topic model, Latent Dirichlet Alloca- tion (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>, by using the nota- tion given in ( <ref type="bibr" target="#b12">Griffiths and Steyvers, 2004</ref>). LDA is a probabilistic language model that generates a corpus as a mixture of hidden topics, and it allows us to infer two parameters: the document-topic distribution θ that represents the mixture rate of topics in each document, and the topic-word dis- tribution ϕ that represents the occurrence rate of words in each topic. For a given corpus w, the model is defined as</p><formula xml:id="formula_35">θ d i ∼ Dirichlet(α) z i |θ d i ∼ Multi(θ d i ) ϕ z i ∼ Dirichlet(β) w i |z i , ϕ z i ∼ Multi(ϕ z i ),</formula><p>where d i and z i are respectively the document that includes the i-th word w i and the hidden topic that is assigned to w i . In the case of infer- ence by Gibbs sampling presented in ( <ref type="bibr" target="#b12">Griffiths and Steyvers, 2004</ref>), we can sample a "good" topic as- signment z i for each word w i with high probabil- ity. Using the assignments z, we obtain the pos- terior distributions of two parameters asˆθasˆ</p><formula xml:id="formula_36">asˆθ d (z) ∝ n (d) z + α andˆϕandˆ andˆϕ z (w) ∝ n (w) z + β, where n (d)</formula><p>z and n (w) z respectively represent the number of times assigning topic z in document d and the number of times topic z is assigned to word w.</p><p>Since an exact analysis is very hard, we will place rough assumptions onˆϕonˆ onˆϕ andˆθandˆ andˆθ to reduce the complexity. The assumption placed onˆϕonˆ onˆϕ is that the word distributionˆϕdistributionˆ distributionˆϕ z of each topic z follows Zipf's law. We think this is acceptable since we can re- gard each topic as a corpus that follows Zipf's law. Sincê ϕ z is normalized for each topic, we can as- sume that for any two topics, z and z ′ , and any two words, w and</p><formula xml:id="formula_37">w ′ , ˆ ϕ z (w) ≈ ˆ ϕ z ′ (w ′ ) holds if r z (w) = r z ′ (w ′ )</formula><p>, where r z (w) is the frequency ranking of w with respect to n (w) z . Note that the above assumption pertains to a posterior, and we do not discuss the fact that a Pitman-Yor process prior is better suited for a power law <ref type="bibr" target="#b10">(Goldwater et al., 2011</ref>).</p><p>The assumption placed onˆϕonˆ onˆϕ may not be reason- able in the case ofˆθofˆ ofˆθ, because we can easily think of a document with only one topic, and we usu- ally use a small number T of topics for LDA, e.g., T = 20. Thus, we consider two extreme cases.</p><p>One is where each document evenly has all topics, and the other is where each document only has one topic. Although these two cases might be unreal- istic, the actual (theoretical) perplexity is expected to be between their values. We believe that analyz- ing such extreme cases is theoretically important, since it would be useful for bounding the compu- tational complexity and predictive performance. We can regard the former case as a unigram model, since the marginal predictive distribution</p><formula xml:id="formula_38">∑ T z=1ˆθ z=1ˆ z=1ˆθ d (z) ˆ ϕ z (w) ∝ ∑ T z=1 n (w) z +β T ∝ ∼ f (w) is in- dependent of d;</formula><p>here we have usedˆθusedˆ usedˆθ d (z) = 1/T from the assumption. In the latter case, we can obtain an exact formula for the perplexity of LDA when the topic assigned to each document follows a discrete uniform distribution, as shown in the next theorem. Note that a mixture of corpora fol- lowing Zipf's law can be approximately regarded as following Zipf's law, when W is sufficiently large. Theorem 7. For any distribution p ′ on the LDA model with T topics learned from a corpus w ′ re- duced from the original corpus w following Zipf's law, assuming that each document only has one topic which is assigned based on a discrete uni- form distribution, the perplexityˆPPperplexityˆ perplexityˆPP Mix of the λ * - restored distributionˆpdistributionˆ distributionˆp of p ′ from w ′ to w is calcu- </p><formula xml:id="formula_39">byˆ byˆPP Mix (W ′ ) =H(W/T ) exp ( B(W ′ /T ) H(W/T ) ) ( W − W ′ H(W/T ) − H(W ′ /T ) ) 1− H(W ′ /T ) H(W/T )</formula><p>Proof. We can prove this by using a similar argu- ment to that of Theorem 3 for each topic.</p><p>The formula of the theorem is nearly identical to the one of Theorem 3 for a 1/T corpus. This implies that the growth rate of the perplexity of LDA models is larger than that of unigram mod- els, whereas the perplexity of LDA models for the original corpus is smaller than that of unigram models. In fact, a similar argument to the one in the approximate analysis in Section 3.1 leads to an approximate formulã PP Mix ofˆPPofˆ ofˆPP Mix as˜PP</p><formula xml:id="formula_39">byˆ byˆPP Mix (W ′ ) =H(W/T ) exp ( B(W ′ /T ) H(W/T ) ) ( W − W ′ H(W/T ) − H(W ′ /T ) ) 1− H(W ′ /T ) H(W/T )</formula><p>when W is sufficiently large. That is, ˜ PP Mix (W ′ ) also has a quadratic behavior in a log-log graph, i.e.,</p><formula xml:id="formula_41">˜ PP Mix (W ′ ) = O(W ′ ln W ′ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We performed experiments on three real corpora (Reuters, 20news, and Enwiki) and two syn- thetic corpora (Zipf1 and ZipfMix) to verify the correctness of our theory and to examine the gap between theory and practice. Reuters and 20news here denote corpora extracted from the Reuters-21578 and 20 Newsgroups data sets, re- spectively. Enwiki is a 1/100 corpus of the En- glish Wikipedia. Zipf1 is a synthetic corpus gen- erated by Zipf's law, whose corpus is the same size as Reuters, and ZipfMix is a mixture of 20 syn- thetic corpora, sizes are 1/20th of Reuters. We used ZipfMix only for the experiments on topic models. <ref type="table" target="#tab_0">Table 1</ref> lists the details of all five corpora. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows the word frequency of Reuters, 20news, Enwiki, and Zipf1 versus frequency ranking on a log-log graph. In all cor- pora, we can regard each curve as linear with a gradient close to 1. This means that all corpora roughly follow Zipf's law. Furthermore, since the curve of Zipf1 is similar to that of Reuters, Zipf1 can be regarded as acceptable. <ref type="figure" target="#fig_0">Fig. 1(b)</ref> plots the perplexity of unigram mod- els learned from Reuters, 20news, Enwiki, and Zipf1 versus the size of reduced vocabu- lary on a log-log graph. Each value is the aver- age over different test sets of five-fold cross val- idation. Theory1 is calculated using the for- mula in Theorem 3. The graph shows that the curve of Theory1 is nearly identical to that of Zipf1. Since the vocabulary size W τ of each test set is small in this experiment, some errors appear when W ′ is large, i.e., W τ &lt; W ′ . This clearly means that our theory is theoretically correct for an ideal corpus Zipf1. Comparing Zipf1 with Reuters, however, we find that their perplex- ities are quite different. The reason is that the gap between the frequencies of low-ranking (high- frequency) words is considerably large. For ex- ample, the frequency of the 1st-rank word of Reuters is f (w) = 136, 371, while that of Zipf1 is f (w) = 234, 705. Our theory seems to be suited for inferring the growth rate of perplexity rather than the perplexity value itself.</p><p>As for the approximate formulã PP 1 of Theo- rem 3, we can surely regard the curve of Zipf1 as being roughly quadratic. The curves of real corpora also have a similar tendency, although their gradients are slightly steeper. This difference might have been caused by the above-mentioned errors. However, at least, we can ascertain the important fact that the results for the corpora re- duced by 1/100 are not so different from those of the original corpora from the perspective of their perplexity measures. <ref type="figure" target="#fig_0">Fig. 1(c)</ref> plots the frequency of k-grams (k ∈ {1, 2, 3}) in Reuters versus frequency ranking on a log-log graph. TheoryFreq (1-3) are calcu- lated using C k in Lemma 4 and π k in Lemma 5. A comparison of TheoryFreq and Zipf verifies the correctness of our theory. However, comparing Zipf and Reuters, we see that C k is poorly es- timated when the gram size is large, whereas π k is roughly correct. This may have happened because we did not put any assumptions on the word se-  quences in our simple model. The frequencies of high-order k-grams tend to be lower than in real- ity. We might need to place a hierarchical assump- tion on the a power law, as in done in hierarchical Pitman-Yor processes <ref type="bibr" target="#b22">(Wood et al., 2011</ref>). TheoryGrad is calculated using π k in Lemma 5. Surprisingly, the real exponents of Reuters are almost the same as the theoretical estimate π k based on our "stupid" model that does not care about the order of words. Note that we do not use any information other than the vocabulary size W and the gram size k for estimating π k . <ref type="figure" target="#fig_0">Fig. 1(e)</ref> plots the perplexity of k-gram mod- els (k ∈ {1, 2, 3}) learned from Reuters versus the size of reduced vocabulary on a log-log graph.</p><p>Theory2 and Theory3 are calculated using the formula in Corollary 6. In the case of bigrams, the perplexities of Theory2 are almost the same as that of Zipf2 when the size of reduced vocab- ulary is large. However, in the case of trigrams, the perplexities of Theory3 are far from those of Zipf3. This difference may be due to the sparse- ness of trigrams in Zipf3. To verify the correct- ness of our theory for higher order k-gram models, we need to make assumptions that include backoff and smoothing. <ref type="figure" target="#fig_0">Fig. 1(f)</ref> plots the perplexity of LDA models with 20 topics learned from Reuters, 20news, Enwiki, Zipf1, and ZipfMix versus the size of reduced vocabulary on a log-log graph. We used a collapsed Gibbs sampler with 100 iterations to infer the parameters and set the hyper parameters, α = 0.1 and β = 0.1. In evaluating the perplexity, we estimated a posterior document-topic distribu- </p><formula xml:id="formula_42">)/PP(W ) ≈ exp (lnˆPPlnˆ lnˆPP(W ′ ) + c)/ exp (lnˆPPlnˆ lnˆPP(W ) + c) = ˆ PP(W ′ )/ ˆ PP(W ).</formula><p>Therefore, we can use TheoryAve as a heuristic function for estimat- ing the perplexity of topic models. Since we can calculate an inverse of TheoryAve from the bisection or Newton-Raphson method, we can maximize the reduction rate and ensure an acceptable perplexity based on a user-specified deterioration rate. According to the fact that the three real corpora with different sizes have a similar tendency, it is expected that we can use our theory for a larger corpus.</p><p>Finally, let us examine the computational costs for LDA learning. <ref type="table" target="#tab_2">Table 2</ref> shows computa- tional time and memory size for LDA learning on the original corpus, (1/10)-reduced corpus, and (1/20)-reduced corpus of Reuters. Comparing the memory used in the learning with the origi- nal corpus and with the (1/10)-reduced corpus of Reuters, we find that the learning on the (1/10)- reduced corpus used 60% of the memory used by the learning on the original corpus. While the computational time decreased a little, we believe that reducing the memory size helps to reduce computational time for a larger corpus in the sense that it can relax the constraint for in-memory com- puting. Although we did not examine the accuracy of real tasks in this paper, there is an interesting report that the word error rate of language mod- els follows a power law with respect to perplexity <ref type="bibr" target="#b16">(Klakow and Peters, 2002</ref>). Thus, we conjecture that the word error rate also has a similar tendency as perplexity with respect to the reduced vocabu- lary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We studied the relationship between perplexity and vocabulary size of reduced corpora. We de- rived trade-off formulae for the perplexity of k- gram models and topic models with respect to the size of reduced vocabulary and showed that each formula approximately has a simple behavior on a log-log graph under certain conditions. We veri- fied the correctness of our theory on synthetic cor- pora and examined the gap between theory and practice on real corpora. We found that the es- timation of the perplexity growth rate is reason- able. This means that we can maximize the reduc- tion rate, thereby ensuring an acceptable perplex- ity based on a user-specified deterioration rate. Furthermore, this suggests the possibility that we can theoretically derive empirical parameters, or "rules of thumb", for different NLP problems, as- suming that a corpus follows Zipf's law. We be- lieve that our theoretical estimation has the advan- tages of computational efficiency and scalability especially for very large corpora, although exper- imental estimations such as cross-validation may be more accurate.</p><p>In the future, we want to find out the cause of the gap between theory and practice and extend our theory to bridge the gap, in the same way that we can construct equations of motion with air re- sistance in the example of the landing point of a ball in Section 1. For example, promising re- search directions include using a general law such as the Zipf-Mandelbrot law <ref type="bibr" target="#b18">(Mandelbrot, 1965</ref>), a sophisticated model that cares the order of words such as hierarchical Pitman-Yor processes <ref type="bibr" target="#b22">(Wood et al., 2011)</ref>, and smoothing/backoff methods to handle the sparseness problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Word frequency of Reuters, 20news, Enwiki, and Zipf1 versus frequency ranking. (b) Perplexity of unigram models learned from Reuters, 20news, Enwiki, and Zipf1 versus size of reduced vocabulary. Theory1 is calculated using the formula in Theorem 3. (c) Frequency of k-grams (k ∈ {1, 2, 3}) in Reuters and Zipf1 versus frequency ranking. The suffix digit of each label means its gram size. TheoryFreq (1-3) are calculated using Lemma 4 and Lemma 5. (d) Exponent of a power law over k-grams in Reuters versus gram size. TheoryGrad is calculated using π k in Lemma 5. (e) Perplexity of k-gram models learned from Reuters versus size of reduced vocabulary. Theory2 and Theory3 are calculated using the formula in Corollary 6. (f) Perplexity of topic models learned from Reuters, 20news, Enwiki, Zipf1, and ZipfMix versus size of reduced vocabulary. TheoryMix is calculated using the formula in Theorem 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 (</head><label>1</label><figDesc>Fig. 1(d) plots the exponent of the power law over k-grams in Reuters versus the gram size on a normal graph. We estimated each exponent of Reuters by using the least-squares method. TheoryGrad is calculated using π k in Lemma 5. Surprisingly, the real exponents of Reuters are almost the same as the theoretical estimate π k based on our "stupid" model that does not care about the order of words. Note that we do not use any information other than the vocabulary size W and the gram size k for estimating π k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Details of Reuters, 20news,</head><label>1</label><figDesc></figDesc><table>Enwiki, 
Zipf1, and ZipfMix. 
vocab. size corpus size doc. size 
Reuters 
70,258 
2,754,800 
18,118 
20news 
192,667 
4,471,151 
19,997 
Enwiki 
409,902 16,711,226 
51,231 
Zipf1 
69,786 
2,754,800 
18,118 
ZipfMix 
70,093 
2,754,800 
18,118 

lated byˆPP 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Computational time and memory size for LDA learning on the original corpus, (1/10)- reduced corpus, and (1/20)-reduced corpus ofW ′</head><label>2</label><figDesc></figDesc><table>Reuters. 
corpus 
time 
memory perplexity 
original 4m3.80s 71,548KB 
500 
(1/10) 3m55.70s 46,648KB 
550 
(1/20) 3m42.63s 34,024KB 
611 

tionˆθtionˆ tionˆθ d by using the first half of each test document 

and calculated the perplexity on the second half, 
as is done in (Asuncion et al., 2009). Each value 
is the average over different test sets of five-fold 
cross validation. Theory1 and TheoryMix 
are calculated using the formulae in Theorem 3 
and Theorem 7, respectively. Comparing Zipf1 
with Theory1, and ZipfMix with TheoryMix, 
we find that our theory of the extreme cases 
discussed in Section 3.3 is theoretically cor-
rect. TheoryAve is the average of Theory1 
and TheoryMix. Comparing Reuters and 
TheoryAve, we see that their curves are almost 
the same. If theoretical perplexityˆPPperplexityˆ perplexityˆPP has a 
similar tendency as real perplexity PP on a 
log-log graph, i.e., ln PP(W ′ ) ≈ lnˆPPlnˆ lnˆPP(W ′ ) + c 
for some constant c, we can approximate 
its deterioration rate as PP(</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author would like to thank the reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On smoothing and inference for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI2009)</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence (UAI2009)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On power-law distributed balls in bins and its applications to view size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Atsonios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Hanusse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusik</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Symposium on Algorithms and Computation</title>
		<meeting>the 22nd International Symposium on Algorithms and Computation</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">General asymptotic estimates for the coupon collector problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vassilis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papanicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="289" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large Language Models in Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL2007)</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shrinking Language Models by Robust Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Buchsbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Giancarlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><forename type="middle">R</forename><surname>Westbrook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="685" to="688" />
		</imprint>
	</monogr>
	<note>ICASSP 1998</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL 1996)</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics (ACL 1996)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compressing Trigram Language Models with Golomb Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Power-Law Distributions in Empirical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosma</forename><forename type="middle">Rohilla</forename><surname>Shalizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="661" to="703" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving Language Model Size Reduction using Better Pruning Criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="176" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Producing Power-Law Distributions and Damping Word Frequencies with TwoStage Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2335" to="2382" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language Model Size Reduction by Pruning and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Spoken Language Processing</title>
		<meeting>the 6th International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="110" to="113" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<publisher>PNAS</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extension of Zipf&apos;s Law to Words and Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">I</forename><surname>Le Quan Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Sicilia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING 2002)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reduced n-gram models for English and Chinese corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Le Quan Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (COLING-ACL 2006)</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, the Conference (COLING-ACL 2006)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="309" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Compressing N-Gram Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teemu</forename><surname>Hirsimaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the 2007 IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="949" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Testing the correlation of word error rate and perplexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On Generalized Euler Constants and an Integral Related to the Piltz Divisor Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">˘ Siauliai Mathematical Seminar</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Information Theory and Psycholinguistics: A Theory of Word Frequencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandelbrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Psychology: Principles and Approaches</title>
		<imprint>
			<publisher>Basic Books</publisher>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The on-line encyclopedia of integer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oeis</forename></persName>
		</author>
		<ptr target="http://oeis.org/A061017/" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Latent Semantic Analysis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="424" to="440" />
		</imprint>
		<respStmt>
			<orgName>Lawrence Erlbaum Associates</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Entropy-based Pruning of Backoff Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>the DARPA Broadcast News Transcription and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lancelot</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<title level="m">The Sequence Memoizer. Communications of the Association for Computing Machines</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Psychobiology of Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kingsley Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
			<pubPlace>Houghton-Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
