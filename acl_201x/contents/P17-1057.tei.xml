<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representations of language in a model of visually grounded speech signal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
							<email>g.chrupala@uvt.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tilburg University</orgName>
								<orgName type="institution" key="instit2">Tilburg University</orgName>
								<orgName type="institution" key="instit3">Tilburg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieke</forename><surname>Gelderloos</surname></persName>
							<email>l.j.gelderloos@uvt.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tilburg University</orgName>
								<orgName type="institution" key="instit2">Tilburg University</orgName>
								<orgName type="institution" key="instit3">Tilburg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
							<email>a.alishahi@uvt.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tilburg University</orgName>
								<orgName type="institution" key="instit2">Tilburg University</orgName>
								<orgName type="institution" key="instit3">Tilburg University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Representations of language in a model of visually grounded speech signal</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="613" to="622"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech recognition is one of the success stories of language technology. It works remarkably well in a range of practical settings. However, this success relies on the use of very heavy supervi- sion where the machine is fed thousands of hours of painstakingly transcribed audio speech signal. Humans are able to learn to recognize and under- stand speech from notably weaker and noisier su- pervision: they manage to learn to extract struc- ture and meaning from speech by simply being ex- posed to utterances situated and grounded in their daily sensory experience. Modeling and emulat- ing this remarkable skill has been the goal of nu- merous studies; however in the overwhelming ma- jority of cases researchers used severely simplified settings where either the language input or the ex- tralinguistic sensory input, or both, are small scale and symbolically represented. Section 2 provides a brief overview of this research.</p><p>More recently several lines of work have moved towards more realistic inputs while modeling or emulating language acquisition in a grounded set- ting. <ref type="bibr" target="#b8">Gelderloos and Chrupała (2016)</ref> use the image captioning dataset MS COCO ( <ref type="bibr" target="#b19">Lin et al., 2014</ref>) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are pho- netically transcribed descriptions of these scenes. The use of such moderately large and low-level data allows the authors to train a multi-layer re- current neural network model, and to explore the nature and localization of the emerging hierarchy of linguistic representations learned in the process. Furthermore, in a series of recent studies <ref type="bibr" target="#b9">Harwath and Glass (2015)</ref>; <ref type="bibr" target="#b11">Harwath et al. (2016)</ref>; <ref type="bibr" target="#b10">Harwath and Glass (2017)</ref> use image captioning datasets to model learning to understand spoken language from visual context with convolutional neural net- work models. Finally, there is a small but grow- ing body of work dedicated to elucidating the na- ture of representations learned by neural networks from language data (see Section 2.2 for a brief overview). In the current work we build on these three strands of research and contribute the follow- ing advances:</p><p>• We use a multi-layer gated recurrent neural network to properly model the temporal na- ture of speech signal and substantially im- prove performance compared to the convolu- tional architecture from Harwath and Glass (2015); • We carry out an in-depth analysis of the rep- resentations used by different components of the trained model and correlate them to repre- sentations learned by a text-based model and to human patterns of judgment on linguistic stimuli. This analysis is especially novel for a model with speech signal as input. The general pattern of findings in our analysis is as follows: The model learns to extract from the acoustic input both form-related and semantics- related information, and encodes it in the activa- tions of the hidden layers. Encoding of semantic aspects tends to become richer as we go up the hi- erarchy of layers. Meanwhile, encoding of form- related aspects of the language input, such as ut- terance length or the presence of specific words, tends to initially increase and then decay.</p><p>We release the code for our models and analyses as open source, available at https://github.com/gchrupala/visually-grounded- speech. We also release a dataset of synthetically spoken image captions based on MS COCO, avail- able at https://doi.org/10.5281/zenodo.400926.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Children learn to recognize and assign meaning to words from continuous perceptual data in ex- tremely noisy context. While there have been many computational studies of human word mean- ing acquisition, they typically make strong sim- plifying assumptions about the nature of the in- put. Often language input is given in the form of word symbols, and the context consists of a set of symbols representing possible referents (e.g. <ref type="bibr" target="#b27">Siskind, 1996;</ref><ref type="bibr" target="#b7">Frank et al., 2007;</ref><ref type="bibr" target="#b6">Fazly et al., 2010</ref>). In contrast, several studies presented mod- els that learn from sensory rather than symbolic in- put, which is rich with regards to the signal itself, but very limited in scale and variation (e.g. <ref type="bibr" target="#b24">Roy and Pentland, 2002;</ref><ref type="bibr" target="#b31">Yu and Ballard, 2004;</ref><ref type="bibr" target="#b17">Lazaridou et al., 2016</ref>). <ref type="bibr" target="#b3">Chrupała et al. (2015)</ref> introduce a model that learns to predict the visual context from image captions. The model is trained on image-caption pairs from MSCOCO ( <ref type="bibr" target="#b19">Lin et al., 2014</ref>), captur- ing both rich visual input as well as larger scale input, but the language input still consists of word symbols. <ref type="bibr" target="#b8">Gelderloos and Chrupała (2016)</ref> propose a similar architecture that instead takes phoneme- level transcriptions as language input, thereby in- corporating the word segmentation problem into the learning task. In this work, we introduce an ar- chitecture that learns from continuous speech and images directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multimodal language acquisition</head><p>This work is related to research on visual grounding of language. The field is large and growing, with most work dedicated to the ground- ing of written text, particularly in image cap- tioning tasks (see <ref type="bibr" target="#b2">Bernardi et al. (2016)</ref> for an overview). However, learning to ground language to visual information is also interesting from an automatic speech recognition point of view. Po- tentially, ASR systems could be trained from nat- urally co-occurring visual context information, without the need for extensive manual annota- tion -a particularly promising prospect for speech recognition in low-resource languages. There have been several attempts along these lines. <ref type="bibr" target="#b28">Synnaeve et al. (2014)</ref> present a method of learning to recognize spoken words in isolation from co- occurrence with image fragments. <ref type="bibr" target="#b9">Harwath and Glass (2015)</ref> present a model that learns to map pre-segmented spoken words in sequence to as- pects of the visual context, while in Harwath and Glass (2017) the model also learns to recognize words in the unsegmented signal.</p><p>Most closely related to our work is that of <ref type="bibr" target="#b11">Harwath et al. (2016)</ref>, as it presents an architecture that learns to project images and unsegmented spoken captions to the same embedding space. The sentence representation is obtained by feed- ing the spectrogram to a convolutional network. The architecture is trained on crowd-sourced spo- ken captions for images from the Places dataset ( <ref type="bibr" target="#b32">Zhou et al., 2014)</ref>, and evaluated on image search and caption retrieval. Unfortunately this dataset is not currently available and we were thus unable to directly compare the performance of our model to <ref type="bibr" target="#b11">Harwath et al. (2016)</ref>. We do compare to <ref type="bibr" target="#b9">Harwath and Glass (2015)</ref> which was tested on a public dataset. We make different architectural choices, as our models are based on recurrent highway net- works ( <ref type="bibr" target="#b34">Zilly et al., 2016)</ref>. As in human cognition, speech is processed incrementally. This also al- lows our architecture to integrate information se- quentially from speech of arbitrary duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Analysis of neural representations</head><p>While analysis of neural methods in NLP is of- ten limited to evaluation of the performance on the training task, recently methods have been in- troduced to peek inside the black box and explore what it is that enables the model to perform the task. One approach is to look at the contribution of specific parts of the input, or specific units in the model, to final representations or decisions. <ref type="bibr">Kádár et al. (2016)</ref> propose omission scores, a method to estimate the contribution of input tokens to the fi-nal representation by removing them from the in- put and comparing the resulting representations to the ones generated by the original input. In a sim- ilar approach,  study the contribu- tion of individual input tokens as well as hidden units and word embedding dimensions by erasing them from the representation and analyzing how this affects the model. <ref type="bibr" target="#b21">Miao et al. (2016)</ref> and <ref type="bibr" target="#b29">Tang et al. (2016)</ref> use vi- sualization techniques for fine-grained analysis of GRU and LSTM models for ASR. Visualization of input and forget gate states allows <ref type="bibr" target="#b21">Miao et al. (2016)</ref> to make informed adaptations to gated re- current architectures, resulting in more efficiently trainable models. <ref type="bibr" target="#b29">Tang et al. (2016)</ref> visualize qualitative differences between LSTM-and GRU- based architectures, regarding the encoding of in- formation, as well as how it is processed through time.</p><p>We specifically study linguistic properties of the information encoded in the trained model. <ref type="bibr" target="#b0">Adi et al. (2016)</ref> introduce prediction tasks to ana- lyze information encoded in sentence embeddings about word order, sentence length, and the pres- ence of individual words. We use related tech- niques to explore encoding of aspects of form and meaning within components of our stacked archi- tecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>We use a multi-layer, gated recurrent neural net- work (RHN) to model the temporal nature of speech signal. Recurrent neural networks are de- signed for modeling sequential data, and gated variants (GRUs, LSTMs) are widely used with speech and text in both cognitive modeling and engineering contexts. RHNs are a simple gener- alization of GRU networks such that the transform between time points can consist of several steps.</p><p>Our multimodal model projects spoken utter- ances and images to a joint semantic space. The idea of projecting different modalities to a shared semantic space via a pair of encoders has been used in work on language and vision (among them <ref type="bibr">Vendrov et al. (2015)</ref>). The core idea is to en- courage inputs representing the same meaning in different modalities to end up nearby, while main- taining a distance from unrelated inputs.</p><p>The model consists of two parts: an utterance encoder, and an image encoder. The utterance en- coder starts from MFCC speech features, while the image encoder starts from features extracted with a VGG-16 pre-trained on ImageNet. Our loss function attempts to make the cosine distance be- tween encodings of matching utterances and im- ages greater than the distance between encodings of mismatching utterance/image pairs, by a mar- gin:</p><formula xml:id="formula_0">(1) u,i u max[0, α + d(u, i) − d(u , i)] + i max[0, α + d(u, i) − d(u, i )]</formula><p>where d(u, i) is the cosine distance between the encoded utterance u and encoded image i. Here (u, i) is the matching utterance-image pair, u ranges over utterances not describing i and i ranges over images not described by u.</p><p>The image encoder enc i is a simple linear pro- jection, followed by normalization to unit L2 norm:</p><formula xml:id="formula_1">enc i (i) = unit(Ai + b)<label>(2)</label></formula><p>where unit(x) =</p><p>x (x T x) 0.5 and with (A, b) as learned parameters. The utterance encoder enc u consists of a 1-dimensional convolutional layer of length s, size d and stride z, whose output feeds into a Recurrent Highway Network with k lay- ers and L microsteps, whose output in turn goes through an attention-like lookback operator, and finally L2 normalization:</p><formula xml:id="formula_2">enc u (u) = unit(Attn(RHN k,L (Conv s,d,z (u))))<label>(3)</label></formula><p>The main function of the convolutional layer Conv s,d,z is to subsample the input along the tem- poral dimension. We use a 1-dimensional convo- lution with full border mode padding. The atten- tion operator simply computes a weighted sum of the RHN activation at all timesteps:</p><formula xml:id="formula_3">Attn(x) = t α t x t<label>(4)</label></formula><p>where the weights α t are determined by learned parameters U and W, and passed through the timewise softmax function:</p><formula xml:id="formula_4">α t = exp(U tanh(Wx t )) t exp(U tanh(Wx t ))<label>(5)</label></formula><p>The main component of the utterance encoder is a recurrent network, specifically a Recurrent High- way Network ( <ref type="bibr" target="#b34">Zilly et al., 2016</ref>). The idea behind RHN is to increase the depth of the transform be- tween timesteps, or the recurrence depth. Other- wise they are a type of gated recurrent networks. The transition from timestep t − 1 to t is then de- fined as:</p><formula xml:id="formula_5">rhn(x t , s (L) t−1 ) = s (L) t<label>(6)</label></formula><p>where x t stands for input at time t, and s (l) t de- notes the state at time t at recurrence layer l, with L being the top layer of recurrence. Furthermore,</p><formula xml:id="formula_6">s (l) t = h (l) t t (l) t + s (l−1) t 1 − t (l) t (7)</formula><p>where is elementwise multiplication, and</p><formula xml:id="formula_7">h (l) t = tanh I[l = 1]W H x t + U H l s (l−1) t (8) t (l) t = σ I[l = 1]W T x t + U T l s (l−1)<label>(9)</label></formula><p>Here I is the indicator function: input is only in- cluded in the computation for the first layer of re- currence l = 1. By applying the rhn function re- peatedly, an RHN layer maps a sequence of inputs to a sequence of states:</p><formula xml:id="formula_8">(10) RHN(X, s 0 ) = rhn(x n , . . . , rhn(x 2 , rhn(x 1 , s (L) 0 )))</formula><p>Two or more RHN layers can be composed into a stack:</p><formula xml:id="formula_9">RHN 2 (RHN 1 (X, s 1 (L) 0 ), s 2 (L) 0 ),<label>(11)</label></formula><p>where s n (l) t stands for the state vector of layer n of the stack, at layer l of recurrence, at time t. In our version of the Stacked RHN architecture we use residualized layers:</p><formula xml:id="formula_10">RHN res (X, s 0 ) = RHN(X, s 0 ) + X<label>(12)</label></formula><p>This formulation tends to ease optimization in multi-layer models (cf. <ref type="bibr" target="#b12">He et al., 2015;</ref><ref type="bibr" target="#b23">Oord et al., 2016)</ref>. In addition to the speech model described above, we also define a comparable text model. As it takes a sequence of words as input, we re- place the convolutional layer with a word embed- ding lookup table. We found the text model did not benefit from the use of the attention mecha- nism, and thus the sentence embedding is simply the L2-normalized activation vector of the topmost layer, at the last timestep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our main goal is to analyze the emerging repre- sentations from different components of the model and to examine the linguistic knowledge they en- code. For this purpose, we employ a number of tasks that cover the spectrum from fully form- based to fully semantic.</p><p>In Section 4.2 we assess the effectiveness of our architecture by evaluating it on the task of rank- ing images given an utterance.</p><note type="other">Sections 4.3 to 4.6 present our analyses. In Sections 4.3 and 4.4 we define auxiliary tasks to investigate to what extent the network encodes information about the surface form of an utterance from the speech input. In Sec- tion 4.5 and 4.6 we focus on where semantic infor- mation is encoded in the model. In the analyses, we use the following features: Utterance embeddings: the weighted sum of the unit activations on the last layer, as calculated by Equation (3). Average unit activations: hidden layer activa- tions averaged over time and L2-normalized for each hidden layer. Average input vectors: the MFCC vectors aver- aged over time.</note><p>We use this feature to exam- ine how much information can be extracted from the input signal only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>For the experiments reported in the remainder of the paper we use two datasets of images with spo- ken captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Flickr8K</head><p>The Flickr8k Audio Caption Corpus was con- structed by having crowdsource workers read aloud the captions in the original We generate the input signal as follows: we ex- tract 12-dimensional mel-frequency cepstral coef- ficients (MFCC) plus log of the total energy. We then compute and add first order and second order differences (deltas) for a total of 37 dimensions. We use 25 milisecond windows, sampled every 10 miliseconds. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Synthetically spoken COCO</head><p>We generated synthetic speech for the captions in the MS COCO dataset ( <ref type="bibr" target="#b19">Lin et al., 2014</ref>) via the Google Text-to-Speech API. <ref type="bibr">2</ref> The audio and the corresponding MFCC features are released as <ref type="bibr" target="#b4">Chrupała et al. (2017)</ref> 3 . This TTS system we used produces high-quality realistic-sounding speech. It is nevertheless much simpler than real human speech as it uses a single voice, and lacks tempo variation or ambient noise. The data consists of over 300,000 images, each with five spoken cap- tions. Five thousand images each are held out for validation and test. We use the splits and image features provided by <ref type="bibr">Vendrov et al. (2015)</ref>. <ref type="bibr">4</ref> The image features also come from the VGG-16 net- work, but are averages of feature vectors for ten crops of each image. For the MS COCO captions we extracted only plain MFCC and total energy features, and did not add deltas in order to keep the amount of computation manageable given the size of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image retrieval</head><p>We evaluate our model on the task of ranking im- ages given a spoken utterance, such that highly ranked images contain scenes described by the ut- terance. The performance on this task on valida- tion data is also used to choose the best variant of the model architecture and to tune the hyperpa- rameters. We compare the speech models to mod- els trained on written sentences split into words. The best settings found for the four models were the following: Flickr8K Text RHN 300-dimensional word em- beddings, 1 hidden layer with 1024 dimen- sions, 1 microstep, initial learning rate 0.001. Flick8K Speech RHN convolutional layer with length 6, size 64, stride 2, 4 hidden layers with 1024 dimensions, 2 microsteps, atten-tion MLP with 128 hidden units, initial learn- ing rate 0.0002 COCO Text RHN 300-dimensional word em- beddings, 1 hidden layer with 1024 dimen- sions, 1 microstep, initial learning rate 0.001 COCO Speech RHN convolutional layer with length 6, size 64, stride 3, 5 hidden layers with 512 dimensions, 2 microsteps, attention MLP with 512 hidden units, initial learning rate 0.0002 All models were optimized with Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2014</ref>) with early stopping: we kept the parameters for the epoch which showed the best recall@10 on validation data.  <ref type="table">Table 2</ref>: Image retrieval performance on MS COCO. R@N stands for recall at N; ˜ r stands for median rank of the correct image. <ref type="table">Table 1</ref> shows the results for the human speech from the Flickr8K dataset. The Speech RHN model scores substantially higher than model of <ref type="bibr" target="#b9">Harwath and Glass (2015)</ref> on the same data. How- ever the large gap between its perfomance and the scores of the text model suggests that Flickr8K is rather small for the speech task. In <ref type="table">Table 2</ref> we present the results on the dataset of synthetic speech from MS COCO. Here the text model is still better, but the gap is much smaller than for Flickr8K. We attribute this to the much larger size of dataset, and to the less noisy and less variable synthetic speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>While the MS COCO text model is overall bet- ter than the speech model, there are cases where it outperforms the text model. We listed the top hundred cases where the ratio of the ranks of the correct image according to the two models was the smallest, as well as another hundred cases where it was the largest. Manual inspection did not turn up any obvious patterns for the cases of text be- ing better than speech. For the cases where speech outperformed text, two patterns stood out: (i) sen- tences with spelling mistakes, (ii) unusually long sentences. For example for the sentence a yellow and white birtd is in flight the text model misses the misspelled word birtd and returns an irrelevant image, while the speech model seems robust to some degree of variation in pronunciation and re- turns the target image at rank 1 (see <ref type="figure" target="#fig_1">Figure 1)</ref>. In an attempt to quantify this effect we counted the number of unique words with training set frequen- cies below 5 in the top 100 utterances with lowest and highest rank ratio: for the utterances where text was better there were 16 such words; for ut- terances where speech was better there were 28, among them misspellings such as streeet, scears (for skiers), contryside, scull, birtd, devise.</p><p>The distribution of utterance lengths in <ref type="figure" target="#fig_2">Fig- ure 2</ref> confirms pattern (ii): the set of 100 sen- tences where speech beats text by a large margin are longer on average and there are extremely long outliers among them. One of them is the 36-word- long utterance depicted in <ref type="figure" target="#fig_3">Figure 3</ref>, with ranks 470 and 2 for text and speech respectively. We suspect that the speech model's attention mechanism en- ables it to cherry pick key fragments of such mon- ster utterances, while the text model lacking this mechanism may struggle. <ref type="figure" target="#fig_3">Figure 3</ref> shows the plot of the attention weights for this utterance from the speech model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Predicting utterance length</head><p>Our first auxiliary task is to predict the length of the utterance, using the features explained at the beginning of Section 4. Since the length of an ut- terance directly corresponds to how long it takes to articulate, we also use the number of time steps <ref type="bibr">5</ref> as a feature and expect it to provide the upper bound for our task, especially for synthetic speech. We use a Ridge Regression model for predicting utter- ance length using each set of features. The model is trained on 80% of the sentences in the validation set, and tested on the remaining 20%. For all fea- tures regularization penalty α = 1.0 gave the best results. <ref type="figure" target="#fig_4">Figure 4</ref> shows the results for this task on hu- man speech from Flickr8K and synthetic speech from COCO. With the exception of the average in- put vectors for Flickr8K, all features can explain a high proportion of variance in the predicted ut- terance length. The pattern observed for the two datasets is slightly different: due to the systematic conversion of words to synthetic speech in COCO, using the number of time steps for this dataset yields the highest R 2 . However, this feature is not as informative for predicting the utterance length in Flickr8K due to noise and variation in human speech, and is in fact outperformed by some of the features extracted from the model. Also, the input vectors from COCO are much more informative than Flickr8K due to larger quantity and simpler structure of the speech signal. However, in both datasets the best (non-ceiling) performance is ob- tained by using average unit activations from the hidden layers (layer 2 for COCO, and layers 3 and 4 for Flickr8K). These features outperform utter- ance embeddings, which are optimized according to the visual grounding objective of the model and most probably learn to ignore the superficial char- acteristics of the utterance that do not contribute to matching the corresponding image.</p><p>Note that the performance on COCO plateaus after the second layer, which might suggest that form-based knowledge is learned by lower layers. Since Flickr8K is much smaller in size, the stabil- ising happens later in layer 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Predicting word presence</head><p>Results from the previous experiment suggest that our model acquires information about higher level building blocks (words) in the continuous speech signal. Here we explore whether it can detect the presence or absence of individual words in an ut- terance. We formulate detecting a word in an ut- terance as a binary classification task, for which we use a multi-layer perceptron with a single hid- den layer of size 1024, optimized by Adam. The input to the model is a concatenation of the fea- ture vector representing an utterance and the one representing a target word. We again use utter- ance embeddings, average unit activations on each layer, and average input vectors as features, and represent each target word as a vector of MFCC features extracted from the audio signal syntheti- cally produced for that word.</p><p>For each utterance in the validation set, we ran- domly pick one positive and one negative target (i.e., one word that does and one that does not ap- pear in the utterance) that is not a stop word. To balance the probability of a word being positive or negative, we use each positive target as a neg- ative target for another utterance in the validation set. The MLP model is trained on the positive and negative examples corresponding to 80% of the ut- terances in the validation set of each dataset, and evaluated on the remaining 20%. <ref type="figure" target="#fig_5">Figure 5</ref> shows the mean accuracy of the MLP on Flickr8K and COCO. All results using features extracted from the model are above chance (0.5), with the average unit activations of the hidden lay- ers yielding the best results (0.65 for Flickr8K on layer 3, and 0.79 for COCO on layer 4). These numbers show that the speech model infers re- liable information about word-level blocks from the low-level audio features it receives as input. The observed trend is similar to the previous task: average unit activations on the higher-level hid- den layers are more informative for this task than the utterance embeddings, but the performance plateaus before the topmost layer. average unit activations, whereas the first (#0) and last point represent average input vectors and ut- terance embeddings, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sentence similarity</head><p>Next we explore to what extent the model's rep- resentations correspond to those of humans. We employ the Sentences Involving Compositional Knowledge (SICK) dataset <ref type="bibr" target="#b20">(Marelli et al., 2014)</ref>. SICK consists of image descriptions taken from  Captions were paired at ran- dom, as well as modified to obtain semantically similar and contrasting counterparts, and the re- sulting pairs were rated for semantic similarity.</p><p>For all sentence pairs in SICK, we generate synthetic spoken sentences and feed them to the COCO Speech RHN, and calculate the cosine sim- ilarity between the averaged MFCC input vectors, the averaged hidden layer activation vectors, and the sentence embeddings. Z-score transformation was applied before calculating the cosine similar- ities. We then correlate these cosine similarities with</p><p>• semantic relatedness according to human rat- ings • cosine similarities according to z-score trans- formed embeddings from COCO Text RHN • edit similarities, a measure of how sim- ilar the sentences are in form, specifi- cally, 1−normalized Levenshtein distance over character sequences <ref type="figure" target="#fig_6">Figure 6</ref> shows a boxplot over 10,000 bootstrap samples for all correlations. We observe that (i) correlation with edit similarity initially increases, then decreases; (ii) correlation with human re- latedness scores and text model embeddings in- creases until layer 4, but decreases for hidden layer 5. The initially increasing and then decreasing cor- relation with edit similarity is consistent with the findings that information about form is encoded by lower layers. The overall growing correlation with both human semantic similarity ratings and  the COCO Text RHN indicate that higher layers learn to represent semantic knowledge. We were somewhat surprised by the pattern for the correla- tion with human ratings and the Text model simi- larities which drops for layer 5. We suspect it may be caused by the model at this point in the layer hierarchy being strongly tuned to the specifics of the COCO dataset. To test this, we checked the correlations with COCO Text embeddings on val- idation sentences from the COCO dataset instead of SICK. These increased monotonically, in sup- port of our conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Homonym disambiguation</head><p>Next we simulate the task of distinguishing be- tween pairs of homonyms, i.e. words with the same acoustic form but different meaning. We group the words in the union of the training and validation data of the COCO dataset by their pho- netic transcription. We then pick pairs of words which have the same pronunciation but different spelling, for example suite/sweet. We impose the following conditions: (a) both forms appear more than 20 times, (b) the two forms have different meaning (i.e. they are not simply variant spellings like theater/theatre), (c) neither form is a func- tion word, and (d) the more frequent form con- stitutes less than 95% of the occurrences. This gives us 34 word pairs. For each pair we gener- ate a binary classification task by taking all the ut- terances where either form appears, using average input vectors, utterance embeddings, and average unit activations as features. Instances for all fea- ture sets are normalized to unit L2 norm. For each task and feature set we run strati- fied 10-fold cross validation using Logistic Re- gression to predict which of the two words the utterance contains. <ref type="figure" target="#fig_8">Figure 7</ref> shows, for each pair, the relative error reduction of each feature set with respect to the majority baseline. There is substantial variation across word pairs, but overall the task becomes easier as the features come from higher layers in the network. Some forms can be disambiguated with very high accu- racy (e.g. sale/sail, cole/coal, pairs/pears), while some others cannot be distinguished at all (peak- ing/peeking, great/grate, mantle/mantel). We ex- amined the sentences containing the failing forms, and found out that almost all occurrences of peak- ing and mantle were misspellings of peeking and mantel, which explains the impossibility of disam- biguating these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a multi-layer recurrent highway net- work model of language acquisition from visually grounded speech signal. Through detailed analy- sis we uncover how information in the input sig- nal is transformed as it flows through the network: formal aspects of language such as word identities that not directly present in the input are discovered and encoded low in the layer hierarchy, while se- mantic information is most strongly expressed in the topmost layers.</p><p>Going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex. This will hopefully lead to a better under- standing of language learning and processing by both artificial and neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Flickr8K cor- pus (Hodosh et al., 2013). For details of the data collection procedure refer to Harwath and Glass (2015). The datasets consist of 8,000 im- ages, each image with five descriptions. One thousand images are held out for validation, and another one thousand for the final test set. We use the splits provided by (Karpathy and Fei-Fei, 2015). The image features come from the final fully connect layer of VGG-16 (Simonyan and Zisserman, 2014) pre-trained on Imagenet (Rus- sakovsky et al., 2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Images returned for utterance a yellow and white birtd is in flight by the text (left) and speech (right) models.</figDesc><graphic url="image-1.png" coords="6,89.53,142.96,96.00,63.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Length distribution for sentences where one model performs much better than the other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention weight distribution for a long utterance.</figDesc><graphic url="image-4.png" coords="7,80.33,164.04,201.60,151.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: R 2 values for predicting utterance length for Flickr8K and COCO. Layers 1-5 represent (normalized) average unit activation, whereas the first (#0) and last point represent average input vectors and utterance embeddings, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mean accuracy values for predicting the presence of a word in an utterance for Flickr8K and COCO. Layers 1-5 represent the (normalized) average unit activations, whereas the first (#0) and last point represent average input vectors and utterance embeddings, respectively.</figDesc><graphic url="image-5.png" coords="7,330.01,432.73,172.80,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Pearson's r of cosine similarities of averaged input MFCCs and COCO Speech RHN hidden layer activation vectors and embeddings of sentence pairs with relatedness scores from SICK, cosine similarity of COCO Text RHN embeddings, and edit similarity.</figDesc><graphic url="image-6.png" coords="8,72.00,62.81,220.00,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Flickr8K</head><label></label><figDesc>and video captions from the SemEval 2012 STS MSRVideo Description data set (STS) (Agirre et al., 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Disambiguation performance per layer. Points #0 and #6 (connected via dotted lines) represent the input vectors and utterance embeddings, respectively. The black line shows the overall mean RER.</figDesc></figure>

			<note place="foot" n="1"> We noticed that for a number of utterances the audio signal was very long: on inspection it turned out that most of these involved failure to switch off the microphone on the part of the workers, and the audio contained ambient noise or unrelated speech. We thus trucated all audio for this dataset at 10,000 miliseconds. 2 Available at https://github.com/pndurette/gTTS. 3 Available at https://doi.org/10.5281/zenodo.400926. 4 See https://github.com/ivendrov/order-embedding.</note>

			<note place="foot" n="5"> This is approximately duration in milliseconds 10×stride .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank David Harwath for mak-ing the Flickr8k Audio Caption Corpus publicly available.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04207</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruket</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03896</idno>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Adrian Muscat, and Barbara Plank</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning language through pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akos</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieke</forename><surname>Gelderloos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Synthetically spoken COCO</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="doi">10.5281/zenodo.400926</idno>
		<ptr target="https://doi.org/10.5281/zenodo.400926" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A probabilistic computational model of cross-situational word learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1017" to="1063" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Bayesian framework for crosssituational word-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieke</forename><surname>Gelderloos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep multimodal semantic embeddings for speech and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning word-like units from joint audio-visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07481</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spoken language with visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1858" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>´ Akos Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alishahi</surname></persName>
		</author>
		<idno>CoRR abs/1602.08952</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal semantic learning from child-directed input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<title level="m">Understanding neural networks through representation erasure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer VisionECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simplifying long short-term memory acoustic models for fast training and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2284" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning words from sights and sounds: a computational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="146" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<editor>Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A computational study of cross-situational techniques for learning word-tomeaning mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="39" to="91" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning words from images and speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08789</idno>
		<title level="m">Memory visualization for gated recurrent neural networks in speech recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06361</idno>
		<title level="m">Sanja Fidler, and Raquel Urtasun. 2015. Order-embeddings of images and language</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multimodal learning interface for grounding spoken language in sensory perceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception (TAP)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="80" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<title level="m">Recurrent highway networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
